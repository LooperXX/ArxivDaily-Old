<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-08-11T01:55:24.473Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04444</id>
        <link href="http://arxiv.org/abs/2108.04444"/>
        <updated>2021-08-11T01:55:24.416Z</updated>
        <summary type="html"><![CDATA[Point cloud completion aims to predict a complete shape in high accuracy from
its partial observation. However, previous methods usually suffered from
discrete nature of point cloud and unstructured prediction of points in local
regions, which makes it hard to reveal fine local geometric details on the
complete shape. To resolve this issue, we propose SnowflakeNet with Snowflake
Point Deconvolution (SPD) to generate the complete point clouds. The
SnowflakeNet models the generation of complete point clouds as the
snowflake-like growth of points in 3D space, where the child points are
progressively generated by splitting their parent points after each SPD. Our
insight of revealing detailed geometry is to introduce skip-transformer in SPD
to learn point splitting patterns which can fit local regions the best.
Skip-transformer leverages attention mechanism to summarize the splitting
patterns used in the previous SPD layer to produce the splitting in the current
SPD layer. The locally compact and structured point cloud generated by SPD is
able to precisely capture the structure characteristic of 3D shape in local
patches, which enables the network to predict highly detailed geometries, such
as smooth regions, sharp edges and corners. Our experimental results outperform
the state-of-the-art point cloud completion methods under widely used
benchmarks. Code will be available at
https://github.com/AllenXiangX/SnowflakeNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1"&gt;Peng Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1"&gt;Xin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1"&gt;Pengfei Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. (arXiv:2108.04521v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04521</id>
        <link href="http://arxiv.org/abs/2108.04521"/>
        <updated>2021-08-11T01:55:24.411Z</updated>
        <summary type="html"><![CDATA[Jointly exploiting multiple different yet complementary domain information
has been proven to be an effective way to perform robust object tracking. This
paper focuses on effectively representing and utilizing complementary features
from the frame domain and event domain for boosting object tracking performance
in challenge scenarios. Specifically, we propose Common Features Extractor
(CFE) to learn potential common representations from the RGB domain and event
domain. For learning the unique features of the two domains, we utilize a
Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract
edge cues in the event domain which may be missed in RGB in some challenging
conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional
Neural Networks to extract texture and semantic information in RGB domain.
Extensive experiments on standard RGB benchmark and real event tracking dataset
demonstrate the effectiveness of the proposed approach. We show our approach
outperforms all compared state-of-the-art tracking algorithms and verify
event-based data is a powerful cue for tracking in challenging scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bo Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yingkai Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Baocai Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Canonical 3D Object Representation for Fine-Grained Recognition. (arXiv:2108.04628v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04628</id>
        <link href="http://arxiv.org/abs/2108.04628"/>
        <updated>2021-08-11T01:55:24.406Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for fine-grained object recognition that learns
to recover object variation in 3D space from a single image, trained on an
image collection without using any ground-truth 3D annotation. We accomplish
this by representing an object as a composition of 3D shape and its appearance,
while eliminating the effect of camera viewpoint, in a canonical configuration.
Unlike conventional methods modeling spatial variation in 2D images only, our
method is capable of reconfiguring the appearance feature in a canonical 3D
space, thus enabling the subsequent object classifier to be invariant under 3D
geometric variation. Our representation also allows us to go beyond existing
methods, by incorporating 3D shape variation as an additional cue for object
recognition. To learn the model without ground-truth 3D annotation, we deploy a
differentiable renderer in an analysis-by-synthesis framework. By incorporating
3D shape and appearance jointly in a deep representation, our method learns the
discriminative representation of the object and achieves competitive
performance on fine-grained image recognition and vehicle re-identification. We
also demonstrate that the performance of 3D shape reconstruction is improved by
learning fine-grained shape deformation in a boosting manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joung_S/0/1/0/all/0/1"&gt;Sunghun Joung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1"&gt;Ig-Jae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Camera Trajectory Forecasting with Trajectory Tensors. (arXiv:2108.04694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04694</id>
        <link href="http://arxiv.org/abs/2108.04694"/>
        <updated>2021-08-11T01:55:24.395Z</updated>
        <summary type="html"><![CDATA[We introduce the problem of multi-camera trajectory forecasting (MCTF), which
involves predicting the trajectory of a moving object across a network of
cameras. While multi-camera setups are widespread for applications such as
surveillance and traffic monitoring, existing trajectory forecasting methods
typically focus on single-camera trajectory forecasting (SCTF), limiting their
use for such applications. Furthermore, using a single camera limits the
field-of-view available, making long-term trajectory forecasting impossible. We
address these shortcomings of SCTF by developing an MCTF framework that
simultaneously uses all estimated relative object locations from several
viewpoints and predicts the object's future location in all possible
viewpoints. Our framework follows a Which-When-Where approach that predicts in
which camera(s) the objects appear and when and where within the camera views
they appear. To this end, we propose the concept of trajectory tensors: a new
technique to encode trajectories across multiple camera views and the
associated uncertainties. We develop several encoder-decoder MCTF models for
trajectory tensors and present extensive experiments on our own database
(comprising 600 hours of video data from 15 camera views) created particularly
for the MCTF task. Results show that our trajectory tensor models outperform
coordinate trajectory-based MCTF models and existing SCTF methods adapted for
MCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Styles_O/0/1/0/all/0/1"&gt;Olly Styles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1"&gt;Tanaya Guha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1"&gt;Victor Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04603</id>
        <link href="http://arxiv.org/abs/2108.04603"/>
        <updated>2021-08-11T01:55:24.388Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel model for recognizing images with composite
attribute-object concepts, notably for composite concepts that are unseen
during model training. We aim to explore the three key properties required by
the task --- relation-aware, consistent, and decoupled --- to learn rich and
robust features for primitive concepts that compose attribute-object pairs. To
this end, we propose the Blocked Message Passing Network (BMP-Net). The model
consists of two modules. The concept module generates semantically meaningful
features for primitive concepts, whereas the visual module extracts visual
features for attributes and objects from input images. A message passing
mechanism is used in the concept module to capture the relations between
primitive concepts. Furthermore, to prevent the model from being biased towards
seen composite concepts and reduce the entanglement between attributes and
objects, we propose a blocking mechanism that equalizes the information
available to the model for both seen and unseen concepts. Extensive experiments
and ablation studies on two benchmarks show the efficacy of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1"&gt;Yongkang Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1"&gt;Mohan Kankanhalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04357</id>
        <link href="http://arxiv.org/abs/2108.04357"/>
        <updated>2021-08-11T01:55:24.373Z</updated>
        <summary type="html"><![CDATA[Touchless computer interaction has become an important consideration during
the COVID-19 pandemic period. Despite progress in machine learning and computer
vision that allows for advanced gesture recognition, an integrated collection
of such open-source methods and a user-customisable approach to utilising them
in a low-cost solution for touchless interaction in existing software is still
missing. In this paper, we introduce the MotionInput v2.0 application. This
application utilises published open-source libraries and additional gesture
definitions developed to take the video stream from a standard RGB webcam as
input. It then maps human motion gestures to input operations for existing
applications and games. The user can choose their own preferred way of
interacting from a series of motion types, including single and bi-modal hand
gesturing, full-body repetitive or extremities-based exercises, head and facial
movements, eye tracking, and combinations of the above. We also introduce a
series of bespoke gesture recognition classifications as DirectInput triggers,
including gestures for idle states, auto calibration, depth capture from a 2D
RGB webcam stream and tracking of facial motions such as mouth motions,
winking, and head direction with rotation. Three use case areas assisted the
development of the modules: creativity software, office and clinical software,
and gaming software. A collection of open-source libraries has been integrated
and provide a layer of modular gesture mapping on top of existing mouse and
keyboard controls in Windows via DirectX. With ease of access to webcams
integrated into most laptops and desktop computers, touchless computing becomes
more available with MotionInput v2.0, in a federated and locally processed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1"&gt;Ashild Kummen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1"&gt;Ali Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1"&gt;Teodora Ganeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qianying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1"&gt;Robert Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1"&gt;Chenuka Ratwatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Lu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1"&gt;Emil Almazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1"&gt;Sheena Visram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1"&gt;Andrew Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1"&gt;Neil J Sebire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1"&gt;Lee Stott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1"&gt;Yvonne Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1"&gt;Graham Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1"&gt;Dean Mohamedally&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Metric Learning for Open World Semantic Segmentation. (arXiv:2108.04562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04562</id>
        <link href="http://arxiv.org/abs/2108.04562"/>
        <updated>2021-08-11T01:55:24.366Z</updated>
        <summary type="html"><![CDATA[Classical close-set semantic segmentation networks have limited ability to
detect out-of-distribution (OOD) objects, which is important for
safety-critical applications such as autonomous driving. Incrementally learning
these OOD objects with few annotations is an ideal way to enlarge the knowledge
base of the deep learning models. In this paper, we propose an open world
semantic segmentation system that includes two modules: (1) an open-set
semantic segmentation module to detect both in-distribution and OOD objects.
(2) an incremental few-shot learning module to gradually incorporate those OOD
objects into its existing knowledge base. This open world semantic segmentation
system behaves like a human being, which is able to identify OOD objects and
gradually learn them with corresponding supervision. We adopt the Deep Metric
Learning Network (DMLNet) with contrastive clustering to implement open-set
semantic segmentation. Compared to other open-set semantic segmentation
methods, our DMLNet achieves state-of-the-art performance on three challenging
open-set semantic segmentation datasets without using additional data or
generative models. On this basis, two incremental few-shot learning methods are
further proposed to progressively improve the DMLNet with the annotations of
OOD objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1"&gt;Jun Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_P/0/1/0/all/0/1"&gt;Peng Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Junhao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Michael Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Ming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Method Towards CVPR 2021 Image Matching Challenge. (arXiv:2108.04453v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04453</id>
        <link href="http://arxiv.org/abs/2108.04453"/>
        <updated>2021-08-11T01:55:24.361Z</updated>
        <summary type="html"><![CDATA[This report describes Megvii-3D team's approach towards CVPR 2021 Image
Matching Workshop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1"&gt;Xiaopeng Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dehao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1"&gt;Ran Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1"&gt;Zheng Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haotian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04728</id>
        <link href="http://arxiv.org/abs/2108.04728"/>
        <updated>2021-08-11T01:55:24.352Z</updated>
        <summary type="html"><![CDATA[Current 3D single object tracking approaches track the target based on a
feature comparison between the target template and the search area. However,
due to the common occlusion in LiDAR scans, it is non-trivial to conduct
accurate feature comparisons on severe sparse and incomplete shapes. In this
work, we exploit the ground truth bounding box given in the first frame as a
strong cue to enhance the feature description of the target object, enabling a
more accurate feature comparison in a simple yet effective way. In particular,
we first propose the BoxCloud, an informative and robust representation, to
depict an object using the point-to-box relation. We further design an
efficient box-aware feature fusion module, which leverages the aforementioned
BoxCloud for reliable feature matching and embedding. Integrating the proposed
general components into an existing model P2B, we construct a superior
box-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms
the previous state-of-the-art by a large margin on both KITTI and NuScenes
benchmarks, achieving a 12.8% improvement in terms of precision while running
~20% faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiantao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weibing Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BIDCD - Bosch Industrial Depth Completion Dataset. (arXiv:2108.04706v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04706</id>
        <link href="http://arxiv.org/abs/2108.04706"/>
        <updated>2021-08-11T01:55:24.347Z</updated>
        <summary type="html"><![CDATA[We introduce BIDCD - the Bosch Industrial Depth Completion Dataset. BIDCD is
a new RGBD dataset of metallic industrial objects, collected with a depth
camera mounted on a robotic manipulator. The main purpose of this dataset is to
facilitate the training of domain-specific depth completion models, to be used
in logistics and manufacturing tasks. We trained a State-of-the-Art depth
completion model on this dataset, and report the results, setting an initial
benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1"&gt;Adam Botach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1"&gt;Yuri Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1"&gt;Yakov Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1"&gt;Yoel Shapiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1"&gt;Dotan Di Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04543</id>
        <link href="http://arxiv.org/abs/2108.04543"/>
        <updated>2021-08-11T01:55:24.342Z</updated>
        <summary type="html"><![CDATA[In this article, we perform a review of the state-of-the-art of hybrid
machine learning in medical imaging. We start with a short summary of the
general developments of the past in machine learning and how general and
specialized approaches have been in competition in the past decades. A
particular focus will be the theoretical and experimental evidence pro and
contra hybrid modelling. Next, we inspect several new developments regarding
hybrid machine learning with a particular focus on so-called known operator
learning and how hybrid approaches gain more and more momentum across
essentially all applications in medical imaging and medical image analysis. As
we will point out by numerous examples, hybrid models are taking over in image
reconstruction and analysis. Even domains such as physical simulation and
scanner and acquisition design are being addressed using machine learning grey
box modelling approaches. Towards the end of the article, we will investigate a
few future directions and point out relevant areas in which hybrid modelling,
meta learning, and other domains will likely be able to drive the
state-of-the-art ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1"&gt;Harald K&amp;#xf6;stler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1"&gt;Marco Heisig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1"&gt;Patrick Krauss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Seung Hee Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Breast Cancer Classification: Enhanced Tangent Function. (arXiv:2108.04663v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.04663</id>
        <link href="http://arxiv.org/abs/2108.04663"/>
        <updated>2021-08-11T01:55:24.326Z</updated>
        <summary type="html"><![CDATA[Background and Aim: Recently, deep learning using convolutional neural
network has been used successfully to classify the images of breast cells
accurately. However, the accuracy of manual classification of those
histopathological images is comparatively low. This research aims to increase
the accuracy of the classification of breast cancer images by utilizing a
Patch-Based Classifier (PBC) along with deep learning architecture.
Methodology: The proposed system consists of a Deep Convolutional Neural
Network (DCNN) that helps in enhancing and increasing the accuracy of the
classification process. This is done by the use of the Patch-based Classifier
(PBC). CNN has completely different layers where images are first fed through
convolutional layers using hyperbolic tangent function together with the
max-pooling layer, drop out layers, and SoftMax function for classification.
Further, the output obtained is fed to a patch-based classifier that consists
of patch-wise classification output followed by majority voting. Results: The
results are obtained throughout the classification stage for breast cancer
images that are collected from breast-histology datasets. The proposed solution
improves the accuracy of classification whether or not the images had normal,
benign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in
processing time from 0.45 s to 0.2s on average. Conclusion: The proposed
solution focused on increasing the accuracy of classifying cancer in the breast
by enhancing the image contrast and reducing the vanishing gradient. Finally,
this solution for the implementation of the Contrast Limited Adaptive Histogram
Equalization (CLAHE) technique and modified tangent function helps in
increasing the accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thapa_A/0/1/0/all/0/1"&gt;Ashu Thapa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alsadoon_A/0/1/0/all/0/1"&gt;Abeer Alsadoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1"&gt;P.W.C. Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bajaj_S/0/1/0/all/0/1"&gt;Simi Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alsadoon_O/0/1/0/all/0/1"&gt;Omar Hisham Alsadoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1"&gt;Rasha S. Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jerew_O/0/1/0/all/0/1"&gt;Oday D. Jerew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TrUMAn: Trope Understanding in Movies and Animations. (arXiv:2108.04542v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.04542</id>
        <link href="http://arxiv.org/abs/2108.04542"/>
        <updated>2021-08-11T01:55:24.283Z</updated>
        <summary type="html"><![CDATA[Understanding and comprehending video content is crucial for many real-world
applications such as search and recommendation systems. While recent progress
of deep learning has boosted performance on various tasks using visual cues,
deep cognition to reason intentions, motivation, or causality remains
challenging. Existing datasets that aim to examine video reasoning capability
focus on visual signals such as actions, objects, relations, or could be
answered utilizing text bias. Observing this, we propose a novel task, along
with a new dataset: Trope Understanding in Movies and Animations (TrUMAn),
intending to evaluate and develop learning systems beyond visual signals.
Tropes are frequently used storytelling devices for creative works. By coping
with the trope understanding task and enabling the deep cognition skills of
machines, we are optimistic that data mining applications and algorithms could
be taken to the next level. To tackle the challenging TrUMAn dataset, we
present a Trope Understanding and Storytelling (TrUSt) with a new Conceptual
Storyteller module, which guides the video encoder by performing video
storytelling on a latent space. The generated story embedding is then fed into
the trope understanding model to provide further signals. Experimental results
demonstrate that state-of-the-art learning systems on existing tasks reach only
12.01% of accuracy with raw input signals. Also, even in the oracle case with
human-annotated descriptions, BERT contextual embedding achieves at most 28% of
accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%
performance. We also provide detailed analysis topave the way for future
research. TrUMAn is publicly available
at:https://www.cmlab.csie.ntu.edu.tw/project/trope]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1"&gt;Po-Wei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1"&gt;Bing-Chen Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1"&gt;Wen-Feng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Ke-Jyun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand Pose Classification Based on Neural Networks. (arXiv:2108.04529v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04529</id>
        <link href="http://arxiv.org/abs/2108.04529"/>
        <updated>2021-08-11T01:55:24.277Z</updated>
        <summary type="html"><![CDATA[In this work, deep learning models are applied to a segment of a robust
hand-washing dataset that has been created with the help of 30 volunteers. This
work demonstrates the classification of presence of one hand, two hands and no
hand in the scene based on transfer learning. The pre-trained model; simplest
NN from Keras library is utilized to train the network with 704 images of hand
gestures and the predictions are carried out for the input image. Due to the
controlled and restricted dataset, 100% accuracy is achieved during the
training with correct predictions for the input image. Complete handwashing
dataset with dense models such as AlexNet for video classification for hand
hygiene stages will be used in the future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1"&gt;Rashmi Bakshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving. (arXiv:2108.04602v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04602</id>
        <link href="http://arxiv.org/abs/2108.04602"/>
        <updated>2021-08-11T01:55:24.272Z</updated>
        <summary type="html"><![CDATA[Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results
of object detection, affinity computation and data association in real time.
This paper presents an efficient multi-modal MOT framework with online joint
detection and tracking schemes and robust data association for autonomous
driving applications. The novelty of this work includes: (1) development of an
end-to-end deep neural network for joint object detection and correlation using
2D and 3D measurements; (2) development of a robust affinity computation module
to compute occlusion-aware appearance and motion affinities in 3D space; (3)
development of a comprehensive data association module for joint optimization
among detection confidences, affinities and start-end probabilities. The
experiment results on the KITTI tracking benchmark demonstrate the superior
performance of the proposed method in terms of both tracking accuracy and
processing speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kemiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1"&gt;Qi Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer. (arXiv:2108.04533v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04533</id>
        <link href="http://arxiv.org/abs/2108.04533"/>
        <updated>2021-08-11T01:55:24.256Z</updated>
        <summary type="html"><![CDATA[Attribute-based person search is the task of finding person images that are
best matched with a set of text attributes given as query. The main challenge
of this task is the large modality gap between attributes and images. To reduce
the gap, we present a new loss for learning cross-modal embeddings in the
context of attribute-based person search. We regard a set of attributes as a
category of people sharing the same traits. In a joint embedding space of the
two modalities, our loss pulls images close to their person categories for
modality alignment. More importantly, it pushes apart a pair of person
categories by a margin determined adaptively by their semantic distance, where
the distance metric is learned end-to-end so that the loss considers importance
of each attribute when relating person categories. Our loss guided by the
adaptive semantic margin leads to more discriminative and semantically
well-arranged distributions of person images. As a consequence, it enables a
simple embedding model to achieve state-of-the-art records on public benchmarks
without bells and whistles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1"&gt;Boseung Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jicheol Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1"&gt;Suha Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White blood cell subtype detection and classification. (arXiv:2108.04614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04614</id>
        <link href="http://arxiv.org/abs/2108.04614"/>
        <updated>2021-08-11T01:55:24.249Z</updated>
        <summary type="html"><![CDATA[Machine learning has endless applications in the health care industry. White
blood cell classification is one of the interesting and promising area of
research. The classification of the white blood cells plays an important part
in the medical diagnosis. In practise white blood cell classification is
performed by the haematologist by taking a small smear of blood and careful
examination under the microscope. The current procedures to identify the white
blood cell subtype is more time taking and error-prone. The computer aided
detection and diagnosis of the white blood cells tend to avoid the human error
and reduce the time taken to classify the white blood cells. In the recent
years several deep learning approaches have been developed in the context of
classification of the white blood cells that are able to identify but are
unable to localize the positions of white blood cells in the blood cell image.
Following this, the present research proposes to utilize YOLOv3 object
detection technique to localize and classify the white blood cells with
bounding boxes. With exhaustive experimental analysis, the proposed work is
found to detect the white blood cell with 99.2% accuracy and classify with 90%
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1"&gt;Nalla Praveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Reverse Image Search Engine for NASAWorldview. (arXiv:2108.04479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04479</id>
        <link href="http://arxiv.org/abs/2108.04479"/>
        <updated>2021-08-11T01:55:24.244Z</updated>
        <summary type="html"><![CDATA[Researchers often spend weeks sifting through decades of unlabeled satellite
imagery(on NASA Worldview) in order to develop datasets on which they can start
conducting research. We developed an interactive, scalable and fast image
similarity search engine (which can take one or more images as the query image)
that automatically sifts through the unlabeled dataset reducing dataset
generation time from weeks to minutes. In this work, we describe key components
of the end to end pipeline. Our similarity search system was created to be able
to identify similar images from a potentially petabyte scale database that are
similar to an input image, and for this we had to break down each query image
into its features, which were generated by a classification layer stripped CNN
trained in a supervised manner. To store and search these features efficiently,
we had to make several scalability improvements. To improve the speed, reduce
the storage, and shrink memory requirements for embedding search, we add a
fully connected layer to our CNN make all images into a 128 length vector
before entering the classification layers. This helped us compress the size of
our image features from 2048 (for ResNet, which was initially tried as our
featurizer) to 128 for our new custom model. Additionally, we utilize existing
approximate nearest neighbor search libraries to significantly speed up
embedding search. Our system currently searches over our entire database of
images at 5 seconds per query on a single virtual machine in the cloud. In the
future, we would like to incorporate a SimCLR based featurizing model which
could be trained without any labelling by a human (since the classification
aspect of the model is irrelevant to this use case).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1"&gt;Abhigya Sodani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1"&gt;Michael Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04454</id>
        <link href="http://arxiv.org/abs/2108.04454"/>
        <updated>2021-08-11T01:55:24.238Z</updated>
        <summary type="html"><![CDATA[Anomaly detection in video streams is a challengingproblem because of the
scarcity of abnormal events andthe difficulty of accurately annotating them.To
allevi-ate these issues, unsupervised learning-based predictionmethods have
been previously applied. These approachestrain the model with only normal
events and predict a fu-ture frame from a sequence of preceding frames by use
ofencoder-decoder architectures so that they result in smallprediction errors
on normal events but large errors on ab-normal events. The architecture,
however, comes with thecomputational burden as some anomaly detection tasks
re-quire low computational cost without sacrificing perfor-mance. In this
paper, Cross-Parallel Network (CPNet) forefficient anomaly detection is
proposed here to minimizecomputations without performance drops. It consists
ofNsmaller parallel U-Net, each of which is designed to handlea single input
frame, to make the calculations significantlymore efficient. Additionally, an
inter-network shift moduleis incorporated to capture temporal relationships
among se-quential frames to enable more accurate future predictions.The
quantitative results show that our model requires lesscomputational cost than
the baseline U-Net while deliver-ing equivalent performance in anomaly
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Youngsaeng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;David Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Hanseok Ko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative Self-consistent Parallel Magnetic Resonance Imaging Reconstruction based on Nonlocal Low-Rank Regularization. (arXiv:2108.04517v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04517</id>
        <link href="http://arxiv.org/abs/2108.04517"/>
        <updated>2021-08-11T01:55:24.233Z</updated>
        <summary type="html"><![CDATA[Iterative self-consistent parallel imaging reconstruction (SPIRiT) is an
effective self-calibrated reconstruction model for parallel magnetic resonance
imaging (PMRI). The joint L1 norm of wavelet coefficients and joint total
variation (TV) regularization terms are incorporated into the SPIRiT model to
improve the reconstruction performance. The simultaneous two-directional
low-rankness (STDLR) in k-space data is incorporated into SPIRiT to realize
improved reconstruction. Recent methods have exploited the nonlocal
self-similarity (NSS) of images by imposing nonlocal low-rankness of similar
patches to achieve a superior performance. To fully utilize both the NSS in
Magnetic resonance (MR) images and calibration consistency in the k-space
domain, we propose a nonlocal low-rank (NLR)-SPIRiT model by incorporating NLR
regularization into the SPIRiT model. We apply the weighted nuclear norm (WNN)
as a surrogate of the rank and employ the Nash equilibrium (NE) formulation and
alternating direction method of multipliers (ADMM) to efficiently solve the
NLR-SPIRiT model. The experimental results demonstrate the superior performance
of NLR-SPIRiT over the state-of-the-art methods via three objective metrics and
visual comparison.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1"&gt;Ting Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jizhong Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04423</id>
        <link href="http://arxiv.org/abs/2108.04423"/>
        <updated>2021-08-11T01:55:24.226Z</updated>
        <summary type="html"><![CDATA[Deep learning models achieve strong performance for radiology image
classification, but their practical application is bottlenecked by the need for
large labeled training datasets. Semi-supervised learning (SSL) approaches
leverage small labeled datasets alongside larger unlabeled datasets and offer
potential for reducing labeling cost. In this work, we introduce NoTeacher, a
novel consistency-based SSL framework which incorporates probabilistic
graphical models. Unlike Mean Teacher which maintains a teacher network updated
via a temporal ensemble, NoTeacher employs two independent networks, thereby
eliminating the need for a teacher network. We demonstrate how NoTeacher can be
customized to handle a range of challenges in radiology image classification.
Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni
and multi-label classification, and class distribution mismatch between labeled
and unlabeled portions of the training data. In realistic empirical evaluations
on three public benchmark datasets spanning the workhorse modalities of
radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the
fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher
outperforms established SSL methods with minimal hyperparameter tuning, and has
implications as a principled and practical option for semisupervised learning
in radiology applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1"&gt;Balagopal Unnikrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1"&gt;Shafa Balaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1"&gt;Chuan Sheng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1"&gt;Pavitra Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network. (arXiv:2108.04644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04644</id>
        <link href="http://arxiv.org/abs/2108.04644"/>
        <updated>2021-08-11T01:55:24.211Z</updated>
        <summary type="html"><![CDATA[Food logo detection plays an important role in the multimedia for its wide
real-world applications, such as food recommendation of the self-service shop
and infringement detection on e-commerce platforms. A large-scale food logo
dataset is urgently needed for developing advanced food logo detection
algorithms. However, there are no available food logo datasets with food brand
information. To support efforts towards food logo detection, we introduce the
dataset FoodLogoDet-1500, a new large-scale publicly available food logo
dataset, which has 1,500 categories, about 100,000 images and about 150,000
manually annotated food logo objects. We describe the collection and annotation
process of FoodLogoDet-1500, analyze its scale and diversity, and compare it
with other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the
first largest publicly available high-quality dataset for food logo detection.
The challenge of food logo detection lies in the large-scale categories and
similarities between food logo categories. For that, we propose a novel food
logo detection method Multi-scale Feature Decoupling Network (MFDNet), which
decouples classification and regression into two branches and focuses on the
classification branch to solve the problem of distinguishing multiple food logo
categories. Specifically, we introduce the feature offset module, which
utilizes the deformation-learning for optimal classification offset and can
effectively obtain the most representative features of classification in
detection. In addition, we adopt a balanced feature pyramid in MFDNet, which
pays attention to global information, balances the multi-scale feature maps,
and enhances feature extraction capability. Comprehensive experiments on
FoodLogoDet-1500 and other two benchmark logo datasets demonstrate the
effectiveness of the proposed method. The FoodLogoDet-1500 can be found at this
https URL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qiang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1"&gt;Weiqing Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1"&gt;Sujuan Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuanjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuqiang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection. (arXiv:1910.06056v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.06056</id>
        <link href="http://arxiv.org/abs/1910.06056"/>
        <updated>2021-08-11T01:55:24.204Z</updated>
        <summary type="html"><![CDATA[The human face has a high potential for biometric identification due to its
many individual traits. At the same time, such identification is vulnerable to
biometric copies. These presentation attacks pose a great challenge in
unsupervised authentication settings. As a countermeasure, we propose a method
that automatically analyzes the plausibility of facial behavior based on a
sequence of 3D face scans. A compact feature representation measures facial
behavior using the temporal curvature change. Finally, we train our method only
on genuine faces in an anomaly detection scenario. Our method can detect
presentation attacks using elastic 3D masks, bent photographs with eye holes,
and monitor replay-attacks. For evaluation, we recorded a challenging database
containing such cases using a high-quality 3D sensor. It features 109 4D face
scans including eleven different types of presentation attacks. We achieve
error rates of 11% and 6% for APCER and BPCER, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thummel_M/0/1/0/all/0/1"&gt;Martin Th&amp;#xfc;mmel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1"&gt;Sven Sickert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1"&gt;Joachim Denzler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04536</id>
        <link href="http://arxiv.org/abs/2108.04536"/>
        <updated>2021-08-11T01:55:24.199Z</updated>
        <summary type="html"><![CDATA[The task of skeleton-based action recognition remains a core challenge in
human-centred scene understanding due to the multiple granularities and large
variation in human motion. Existing approaches typically employ a single neural
representation for different motion patterns, which has difficulty in capturing
fine-grained action classes given limited training data. To address the
aforementioned problems, we propose a novel multi-granular spatio-temporal
graph network for skeleton-based action classification that jointly models the
coarse- and fine-grained skeleton motion patterns. To this end, we develop a
dual-head graph network consisting of two interleaved branches, which enables
us to extract features at two spatio-temporal resolutions in an effective and
efficient manner. Moreover, our network utilises a cross-head communication
strategy to mutually enhance the representations of both heads. We conducted
extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU
RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance
on all the benchmarks, which validates the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tailin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Desen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yu Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08598</id>
        <link href="http://arxiv.org/abs/2107.08598"/>
        <updated>2021-08-11T01:55:24.192Z</updated>
        <summary type="html"><![CDATA[Ensemble models in E-commerce combine predictions from multiple sub-models
for ranking and revenue improvement. Industrial ensemble models are typically
deep neural networks, following the supervised learning paradigm to infer
conversion rate given inputs from sub-models. However, this process has the
following two problems. Firstly, the point-wise scoring approach disregards the
relationships between items and leads to homogeneous displayed results, while
diversified display benefits user experience and revenue. Secondly, the
learning paradigm focuses on the ranking metrics and does not directly optimize
the revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework
RAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)
and explores the best weights of sub-models by the Evaluator-Generator
Optimization (EGO). To achieve the best online performance, we propose a new
rank aggregation algorithm TournamentGreedy as a refinement of classic rank
aggregators, which also produces the best average weighted Kendall Tau Distance
(KTD) amongst all the considered algorithms with quadratic time complexity.
Under the assumption that the best output list should be Pareto Optimal on the
KTD metric for sub-models, we show that our RA algorithm has higher efficiency
and coverage in exploring the optimal weights. Combined with the idea of
Bayesian Optimization and gradient descent, we solve the online contextual
Black-Box Optimization task that finds the optimal weights for sub-models given
a chosen RA model. RA-EGO has been deployed in our online system and has
improved the revenue significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuesi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1"&gt;Guangda Huzhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qianying Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1"&gt;Qing Da&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.14442</id>
        <link href="http://arxiv.org/abs/1910.14442"/>
        <updated>2021-08-11T01:55:24.187Z</updated>
        <summary type="html"><![CDATA[We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;William B. Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1"&gt;Priya Kasimbeg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1"&gt;Alexander Toshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04344</id>
        <link href="http://arxiv.org/abs/2108.04344"/>
        <updated>2021-08-11T01:55:24.170Z</updated>
        <summary type="html"><![CDATA[Due to the limited availability and high cost of the reverse
transcription-polymerase chain reaction (RT-PCR) test, many studies have
proposed machine learning techniques for detecting COVID-19 from medical
imaging. The purpose of this study is to systematically review, assess, and
synthesize research articles that have used different machine learning
techniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.
A structured literature search was conducted in the relevant bibliographic
databases to ensure that the survey solely centered on reproducible and
high-quality research. We selected papers based on our inclusion criteria. In
this survey, we reviewed $98$ articles that fulfilled our inclusion criteria.
We have surveyed a complete pipeline of chest imaging analysis techniques
related to COVID-19, including data collection, pre-processing, feature
extraction, classification, and visualization. We have considered CT scans and
X-rays as both are widely used to describe the latest developments in medical
imaging to detect COVID-19. This survey provides researchers with valuable
insights into different machine learning techniques and their performance in
the detection and diagnosis of COVID-19 from chest imaging. At the end, the
challenges and limitations in detecting COVID-19 using machine learning
techniques and the future direction of research are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1"&gt;Aishwarza Panday&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Muhammad Ashad Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1"&gt;Nihad Karim Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04800</id>
        <link href="http://arxiv.org/abs/2108.04800"/>
        <updated>2021-08-11T01:55:24.165Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) is transforming medicine and showing promise in
improving clinical diagnosis. In breast cancer screening, several recent
studies show that AI has the potential to improve radiologists' accuracy,
subsequently helping in early cancer diagnosis and reducing unnecessary workup.
As the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them in order to reproduce the results
and to compare different approaches. To enable reproducibility of research in
this application area and to enable comparison between different methods, we
release a meta-repository containing deep learning models for classification of
screening mammograms. This meta-repository creates a framework that enables the
evaluation of machine learning models on any private or public screening
mammography data set. At its inception, our meta-repository contains five
state-of-the-art models with open-source implementations and cross-platform
compatibility. We compare their performance on five international data sets:
two private New York University breast cancer screening data sets as well as
three public (DDSM, INbreast and Chinese Mammography Database) data sets. Our
framework has a flexible design that can be generalized to other medical image
analysis tasks. The meta-repository is available at
https://www.github.com/nyukat/mammography_metarepository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1"&gt;Benjamin Stadnick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1"&gt;Jan Witowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1"&gt;Vishwaesh Rajiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1"&gt;Farah E. Shamout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04658</id>
        <link href="http://arxiv.org/abs/2108.04658"/>
        <updated>2021-08-11T01:55:24.159Z</updated>
        <summary type="html"><![CDATA[Development of deep learning systems for biomedical segmentation often
requires access to expert-driven, manually annotated datasets. If more than a
single expert is involved in the annotation of the same images, then the
inter-expert agreement is not necessarily perfect, and no single expert
annotation can precisely capture the so-called ground truth of the regions of
interest on all images. Also, it is not trivial to generate a reference
estimate using annotations from multiple experts. Here we present a deep neural
network, defined as U-Net-and-a-half, which can simultaneously learn from
annotations performed by multiple experts on the same set of images.
U-Net-and-a-half contains a convolutional encoder to generate features from the
input images, multiple decoders that allow simultaneous learning from image
masks obtained from annotations that were independently generated by multiple
experts, and a shared low-dimensional feature space. To demonstrate the
applicability of our framework, we used two distinct datasets from digital
pathology and radiology, respectively. Specifically, we trained two separate
models using pathologist-driven annotations of glomeruli on whole slide images
of human kidney biopsies (10 patients), and radiologist-driven annotations of
lumen cross-sections of human arteriovenous fistulae obtained from
intravascular ultrasound images (10 patients), respectively. The models based
on U-Net-and-a-half exceeded the performance of the traditional U-Net models
trained on single expert annotations alone, thus expanding the scope of
multitask learning in the context of biomedical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1"&gt;Jesper Kers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1"&gt;Clarissa A. Cassol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1"&gt;Joris J. Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1"&gt;Najia Idrees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1"&gt;Alik Farber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1"&gt;Samir Haroon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1"&gt;Kevin P. Daly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Suvranu Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1"&gt;Vipul C. Chitalia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1"&gt;Vijaya B. Kolachalama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.04327</id>
        <link href="http://arxiv.org/abs/2108.04327"/>
        <updated>2021-08-11T01:55:24.154Z</updated>
        <summary type="html"><![CDATA[Natural numerical networks are introduced as a new classification algorithm
based on the numerical solution of nonlinear partial differential equations of
forward-backward diffusion type on complete graphs. The proposed natural
numerical network is applied to open important environmental and nature
conservation task, the automated identification of protected habitats by using
satellite images. In the natural numerical network, the forward diffusion
causes the movement of points in a feature space toward each other. The
opposite effect, keeping the points away from each other, is caused by backward
diffusion. This yields the desired classification. The natural numerical
network contains a few parameters that are optimized in the learning phase of
the method. After learning parameters and optimizing the topology of the
network graph, classification necessary for habitat identification is
performed. A relevancy map for each habitat is introduced as a tool for
validating the classification and finding new Natura 2000 habitat appearances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1"&gt;Karol Mikula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1"&gt;Michal Kollar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1"&gt;Aneta A. Ozvat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1"&gt;Martin Ambroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1"&gt;Lucia Cahojova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1"&gt;Ivan Jarolimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1"&gt;Jozef Sibik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1"&gt;Maria Sibikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A deep generative model for probabilistic energy forecasting in power systems: normalizing flows. (arXiv:2106.09370v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09370</id>
        <link href="http://arxiv.org/abs/2106.09370"/>
        <updated>2021-08-11T01:55:24.148Z</updated>
        <summary type="html"><![CDATA[Greater direct electrification of end-use sectors with a higher share of
renewables is one of the pillars to power a carbon-neutral society by 2050.
However, in contrast to conventional power plants, renewable energy is subject
to uncertainty raising challenges for their interaction with power systems.
Scenario-based probabilistic forecasting models have become an important tool
to equip decision-makers. This paper proposes to present to the power systems
forecasting practitioners a recent deep learning technique, the normalizing
flows, to produce accurate scenario-based probabilistic forecasts that are
crucial to face the new challenges in power systems applications. The strength
of this technique is to directly learn the stochastic multivariate distribution
of the underlying process by maximizing the likelihood. Through comprehensive
empirical evaluations using the open data of the Global Energy Forecasting
Competition 2014, we demonstrate that this methodology is competitive with
other state-of-the-art deep learning generative models: generative adversarial
networks and variational autoencoders. The models producing weather-based wind,
solar power, and load scenarios are properly compared both in terms of forecast
value, by considering the case study of an energy retailer, and quality using
several complementary metrics. The numerical experiments are simple and easily
reproducible. Thus, we hope it will encourage other forecasting practitioners
to test and use normalizing flows in power system applications such as bidding
on electricity markets, scheduling of power systems with high renewable energy
sources penetration, energy management of virtual power plan or microgrids, and
unit commitment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1"&gt;Jonathan Dumas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1"&gt;Antoine Wehenkel Damien Lanaspeze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1"&gt;Bertrand Corn&amp;#xe9;lusse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1"&gt;Antonio Sutera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data. (arXiv:2108.04815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04815</id>
        <link href="http://arxiv.org/abs/2108.04815"/>
        <updated>2021-08-11T01:55:24.133Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) are widely used for image classification
in a variety of fields, including medical imaging. While most studies deploy
cross-entropy as the loss function in such tasks, a growing number of
approaches have turned to a family of contrastive learning-based losses. Even
though performance metrics such as accuracy, sensitivity and specificity are
regularly used for the evaluation of CNN classifiers, the features that these
classifiers actually learn are rarely identified and their effect on the
classification performance on out-of-distribution test samples is
insufficiently explored. In this paper, motivated by the real-world task of
lung nodule classification, we investigate the features that a CNN learns when
trained and tested on different distributions of a synthetic dataset with
controlled modes of variation. We show that different loss functions lead to
different features being learned and consequently affect the generalization
ability of the classifier on unseen data. This study provides some important
insights into the design of deep learning solutions for medical imaging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1"&gt;Vasileios Baltatzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1"&gt;Loic Le Folgoc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1"&gt;Sam Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1"&gt;Octavio E. Martinez Manzanera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1"&gt;Kyriaki-Margarita Bintsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Arjun Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sujal Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04814</id>
        <link href="http://arxiv.org/abs/2108.04814"/>
        <updated>2021-08-11T01:55:24.124Z</updated>
        <summary type="html"><![CDATA[While self-supervised monocular depth estimation in driving scenarios has
achieved comparable performance to supervised approaches, violations of the
static world assumption can still lead to erroneous depth predictions of
traffic participants, posing a potential safety issue. In this paper, we
present R4Dyn, a novel set of techniques to use cost-efficient radar data on
top of a self-supervised depth estimation framework. In particular, we show how
radar can be used during training as weak supervision signal, as well as an
extra input to enhance the estimation robustness at inference time. Since
automotive radars are readily available, this allows to collect training data
from a variety of existing vehicles. Moreover, by filtering and expanding the
signal to make it compatible with learning-based approaches, we address radar
inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome
a major limitation of self-supervised depth estimation, i.e. the prediction of
traffic participants. We substantially improve the estimation on dynamic
objects, such as cars by 37% on the challenging nuScenes dataset, hence
demonstrating that radar is a valuable additional sensor for monocular depth
estimation in autonomous vehicles. Additionally, we plan on making the code
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1"&gt;Stefano Gasperini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1"&gt;Patrick Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1"&gt;Vinzenz Dallabetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1"&gt;Benjamin Busam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01289</id>
        <link href="http://arxiv.org/abs/2108.01289"/>
        <updated>2021-08-11T01:55:24.118Z</updated>
        <summary type="html"><![CDATA[Deep neural networks continue to awe the world with their remarkable
performance. Their predictions, however, are prone to be corrupted by
adversarial examples that are imperceptible to humans. Current efforts to
improve the robustness of neural networks against adversarial examples are
focused on developing robust training methods, which update the weights of a
neural network in a more robust direction. In this work, we take a step beyond
training of the weight parameters and consider the problem of designing an
adversarially robust neural architecture with high intrinsic robustness. We
propose AdvRush, a novel adversarial robustness-aware neural architecture
search algorithm, based upon a finding that independent of the training method,
the intrinsic robustness of a neural network can be represented with the
smoothness of its input loss landscape. Through a regularizer that favors a
candidate architecture with a smoother input loss landscape, AdvRush
successfully discovers an adversarially robust neural architecture. Along with
a comprehensive theoretical motivation for AdvRush, we conduct an extensive
amount of experiments to demonstrate the efficacy of AdvRush on various
benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust
accuracy under FGSM attack after standard training and 50.04% robust accuracy
under AutoAttack after 7-step PGD adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1"&gt;Jisoo Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1"&gt;Byunggook Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1"&gt;Hyeokjun Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition. (arXiv:2002.09674v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.09674</id>
        <link href="http://arxiv.org/abs/2002.09674"/>
        <updated>2021-08-11T01:55:24.113Z</updated>
        <summary type="html"><![CDATA[Gait recognition is widely used in social security applications due to its
advantages in long-distance human identification. Recently, sequence-based
methods have achieved high accuracy by learning abundant temporal and spatial
information. However, their robustness under adversarial attacks has not been
clearly explored. In this paper, we demonstrate that the state-of-the-art gait
recognition model is vulnerable to such attacks. To this end, we propose a
novel temporal sparse adversarial attack method. Different from previous
additive noise models which add perturbations on original samples, we employ a
generative adversarial network based architecture to semantically generate
adversarial high-quality gait silhouettes or video frames. Moreover, by
sparsely substituting or inserting a few adversarial gait silhouettes, the
proposed method ensures its imperceptibility and achieves a high attack success
rate. The experimental results show that if only one-fortieth of the frames are
attacked, the accuracy of the target model drops dramatically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Ziwen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Analysis for Derivative Compressive Sampling. (arXiv:2108.04355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04355</id>
        <link href="http://arxiv.org/abs/2108.04355"/>
        <updated>2021-08-11T01:55:24.107Z</updated>
        <summary type="html"><![CDATA[Derivative compressive sampling (DCS) is a signal reconstruction method from
measurements of the spatial gradient with sub-Nyquist sampling rate.
Applications of DCS include optical image reconstruction, photometric stereo,
and shape-from-shading. In this work, we study the sensitivity of DCS with
respect to algorithmic hyperparameters using a brute-force search algorithm. We
perform experiments on a dataset of surface images and deduce guidelines for
the user to setup values for the hyperparameters for improved signal recovery
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1"&gt;Md Fazle Rabbi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04349</id>
        <link href="http://arxiv.org/abs/2108.04349"/>
        <updated>2021-08-11T01:55:24.089Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUNet: Symmetric Undistortion Network for Rolling Shutter Correction. (arXiv:2108.04775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04775</id>
        <link href="http://arxiv.org/abs/2108.04775"/>
        <updated>2021-08-11T01:55:24.084Z</updated>
        <summary type="html"><![CDATA[The vast majority of modern consumer-grade cameras employ a rolling shutter
mechanism, leading to image distortions if the camera moves during image
acquisition. In this paper, we present a novel deep network to solve the
generic rolling shutter correction problem with two consecutive frames. Our
pipeline is symmetrically designed to predict the global shutter image
corresponding to the intermediate time of these two frames, which is difficult
for existing methods because it corresponds to a camera pose that differs most
from the two frames. First, two time-symmetric dense undistortion flows are
estimated by using well-established principles: pyramidal construction,
warping, and cost volume processing. Then, both rolling shutter images are
warped into a common global shutter one in the feature space, respectively.
Finally, a symmetric consistency constraint is constructed in the image decoder
to effectively aggregate the contextual cues of two rolling shutter images,
thereby recovering the high-quality global shutter image. Extensive experiments
with both synthetic and real data from public benchmarks demonstrate the
superiority of our proposed approach over the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1"&gt;Bin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mingyi He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04212</id>
        <link href="http://arxiv.org/abs/2108.04212"/>
        <updated>2021-08-11T01:55:24.079Z</updated>
        <summary type="html"><![CDATA[Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at https://github.com/datamllab/autovideo]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1"&gt;Zaid Pervaiz Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Sirui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anmoll Kumar Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1"&gt;Mohammad Qazim Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1"&gt;Kwei-Herng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaben Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1"&gt;Na Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1907.12975</id>
        <link href="http://arxiv.org/abs/1907.12975"/>
        <updated>2021-08-11T01:55:24.073Z</updated>
        <summary type="html"><![CDATA[Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1"&gt;Florian Kromp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1"&gt;Lukas Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1"&gt;Eva Bozsaky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1"&gt;Inge Ambros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1"&gt;Wolfgang Doerr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1"&gt;Sabine Taschner-Mandl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1"&gt;Peter Ambros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06895</id>
        <link href="http://arxiv.org/abs/2106.06895"/>
        <updated>2021-08-11T01:55:24.067Z</updated>
        <summary type="html"><![CDATA[To reduce the time-to-market and access to state-of-the-art techniques, CNN
hardware mapping and deployment on embedded accelerators are often outsourced
to untrusted third parties, which is going to be more prevalent in futuristic
artificial intelligence of things (AIoT) systems. These AIoT systems anticipate
horizontal collaboration among different resource-constrained AIoT node
devices, where CNN layers are partitioned and these devices collaboratively
compute complex CNN tasks. This horizontal collaboration opens another attack
surface to the CNN-based application, like inserting the hardware Trojans (HT)
into the embedded accelerators designed for the CNN. Therefore, there is a dire
need to explore this attack surface for designing secure embedded hardware
accelerators for CNNs. Towards this goal, in this paper, we exploited this
attack surface to propose an HT-based attack called FeSHI. Since in horizontal
collaboration of RC AIoT devices different sections of CNN architectures are
outsourced to different untrusted third parties, the attacker may not know the
input image, but it has access to the layer-by-layer output feature maps
information for the assigned sections of the CNN architecture. This attack
exploits the statistical distribution, i.e., Gaussian distribution, of the
layer-by-layer feature maps of the CNN to design two triggers for stealthy HT
with a very low probability of triggering. Also, three different novel,
stealthy and effective trigger designs are proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1"&gt;Tolulope Odetola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1"&gt;Faiq Khalid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1"&gt;Travis Sandefur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1"&gt;Hawzhin Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Syed Rafay Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14330</id>
        <link href="http://arxiv.org/abs/2107.14330"/>
        <updated>2021-08-11T01:55:24.033Z</updated>
        <summary type="html"><![CDATA[Education should not be a privilege but a common good. It should be openly
accessible to everyone, with as few barriers as possible; even more so for key
technologies such as Machine Learning (ML) and Data Science (DS). Open
Educational Resources (OER) are a crucial factor for greater educational
equity. In this paper, we describe the specific requirements for OER in ML and
DS and argue that it is especially important for these fields to make source
files publicly available, leading to Open Source Educational Resources (OSER).
We present our view on the collaborative development of OSER, the challenges
this poses, and first steps towards their solutions. We outline how OSER can be
used for blended learning scenarios and share our experiences in university
education. Finally, we discuss additional challenges such as credit assignment
or granting certificates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1"&gt;Ludwig Bothmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1"&gt;Sven Strickroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1"&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1"&gt;Fabian Scheipl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04351</id>
        <link href="http://arxiv.org/abs/2108.04351"/>
        <updated>2021-08-11T01:55:24.027Z</updated>
        <summary type="html"><![CDATA[This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VirtualConductor: Music-driven Conducting Video Generation System. (arXiv:2108.04350v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04350</id>
        <link href="http://arxiv.org/abs/2108.04350"/>
        <updated>2021-08-11T01:55:24.016Z</updated>
        <summary type="html"><![CDATA[In this demo, we present VirtualConductor, a system that can generate
conducting video from any given music and a single user's image. First, a
large-scale conductor motion dataset is collected and constructed. Then, we
propose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual
learning to learn the cross-modal relationship and generate diverse, plausible,
music-synchronized motion. Finally, we combine 3D animation rendering and a
pose transfer model to synthesize conducting video from a single given user's
image. Therefore, any user can become a virtual conductor through the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Delong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AnyoneNet: Synchronized Speech and Talking Head Generation for arbitrary person. (arXiv:2108.04325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04325</id>
        <link href="http://arxiv.org/abs/2108.04325"/>
        <updated>2021-08-11T01:55:24.001Z</updated>
        <summary type="html"><![CDATA[Automatically generating videos in which synthesized speech is synchronized
with lip movements in a talking head has great potential in many human-computer
interaction scenarios. In this paper, we present an automatic method to
generate synchronized speech and talking-head videos on the basis of text and a
single face image of an arbitrary person as input. In contrast to previous
text-driven talking head generation methods, which can only synthesize the
voice of a specific person, the proposed method is capable of synthesizing
speech for any person that is inaccessible in the training stage. Specifically,
the proposed method decomposes the generation of synchronized speech and
talking head videos into two stages, i.e., a text-to-speech (TTS) stage and a
speech-driven talking head generation stage. The proposed TTS module is a
face-conditioned multi-speaker TTS model that gets the speaker identity
information from face images instead of speech, which allows us to synthesize a
personalized voice on the basis of the input face image. To generate the
talking head videos from the face images, a facial landmark-based method that
can predict both lip movements and head rotations is proposed. Extensive
experiments demonstrate that the proposed method is able to generate
synchronized speech and talking head videos for arbitrary persons and
non-persons. Synthesized speech shows consistency with the given face regarding
to the synthesized voice's timbre and one's appearance in the image, and the
proposed landmark-based talking head method outperforms the state-of-the-art
landmark-based method on generating natural talking head videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinsheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qicong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharenborg/0/1/0/all/0/1"&gt;Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11686</id>
        <link href="http://arxiv.org/abs/2105.11686"/>
        <updated>2021-08-11T01:55:23.984Z</updated>
        <summary type="html"><![CDATA[Studying the implicit regularization effect of the nonlinear training
dynamics of neural networks (NNs) is important for understanding why
over-parameterized neural networks often generalize well on real dataset.
Empirically, for two-layer NN, existing works have shown that input weights of
hidden neurons (the input weight of a hidden neuron consists of the weight from
its input layer to the hidden neuron and its bias term) condense on isolated
orientations with a small initialization. The condensation dynamics implies
that NNs can learn features from the training data with a network configuration
effectively equivalent to a much smaller network during the training. In this
work, we show that the multiple roots of activation function at origin
(referred as ``multiplicity'') is a key factor for understanding the
condensation at the initial stage of training. Our experiments of multilayer
networks suggest that the maximal number of condensed orientations is twice the
multiplicity of the activation function used. Our theoretical analysis of
two-layer networks confirms experiments for two cases, one is for the
activation function of multiplicity one, which contains many common activation
functions, and the other is for the one-dimensional input. This work makes a
step towards understanding how small initialization implicitly leads NNs to
condensation at initial training stage, which lays a foundation for the future
study of the nonlinear dynamics of NNs and its implicit regularization effect
at a later stage of training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hanxu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yaoyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00793</id>
        <link href="http://arxiv.org/abs/2103.00793"/>
        <updated>2021-08-11T01:55:23.977Z</updated>
        <summary type="html"><![CDATA[In real applications, different computation-resource devices need
different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,
existing methods either design multiple networks and train them independently,
or construct depth-level/width-level dynamic neural networks which is hard to
prove the accuracy of each sub-net. In this article, we propose an elegant
Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets
of similar architectures. To improve the generalization of sub-nets, we design
the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to
implement knowledge transfer from the teacher (full-net) to multiple students
(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to
constrain the posterior class probability consistency between full-net and
sub-nets, and self-attention distillation on the same resolution feature of
different depth is addressed to drive more abundant feature representations of
sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in
a DDNN via the online knowledge distillation in each training iteration without
extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet
datasets demonstrate that sub-nets in DDNN with EKD training achieve better
performance than individually training networks while preserving the original
performance of full-nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shuchang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Ting-Bing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13430</id>
        <link href="http://arxiv.org/abs/2107.13430"/>
        <updated>2021-08-11T01:55:23.972Z</updated>
        <summary type="html"><![CDATA[This study proposes multivariate kernel density estimation by stagewise
minimization algorithm based on $U$-divergence and a simple dictionary. The
dictionary consists of an appropriate scalar bandwidth matrix and a part of the
original data. The resulting estimator brings us data-adaptive weighting
parameters and bandwidth matrices, and realizes a sparse representation of
kernel density estimation. We develop the non-asymptotic error bound of
estimator obtained via the proposed stagewise minimization algorithm. It is
confirmed from simulation studies that the proposed estimator performs
competitive to or sometime better than other well-known density estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1"&gt;Kiheiji Nishida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1"&gt;Kanta Naito&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04267</id>
        <link href="http://arxiv.org/abs/2108.04267"/>
        <updated>2021-08-11T01:55:23.966Z</updated>
        <summary type="html"><![CDATA[The neuroimage analysis community has neglected the automated segmentation of
the olfactory bulb (OB) despite its crucial role in olfactory function. The
lack of an automatic processing method for the OB can be explained by its
challenging properties. Nonetheless, recent advances in MRI acquisition
techniques and resolution have allowed raters to generate more reliable manual
annotations. Furthermore, the high accuracy of deep learning methods for
solving semantic segmentation problems provides us with an option to reliably
assess even small structures. In this work, we introduce a novel, fast, and
fully automated deep learning pipeline to accurately segment OB tissue on
sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we
designed a three-stage pipeline: (1) Localization of a region containing both
OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized
region through four independent AttFastSurferCNN - a novel deep learning
architecture with a self-attention mechanism to improve modeling of contextual
information, and (3) Ensemble of the predicted label maps. The OB pipeline
exhibits high performance in terms of boundary delineation, OB localization,
and volume estimation across a wide range of ages in 203 participants of the
Rhineland Study. Moreover, it also generalizes to scans of an independent
dataset never encountered during training, the Human Connectome Project (HCP),
with different acquisition parameters and demographics, evaluated in 30 cases
at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.
We extensively validated our pipeline not only with respect to segmentation
accuracy but also to known OB volume effects, where it can sensitively
replicate age effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1"&gt;Santiago Estrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Ran Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1"&gt;Kersten Diers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Weiyi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1"&gt;Philipp Ehses&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1"&gt;Tony St&amp;#xf6;cker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1"&gt;Monique M.B Breteler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1"&gt;Martin Reuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04281</id>
        <link href="http://arxiv.org/abs/2108.04281"/>
        <updated>2021-08-11T01:55:23.961Z</updated>
        <summary type="html"><![CDATA[This paper presents a semantic planar SLAM system that improves pose
estimation and mapping using cues from an instance planar segmentation network.
While the mainstream approaches are using RGB-D sensors, employing a monocular
camera with such a system still faces challenges such as robust data
association and precise geometric model fitting. In the majority of existing
work, geometric model estimation problems such as homography estimation and
piece-wise planar reconstruction (PPR) are usually solved by standard (greedy)
RANSAC separately and sequentially. However, setting the inlier-outlier
threshold is difficult in absence of information about the scene (i.e. the
scale). In this work, we revisit these problems and argue that two mentioned
geometric models (homographies/3D planes) can be solved by minimizing an energy
function that exploits the spatial coherence, i.e. with graph-cut optimization,
which also tackles the practical issue when the output of a trained CNN is
inaccurate. Moreover, we propose an adaptive parameter setting strategy based
on our experiments, and report a comprehensive evaluation on various
open-source datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1"&gt;Fangwen Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yaxu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1"&gt;Jason Rambach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1"&gt;Alain Pagani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04324</id>
        <link href="http://arxiv.org/abs/2108.04324"/>
        <updated>2021-08-11T01:55:23.955Z</updated>
        <summary type="html"><![CDATA[Storytelling is an open-ended task that entails creative thinking and
requires a constant flow of ideas. Natural language generation (NLG) for
storytelling is especially challenging because it requires the generated text
to follow an overall theme while remaining creative and diverse to engage the
reader. In this work, we introduce a system and a web-based demo, FairyTailor,
for human-in-the-loop visual story co-creation. Users can create a cohesive
children's fairytale by weaving generated texts and retrieved images with their
input. FairyTailor adds another modality and modifies the text generation
process to produce a coherent and creative sequence of text and images. To our
knowledge, this is the first dynamic tool for multimodal story generation that
allows interactive co-formation of both texts and images. It allows users to
give feedback on co-created stories and share their results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1"&gt;Eden Bensaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1"&gt;Mauro Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1"&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1"&gt;Hendrik Strobelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1907.12975</id>
        <link href="http://arxiv.org/abs/1907.12975"/>
        <updated>2021-08-11T01:55:22.683Z</updated>
        <summary type="html"><![CDATA[Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1"&gt;Florian Kromp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1"&gt;Lukas Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1"&gt;Eva Bozsaky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1"&gt;Inge Ambros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1"&gt;Wolfgang Doerr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1"&gt;Sabine Taschner-Mandl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1"&gt;Peter Ambros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07770</id>
        <link href="http://arxiv.org/abs/2103.07770"/>
        <updated>2021-08-11T01:55:22.677Z</updated>
        <summary type="html"><![CDATA[Video quality assessment (VQA) is now a fastgrowing subject, beginning to
mature in the full reference (FR) case, while the burgeoning no reference (NR)
case remains challenging. We investigate variants of the popular VMAF video
quality assessment algorithm for the FR case, using support vector regression
and feedforward neural networks, and extend it to the NR case, using the same
learning architectures, to develop a partially unified framework for VQA. When
heavily trained, algorithms such as VMAF perform well on test datasets, with
90%+ match; but predicting performance in the wild is better done by
training/testing from scratch, as we do. Even from scratch, we achieve 90%+
performance in FR, with gains over VMAF. And we greatly reduce complexity vs.
leading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our
preliminary testing, we find the improvements in trainability, while also
constraining computational complexity, as quite encouraging, suggesting further
study and analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1"&gt;Pankaj Topiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1"&gt;Jiangfeng Pian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1"&gt;Katalina Biondi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1"&gt;Arvind Krovvidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetrievalFuse: Neural 3D Scene Reconstruction with a Database. (arXiv:2104.00024v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00024</id>
        <link href="http://arxiv.org/abs/2104.00024"/>
        <updated>2021-08-11T01:55:22.672Z</updated>
        <summary type="html"><![CDATA[3D reconstruction of large scenes is a challenging problem due to the
high-complexity nature of the solution space, in particular for generative
neural networks. In contrast to traditional generative learned models which
encode the full generative process into a neural network and can struggle with
maintaining local details at the scene level, we introduce a new method that
directly leverages scene geometry from the training database. First, we learn
to synthesize an initial estimate for a 3D scene, constructed by retrieving a
top-k set of volumetric chunks from the scene database. These candidates are
then refined to a final scene generation with an attention-based refinement
that can effectively select the most consistent set of geometry from the
candidates and combine them together to create an output scene, facilitating
transfer of coherent structures and local detail from train scene geometry. We
demonstrate our neural scene reconstruction with a database for the tasks of 3D
super resolution and surface reconstruction from sparse point clouds, showing
that our approach enables generation of more coherent, accurate 3D scenes,
improving on average by over 8% in IoU over state-of-the-art scene
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1"&gt;Yawar Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1"&gt;Justus Thies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fangchang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1"&gt;Qi Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Angela Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Search on Binary Codes by Weighted Hamming Distance. (arXiv:2009.08591v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08591</id>
        <link href="http://arxiv.org/abs/2009.08591"/>
        <updated>2021-08-11T01:55:22.666Z</updated>
        <summary type="html"><![CDATA[Weighted Hamming distance, as a similarity measure between binary codes and
binary queries, provides superior accuracy in search tasks than Hamming
distance. However, how to efficiently and accurately find $K$ binary codes that
have the smallest weighted Hamming distance to the query remains an open issue.
In this paper, a fast search algorithm is proposed to perform the
non-exhaustive search for $K$ nearest binary codes by weighted Hamming
distance. By using binary codes as direct bucket indices in a hash table, the
search algorithm generates a sequence to probe the buckets based on the
independence characteristic of the weights for each bit. Furthermore, a fast
search framework based on the proposed search algorithm is designed to solve
the problem of long binary codes. Specifically, long binary codes are split
into substrings and multiple hash tables are built on them. Then, the search
algorithm probes the buckets to obtain candidates according to the generated
substring indices, and a merging algorithm is proposed to find the nearest
binary codes by merging the candidates. Theoretical analysis and experimental
results demonstrate that the search algorithm improves the search accuracy
compared to other non-exhaustive algorithms and provides orders-of-magnitude
faster search than the linear scan baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1"&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuesheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruixin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13689</id>
        <link href="http://arxiv.org/abs/2103.13689"/>
        <updated>2021-08-11T01:55:22.652Z</updated>
        <summary type="html"><![CDATA[Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1"&gt;Xianbo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Shunquan Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation. (arXiv:2108.04547v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04547</id>
        <link href="http://arxiv.org/abs/2108.04547"/>
        <updated>2021-08-11T01:55:22.646Z</updated>
        <summary type="html"><![CDATA[Contrastive learning shows great potential in unpaired image-to-image
translation, but sometimes the translated results are in poor quality and the
contents are not preserved consistently. In this paper, we uncover that the
negative examples play a critical role in the performance of contrastive
learning for image translation. The negative examples in previous methods are
randomly sampled from the patches of different positions in the source image,
which are not effective to push the positive examples close to the query
examples. To address this issue, we present instance-wise hard Negative Example
Generation for Contrastive learning in Unpaired image-to-image
Translation~(NEGCUT). Specifically, we train a generator to produce negative
examples online. The generator is novel from two perspectives: 1) it is
instance-wise which means that the generated examples are based on the input
image, and 2) it can generate hard negative examples since it is trained with
an adversarial loss. With the generator, the performance of unpaired
image-to-image translation is significantly improved. Experiments on three
benchmark datasets demonstrate that the proposed NEGCUT framework achieves
state-of-the-art performance compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weilun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jianmin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12664</id>
        <link href="http://arxiv.org/abs/2107.12664"/>
        <updated>2021-08-11T01:55:22.641Z</updated>
        <summary type="html"><![CDATA[Arbitrary shape text detection is a challenging task due to the high
complexity and variety of scene texts. In this work, we propose a novel
adaptive boundary proposal network for arbitrary shape text detection, which
can learn to directly produce accurate boundary for arbitrary shape text
without any post-processing. Our method mainly consists of a boundary proposal
model and an innovative adaptive boundary deformation model. The boundary
proposal model constructed by multi-layer dilated convolutions is adopted to
produce prior information (including classification map, distance field, and
direction field) and coarse boundary proposals. The adaptive boundary
deformation model is an encoder-decoder network, in which the encoder mainly
consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network
(RNN). It aims to perform boundary deformation in an iterative way for
obtaining text instance shape guided by prior information from the boundary
proposal model. In this way, our method can directly and efficiently generate
accurate text boundaries without complex post-processing. Extensive experiments
on publicly available datasets demonstrate the state-of-the-art performance of
our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shi-Xue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaobin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongfa Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xu-Cheng Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization. (arXiv:2108.04508v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04508</id>
        <link href="http://arxiv.org/abs/2108.04508"/>
        <updated>2021-08-11T01:55:22.635Z</updated>
        <summary type="html"><![CDATA[Finding tampered regions in images is a hot research topic in machine
learning and computer vision. Although many image manipulation location
algorithms have been proposed, most of them only focus on the RGB images with
different color spaces, and the frequency information that contains the
potential tampering clues is often ignored. In this work, a novel end-to-end
two-stream boundary-aware network (abbreviated as TBNet) is proposed for
generic image manipulation localization in which the RGB stream, the frequency
stream, and the boundary artifact location are explored in a unified framework.
Specifically, we first design an adaptive frequency selection module (AFS) to
adaptively select the appropriate frequency to mine inconsistent statistics and
eliminate the interference of redundant statistics. Then, an adaptive
cross-attention fusion module (ACF) is proposed to adaptively fuse the RGB
feature and the frequency feature. Finally, the boundary artifact location
network (BAL) is designed to locate the boundary artifacts for which the
parameters are jointly updated by the outputs of the ACF, and its results are
further fed into the decoder. Thus, the parameters of the RGB stream, the
frequency stream, and the boundary artifact location network are jointly
optimized, and their latent complementary relationships are fully mined. The
results of extensive experiments performed on four public benchmarks of the
image manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and
In-The-Wild, demonstrate that the proposed TBNet can significantly outperform
state-of-the-art generic image manipulation localization methods in terms of
both MCC and F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1"&gt;Weili Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning point embedding for 3D data processing. (arXiv:2107.08565v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08565</id>
        <link href="http://arxiv.org/abs/2107.08565"/>
        <updated>2021-08-11T01:55:22.630Z</updated>
        <summary type="html"><![CDATA[Among 2D convolutional networks on point clouds, point-based approaches
consume point clouds of fixed size directly. By analysis of PointNet, a pioneer
in introducing deep learning into point sets, we reveal that current
point-based methods are essentially spatial relationship processing networks.
In this paper, we take a different approach. Our architecture, named PE-Net,
learns the representation of point clouds in high-dimensional space, and
encodes the unordered input points to feature vectors, which standard 2D CNNs
can be applied to. The recommended network can adapt to changes in the number
of input points which is the limit of current methods. Experiments show that in
the tasks of classification and part segmentation, PE-Net achieves the
state-of-the-art performance in multiple challenging datasets, such as ModelNet
and ShapeNetPart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1"&gt;Yuan li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12449</id>
        <link href="http://arxiv.org/abs/2106.12449"/>
        <updated>2021-08-11T01:55:22.624Z</updated>
        <summary type="html"><![CDATA[Accurate detection of obstacles in 3D is an essential task for autonomous
driving and intelligent transportation. In this work, we propose a general
multimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D
point clouds at a semantic level for boosting the 3D object detection task.
Especially, the FusionPainting framework consists of three main modules: a
multi-modal semantic segmentation module, an adaptive attention-based semantic
fusion module, and a 3D object detector. First, semantic information is
obtained for 2D images and 3D Lidar point clouds based on 2D and 3D
segmentation approaches. Then the segmentation results from different sensors
are adaptively fused based on the proposed attention-based semantic fusion
module. Finally, the point clouds painted with the fused semantic label are
sent to the 3D detector for obtaining the 3D objection results. The
effectiveness of the proposed framework has been verified on the large-scale
nuScenes detection benchmark by comparing it with three different baselines.
The experimental results show that the fusion strategy can significantly
improve the detection performance compared to the methods using only point
clouds, and the methods using point clouds only painted with 2D segmentation
information. Furthermore, the proposed approach outperforms other
state-of-the-art methods on the nuScenes testing benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shaoqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dingfu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junbo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1"&gt;Zhou Bin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangjun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07588</id>
        <link href="http://arxiv.org/abs/2008.07588"/>
        <updated>2021-08-11T01:55:22.618Z</updated>
        <summary type="html"><![CDATA[Deep learning motivated by convolutional neural networks has been highly
successful in a range of medical imaging problems like image classification,
image segmentation, image synthesis etc. However for validation and
interpretability, not only do we need the predictions made by the model but
also how confident it is while making those predictions. This is important in
safety critical applications for the people to accept it. In this work, we used
an encoder decoder architecture based on variational inference techniques for
segmenting brain tumour images. We evaluate our work on the publicly available
BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over
Union (IOU) as the evaluation metrics. Our model is able to segment brain
tumours while taking into account both aleatoric uncertainty and epistemic
uncertainty in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MiniVLM: A Smaller and Faster Vision-Language Model. (arXiv:2012.06946v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06946</id>
        <link href="http://arxiv.org/abs/2012.06946"/>
        <updated>2021-08-11T01:55:22.612Z</updated>
        <summary type="html"><![CDATA[Recent vision-language (VL) studies have shown remarkable progress by
learning generic representations from massive image-text pairs with transformer
models and then fine-tuning on downstream VL tasks. While existing research has
been focused on achieving high accuracy with large pre-trained models, building
a lightweight model is of great value in practice but is less explored. In this
paper, we propose a smaller and faster VL model, MiniVLM, which can be
finetuned with good performance on various downstream tasks like its larger
counterpart. MiniVLM consists of two modules, a vision feature extractor and a
transformer-based vision-language fusion module. We design a Two-stage
Efficient feature Extractor (TEE), inspired by the one-stage EfficientDet
network, to significantly reduce the time cost of visual feature extraction by
$95\%$, compared to a baseline model. We adopt the MiniLM structure to reduce
the computation cost of the transformer module after comparing different
compact BERT models. In addition, we improve the MiniVLM pre-training by adding
$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art
captioning model. We also pre-train with high-quality image tags obtained from
a strong tagging model to enhance cross-modality alignment. The large models
are used offline without adding any overhead in fine-tuning and inference. With
the above design choices, our MiniVLM reduces the model size by $73\%$ and the
inference time cost by $94\%$ while being able to retain $94-97\%$ of the
accuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the
state-of-the-art VL research for on-the-edge applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaowei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiujun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11890</id>
        <link href="http://arxiv.org/abs/2011.11890"/>
        <updated>2021-08-11T01:55:22.587Z</updated>
        <summary type="html"><![CDATA[We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is
a practical solution to the problem of calibration-free automatic white balance
for mobile photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1"&gt;Mahmoud Afifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1"&gt;Chloe LeGendre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yun-Ta Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1"&gt;Francois Bleibel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification. (arXiv:2002.04264v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04264</id>
        <link href="http://arxiv.org/abs/2002.04264"/>
        <updated>2021-08-11T01:55:22.580Z</updated>
        <summary type="html"><![CDATA[Key for solving fine-grained image categorization is finding discriminate and
local regions that correspond to subtle visual traits. Great strides have been
made, with complex networks designed specifically to learn part-level
discriminate feature representations. In this paper, we show it is possible to
cultivate subtle details without the need for overly complicated network
designs or training mechanisms -- a single loss is all it takes. The main trick
lies with how we delve into individual feature channels early on, as opposed to
the convention of starting from a consolidated feature map. The proposed loss
function, termed as mutual-channel loss (MC-Loss), consists of two
channel-specific components: a discriminality component and a diversity
component. The discriminality component forces all feature channels belonging
to the same class to be discriminative, through a novel channel-wise attention
mechanism. The diversity component additionally constraints channels so that
they become mutually exclusive on spatial-wise. The end result is therefore a
set of feature channels that each reflects different locally discriminative
regions for a specific class. The MC-Loss can be trained end-to-end, without
the need for any bounding-box/part annotations, and yields highly
discriminative regions during inference. Experimental results show our MC-Loss
when implemented on top of common base networks can achieve state-of-the-art
performance on all four fine-grained categorization datasets (CUB-Birds,
FGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further
demonstrate the superiority of MC-Loss when compared with other recently
proposed general-purpose losses for visual classification, on two different
base networks. Code available at
https://github.com/dongliangchang/Mutual-Channel-Loss]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongliang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yifeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1"&gt;Ayan Kumar Bhunia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Ming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11170</id>
        <link href="http://arxiv.org/abs/2107.11170"/>
        <updated>2021-08-11T01:55:22.557Z</updated>
        <summary type="html"><![CDATA[Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1"&gt;Lusine Abrahamyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1"&gt;Valentin Ziatchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1"&gt;Nikos Deligiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Enhanced Prohibited Items Recognition Model. (arXiv:2102.12256v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12256</id>
        <link href="http://arxiv.org/abs/2102.12256"/>
        <updated>2021-08-11T01:55:22.550Z</updated>
        <summary type="html"><![CDATA[We proposed a new modeling method to promote the performance of prohibited
items recognition via X-ray image. We analyzed the characteristics of
prohibited items and X-ray images. We found the fact that the scales of some
items are too small to be recognized which encumber the model performance. Then
we adopted a set of data augmentation and modified the model to adapt the field
of prohibited items recognition. The Convolutional Block Attention Module(CBAM)
and rescoring mechanism has been assembled into the model. By the modification,
our model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1"&gt;Tianze Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongxiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yichao Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Solution to Product detection in Densely Packed Scenes. (arXiv:2007.11946v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11946</id>
        <link href="http://arxiv.org/abs/2007.11946"/>
        <updated>2021-08-11T01:55:22.539Z</updated>
        <summary type="html"><![CDATA[This work is a solution to densely packed scenes dataset SKU-110k. Our work
is modified from Cascade R-CNN. To solve the problem, we proposed a random crop
strategy to ensure both the sampling rate and input scale is relatively
sufficient as a contrast to the regular random crop. And we adopted some of
trick and optimized the hyper-parameters. To grasp the essential feature of the
densely packed scenes, we analysis the stages of a detector and investigate the
bottleneck which limits the performance. As a result, our method obtains 58.7
mAP on test set of SKU-110k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1"&gt;Tianze Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanjia Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongxiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yichao Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10850</id>
        <link href="http://arxiv.org/abs/2011.10850"/>
        <updated>2021-08-11T01:55:22.524Z</updated>
        <summary type="html"><![CDATA[Data hiding is the procedure of encoding desired information into an image to
resist potential noises while ensuring the embedded image has little perceptual
perturbations from the original image. Recently, with the tremendous successes
gained by deep neural networks in various fields, data hiding areas have
attracted increasing number of attentions. The neglect of considering the pixel
sensitivity within the cover image of deep neural methods will inevitably
affect the model robustness for information hiding. Targeting at the problem,
in this paper, we propose a novel deep data hiding scheme with Inverse Gradient
Attention (IGA), combing the ideas of adversarial learning and attention
mechanism to endow different sensitivity to different pixels. With the proposed
component, the model can spotlight pixels with more robustness for embedding
data. Empirically, extensive experiments show that the proposed model
outperforms the state-of-the-art methods on two prevalent datasets under
multiple settings. Besides, we further identify and discuss the connections
between the proposed inverse gradient attention and high-frequency regions
within images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honglei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yuanzhouhan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yidong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmentation of VHR EO Images using Unsupervised Learning. (arXiv:2108.04222v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04222</id>
        <link href="http://arxiv.org/abs/2108.04222"/>
        <updated>2021-08-11T01:55:22.513Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a crucial step in many Earth observation tasks.
Large quantity of pixel-level annotation is required to train deep networks for
semantic segmentation. Earth observation techniques are applied to varieties of
applications and since classes vary widely depending on the applications,
therefore, domain knowledge is often required to label Earth observation
images, impeding availability of labeled training data in many Earth
observation applications. To tackle these challenges, in this paper we propose
an unsupervised semantic segmentation method that can be trained using just a
single unlabeled scene. Remote sensing scenes are generally large. The proposed
method exploits this property to sample smaller patches from the larger scene
and uses deep clustering and contrastive learning to refine the weights of a
lightweight deep model composed of a series of the convolution layers along
with an embedded channel attention. After unsupervised training on the target
image/scene, the model automatically segregates the major classes present in
the scene and produces the segmentation map. Experimental results on the
Vaihingen dataset demonstrate the efficacy of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sudipan Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1"&gt;Lichao Mou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1"&gt;Muhammad Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No-Reference Image Quality Assessment by Hallucinating Pristine Features. (arXiv:2108.04165v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04165</id>
        <link href="http://arxiv.org/abs/2108.04165"/>
        <updated>2021-08-11T01:55:22.499Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a no-reference (NR) image quality assessment (IQA)
method via feature level pseudo-reference (PR) hallucination. The proposed
quality assessment framework is grounded on the prior models of natural image
statistical behaviors and rooted in the view that the perceptually meaningful
features could be well exploited to characterize the visual quality. Herein,
the PR features from the distorted images are learned by a mutual learning
scheme with the pristine reference as the supervision, and the discriminative
characteristics of PR features are further ensured with the triplet
constraints. Given a distorted image for quality inference, the feature level
disentanglement is performed with an invertible neural layer for final quality
prediction, leading to the PR and the corresponding distortion features for
comparison. The effectiveness of our proposed method is demonstrated on four
popular IQA databases, and superior performance on cross-database evaluation
also reveals the high generalization capability of our method. The
implementation of our method is publicly available on
https://github.com/Baoliang93/FPR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1"&gt;Chenqi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hanwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Featureswith Split-and-Share Module. (arXiv:2108.04500v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04500</id>
        <link href="http://arxiv.org/abs/2108.04500"/>
        <updated>2021-08-11T01:55:22.493Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) have shown state-of-the-art
performances in various computer vision tasks. Advances on CNN architectures
have focused mainly on designing convolutional blocks of the feature
extractors, but less on the classifiers that exploit extracted features. In
this work, we propose Split-and-Share Module (SSM),a classifier that splits a
given feature into parts, which are partially shared by multiple
sub-classifiers. Our intuition is that the more the features are shared, the
more common they will become, and SSM can encourage such structural
characteristics in the split features. SSM can be easily integrated into any
architecture without bells and whistles. We have extensively validated the
efficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent
and significant improvements over baseline architectures. In addition, we
analyze the effect of SSM using the Grad-CAM visualization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaemin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1"&gt;Minseok Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jongchan Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Dong-Geol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attribute Guided Sparse Tensor-Based Model for Person Re-Identification. (arXiv:2108.04352v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04352</id>
        <link href="http://arxiv.org/abs/2108.04352"/>
        <updated>2021-08-11T01:55:22.482Z</updated>
        <summary type="html"><![CDATA[Visual perception of a person is easily influenced by many factors such as
camera parameters, pose and viewpoint variations. These variations make person
Re-Identification (ReID) a challenging problem. Nevertheless, human attributes
usually stand as robust visual properties to such variations. In this paper, we
propose a new method to leverage features from human attributes for person
ReID. Our model uses a tensor to non-linearly fuse identity and attribute
features, and then forces the parameters of the tensor in the loss function to
generate discriminative fused features for ReID. Since tensor-based methods
usually contain a large number of parameters, training all of these parameters
becomes very slow, and the chance of overfitting increases as well. To address
this issue, we propose two new techniques based on Structural Sparsity Learning
(SSL) and Tensor Decomposition (TD) methods to create an accurate and stable
learning problem. We conducted experiments on several standard pedestrian
datasets, and experimental results indicate that our tensor-based approach
significantly improves person ReID baselines and also outperforms state of the
art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1"&gt;Fariborz Taherkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1"&gt;Ali Dabouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04212</id>
        <link href="http://arxiv.org/abs/2108.04212"/>
        <updated>2021-08-11T01:55:22.464Z</updated>
        <summary type="html"><![CDATA[Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at https://github.com/datamllab/autovideo]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1"&gt;Daochen Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1"&gt;Zaid Pervaiz Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yicheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Sirui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anmoll Kumar Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1"&gt;Mohammad Qazim Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1"&gt;Kwei-Herng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaben Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1"&gt;Na Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01622</id>
        <link href="http://arxiv.org/abs/2105.01622"/>
        <updated>2021-08-11T01:55:22.458Z</updated>
        <summary type="html"><![CDATA[Semi-supervised machine learning models learn from a (small) set of labeled
training examples, and a (large) set of unlabeled training examples.
State-of-the-art models can reach within a few percentage points of
fully-supervised training, while requiring 100x less labeled data.

We study a new class of vulnerabilities: poisoning attacks that modify the
unlabeled dataset. In order to be useful, unlabeled datasets are given strictly
less review than labeled datasets, and adversaries can therefore poison them
easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%
of the dataset size, we can manipulate a model trained on this poisoned dataset
to misclassify arbitrary examples at test time (as any desired label). Our
attacks are highly effective across datasets and semi-supervised learning
methods.

We find that more accurate methods (thus more likely to be used) are
significantly more vulnerable to poisoning attacks, and as such better training
methods are unlikely to prevent this attack. To counter this we explore the
space of defenses, and propose two methods that mitigate our attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v6 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-08-11T01:55:22.440Z</updated>
        <summary type="html"><![CDATA[We present iGibson 1.0, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson 1.0 integrates several key features to facilitate
the study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson 1.0
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson 1.0 is open-source, equipped with comprehensive examples and
documentation. For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Josiah Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04353</id>
        <link href="http://arxiv.org/abs/2108.04353"/>
        <updated>2021-08-11T01:55:22.424Z</updated>
        <summary type="html"><![CDATA[The great success of Convolutional Neural Networks (CNN) for facial attribute
prediction relies on a large amount of labeled images. Facial image datasets
are usually annotated by some commonly used attributes (e.g., gender), while
labels for the other attributes (e.g., big nose) are limited which causes their
prediction challenging. To address this problem, we use a new Multi-Task
Learning (MTL) paradigm in which a facial attribute predictor uses the
knowledge of other related attributes to obtain a better generalization
performance. Here, we leverage MLT paradigm in two problem settings. First, it
is assumed that the structure of the tasks (e.g., grouping pattern of facial
attributes) is known as a prior knowledge, and parameters of the tasks (i.e.,
predictors) within the same group are represented by a linear combination of a
limited number of underlying basis tasks. Here, a sparsity constraint on the
coefficients of this linear combination is also considered such that each task
is represented in a more structured and simpler manner. Second, it is assumed
that the structure of the tasks is unknown, and then structure and parameters
of the tasks are learned jointly by using a Laplacian regularization framework.
Our MTL methods are compared with competing methods for facial attribute
prediction to show its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1"&gt;Fariborz Taherkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1"&gt;Ali Dabouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1"&gt;Sobhan Soleymani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Morphometry of Closed, Implicit Surfaces. (arXiv:2108.04354v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04354</id>
        <link href="http://arxiv.org/abs/2108.04354"/>
        <updated>2021-08-11T01:55:22.418Z</updated>
        <summary type="html"><![CDATA[Anatomical structures such as the hippocampus, liver, and bones can be
analyzed as orientable, closed surfaces. This permits the computation of
volume, surface area, mean curvature, Gaussian curvature, and the
Euler-Poincar\'e characteristic as well as comparison of these morphometrics
between structures of different topology. The structures are commonly
represented implicitly in curve evolution problems as the zero level set of an
embedding. Practically, binary images of anatomical structures are embedded
using a signed distance transform. However, quantization prevents the accurate
computation of curvatures, leading to considerable errors in morphometry. This
paper presents a fast, simple embedding procedure for accurate local
morphometry as the zero crossing of the Gaussian blurred binary image. The
proposed method was validated based on the femur and fourth lumbar vertebrae of
50 clinical computed tomography datasets. The results show that the signed
distance transform leads to large quantization errors in the computed local
curvature. Global validation of morphometry using regression and Bland-Altman
analysis revealed that the coefficient of determination for the average mean
curvature is improved from 93.8% with the signed distance transform to 100%
with the proposed method. For the surface area, the proportional bias is
improved from -5.0% for the signed distance transform to +0.6% for the proposed
method. The Euler-Poincar\'e characteristic is improved from unusable in the
signed distance transform to 98% accuracy for the proposed method. The proposed
method enables an improved local and global evaluation of curvature for
purposes of morphometry on closed, implicit surfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Besler_B/0/1/0/all/0/1"&gt;Bryce A Besler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1"&gt;Tannis D. Kemp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalski_A/0/1/0/all/0/1"&gt;Andrew S. Michalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1"&gt;Nils D. Forkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1"&gt;Steven K. Boyd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04584</id>
        <link href="http://arxiv.org/abs/2108.04584"/>
        <updated>2021-08-11T01:55:22.413Z</updated>
        <summary type="html"><![CDATA[Scene understanding is crucial for autonomous systems which intend to operate
in the real world. Single task vision networks extract information only based
on some aspects of the scene. In multi-task learning (MTL), on the other hand,
these single tasks are jointly learned, thereby providing an opportunity for
tasks to share information and obtain a more comprehensive understanding. To
this end, we develop UniNet, a unified scene understanding network that
accurately and efficiently infers vital vision tasks including object
detection, semantic segmentation, instance segmentation, monocular depth
estimation, and monocular instance depth prediction. As these tasks look at
different semantic and geometric information, they can either complement or
conflict with each other. Therefore, understanding inter-task relationships can
provide useful cues to enable complementary information sharing. We evaluate
the task relationships in UniNet through the lens of adversarial attacks based
on the notion that they can exploit learned biases and task interactions in the
neural network. Extensive experiments on the Cityscapes dataset, using
untargeted and targeted attacks reveal that semantic tasks strongly interact
amongst themselves, and the same holds for geometric tasks. Additionally, we
show that the relationship between semantic and geometric tasks is asymmetric
and their interaction becomes weaker as we move towards higher-level
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1"&gt;NareshKumar Gurulingan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05568</id>
        <link href="http://arxiv.org/abs/2103.05568"/>
        <updated>2021-08-11T01:55:22.405Z</updated>
        <summary type="html"><![CDATA[Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Aman Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1"&gt;Mayank Kothyari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vishwajeet Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00793</id>
        <link href="http://arxiv.org/abs/2103.00793"/>
        <updated>2021-08-11T01:55:22.389Z</updated>
        <summary type="html"><![CDATA[In real applications, different computation-resource devices need
different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,
existing methods either design multiple networks and train them independently,
or construct depth-level/width-level dynamic neural networks which is hard to
prove the accuracy of each sub-net. In this article, we propose an elegant
Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets
of similar architectures. To improve the generalization of sub-nets, we design
the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to
implement knowledge transfer from the teacher (full-net) to multiple students
(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to
constrain the posterior class probability consistency between full-net and
sub-nets, and self-attention distillation on the same resolution feature of
different depth is addressed to drive more abundant feature representations of
sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in
a DDNN via the online knowledge distillation in each training iteration without
extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet
datasets demonstrate that sub-nets in DDNN with EKD training achieve better
performance than individually training networks while preserving the original
performance of full-nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shuchang Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Ting-Bing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An optical biomimetic eyes with interested object imaging. (arXiv:2108.04236v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04236</id>
        <link href="http://arxiv.org/abs/2108.04236"/>
        <updated>2021-08-11T01:55:22.361Z</updated>
        <summary type="html"><![CDATA[We presented an optical system to perform imaging interested objects in
complex scenes, like the creature easy see the interested prey in the hunt for
complex environments. It utilized Deep-learning network to learn the interested
objects's vision features and designed the corresponding "imaging matrices",
furthermore the learned matrixes act as the measurement matrix to complete
compressive imaging with a single-pixel camera, finally we can using the
compressed image data to only image the interested objects without the rest
objects and backgrounds of the scenes with the previous Deep-learning network.
Our results demonstrate that no matter interested object is single feature or
rich details, the interference can be successfully filtered out and this idea
can be applied in some common applications that effectively improve the
performance. This bio-inspired optical system can act as the creature eye to
achieve success on interested-based object imaging, object detection, object
recognition and object tracking, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shimei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shangyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1"&gt;Miao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiaofang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chuangxue Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kunyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shuxin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yuer Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1"&gt;Ting Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. (arXiv:2102.03322v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03322</id>
        <link href="http://arxiv.org/abs/2102.03322"/>
        <updated>2021-08-11T01:55:22.356Z</updated>
        <summary type="html"><![CDATA[Given the increasing promise of Graph Neural Networks (GNNs) in real-world
applications, several methods have been developed for explaining their
predictions. So far, these methods have primarily focused on generating
subgraphs that are especially relevant for a particular prediction. However,
such methods do not provide a clear opportunity for recourse: given a
prediction, we want to understand how the prediction can be changed in order to
achieve a more desirable outcome. In this work, we propose a method for
generating counterfactual (CF) explanations for GNNs: the minimal perturbation
to the input (graph) data such that the prediction changes. Using only edge
deletions, we find that our method, CF-GNNExplainer can generate CF
explanations for the majority of instances across three widely used datasets
for GNN explanations, while removing less than 3 edges on average, with at
least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes
edges that are crucial for the original predictions, resulting in minimal CF
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1"&gt;Ana Lucic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1"&gt;Maartje ter Hoeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1"&gt;Gabriele Tolomei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1"&gt;Fabrizio Silvestri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13731</id>
        <link href="http://arxiv.org/abs/2107.13731"/>
        <updated>2021-08-11T01:55:22.350Z</updated>
        <summary type="html"><![CDATA[To improve the accessibility of smart devices and to simplify their usage,
building models which understand user interfaces (UIs) and assist users to
complete their tasks is critical. However, unique challenges are proposed by
UI-specific characteristics, such as how to effectively leverage multimodal UI
features that involve image, text, and structural metadata and how to achieve
good performance when high-quality labeled data is unavailable. To address such
challenges we introduce UIBert, a transformer-based joint image-text model
trained through novel pre-training tasks on large-scale unlabeled UI data to
learn generic feature representations for a UI and its components. Our key
intuition is that the heterogeneous features in a UI are self-aligned, i.e.,
the image and text features of UI components, are predictive of each other. We
propose five pretraining tasks utilizing this self-alignment among different
features of a UI component and across various components in the same UI. We
evaluate our method on nine real-world downstream UI tasks where UIBert
outperforms strong multimodal baselines by up to 9.26% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1"&gt;Chongyang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Ying Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1"&gt;Srinivas Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Abhinav Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1"&gt;Blaise Aguera y Arcas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04345</id>
        <link href="http://arxiv.org/abs/2108.04345"/>
        <updated>2021-08-11T01:55:22.345Z</updated>
        <summary type="html"><![CDATA[Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1"&gt;Hamza Rasaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1"&gt;Hassan Rivaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating synthetic meteorology satellite visible light images during night based on GAN method. (arXiv:2108.04330v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04330</id>
        <link href="http://arxiv.org/abs/2108.04330"/>
        <updated>2021-08-11T01:55:22.330Z</updated>
        <summary type="html"><![CDATA[Meteorology satellite visible light images is critical for meteorology
support and forecast. However, there is no such kind of data during night time.
To overcome this, we propose a method based on deep learning to create
synthetic satellite visible light images during night. Specifically, to produce
more realistic products, we train a Generative Adversarial Networks (GAN) model
to generate visible light images given the corresponding satellite infrared
images and numerical weather prediction(NWP) products. To better model the
nonlinear relationship from infrared data and NWP products to visible light
images, we propose to use the channel-wise attention mechanics, e.g., SEBlock
to quantitative weight the input channels. The experiments based on the ECMWF
NWP products and FY-4A meteorology satellite visible light and infrared
channels date show that the proposed methods can be effective to create
realistic synthetic satellite visible light images during night.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wencong_C/0/1/0/all/0/1"&gt;CHENG Wencong&lt;/a&gt; (1) ((1) Beijing Aviation Meteorological Institute)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-11T01:55:22.324Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Gradient Adversarial Attack. (arXiv:2108.04204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.04204</id>
        <link href="http://arxiv.org/abs/2108.04204"/>
        <updated>2021-08-11T01:55:22.318Z</updated>
        <summary type="html"><![CDATA[In recent years, research on adversarial attacks has become a hot spot.
Although current literature on the transfer-based adversarial attack has
achieved promising results for improving the transferability to unseen
black-box models, it still leaves a long way to go. Inspired by the idea of
meta-learning, this paper proposes a novel architecture called Meta Gradient
Adversarial Attack (MGAA), which is plug-and-play and can be integrated with
any existing gradient-based attack method for improving the cross-model
transferability. Specifically, we randomly sample multiple models from a model
zoo to compose different tasks and iteratively simulate a white-box attack and
a black-box attack in each task. By narrowing the gap between the gradient
directions in white-box and black-box attacks, the transferability of
adversarial examples on the black-box setting can be improved. Extensive
experiments on the CIFAR10 and ImageNet datasets show that our architecture
outperforms the state-of-the-art methods for both black-box and white-box
attack settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zheng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yunpei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1"&gt;Tao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[INeRF: Inverting Neural Radiance Fields for Pose Estimation. (arXiv:2012.05877v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05877</id>
        <link href="http://arxiv.org/abs/2012.05877"/>
        <updated>2021-08-11T01:55:22.312Z</updated>
        <summary type="html"><![CDATA[We present iNeRF, a framework that performs mesh-free pose estimation by
"inverting" a Neural RadianceField (NeRF). NeRFs have been shown to be
remarkably effective for the task of view synthesis - synthesizing
photorealistic novel views of real-world scenes or objects. In this work, we
investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,
RGB-only 6DoF pose estimation - given an image, find the translation and
rotation of a camera relative to a 3D object or scene. Our method assumes that
no object mesh models are available during either training or test time.
Starting from an initial pose estimate, we use gradient descent to minimize the
residual between pixels rendered from a NeRF and pixels in an observed image.
In our experiments, we first study 1) how to sample rays during pose refinement
for iNeRF to collect informative gradients and 2) how different batch sizes of
rays affect iNeRF on a synthetic dataset. We then show that for complex
real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating
the camera poses of novel images and using these images as additional training
data for NeRF. Finally, we show iNeRF can perform category-level object pose
estimation, including object instances not seen during training, with RGB
images by inverting a NeRF model inferred from a single view.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1"&gt;Lin Yen-Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1"&gt;Pete Florence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1"&gt;Alberto Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts. (arXiv:2011.11962v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11962</id>
        <link href="http://arxiv.org/abs/2011.11962"/>
        <updated>2021-08-11T01:55:22.307Z</updated>
        <summary type="html"><![CDATA[Ultrasound 3D compounding is important for volumetric reconstruction, but as
of yet there is no consensus on best practices for compounding. Ultrasound
images depend on probe direction and the path sound waves pass through, so when
multiple intersecting B-scans of the same spot from different perspectives
yield different pixel values, there is not a single, ideal representation for
compounding (i.e. combining) the overlapping pixel values. Current popular
methods inevitably suppress or altogether leave out bright or dark regions that
are useful, and potentially introduce new artifacts. In this work, we establish
a new algorithm to compound the overlapping pixels from different view points
in ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve
the maximum boundary contrast without overemphasizing noise and speckle. We
evaluate our algorithm by comparing ours with previous algorithms, and we show
that our approach not only preserves both light and dark details, but also
somewhat suppresses artifacts, rather than amplifying them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1"&gt;Alex Ling Yu Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1"&gt;John Galeotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14582</id>
        <link href="http://arxiv.org/abs/2107.14582"/>
        <updated>2021-08-11T01:55:22.291Z</updated>
        <summary type="html"><![CDATA[The application of differential privacy to the training of deep neural
networks holds the promise of allowing large-scale (decentralized) use of
sensitive data while providing rigorous privacy guarantees to the individual.
The predominant approach to differentially private training of neural networks
is DP-SGD, which relies on norm-based gradient clipping as a method for
bounding sensitivity, followed by the addition of appropriately calibrated
Gaussian noise. In this work we propose NeuralDP, a technique for privatising
activations of some layer within a neural network, which by the post-processing
properties of differential privacy yields a differentially private network. We
experimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia
Dataset (PPD)) that our method offers substantially improved privacy-utility
trade-offs compared to DP-SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04294</id>
        <link href="http://arxiv.org/abs/2108.04294"/>
        <updated>2021-08-11T01:55:22.267Z</updated>
        <summary type="html"><![CDATA[Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1"&gt;Alejandro Pardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1"&gt;Fabian Caba Heilbron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1"&gt;Juan Le&amp;#xf3;n Alc&amp;#xe1;zar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1"&gt;Ali Thabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2108.02940</id>
        <link href="http://arxiv.org/abs/2108.02940"/>
        <updated>2021-08-11T01:55:22.260Z</updated>
        <summary type="html"><![CDATA[In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jindi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kejie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaohua Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04308</id>
        <link href="http://arxiv.org/abs/2108.04308"/>
        <updated>2021-08-11T01:55:22.254Z</updated>
        <summary type="html"><![CDATA[Data is a crucial component of machine learning. The field is reliant on data
to train, validate, and test models. With increased technical capabilities,
machine learning research has boomed in both academic and industry settings,
and one major focus has been on computer vision. Computer vision is a popular
domain of machine learning increasingly pertinent to real-world applications,
from facial recognition in policing to object detection for autonomous
vehicles. Given computer vision's propensity to shape machine learning research
and impact human life, we seek to understand disciplinary practices around
dataset documentation - how data is collected, curated, annotated, and packaged
into datasets for computer vision researchers and practitioners to use for
model tuning and development. Specifically, we examine what dataset
documentation communicates about the underlying values of vision data and the
larger practices and goals of computer vision as a field. To conduct this
study, we collected a corpus of about 500 computer vision datasets, from which
we sampled 114 dataset publications across different vision tasks. Through both
a structured and thematic content analysis, we document a number of values
around accepted data practices, what makes desirable data, and the treatment of
humans in the dataset construction process. We discuss how computer vision
datasets authors value efficiency at the expense of care; universality at the
expense of contextuality; impartiality at the expense of positionality; and
model work at the expense of data work. Many of the silenced values we identify
sit in opposition with social computing practices. We conclude with suggestions
on how to better incorporate silenced values into the dataset creation and
curation process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1"&gt;Morgan Klaus Scheuerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1"&gt;Emily Denton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1"&gt;Alex Hanna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11732</id>
        <link href="http://arxiv.org/abs/2107.11732"/>
        <updated>2021-08-11T01:55:22.248Z</updated>
        <summary type="html"><![CDATA[Analyzing observational data from multiple sources can be useful for
increasing statistical power to detect a treatment effect; however, practical
constraints such as privacy considerations may restrict individual-level
information sharing across data sets. This paper develops federated methods
that only utilize summary-level information from heterogeneous data sets. Our
federated methods provide doubly-robust point estimates of treatment effects as
well as variance estimates. We derive the asymptotic distributions of our
federated estimators, which are shown to be asymptotically equivalent to the
corresponding estimators from the combined, individual-level data. We show that
to achieve these properties, federated methods should be adjusted based on
conditions such as whether models are correctly specified and stable across
heterogeneous data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Ruoxuan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1"&gt;Allison Koenecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1"&gt;Michael Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04238</id>
        <link href="http://arxiv.org/abs/2108.04238"/>
        <updated>2021-08-11T01:55:22.232Z</updated>
        <summary type="html"><![CDATA[Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is "counter-facted", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Haocheng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Caleb Chen Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13689</id>
        <link href="http://arxiv.org/abs/2103.13689"/>
        <updated>2021-08-11T01:55:22.204Z</updated>
        <summary type="html"><![CDATA[Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1"&gt;Xianbo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Shunquan Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Average Treatment Effects via Orthogonal Regularization. (arXiv:2101.08490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08490</id>
        <link href="http://arxiv.org/abs/2101.08490"/>
        <updated>2021-08-11T01:55:22.198Z</updated>
        <summary type="html"><![CDATA[Decision-making often requires accurate estimation of treatment effects from
observational data. This is challenging as outcomes of alternative decisions
are not observed and have to be estimated. Previous methods estimate outcomes
based on unconfoundedness but neglect any constraints that unconfoundedness
imposes on the outcomes. In this paper, we propose a novel regularization
framework for estimating average treatment effects that exploits
unconfoundedness. To this end, we formalize unconfoundedness as an
orthogonality constraint, which ensures that the outcomes are orthogonal to the
treatment assignment. This orthogonality constraint is then included in the
loss function via a regularization. Based on our regularization framework, we
develop deep orthogonal networks for unconfounded treatments (DONUT), which
learn outcomes that are orthogonal to the treatment assignment. Using a variety
of benchmark datasets for estimating average treatment effects, we demonstrate
that DONUT outperforms the state-of-the-art substantially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hatt_T/0/1/0/all/0/1"&gt;Tobias Hatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1"&gt;Stefan Feuerriegel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06578</id>
        <link href="http://arxiv.org/abs/2107.06578"/>
        <updated>2021-08-11T01:55:22.192Z</updated>
        <summary type="html"><![CDATA[To enable process analysis based on an event log without compromising the
privacy of individuals involved in process execution, a log may be anonymized.
Such anonymization strives to transform a log so that it satisfies provable
privacy guarantees, while largely maintaining its utility for process analysis.
Existing techniques perform anonymization using simple, syntactic measures to
identify suitable transformation operations. This way, the semantics of the
activities referenced by the events in a trace are neglected, potentially
leading to transformations in which events of unrelated activities are merged.
To avoid this and incorporate the semantics of activities during anonymization,
we propose to instead incorporate a distance measure based on feature learning.
Specifically, we show how embeddings of events enable the definition of a
distance measure for traces to guide event log anonymization. Our experiments
with real-world data indicate that anonymization using this measure, compared
to a syntactic one, yields logs that are closer to the original log in various
dimensions and, hence, have higher utility for process analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1"&gt;Fabian R&amp;#xf6;sel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1"&gt;Stephan A. Fahrenkrog-Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1"&gt;Han van der Aa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1"&gt;Matthias Weidlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-11T01:55:22.185Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Training Set Debugging for Linear Regression. (arXiv:2006.09009v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09009</id>
        <link href="http://arxiv.org/abs/2006.09009"/>
        <updated>2021-08-11T01:55:22.169Z</updated>
        <summary type="html"><![CDATA[We investigate problems in penalized $M$-estimation, inspired by applications
in machine learning debugging. Data are collected from two pools, one
containing data with possibly contaminated labels, and the other which is known
to contain only cleanly labeled points. We first formulate a general
statistical algorithm for identifying buggy points and provide rigorous
theoretical guarantees under the assumption that the data follow a linear
model. We then present two case studies to illustrate the results of our
general theory and the dependence of our estimator on clean versus buggy
points. We further propose an algorithm for tuning parameter selection of our
Lasso-based algorithm and provide corresponding theoretical guarantees.
Finally, we consider a two-person "game" played between a bug generator and a
debugger, where the debugger can augment the contaminated data set with cleanly
labeled versions of points in the original data pool. We establish a
theoretical result showing a sufficient condition under which the bug generator
can always fool the debugger. Nonetheless, we provide empirical results showing
that such a situation may not occur in practice, making it possible for natural
augmentation strategies combined with our Lasso debugging algorithm to succeed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaomin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1"&gt;Po-Ling Loh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12344</id>
        <link href="http://arxiv.org/abs/2102.12344"/>
        <updated>2021-08-11T01:55:22.163Z</updated>
        <summary type="html"><![CDATA[A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Processes (MDPs). In real-world
robotics, this assumption is unpractical, because of issues such as sensor
sensitivity limitations and sensor noise, and the lack of knowledge about
whether the observation design is complete or not. These scenarios lead to
Partially Observable MDPs (POMDPs). In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1"&gt;Rob Gorbet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1"&gt;Dana Kuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Super-Convergence with a Large Cyclical Learning Rate. (arXiv:2102.10734v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10734</id>
        <link href="http://arxiv.org/abs/2102.10734"/>
        <updated>2021-08-11T01:55:22.158Z</updated>
        <summary type="html"><![CDATA[Conventional wisdom dictates that learning rate should be in the stable
regime so that gradient-based algorithms don't blow up. This letter introduces
a simple scenario where an unstably large learning rate scheme leads to a super
fast convergence, with the convergence rate depending only logarithmically on
the condition number of the problem. Our scheme uses a Cyclical Learning Rate
(CLR) where we periodically take one large unstable step and several small
stable steps to compensate for the instability. These findings also help
explain the empirical observations of [Smith and Topin, 2019] where they show
that CLR with a large maximum learning rate can dramatically accelerate
learning and lead to so-called "super-convergence". We prove that our scheme
excels in the problems where Hessian exhibits a bimodal spectrum and the
eigenvalues can be grouped into two clusters (small and large). The unstably
large step is the key to enabling fast convergence over the small
eigen-spectrum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1"&gt;Samet Oymak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07160</id>
        <link href="http://arxiv.org/abs/2106.07160"/>
        <updated>2021-08-11T01:55:22.152Z</updated>
        <summary type="html"><![CDATA[We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1"&gt;Kim Hammar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1"&gt;Rolf Stadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04235</id>
        <link href="http://arxiv.org/abs/2108.04235"/>
        <updated>2021-08-11T01:55:22.146Z</updated>
        <summary type="html"><![CDATA[Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuting Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1"&gt;Gang Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01139</id>
        <link href="http://arxiv.org/abs/2108.01139"/>
        <updated>2021-08-11T01:55:22.131Z</updated>
        <summary type="html"><![CDATA[EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1"&gt;Andrei-Marius Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1"&gt;Vasile Pais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1"&gt;Dan Tufis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Method Towards CVPR 2021 SimLocMatch Challenge. (arXiv:2108.04466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04466</id>
        <link href="http://arxiv.org/abs/2108.04466"/>
        <updated>2021-08-11T01:55:22.125Z</updated>
        <summary type="html"><![CDATA[This report describes Megvii-3D team's approach to-wards SimLocMatch
Challenge @ CVPR 2021 Image Matching Workshop.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1"&gt;Xiaopeng Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1"&gt;Ran Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1"&gt;Zheng Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haotian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Cellular Automata. (arXiv:2108.04328v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2108.04328</id>
        <link href="http://arxiv.org/abs/2108.04328"/>
        <updated>2021-08-11T01:55:22.120Z</updated>
        <summary type="html"><![CDATA[Motivated by the interaction between cells, the recently introduced concept
of Neural Cellular Automata shows promising results in a variety of tasks. So
far, this concept was mostly used to generate images for a single scenario. As
each scenario requires a new model, this type of generation seems contradictory
to the adaptability of cells in nature. To address this contradiction, we
introduce a concept using different initial environments as input while using a
single Neural Cellular Automata to produce several outputs. Additionally, we
introduce GANCA, a novel algorithm that combines Neural Cellular Automata with
Generative Adversarial Networks, allowing for more generalization through
adversarial training. The experiments show that a single model is capable of
learning several images when presented with different inputs, and that the
adversarially trained model improves drastically on out-of-distribution data
compared to a supervised trained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otte_M/0/1/0/all/0/1"&gt;Maximilian Otte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1"&gt;Quentin Delfosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czech_J/0/1/0/all/0/1"&gt;Johannes Czech&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15380</id>
        <link href="http://arxiv.org/abs/2106.15380"/>
        <updated>2021-08-11T01:55:22.114Z</updated>
        <summary type="html"><![CDATA[In this work we present a novel approach to hierarchical reinforcement
learning for linearly-solvable Markov decision processes. Our approach assumes
that the state space is partitioned, and the subtasks consist in moving between
the partitions. We represent value functions on several levels of abstraction,
and use the compositionality of subtasks to estimate the optimal values of the
states in each partition. The policy is implicitly defined on these optimal
value estimates, rather than being decomposed among the subtasks. As a
consequence, our approach can learn the globally optimal policy, and does not
suffer from the non-stationarity of high-level decisions. If several partitions
have equivalent dynamics, the subtasks of those partitions can be shared. If
the set of boundary states is smaller than the entire state space, our approach
can have significantly smaller sample complexity than that of a flat learner,
and we validate this empirically in several experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1"&gt;Guillermo Infante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1"&gt;Anders Jonsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1"&gt;Vicen&amp;#xe7; G&amp;#xf3;mez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11170</id>
        <link href="http://arxiv.org/abs/2107.11170"/>
        <updated>2021-08-11T01:55:22.109Z</updated>
        <summary type="html"><![CDATA[Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1"&gt;Lusine Abrahamyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1"&gt;Valentin Ziatchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1"&gt;Nikos Deligiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChemiRise: a data-driven retrosynthesis engine. (arXiv:2108.04682v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2108.04682</id>
        <link href="http://arxiv.org/abs/2108.04682"/>
        <updated>2021-08-11T01:55:22.102Z</updated>
        <summary type="html"><![CDATA[We have developed an end-to-end, retrosynthesis system, named ChemiRise, that
can propose complete retrosynthesis routes for organic compounds rapidly and
reliably. The system was trained on a processed patent database of over 3
million organic reactions. Experimental reactions were atom-mapped, clustered,
and extracted into reaction templates. We then trained a graph convolutional
neural network-based one-step reaction proposer using template embeddings and
developed a guiding algorithm on the directed acyclic graph (DAG) of chemical
compounds to find the best candidate to explore. The atom-mapping algorithm and
the one-step reaction proposer were benchmarked against previous studies and
showed better results. The final product was demonstrated by retrosynthesis
routes reviewed and rated by human experts, showing satisfying functionality
and a potential productivity boost in real-life use cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiangyan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1"&gt;Ke Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuquan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingjie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xing_H/0/1/0/all/0/1"&gt;Haoming Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gao_M/0/1/0/all/0/1"&gt;Minghong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1"&gt;Suocheng Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ni_Z/0/1/0/all/0/1"&gt;Zekun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Han_Q/0/1/0/all/0/1"&gt;Qi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junqiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jie Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction. (arXiv:2108.04740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04740</id>
        <link href="http://arxiv.org/abs/2108.04740"/>
        <updated>2021-08-11T01:55:22.084Z</updated>
        <summary type="html"><![CDATA[Predicting the movement trajectories of multiple classes of road users in
real-world scenarios is a challenging task due to the diverse trajectory
patterns. While recent works of pedestrian trajectory prediction successfully
modelled the influence of surrounding neighbours based on the relative
distances, they are ineffective on multi-class trajectory prediction. This is
because they ignore the impact of the implicit correlations between different
types of road users on the trajectory to be predicted - for example, a nearby
pedestrian has a different level of influence from a nearby car. In this paper,
we propose to introduce class information into a graph convolutional neural
network to better predict the trajectory of an individual. We embed the class
labels of the surrounding objects into the label adjacency matrix (LAM), which
is combined with the velocity-based adjacency matrix (VAM) comprised of the
objects' velocity, thereby generating a semantics-guided graph adjacency (SAM).
SAM effectively models semantic information with trainable parameters to
automatically learn the embedded label features that will contribute to the
fixed velocity-based trajectory. Such information of spatial and temporal
dependencies is passed to a graph convolutional and temporal convolutional
network to estimate the predicted trajectory distributions. We further propose
new metrics, known as Average2 Displacement Error (aADE) and Average Final
Displacement Error (aFDE), that assess network accuracy more accurately. We
call our framework Semantics-STGCNN. It consistently shows superior performance
to the state-of-the-arts in existing and the newly proposed metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rainbow_B/0/1/0/all/0/1"&gt;Ben A. Rainbow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1"&gt;Qianhui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1"&gt;Hubert P. H. Shum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification. (arXiv:2108.04527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04527</id>
        <link href="http://arxiv.org/abs/2108.04527"/>
        <updated>2021-08-11T01:55:22.079Z</updated>
        <summary type="html"><![CDATA[Person reidentification (ReID) is a very hot research topic in machine
learning and computer vision, and many person ReID approaches have been
proposed; however, most of these methods assume that the same person has the
same clothes within a short time interval, and thus their visual appearance
must be similar. However, in an actual surveillance environment, a given person
has a great probability of changing clothes after a long time span, and they
also often take different personal belongings with them. When the existing
person ReID methods are applied in this type of case, almost all of them fail.
To date, only a few works have focused on the cloth-changing person ReID task,
but since it is very difficult to extract generalized and robust features for
representing people with different clothes, their performances need to be
improved. Moreover, visual-semantic information is often ignored. To solve
these issues, in this work, a novel multigranular visual-semantic embedding
algorithm (MVSE) is proposed for cloth-changing person ReID, where visual
semantic information and human attributes are embedded into the network, and
the generalized features of human appearance can be well learned to effectively
solve the problem of clothing changes. Specifically, to fully represent a
person with clothing changes, a multigranular feature representation scheme
(MGR) is employed to focus on the unchanged part of the human, and then a cloth
desensitization network (CDN) is designed to improve the feature robustness of
the approach for the person with different clothing, where different high-level
human attributes are fully utilized. Moreover, to further solve the issue of
pose changes and occlusion under different camera perspectives, a partially
semantically aligned network (PSA) is proposed to obtain the visual-semantic
information that is used to align the human attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hongwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1"&gt;Weili Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1"&gt;Weizhi Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Meng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference-based Defect Detection Network. (arXiv:2108.04456v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04456</id>
        <link href="http://arxiv.org/abs/2108.04456"/>
        <updated>2021-08-11T01:55:22.073Z</updated>
        <summary type="html"><![CDATA[The defect detection task can be regarded as a realistic scenario of object
detection in the computer vision field and it is widely used in the industrial
field. Directly applying vanilla object detector to defect detection task can
achieve promising results, while there still exists challenging issues that
have not been solved. The first issue is the texture shift which means a
trained defect detector model will be easily affected by unseen texture, and
the second issue is partial visual confusion which indicates that a partial
defect box is visually similar with a complete box. To tackle these two
problems, we propose a Reference-based Defect Detection Network (RDDN).
Specifically, we introduce template reference and context reference to against
those two problems, respectively. Template reference can reduce the texture
shift from image, feature or region levels, and encourage the detectors to
focus more on the defective area as a result. We can use either well-aligned
template images or the outputs of a pseudo template generator as template
references in this work, and they are jointly trained with detectors by the
supervision of normal samples. To solve the partial visual confusion issue, we
propose to leverage the carried context information of context reference, which
is the concentric bigger box of each region proposal, to perform more accurate
region classification and regression. Experiments on two defect detection
datasets demonstrate the effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04359</id>
        <link href="http://arxiv.org/abs/2108.04359"/>
        <updated>2021-08-11T01:55:22.067Z</updated>
        <summary type="html"><![CDATA[The performance of many medical image analysis tasks are strongly associated
with image data quality. When developing modern deep learning algorithms,
rather than relying on subjective (human-based) image quality assessment (IQA),
task amenability potentially provides an objective measure of task-specific
image quality. To predict task amenability, an IQA agent is trained using
reinforcement learning (RL) with a simultaneously optimised task predictor,
such as a classification or segmentation neural network. In this work, we
develop transfer learning or adaptation strategies to increase the adaptability
of both the IQA agent and the task predictor so that they are less dependent on
high-quality, expert-labelled training data. The proposed transfer learning
strategy re-formulates the original RL problem for task amenability in a
meta-reinforcement learning (meta-RL) framework. The resulting algorithm
facilitates efficient adaptation of the agent to different definitions of image
quality, each with its own Markov decision process environment including
different images, labels and an adaptable task predictor. Our work demonstrates
that the IQA agents pre-trained on non-expert task labels can be adapted to
predict task amenability as defined by expert task labels, using only a small
set of expert labels. Using 6644 clinical ultrasound images from 249 prostate
cancer patients, our results for image classification and segmentation tasks
show that the proposed IQA method can be adapted using data with as few as
respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve
comparable IQA and task performance, which would otherwise require a training
dataset with 100% expert labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1"&gt;Shaheer U. Saeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yunguan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1"&gt;Vasilis Stavrinides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1"&gt;Zachary M. C. Baum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1"&gt;Mirabela Rusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Richard E. Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1"&gt;Geoffrey A. Sonn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1"&gt;J. Alison Noble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1"&gt;Dean C. Barratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. (arXiv:2102.06559v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06559</id>
        <link href="http://arxiv.org/abs/2102.06559"/>
        <updated>2021-08-11T01:55:22.062Z</updated>
        <summary type="html"><![CDATA[We perform scalable approximate inference in a continuous-depth Bayesian
neural network family. In this model class, uncertainty about separate weights
in each layer gives hidden units that follow a stochastic differential
equation. We demonstrate gradient-based stochastic variational inference in
this infinite-parameter setting, producing arbitrarily-flexible approximate
posteriors. We also derive a novel gradient estimator that approaches zero
variance as the approximate posterior over weights approaches the true
posterior. This approach brings continuous-depth Bayesian neural nets to a
competitive comparison against discrete-depth alternatives, while inheriting
the memory-efficient training and tunable precision of Neural ODEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1"&gt;Winnie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ricky T.Q. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuechen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05568</id>
        <link href="http://arxiv.org/abs/2103.05568"/>
        <updated>2021-08-11T01:55:22.046Z</updated>
        <summary type="html"><![CDATA[Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Aman Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1"&gt;Mayank Kothyari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vishwajeet Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01622</id>
        <link href="http://arxiv.org/abs/2105.01622"/>
        <updated>2021-08-11T01:55:22.041Z</updated>
        <summary type="html"><![CDATA[Semi-supervised machine learning models learn from a (small) set of labeled
training examples, and a (large) set of unlabeled training examples.
State-of-the-art models can reach within a few percentage points of
fully-supervised training, while requiring 100x less labeled data.

We study a new class of vulnerabilities: poisoning attacks that modify the
unlabeled dataset. In order to be useful, unlabeled datasets are given strictly
less review than labeled datasets, and adversaries can therefore poison them
easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%
of the dataset size, we can manipulate a model trained on this poisoned dataset
to misclassify arbitrary examples at test time (as any desired label). Our
attacks are highly effective across datasets and semi-supervised learning
methods.

We find that more accurate methods (thus more likely to be used) are
significantly more vulnerable to poisoning attacks, and as such better training
methods are unlikely to prevent this attack. To counter this we explore the
space of defenses, and propose two methods that mitigate our attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANmapper: geographical content filling. (arXiv:2108.04232v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04232</id>
        <link href="http://arxiv.org/abs/2108.04232"/>
        <updated>2021-08-11T01:55:22.034Z</updated>
        <summary type="html"><![CDATA[We present a new method to create spatial data using a generative adversarial
network (GAN). Our contribution uses coarse and widely available geospatial
data to create maps of less available features at the finer scale in the built
environment, bypassing their traditional acquisition techniques (e.g. satellite
imagery or land surveying). In the work, we employ land use data and road
networks as input to generate building footprints, and conduct experiments in 9
cities around the world. The method, which we implement in a tool we release
openly, enables generating approximate maps of the urban form, and it is
generalisable to augment other types of geoinformation, enhancing the
completeness and quality of spatial data infrastructure. It may be especially
useful in locations missing detailed and high-resolution data and those that
are mapped with uncertain or heterogeneous quality, such as much of
OpenStreetMap. The quality of the results is influenced by the urban form and
scale. In most cases, experiments suggest promising performance as the method
tends to truthfully indicate the locations, amount, and shape of buildings. The
work has the potential to support several applications, such as energy,
climate, and urban morphology studies in areas previously lacking required
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Abraham Noah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1"&gt;Filip Biljecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Lane Detection Associated with Quaternion Hardy Filter. (arXiv:2108.04356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04356</id>
        <link href="http://arxiv.org/abs/2108.04356"/>
        <updated>2021-08-11T01:55:22.029Z</updated>
        <summary type="html"><![CDATA[In this article, a robust color-edge feature extraction method based on the
Quaternion Hardy filter is proposed. The Quaternion Hardy filter is an emerging
edge detection theory. It is along with the Poisson and conjugate Poisson
smoothing kernels to handle various types of noise. Combining with the
Quaternion Hardy filter, Jin's color gradient operator and Hough transform, the
color-edge feature detection algorithm is proposed and applied to the lane
marking detection. Experiments are presented to demonstrate the validity of the
proposed algorithm. The results are accurate and robust with respect to the
complex environment lane markings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1"&gt;Wenshan Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Dong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1"&gt;Kit Ian Kou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays. (arXiv:2108.04329v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04329</id>
        <link href="http://arxiv.org/abs/2108.04329"/>
        <updated>2021-08-11T01:55:22.022Z</updated>
        <summary type="html"><![CDATA[Tuberculosis is an infectious disease that is leading to the death of
millions of people across the world. The mortality rate of this disease is high
in patients suffering from immuno-compromised disorders. The early diagnosis of
this disease can save lives and can avoid further complications. But the
diagnosis of TB is a very complex task. The standard diagnostic tests still
rely on traditional procedures developed in the last century. These procedures
are slow and expensive. So this paper presents an automatic approach for the
diagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,
where in the first step the lung regions are segmented from the chest x-rays
using the graph cut method, and then in the second step the transfer learning
of VGG16 combined with Bi-directional LSTM is used for extracting high-level
discriminative features from the segmented lung regions and then classification
is performed using a fully connected layer. The proposed model is evaluated
using data from two publicly available databases namely Montgomery Country set
and Schezien set. The proposed model achieved accuracy and sensitivity of
97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.
This model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on
Schezien and Montgomery county datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1"&gt;G Jignesh Chowdary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1"&gt;Suganya G&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1"&gt;Premalatha M&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1"&gt;Karunamurthy K&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GIPA: General Information Propagation Algorithm for Graph Learning. (arXiv:2105.06035v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06035</id>
        <link href="http://arxiv.org/abs/2105.06035"/>
        <updated>2021-08-11T01:55:21.993Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been popularly used in analyzing
graph-structured data, showing promising results in various applications such
as node classification, link prediction and network recommendation. In this
paper, we present a new graph attention neural network, namely GIPA, for
attributed graph data learning. GIPA consists of three key components:
attention, feature propagation and aggregation. Specifically, the attention
component introduces a new multi-layer perceptron based multi-head to generate
better non-linear feature mapping and representation than conventional
implementations such as dot-product. The propagation component considers not
only node features but also edge features, which differs from existing GNNs
that merely consider node features. The aggregation component uses a residual
connection to generate the final embedding. We evaluate the performance of GIPA
using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The
experimental results reveal that GIPA can beat the state-of-the-art models in
terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of
$0.8700\pm 0.0010$ and outperforms all the previous methods listed in the
ogbn-proteins leaderboard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinkai Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhixiong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guowei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xintan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongchao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Aware Universal Style Transfer. (arXiv:2108.04441v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04441</id>
        <link href="http://arxiv.org/abs/2108.04441"/>
        <updated>2021-08-11T01:55:21.935Z</updated>
        <summary type="html"><![CDATA[Style transfer aims to reproduce content images with the styles from
reference images. Existing universal style transfer methods successfully
deliver arbitrary styles to original images either in an artistic or a
photo-realistic way. However, the range of 'arbitrary style' defined by
existing works is bounded in the particular domain due to their structural
limitation. Specifically, the degrees of content preservation and stylization
are established according to a predefined target domain. As a result, both
photo-realistic and artistic models have difficulty in performing the desired
style transfer for the other domain. To overcome this limitation, we propose a
unified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer
not only the style but also the property of domain (i.e., domainness) from a
given reference image. To this end, we design a novel domainness indicator that
captures the domainness value from the texture and structural features of
reference images. Moreover, we introduce a unified framework with domain-aware
skip connection to adaptively transfer the stroke and palette to the input
contents guided by the domainness indicator. Our extensive experiments validate
that our model produces better qualitative results and outperforms previous
methods in terms of proxy metrics on both artistic and photo-realistic
stylizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1"&gt;Kibeom Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jianlong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1"&gt;Hyeran Byun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation. (arXiv:2108.04476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04476</id>
        <link href="http://arxiv.org/abs/2108.04476"/>
        <updated>2021-08-11T01:55:21.930Z</updated>
        <summary type="html"><![CDATA[We present SP-GAN, a new unsupervised sphere-guided generative model for
direct synthesis of 3D shapes in the form of point clouds. Compared with
existing models, SP-GAN is able to synthesize diverse and high-quality shapes
with fine details and promote controllability for part-aware shape generation
and manipulation, yet trainable without any parts annotations. In SP-GAN, we
incorporate a global prior (uniform points on a sphere) to spatially guide the
generative process and attach a local prior (a random latent code) to each
sphere point to provide local details. The key insight in our design is to
disentangle the complex 3D shape generation task into a global shape modeling
and a local structure adjustment, to ease the learning process and enhance the
shape generation quality. Also, our model forms an implicit dense
correspondence between the sphere points and points in every generated shape,
enabling various forms of structure-aware shape manipulations such as part
editing, part-wise shape interpolation, and multi-shape part composition, etc.,
beyond the existing generative models. Experimental results, which include both
visual and quantitative evaluations, demonstrate that our model is able to
synthesize diverse point clouds with fine details and less noise, as compared
with the state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1"&gt;Ka-Hei Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chi-Wing Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04384</id>
        <link href="http://arxiv.org/abs/2108.04384"/>
        <updated>2021-08-11T01:55:21.919Z</updated>
        <summary type="html"><![CDATA[For the past ten years, CNN has reigned supreme in the world of computer
vision, but recently, Transformer is on the rise. However, the quadratic
computational cost of self-attention has become a severe problem of practice.
There has been much research on architectures without CNN and self-attention in
this context. In particular, MLP-Mixer is a simple idea designed using MLPs and
hit an accuracy comparable to the Vision Transformer. However, the only
inductive bias in this architecture is the embedding of tokens. Thus, there is
still a possibility to build a non-convolutional inductive bias into the
architecture itself, and we built in an inductive bias using two simple ideas.
A way is to divide the token-mixing block vertically and horizontally. Another
way is to make spatial correlations denser among some channels of token-mixing.
With this approach, we were able to improve the accuracy of the MLP-Mixer while
reducing its parameters and computational complexity. Compared to other
MLP-based models, the proposed model, named RaftMLP has a good balance of
computational complexity, the number of parameters, and actual memory usage. In
addition, our work indicates that MLP-based models have the potential to
replace CNNs by adopting inductive bias. The source code in PyTorch version is
available at \url{https://github.com/okojoalg/raft-mlp}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1"&gt;Yuki Tatsunami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1"&gt;Masato Taki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks. (arXiv:2108.04558v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04558</id>
        <link href="http://arxiv.org/abs/2108.04558"/>
        <updated>2021-08-11T01:55:21.913Z</updated>
        <summary type="html"><![CDATA[Human observers engage in selective information uptake when classifying
visual patterns. The same is true of deep neural networks, which currently
constitute the best performing artificial vision systems. Our goal is to
examine the congruence, or lack thereof, in the information-gathering
strategies of the two systems. We have operationalized our investigation as a
character recognition task. We have used eye-tracking to assay the spatial
distribution of information hotspots for humans via fixation maps and an
activation mapping technique for obtaining analogous distributions for deep
networks through visualization maps. Qualitative comparison between
visualization maps and fixation maps reveals an interesting correlate of
congruence. The deep learning model considered similar regions in character,
which humans have fixated in the case of correctly classified characters. On
the other hand, when the focused regions are different for humans and deep
nets, the characters are typically misclassified by the latter. Hence, we
propose to use the visual fixation maps obtained from the eye-tracking
experiment as a supervisory input to align the model's focus on relevant
character regions. We find that such supervision improves the model's
performance significantly and does not require any additional parameters. This
approach has the potential to find applications in diverse domains such as
medical analysis and surveillance in which explainability helps to determine
system fidelity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1"&gt;Chetan Ralekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Shubham Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1"&gt;Tapan Kumar Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1"&gt;Santanu Chaudhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04409</id>
        <link href="http://arxiv.org/abs/2108.04409"/>
        <updated>2021-08-11T01:55:21.899Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small per-
turbations on the input images. Researchers have been devoted to promoting the
research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise at- tack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiaoyang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huilin Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Wancheng Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04812</id>
        <link href="http://arxiv.org/abs/2108.04812"/>
        <updated>2021-08-11T01:55:21.891Z</updated>
        <summary type="html"><![CDATA[We study continual learning for natural language instruction generation, by
observing human users' instruction execution. We focus on a collaborative
scenario, where the system both acts and delegates tasks to human users using
natural language. We compare user execution of generated instructions to the
original system intent as an indication to the system's success communicating
its intent. We show how to use this signal to improve the system's ability to
generate instructions via contextual bandit learning. In interaction with real
users, our system demonstrates dramatic improvements in its ability to generate
language over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1"&gt;Noriyuki Kojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1"&gt;Alane Suhr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures. (arXiv:1808.00560v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.00560</id>
        <link href="http://arxiv.org/abs/1808.00560"/>
        <updated>2021-08-11T01:55:21.885Z</updated>
        <summary type="html"><![CDATA[Spectral mixture (SM) kernels comprise a powerful class of kernels for
Gaussian processes (GPs) capable of discovering structurally complex patterns
and modeling negative covariances. Being a linear superposition of
quasi-periodical kernel components, the state-of-the-art SM kernel does not
consider component compression and dependency structures between components. In
this paper, we investigate the benefits of component compression and modeling
of both time and phase delay structures between basis components in the SM
kernel. By verifying the presence of dependencies between function components
using Gaussian conditionals and posterior covariance, we first propose a new SM
kernel variant with a time and phase delay dependency structure (SMD) and then
provide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is
constructed in two steps: first, time delay and phase delay are incorporated
into each basis component; next, cross-convolution between a basis component
and the reversed complex conjugate of another basis component is performed,
which yields a complex-valued and positive definite kernel incorporating
dependency structures between basis components. The model compression and
dependency sparsity of the SMD kernel can be obtained by using automatic
pruning in SA. We perform a thorough comparative experimental analysis of the
SMD on both synthetic and real-life datasets. The results corroborate the
efficacy of the dependency structure and SA in the SMD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yijue Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1"&gt;Feng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1"&gt;Elena Marchiori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1"&gt;Sergios Theodoridis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Backdoor. (arXiv:2006.11890v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11890</id>
        <link href="http://arxiv.org/abs/2006.11890"/>
        <updated>2021-08-11T01:55:21.878Z</updated>
        <summary type="html"><![CDATA[One intriguing property of deep neural networks (DNNs) is their inherent
vulnerability to backdoor attacks -- a trojan model responds to
trigger-embedded inputs in a highly predictable manner while functioning
normally otherwise. Despite the plethora of prior work on DNNs for continuous
data (e.g., images), the vulnerability of graph neural networks (GNNs) for
discrete-structured data (e.g., graphs) is largely unexplored, which is highly
concerning given their increasing use in security-sensitive domains. To bridge
this gap, we present GTA, the first backdoor attack on GNNs. Compared with
prior work, GTA departs in significant ways: graph-oriented -- it defines
triggers as specific subgraphs, including both topological structures and
descriptive features, entailing a large design spectrum for the adversary;
input-tailored -- it dynamically adapts triggers to individual graphs, thereby
optimizing both attack effectiveness and evasiveness; downstream model-agnostic
-- it can be readily launched without knowledge regarding downstream models or
fine-tuning strategies; and attack-extensible -- it can be instantiated for
both transductive (e.g., node classification) and inductive (e.g., graph
classification) tasks, constituting severe threats for a range of
security-critical applications. Through extensive evaluation using benchmark
datasets and state-of-the-art models, we demonstrate the effectiveness of GTA.
We further provide analytical justification for its effectiveness and discuss
potential countermeasures, pointing to several promising research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1"&gt;Zhaohan Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Ren Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Ting Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Baseline for Shapley Values in MLPs: from Missingness to Neutrality. (arXiv:2006.04896v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04896</id>
        <link href="http://arxiv.org/abs/2006.04896"/>
        <updated>2021-08-11T01:55:21.873Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have gained momentum based on their accuracy, but their
interpretability is often criticised. As a result, they are labelled as black
boxes. In response, several methods have been proposed in the literature to
explain their predictions. Among the explanatory methods, Shapley values is a
feature attribution method favoured for its robust theoretical foundation.
However, the analysis of feature attributions using Shapley values requires
choosing a baseline that represents the concept of missingness. An arbitrary
choice of baseline could negatively impact the explanatory power of the method
and possibly lead to incorrect interpretations. In this paper, we present a
method for choosing a baseline according to a neutrality value: as a parameter
selected by decision-makers, the point at which their choices are determined by
the model predictions being either above or below it. Hence, the proposed
baseline is set based on a parameter that depends on the actual use of the
model. This procedure stands in contrast to how other baselines are set, i.e.
without accounting for how the model is used. We empirically validate our
choice of baseline in the context of binary classification tasks, using two
datasets: a synthetic dataset and a dataset derived from the financial domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izzo_C/0/1/0/all/0/1"&gt;Cosimo Izzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1"&gt;Aldo Lipani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhrati_R/0/1/0/all/0/1"&gt;Ramin Okhrati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1"&gt;Francesca Medda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04424</id>
        <link href="http://arxiv.org/abs/2108.04424"/>
        <updated>2021-08-11T01:55:21.867Z</updated>
        <summary type="html"><![CDATA[Blind face inpainting refers to the task of reconstructing visual contents
without explicitly indicating the corrupted regions in a face image.
Inherently, this task faces two challenges: (1) how to detect various mask
patterns of different shapes and contents; (2) how to restore visually
plausible and pleasing contents in the masked regions. In this paper, we
propose a novel two-stage blind face inpainting method named Frequency-guided
Transformer and Top-Down Refinement Network (FT-TDR) to tackle these
challenges. Specifically, we first use a transformer-based network to detect
the corrupted regions to be inpainted as masks by modeling the relation among
different patches. We also exploit the frequency modality as complementary
information for improved detection results and capture the local contextual
incoherence to enhance boundary consistency. Then a top-down refinement network
is proposed to hierarchically restore features at different levels and generate
contents that are semantically consistent with the unmasked face regions.
Extensive experiments demonstrate that our method outperforms current
state-of-the-art blind and non-blind face inpainting methods qualitatively and
quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaoxiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yu-Gang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04658</id>
        <link href="http://arxiv.org/abs/2108.04658"/>
        <updated>2021-08-11T01:55:21.851Z</updated>
        <summary type="html"><![CDATA[Development of deep learning systems for biomedical segmentation often
requires access to expert-driven, manually annotated datasets. If more than a
single expert is involved in the annotation of the same images, then the
inter-expert agreement is not necessarily perfect, and no single expert
annotation can precisely capture the so-called ground truth of the regions of
interest on all images. Also, it is not trivial to generate a reference
estimate using annotations from multiple experts. Here we present a deep neural
network, defined as U-Net-and-a-half, which can simultaneously learn from
annotations performed by multiple experts on the same set of images.
U-Net-and-a-half contains a convolutional encoder to generate features from the
input images, multiple decoders that allow simultaneous learning from image
masks obtained from annotations that were independently generated by multiple
experts, and a shared low-dimensional feature space. To demonstrate the
applicability of our framework, we used two distinct datasets from digital
pathology and radiology, respectively. Specifically, we trained two separate
models using pathologist-driven annotations of glomeruli on whole slide images
of human kidney biopsies (10 patients), and radiologist-driven annotations of
lumen cross-sections of human arteriovenous fistulae obtained from
intravascular ultrasound images (10 patients), respectively. The models based
on U-Net-and-a-half exceeded the performance of the traditional U-Net models
trained on single expert annotations alone, thus expanding the scope of
multitask learning in the context of biomedical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1"&gt;Jesper Kers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1"&gt;Clarissa A. Cassol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1"&gt;Joris J. Roelofs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1"&gt;Najia Idrees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1"&gt;Alik Farber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1"&gt;Samir Haroon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1"&gt;Kevin P. Daly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Suvranu Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1"&gt;Vipul C. Chitalia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1"&gt;Vijaya B. Kolachalama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Factors Aware Dual-Attentional Knowledge Tracing. (arXiv:2108.04741v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.04741</id>
        <link href="http://arxiv.org/abs/2108.04741"/>
        <updated>2021-08-11T01:55:21.846Z</updated>
        <summary type="html"><![CDATA[With the increasing demands of personalized learning, knowledge tracing has
become important which traces students' knowledge states based on their
historical practices. Factor analysis methods mainly use two kinds of factors
which are separately related to students and questions to model students'
knowledge states. These methods use the total number of attempts of students to
model students' learning progress and hardly highlight the impact of the most
recent relevant practices. Besides, current factor analysis methods ignore rich
information contained in questions. In this paper, we propose Multi-Factors
Aware Dual-Attentional model (MF-DAKT) which enriches question representations
and utilizes multiple factors to model students' learning progress based on a
dual-attentional mechanism. More specifically, we propose a novel
student-related factor which records the most recent attempts on relevant
concepts of students to highlight the impact of recent exercises. To enrich
questions representations, we use a pre-training method to incorporate two
kinds of question information including questions' relation and difficulty
level. We also add a regularization term about questions' difficulty level to
restrict pre-trained question representations to fine-tuning during the process
of predicting students' performance. Moreover, we apply a dual-attentional
mechanism to differentiate contributions of factors and factor interactions to
final prediction in different practice records. At last, we conduct experiments
on several real-world datasets and results show that MF-DAKT can outperform
existing knowledge tracing methods. We also conduct several studies to validate
the effects of each component of MF-DAKT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Moyu Zhang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinning Zhu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chunhong Zhang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yang Ji&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feng Pan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Changchuan Yin&lt;/a&gt; (1) ((1) Beijing University of Posts and Telecommunications)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04392</id>
        <link href="http://arxiv.org/abs/2108.04392"/>
        <updated>2021-08-11T01:55:21.840Z</updated>
        <summary type="html"><![CDATA[Differentiable Neural Architecture Search is one of the most popular Neural
Architecture Search (NAS) methods for its search efficiency and simplicity,
accomplished by jointly optimizing the model weight and architecture parameters
in a weight-sharing supernet via gradient-based algorithms. At the end of the
search phase, the operations with the largest architecture parameters will be
selected to form the final architecture, with the implicit assumption that the
values of architecture parameters reflect the operation strength. While much
has been discussed about the supernet's optimization, the architecture
selection process has received little attention. We provide empirical and
theoretical analysis to show that the magnitude of architecture parameters does
not necessarily indicate how much the operation contributes to the supernet's
performance. We propose an alternative perturbation-based architecture
selection that directly measures each operation's influence on the supernet. We
re-evaluate several differentiable NAS methods with the proposed architecture
selection and find that it is able to extract significantly improved
architectures from the underlying supernets consistently. Furthermore, we find
that several failure modes of DARTS can be greatly alleviated with the proposed
selection method, indicating that much of the poor generalization observed in
DARTS can be attributed to the failure of magnitude-based architecture
selection rather than entirely the optimization of its supernet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Minhao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04358</id>
        <link href="http://arxiv.org/abs/2108.04358"/>
        <updated>2021-08-11T01:55:21.827Z</updated>
        <summary type="html"><![CDATA[Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1"&gt;Ipsita Sutradhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mahziba Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Mehedi Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1"&gt;Malabika Sarker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of Deep Neural Networks. (arXiv:2009.05673v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05673</id>
        <link href="http://arxiv.org/abs/2009.05673"/>
        <updated>2021-08-11T01:55:21.821Z</updated>
        <summary type="html"><![CDATA[Deep learning is a group of exciting new technologies for neural networks.
Through a combination of advanced training techniques and neural network
architectural components, it is now possible to create neural networks that can
handle tabular data, images, text, and audio as both input and output. Deep
learning allows a neural network to learn hierarchies of information in a way
that is like the function of the human brain. This course will introduce the
student to classic neural network structures, Convolution Neural Networks
(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),
General Adversarial Networks (GAN), and reinforcement learning. Application of
these architectures to computer vision, time series, security, natural language
processing (NLP), and data generation will be covered. High-Performance
Computing (HPC) aspects will demonstrate how deep learning can be leveraged
both on graphical processing units (GPUs), as well as grids. Focus is primarily
upon the application of deep learning to problems, with some introduction to
mathematical foundations. Readers will use the Python programming language to
implement deep learning using Google TensorFlow and Keras. It is not necessary
to know Python prior to this book; however, familiarity with at least one
programming language is assumed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1"&gt;Jeff Heaton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning. (arXiv:2108.04755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04755</id>
        <link href="http://arxiv.org/abs/2108.04755"/>
        <updated>2021-08-11T01:55:21.815Z</updated>
        <summary type="html"><![CDATA[Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)
is a classical federated learning algorithm in which clients run multiple local
SGD steps before communicating their update to an orchestrating server. We
propose a new federated learning algorithm, FedPAGE, able to further reduce the
communication complexity by utilizing the recent optimal PAGE method (Li et
al., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer
communication rounds than previous local methods for both federated convex and
nonconvex optimization. Concretely, 1) in the convex setting, the number of
communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD (Karimireddy et
al.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients
(usually is very large in federated learning), $S$ is the sampled subset of
clients in each communication round, and $\epsilon$ is the target error; 2) in
the nonconvex setting, the number of communication rounds of FedPAGE is
$O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving the best-known result
$O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by
a factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\leq \sqrt{N}$. Note
that in both settings, the communication cost for each round is the same for
both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art
results in terms of communication complexity for both federated convex and
nonconvex optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhize Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data. (arXiv:2108.04742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04742</id>
        <link href="http://arxiv.org/abs/2108.04742"/>
        <updated>2021-08-11T01:55:21.809Z</updated>
        <summary type="html"><![CDATA[Errors in measurements are key to weighting the value of data, but are often
neglected in Machine Learning (ML). We show how Convolutional Neural Networks
(CNNs) are able to learn about the context and patterns of signal and noise,
leading to improvements in the performance of classification methods. We
construct a model whereby two classes of objects follow an underlying Gaussian
distribution, and where the features (the input data) have varying, but known,
levels of noise. This model mimics the nature of scientific data sets, where
the noises arise as realizations of some random processes whose underlying
distributions are known. The classification of these objects can then be
performed using standard statistical techniques (e.g., least-squares
minimization or Markov-Chain Monte Carlo), as well as ML techniques. This
allows us to take advantage of a maximum likelihood approach to object
classification, and to measure the amount by which the ML methods are
incorporating the information in the input data uncertainties. We show that,
when each data point is subject to different levels of noise (i.e., noises with
different distribution functions), that information can be learned by the CNNs,
raising the ML performance to at least the same level of the least-squares
method -- and sometimes even surpassing it. Furthermore, we show that, with
varying noise levels, the confidence of the ML classifiers serves as a proxy
for the underlying cumulative distribution function, but only if the
information about specific input data uncertainties is provided to the CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1"&gt;Nat&amp;#xe1;lia V. N. Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abramo_L/0/1/0/all/0/1"&gt;L. Raul Abramo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirata_N/0/1/0/all/0/1"&gt;Nina S. Hirata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization. (arXiv:2010.05244v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05244</id>
        <link href="http://arxiv.org/abs/2010.05244"/>
        <updated>2021-08-11T01:55:21.803Z</updated>
        <summary type="html"><![CDATA[Due to lack of data, overfitting ubiquitously exists in real-world
applications of deep neural networks (DNNs). We propose advanced dropout, a
model-free methodology, to mitigate overfitting and improve the performance of
DNNs. The advanced dropout technique applies a model-free and easily
implemented distribution with parametric prior, and adaptively adjusts dropout
rate. Specifically, the distribution parameters are optimized by stochastic
gradient variational Bayes in order to carry out an end-to-end training. We
evaluate the effectiveness of the advanced dropout against nine dropout
techniques on seven computer vision datasets (five small-scale datasets and two
large-scale datasets) with various base models. The advanced dropout
outperforms all the referred techniques on all the datasets.We further compare
the effectiveness ratios and find that advanced dropout achieves the highest
one on most cases. Next, we conduct a set of analysis of dropout rate
characteristics, including convergence of the adaptive dropout rate, the
learned distributions of dropout masks, and a comparison with dropout rate
generation without an explicit distribution. In addition, the ability of
overfitting prevention is evaluated and confirmed. Finally, we extend the
application of the advanced dropout to uncertainty inference, network pruning,
text classification, and regression. The proposed advanced dropout is also
superior to the corresponding referred methods. Codes are available at
https://github.com/PRIS-CV/AdvancedDropout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_a/0/1/0/all/0/1"&gt;and Jianjun Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zheng-Hua Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jun Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06429</id>
        <link href="http://arxiv.org/abs/2009.06429"/>
        <updated>2021-08-11T01:55:21.797Z</updated>
        <summary type="html"><![CDATA[Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1"&gt;Anna Lukina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1"&gt;Thomas A. Henzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems. (arXiv:2108.04585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04585</id>
        <link href="http://arxiv.org/abs/2108.04585"/>
        <updated>2021-08-11T01:55:21.791Z</updated>
        <summary type="html"><![CDATA[Owing to their superior modeling capabilities, gated Recurrent Neural
Networks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term
Memory networks (LSTMs), have become popular tools for learning dynamical
systems. This paper aims to discuss how these networks can be adopted for the
synthesis of Internal Model Control (IMC) architectures. To this end, a first
gated RNN is used to learn a model of the unknown input-output stable plant.
Then, another gated RNN approximating the model inverse is trained. The
proposed scheme is able to cope with the saturation of the control variables,
and it can be deployed on low-power embedded controllers since it does not
require any online computation. The approach is then tested on the Quadruple
Tank benchmark system, resulting in satisfactory closed-loop performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stroke Correspondence by Labeling Closed Areas. (arXiv:2108.04393v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2108.04393</id>
        <link href="http://arxiv.org/abs/2108.04393"/>
        <updated>2021-08-11T01:55:21.785Z</updated>
        <summary type="html"><![CDATA[Constructing stroke correspondences between keyframes is one of the most
important processes in the production pipeline of hand-drawn inbetweening
frames. This process requires time-consuming manual work imposing a tremendous
burden on the animators. We propose a method to estimate stroke correspondences
between raster character images (keyframes) without vectorization processes.
First, the proposed system separates the closed areas in each keyframe and
estimates the correspondences between closed areas by using the characteristics
of shape, depth, and closed area connection. Second, the proposed system
estimates stroke correspondences from the estimated closed area
correspondences. We demonstrate the effectiveness of our method by performing a
user study and comparing the proposed system with conventional approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miyauchi_R/0/1/0/all/0/1"&gt;Ryoma Miyauchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1"&gt;Tsukasa Fukusato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haoran Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1"&gt;Kazunori Miyata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell. (arXiv:2108.04656v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.04656</id>
        <link href="http://arxiv.org/abs/2108.04656"/>
        <updated>2021-08-11T01:55:21.780Z</updated>
        <summary type="html"><![CDATA[A code smell is a surface indicator of an inherent problem in the system,
most often due to deviation from standard coding practices on the developers
part during the development phase. Studies observe that code smells made the
code more susceptible to call for modifications and corrections than code that
did not contain code smells. Restructuring the code at the early stage of
development saves the exponentially increasing amount of effort it would
require to address the issues stemming from the presence of these code smells.
Instead of using traditional features to detect code smells, we use user
comments to manually construct features to predict code smells. We use three
Extreme learning machine kernels over 629 packages to identify eight code
smells by leveraging feature engineering aspects and using sampling techniques.
Our findings indicate that the radial basis functional kernel performs best out
of the three kernel methods with a mean accuracy of 98.52.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Himanshu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulanikar_A/0/1/0/all/0/1"&gt;Abhiram Anand Gulanikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1"&gt;Lov Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1"&gt;Lalita Bhanu Murthy Neti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic. (arXiv:2012.03728v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03728</id>
        <link href="http://arxiv.org/abs/2012.03728"/>
        <updated>2021-08-11T01:55:21.757Z</updated>
        <summary type="html"><![CDATA[As a reaction to the high infectiousness and lethality of the COVID-19 virus,
countries around the world have adopted drastic policy measures to contain the
pandemic. However, it remains unclear which effect these measures, so-called
non-pharmaceutical interventions (NPIs), have on the spread of the virus. In
this article, we use machine learning and apply drift detection methods in a
novel way to predict the time lag of policy interventions with respect to the
development of daily case numbers of COVID-19 across 9 European countries and
28 US states. Our analysis shows that there are, on average, more than two
weeks between NPI enactment and a drift in the case numbers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1"&gt;Lucas Baier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1"&gt;Niklas K&amp;#xfc;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1"&gt;Jakob Sch&amp;#xf6;ffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1"&gt;Gerhard Satzger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep tree-ensembles for multi-output prediction. (arXiv:2011.02829v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02829</id>
        <link href="http://arxiv.org/abs/2011.02829"/>
        <updated>2021-08-11T01:55:21.749Z</updated>
        <summary type="html"><![CDATA[Recently, deep neural networks have expanded the state-of-art in various
scientific fields and provided solutions to long standing problems across
multiple application domains. Nevertheless, they also suffer from weaknesses
since their optimal performance depends on massive amounts of training data and
the tuning of an extended number of parameters. As a countermeasure, some
deep-forest methods have been recently proposed, as efficient and low-scale
solutions. Despite that, these approaches simply employ label classification
probabilities as induced features and primarily focus on traditional
classification and regression tasks, leaving multi-output prediction
under-explored. Moreover, recent work has demonstrated that tree-embeddings are
highly representative, especially in structured output prediction. In this
direction, we propose a novel deep tree-ensemble (DTE) model, where every layer
enriches the original feature set with a representation learning component
based on tree-embeddings. In this paper, we specifically focus on two
structured output prediction tasks, namely multi-label classification and
multi-target regression. We conducted experiments using multiple benchmark
datasets and the obtained results confirm that our method provides superior
results to state-of-the-art methods in both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakano_F/0/1/0/all/0/1"&gt;Felipe Kenji Nakano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1"&gt;Konstantinos Pliakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1"&gt;Celine Vens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Reinforcement Learning for Data Processing and Analytics. (arXiv:2108.04526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04526</id>
        <link href="http://arxiv.org/abs/2108.04526"/>
        <updated>2021-08-11T01:55:21.725Z</updated>
        <summary type="html"><![CDATA[Data processing and analytics are fundamental and pervasive. Algorithms play
a vital role in data processing and analytics where many algorithm designs have
incorporated heuristics and general rules from human knowledge and experience
to improve their effectiveness. Recently, reinforcement learning, deep
reinforcement learning (DRL) in particular, is increasingly explored and
exploited in many areas because it can learn better strategies in complicated
environments it is interacting with than statically designed algorithms.
Motivated by this trend, we provide a comprehensive review of recent works
focusing on utilizing deep reinforcement learning to improve data processing
and analytics. First, we present an introduction to key concepts, theories, and
methods in deep reinforcement learning. Next, we discuss deep reinforcement
learning deployment on database systems, facilitating data processing and
analytics in various aspects, including data organization, scheduling, tuning,
and indexing. Then, we survey the application of deep reinforcement learning in
data processing and analytics, ranging from data preparation, natural language
interface to healthcare, fintech, etc. Finally, we discuss important open
challenges and future research directions of using deep reinforcement learning
in data processing and analytics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Qingpeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yiyuan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongle Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meihui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01184</id>
        <link href="http://arxiv.org/abs/2010.01184"/>
        <updated>2021-08-11T01:55:21.718Z</updated>
        <summary type="html"><![CDATA[In supervised learning, training and test datasets are often sampled from
distinct distributions. Domain adaptation techniques are thus required.
Covariate shift adaptation yields good generalization performance when domains
differ only by the marginal distribution of features. Covariate shift
adaptation is usually implemented using importance weighting, which may fail,
according to common wisdom, due to small effective sample sizes (ESS). Previous
research argues this scenario is more common in high-dimensional settings.
However, how effective sample size, dimensionality, and model
performance/generalization are formally related in supervised learning,
considering the context of covariate shift adaptation, is still somewhat
obscure in the literature. Thus, a main challenge is presenting a unified
theory connecting those points. Hence, in this paper, we focus on building a
unified view connecting the ESS, data dimensionality, and generalization in the
context of covariate shift adaptation. Moreover, we also demonstrate how
dimensionality reduction or feature selection can increase the ESS, and argue
that our results support dimensionality reduction before covariate shift
adaptation as a good practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Renato Vicente&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Laplace for Bayesian neural networks. (arXiv:2011.10443v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10443</id>
        <link href="http://arxiv.org/abs/2011.10443"/>
        <updated>2021-08-11T01:55:21.705Z</updated>
        <summary type="html"><![CDATA[We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1"&gt;Ali Unlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07588</id>
        <link href="http://arxiv.org/abs/2008.07588"/>
        <updated>2021-08-11T01:55:21.693Z</updated>
        <summary type="html"><![CDATA[Deep learning motivated by convolutional neural networks has been highly
successful in a range of medical imaging problems like image classification,
image segmentation, image synthesis etc. However for validation and
interpretability, not only do we need the predictions made by the model but
also how confident it is while making those predictions. This is important in
safety critical applications for the people to accept it. In this work, we used
an encoder decoder architecture based on variational inference techniques for
segmenting brain tumour images. We evaluate our work on the publicly available
BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over
Union (IOU) as the evaluation metrics. Our model is able to segment brain
tumours while taking into account both aleatoric uncertainty and epistemic
uncertainty in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention. (arXiv:2003.04575v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04575</id>
        <link href="http://arxiv.org/abs/2003.04575"/>
        <updated>2021-08-11T01:55:21.678Z</updated>
        <summary type="html"><![CDATA[Channel attention mechanisms have been commonly applied in many visual tasks
for effective performance improvement. It is able to reinforce the informative
channels as well as to suppress the useless channels. Recently, different
channel attention modules have been proposed and implemented in various ways.
Generally speaking, they are mainly based on convolution and pooling
operations. In this paper, we propose Gaussian process embedded channel
attention (GPCA) module and further interpret the channel attention schemes in
a probabilistic way. The GPCA module intends to model the correlations among
the channels, which are assumed to be captured by beta distributed variables.
As the beta distribution cannot be integrated into the end-to-end training of
convolutional neural networks (CNNs) with a mathematically tractable solution,
we utilize an approximation of the beta distribution to solve this problem. To
specify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian
distributed variables are transferred into the interval [0,1]. The Gaussian
process is then utilized to model the correlations among different channels. In
this case, a mathematically tractable solution is derived. The GPCA module can
be efficiently implemented and integrated into the end-to-end training of the
CNNs. Experimental results demonstrate the promising performance of the
proposed GPCA module. Codes are available at https://github.com/PRIS-CV/GPCA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dongliang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jun Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models. (arXiv:2108.04659v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.04659</id>
        <link href="http://arxiv.org/abs/2108.04659"/>
        <updated>2021-08-11T01:55:21.663Z</updated>
        <summary type="html"><![CDATA[Code Smell, similar to a bad smell, is a surface indication of something
tainted but in terms of software writing practices. This metric is an
indication of a deeper problem lies within the code and is associated with an
issue which is prominent to experienced software developers with acceptable
coding practices. Recent studies have often observed that codes having code
smells are often prone to a higher probability of change in the software
development cycle. In this paper, we developed code smell prediction models
with the help of features extracted from source code to predict eight types of
code smell. Our work also presents the application of data sampling techniques
to handle class imbalance problem and feature selection techniques to find
relevant feature sets. Previous studies had made use of techniques such as
Naive - Bayes and Random forest but had not explored deep learning methods to
predict code smell. A total of 576 distinct Deep Learning models were trained
using the features and datasets mentioned above. The study concluded that the
deep learning models which used data from Synthetic Minority Oversampling
Technique gave better results in terms of accuracy, AUC with the accuracy of
some models improving from 88.47 to 96.84.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Himanshu Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1"&gt;Tanmay G. Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1"&gt;Lov Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1"&gt;Lalita Bhanu Murthy Neti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1"&gt;Aneesh Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04655</id>
        <link href="http://arxiv.org/abs/2108.04655"/>
        <updated>2021-08-11T01:55:21.655Z</updated>
        <summary type="html"><![CDATA[Collaborative Metric Learning (CML) recently emerged as a powerful paradigm
for recommendation based on implicit feedback collaborative filtering. However,
standard CML methods learn fixed user and item representations, which fails to
capture the complex interests of users. Existing extensions of CML also either
ignore the heterogeneity of user-item relations, i.e. that a user can
simultaneously like very different items, or the latent item-item relations,
i.e. that a user's preference for an item depends, not only on its intrinsic
characteristics, but also on items they previously interacted with. In this
paper, we present a hierarchical CML model that jointly captures latent
user-item and item-item relations from implicit data. Our approach is inspired
by translation mechanisms from knowledge graph embedding and leverages
memory-based attention networks. We empirically show the relevance of this
joint relational modeling, by outperforming existing CML models on
recommendation tasks on several real-world datasets. Our experiments also
emphasize the limits of current CML relational models on very sparse datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Viet-Anh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1"&gt;Guillaume Salha-Galvan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1"&gt;Romain Hennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1"&gt;Manuel Moussallam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04800</id>
        <link href="http://arxiv.org/abs/2108.04800"/>
        <updated>2021-08-11T01:55:21.639Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) is transforming medicine and showing promise in
improving clinical diagnosis. In breast cancer screening, several recent
studies show that AI has the potential to improve radiologists' accuracy,
subsequently helping in early cancer diagnosis and reducing unnecessary workup.
As the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them in order to reproduce the results
and to compare different approaches. To enable reproducibility of research in
this application area and to enable comparison between different methods, we
release a meta-repository containing deep learning models for classification of
screening mammograms. This meta-repository creates a framework that enables the
evaluation of machine learning models on any private or public screening
mammography data set. At its inception, our meta-repository contains five
state-of-the-art models with open-source implementations and cross-platform
compatibility. We compare their performance on five international data sets:
two private New York University breast cancer screening data sets as well as
three public (DDSM, INbreast and Chinese Mammography Database) data sets. Our
framework has a flexible design that can be generalized to other medical image
analysis tasks. The meta-repository is available at
https://www.github.com/nyukat/mammography_metarepository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1"&gt;Benjamin Stadnick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1"&gt;Jan Witowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1"&gt;Vishwaesh Rajiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1"&gt;Jakub Ch&amp;#x142;&amp;#x119;dowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1"&gt;Farah E. Shamout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof J. Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston. (arXiv:2108.04787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04787</id>
        <link href="http://arxiv.org/abs/2108.04787"/>
        <updated>2021-08-11T01:55:21.624Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic has resulted in significant social and economic impacts
throughout the world. In addition to the health consequences, the impacts on
traffic behaviors have also been sudden and dramatic. We have analyzed how the
road traffic safety of New York City, Los Angeles, and Boston in the U.S. have
been impacted by the pandemic and corresponding local government orders and
restrictions. To be specific, we have studied the accident hotspots'
distributions before and after the outbreak of the pandemic and found that
traffic accidents have shifted in both location and time compared to previous
years. In addition, we have studied the road network characteristics in those
hotspot regions with the hope to understand the underlying cause of the hotspot
shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karadla_L/0/1/0/all/0/1"&gt;Lahari Karadla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weizi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems. (arXiv:2011.04170v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04170</id>
        <link href="http://arxiv.org/abs/2011.04170"/>
        <updated>2021-08-11T01:55:21.618Z</updated>
        <summary type="html"><![CDATA[Class imbalance is a substantial challenge in classifying many real-world
cases. Synthetic over-sampling methods have been effective to improve the
performance of classifiers for imbalance problems. However, most synthetic
over-sampling methods generate non-diverse synthetic instances within the
convex hull formed by the existing minority instances as they only concentrate
on the minority class and ignore the vast information provided by the majority
class. They also often do not perform well for extremely imbalanced data as the
fewer the minority instances, the less information to generate synthetic
instances. Moreover, existing methods that generate synthetic instances using
the majority class distributional information cannot perform effectively when
the majority class has a multi-modal distribution. We propose a new method to
generate diverse and adaptable synthetic instances using Synthetic
Over-sampling with Minority and Majority classes (SOMM). SOMM generates
synthetic instances diversely within the minority data space. It updates the
generated instances adaptively to the neighbourhood including both classes.
Thus, SOMM performs well for both binary and multiclass imbalance problems. We
examine the performance of SOMM for binary and multiclass problems using
benchmark data sets for different imbalance levels. The empirical results show
the superiority of SOMM compared to other existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorshidi_H/0/1/0/all/0/1"&gt;Hadi A. Khorshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aickelin_U/0/1/0/all/0/1"&gt;Uwe Aickelin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP. (arXiv:2108.04578v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04578</id>
        <link href="http://arxiv.org/abs/2108.04578"/>
        <updated>2021-08-11T01:55:21.612Z</updated>
        <summary type="html"><![CDATA[The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the
total distance of the routes under the capacity constraints of the vehicles.
But more often, the objective involves multiple criteria including not only the
total distance of the tour but also other factors such as travel costs, travel
time, and fuel consumption.Moreover, in reality, there are numerous implicit
preferences ingrained in the minds of the route planners and the drivers.
Drivers, for instance, have familiarity with certain neighborhoods and
knowledge of the state of roads, and often consider the best places for rest
and lunch breaks. This knowledge is difficult to formulate and balance when
operational routing decisions have to be made. This motivates us to learn the
implicit preferences from past solutions and to incorporate these learned
preferences in the optimization process. These preferences are in the form of
arc probabilities, i.e., the more preferred a route is, the higher is the joint
probability. The novelty of this work is the use of a neural network model to
estimate the arc probabilities, which allows for additional features and
automatic parameter estimation. This first requires identifying suitable
features, neural architectures and loss functions, taking into account that
there is typically few data available. We investigate the difference with a
prior weighted Markov counting approach, and study the applicability of neural
networks in this setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1"&gt;Jayanta Mandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1"&gt;Rocsildes Canoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Bucarey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1"&gt;Tias Guns&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.14442</id>
        <link href="http://arxiv.org/abs/1910.14442"/>
        <updated>2021-08-11T01:55:21.597Z</updated>
        <summary type="html"><![CDATA[We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;William B. Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1"&gt;Priya Kasimbeg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1"&gt;Alexander Toshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14408</id>
        <link href="http://arxiv.org/abs/2005.14408"/>
        <updated>2021-08-11T01:55:21.591Z</updated>
        <summary type="html"><![CDATA[Named Entity Recognition (NER) and Entity Linking (EL) play an essential role
in voice assistant interaction, but are challenging due to the special
difficulties associated with spoken user queries. In this paper, we propose a
novel architecture that jointly solves the NER and EL tasks by combining them
in a joint reranking module. We show that our proposed framework improves NER
accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features
used also lead to better accuracies in other natural language understanding
tasks, such as domain classification and semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1"&gt;Deepak Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1"&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Sida Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1"&gt;Justine Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1"&gt;Stephen Pulman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1"&gt;Atish Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ray Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yinying Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1"&gt;Vivek Kaul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1"&gt;Mubarak Seyed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1"&gt;Gang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1"&gt;Nan Dun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yidan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1"&gt;Andy O&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1"&gt;Pooja Chitkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1"&gt;Alkesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1"&gt;Kushal Tayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Roger Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1"&gt;Peter Grasch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1"&gt;Jason D. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label-informed Graph Structure Learning for Node Classification. (arXiv:2108.04595v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04595</id>
        <link href="http://arxiv.org/abs/2108.04595"/>
        <updated>2021-08-11T01:55:21.585Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have achieved great success among various
domains. Nevertheless, most GNN methods are sensitive to the quality of graph
structures. To tackle this problem, some studies exploit different graph
structure learning strategies to refine the original graph structure. However,
these methods only consider feature information while ignoring available label
information. In this paper, we propose a novel label-informed graph structure
learning framework which incorporates label information explicitly through a
class transition matrix. We conduct extensive experiments on seven node
classification benchmark datasets and the results show that our method
outperforms or matches the state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fenyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-based computation of metastable and coherent sets. (arXiv:1908.04741v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.04741</id>
        <link href="http://arxiv.org/abs/1908.04741"/>
        <updated>2021-08-11T01:55:21.580Z</updated>
        <summary type="html"><![CDATA[Recent years have seen rapid advances in the data-driven analysis of
dynamical systems based on Koopman operator theory and related approaches. On
the other hand, low-rank tensor product approximations -- in particular the
tensor train (TT) format -- have become a valuable tool for the solution of
large-scale problems in a number of fields. In this work, we combine
Koopman-based models and the TT format, enabling their application to
high-dimensional problems in conjunction with a rich set of basis functions or
features. We derive efficient algorithms to obtain a reduced matrix
representation of the system's evolution operator starting from an appropriate
low-rank representation of the data. These algorithms can be applied to both
stationary and non-stationary systems. We establish the infinite-data limit of
these matrix representations, and demonstrate our methods' capabilities using
several benchmark data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1"&gt;Feliks N&amp;#xfc;ske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gelss_P/0/1/0/all/0/1"&gt;Patrick Gel&amp;#xdf;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1"&gt;Stefan Klus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Clementi_C/0/1/0/all/0/1"&gt;Cecilia Clementi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Levels From Nothing. (arXiv:2002.05259v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05259</id>
        <link href="http://arxiv.org/abs/2002.05259"/>
        <updated>2021-08-11T01:55:21.565Z</updated>
        <summary type="html"><![CDATA[Machine learning for procedural content generation has recently become an
active area of research. Levels vary in both form and function and are mostly
unrelated to each other across games. This has made it difficult to assemble
suitably large datasets to bring machine learning to level design in the same
way as it's been used for image generation. Here we propose Generative Playing
Networks which design levels for itself to play. The algorithm is built in two
parts; an agent that learns to play game levels, and a generator that learns
the distribution of playable levels. As the agent learns and improves its
ability, the space of playable levels, as defined by the agent, grows. The
generator targets the agent's playability estimates to then update its
understanding of what constitutes a playable level. We call this process of
learning the distribution of data found through self-discovery with an
environment, self-supervised inductive learning. Unlike previous approaches to
procedural content generation, Generative Playing Networks are end-to-end
differentiable and do not require human-designed examples or domain knowledge.
We demonstrate the capability of this framework by training an agent and level
generator for a 2D dungeon crawler game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bontrager_P/0/1/0/all/0/1"&gt;Philip Bontrager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1"&gt;Julian Togelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Maximize Influence. (arXiv:2108.04623v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04623</id>
        <link href="http://arxiv.org/abs/2108.04623"/>
        <updated>2021-08-11T01:55:21.556Z</updated>
        <summary type="html"><![CDATA[As the field of machine learning for combinatorial optimization advances,
traditional problems are resurfaced and readdressed through this new
perspective. The overwhelming majority of the literature focuses on small graph
problems, while several real-world problems are devoted to large graphs. Here,
we focus on two such problems that are related: influence estimation, a
\#P-hard counting problem, and influence maximization, an NP-hard problem. We
develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an
upper bound of influence estimation and train it on small simulated graphs.
Experiments show that GLIE can provide accurate predictions faster than the
alternatives for graphs 10 times larger than the train set. More importantly,
it can be used on arbitrary large graphs for influence maximization, as the
predictions can rank effectively seed sets even when the accuracy deteriorates.
To showcase this, we propose a version of a standard Influence Maximization
(IM) algorithm where we substitute traditional influence estimation with the
predictions of GLIE.We also transfer GLIE into a reinforcement learning model
that learns how to choose seeds to maximize influence sequentially using GLIE's
hidden representations and predictions. The final results show that the
proposed methods surpasses a previous GNN-RL approach and perform on par with a
state-of-the-art IM algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1"&gt;George Panagopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1"&gt;Nikolaos Tziortziotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1"&gt;Fragkiskos D. Malliaros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1"&gt;Michalis Vazirgiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning for Transition State Calculation. (arXiv:2108.04698v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.04698</id>
        <link href="http://arxiv.org/abs/2108.04698"/>
        <updated>2021-08-11T01:55:21.550Z</updated>
        <summary type="html"><![CDATA[The transition state (TS) calculation is a grand challenge for computational
intensive energy function. The traditional methods need to evaluate the
gradients of the energy function at a very large number of locations. To reduce
the number of expensive computations of the true gradients, we propose an
active learning framework consisting of a statistical surrogate model, Gaussian
process regression (GPR) for the energy function, and a single-walker dynamics
method, gentle accent dynamics (GAD), for the saddle-type transition states. TS
is detected by the GAD applied to the GPR surrogate for the gradient vector and
the Hessian matrix. Our key ingredient for efficiency improvements is an active
learning method which sequentially designs the most informative locations and
takes evaluations of the original model at these locations to train GPR. We
formulate this active learning task as the optimal experimental design problem
and propose a very efficient sample-based sub-optimal criterion to construct
the optimal locations. We show that the new method significantly decreases the
required number of energy or force evaluations of the original model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuting Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongqiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiang Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning. (arXiv:2108.04809v1 [cond-mat.mes-hall])]]></title>
        <id>http://arxiv.org/abs/2108.04809</id>
        <link href="http://arxiv.org/abs/2108.04809"/>
        <updated>2021-08-11T01:55:21.539Z</updated>
        <summary type="html"><![CDATA[From ultra-sensitive detectors of fundamental forces to quantum networks and
sensors, mechanical resonators are enabling next-generation technologies to
operate in room temperature environments. Currently, silicon nitride
nanoresonators stand as a leading microchip platform in these advances by
allowing for mechanical resonators whose motion is remarkably isolated from
ambient thermal noise. However, to date, human intuition has remained the
driving force behind design processes. Here, inspired by nature and guided by
machine learning, a spiderweb nanomechanical resonator is developed that
exhibits vibration modes which are isolated from ambient thermal environments
via a novel "torsional soft-clamping" mechanism discovered by the data-driven
optimization algorithm. This bio-inspired resonator is then fabricated;
experimentally confirming a new paradigm in mechanics with quality factors
above 1 billion in room temperature environments. In contrast to other
state-of-the-art resonators, this milestone is achieved with a compact design
which does not require sub-micron lithographic features or complex phononic
bandgaps, making it significantly easier and cheaper to manufacture at large
scales. Here we demonstrate the ability of machine learning to work in tandem
with human intuition to augment creative possibilities and uncover new
strategies in computing and nanotechnology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Shin_D/0/1/0/all/0/1"&gt;Dongil Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Cupertino_A/0/1/0/all/0/1"&gt;Andrea Cupertino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Jong_M/0/1/0/all/0/1"&gt;Matthijs H. J. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Steeneken_P/0/1/0/all/0/1"&gt;Peter G. Steeneken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Bessa_M/0/1/0/all/0/1"&gt;Miguel A. Bessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Norte_R/0/1/0/all/0/1"&gt;Richard A. Norte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04452</id>
        <link href="http://arxiv.org/abs/2108.04452"/>
        <updated>2021-08-11T01:55:21.533Z</updated>
        <summary type="html"><![CDATA["High Quality Related Search Query Suggestions" task aims at recommending
search queries which are real, accurate, diverse, relevant and engaging.
Obtaining large amounts of query-quality human annotations is expensive. Prior
work on supervised query suggestion models suffered from selection and exposure
bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),
leading to low quality suggestions. Reinforcement Learning techniques employed
to reformulate a query using terms from search results, have limited
scalability to large-scale industry applications. To recommend high quality
related search queries, we train a Deep Reinforcement Learning model to predict
the query a user would enter next. The reward signal is composed of long-term
session-based user feedback, syntactic relatedness and estimated naturalness of
generated query. Over the baseline supervised model, our proposed approach
achieves a significant relative improvement in terms of recommendation
diversity (3%), down-stream user-engagement (4.2%) and per-sentence word
repetitions (82%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1"&gt;Praveen Kumar Bodigutla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical investigation into audio pipeline approaches for classifying bird species. (arXiv:2108.04449v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.04449</id>
        <link href="http://arxiv.org/abs/2108.04449"/>
        <updated>2021-08-11T01:55:21.517Z</updated>
        <summary type="html"><![CDATA[This paper is an investigation into aspects of an audio classification
pipeline that will be appropriate for the monitoring of bird species on edges
devices. These aspects include transfer learning, data augmentation and model
optimization. The hope is that the resulting models will be good candidates to
deploy on edge devices to monitor bird populations. Two classification
approaches will be taken into consideration, one which explores the
effectiveness of a traditional Deep Neural Network(DNN) and another that makes
use of Convolutional layers.This study aims to contribute empirical evidence of
the merits and demerits of each approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Behr_D/0/1/0/all/0/1"&gt;David Behr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maina_C/0/1/0/all/0/1"&gt;Ciira wa Maina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Complex Neural Network Acceleration on FPGA. (arXiv:2108.04811v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04811</id>
        <link href="http://arxiv.org/abs/2108.04811"/>
        <updated>2021-08-11T01:55:21.512Z</updated>
        <summary type="html"><![CDATA[Being able to learn from complex data with phase information is imperative
for many signal processing applications. Today' s real-valued deep neural
networks (DNNs) have shown efficiency in latent information analysis but fall
short when applied to the complex domain. Deep complex networks (DCN), in
contrast, can learn from complex data, but have high computational costs;
therefore, they cannot satisfy the instant decision-making requirements of many
deployable systems dealing with short observations or short signal bursts.
Recent, Binarized Complex Neural Network (BCNN), which integrates DCNs with
binarized neural networks (BNN), shows great potential in classifying complex
data in real-time. In this paper, we propose a structural pruning based
accelerator of BCNN, which is able to provide more than 5000 frames/s inference
throughput on edge devices. The high performance comes from both the algorithm
and hardware sides. On the algorithm side, we conduct structural pruning to the
original BCNN models and obtain 20 $\times$ pruning rates with negligible
accuracy loss; on the hardware side, we propose a novel 2D convolution
operation accelerator for the binary complex neural network. Experimental
results show that the proposed design works with over 90% utilization and is
able to achieve the inference throughput of 5882 frames/s and 4938 frames/s for
complex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hongwu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shanglin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitze_S/0/1/0/all/0/1"&gt;Scott Weitze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1"&gt;Sahidul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1"&gt;Tong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Minghu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1"&gt;Mimi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions. (arXiv:2108.04620v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.04620</id>
        <link href="http://arxiv.org/abs/2108.04620"/>
        <updated>2021-08-11T01:55:21.484Z</updated>
        <summary type="html"><![CDATA[Gradient descent (GD) type optimization methods are the standard instrument
to train artificial neural networks (ANNs) with rectified linear unit (ReLU)
activation. Despite the great success of GD type optimization methods in
numerical simulations for the training of ANNs with ReLU activation, it remains
- even in the simplest situation of the plain vanilla GD optimization method
with random initializations and ANNs with one hidden layer - an open problem to
prove (or disprove) the conjecture that the risk of the GD optimization method
converges in the training of such ANNs to zero as the width of the ANNs, the
number of independent random initializations, and the number of GD steps
increase to infinity. In this article we prove this conjecture in the situation
where the probability distribution of the input data is equivalent to the
continuous uniform distribution on a compact interval, where the probability
distributions for the random initializations of the ANN parameters are standard
normal distributions, and where the target function under consideration is
continuous and piecewise affine linear. Roughly speaking, the key ingredients
in our mathematical convergence analysis are (i) to prove that suitable sets of
global minima of the risk functions are \emph{twice continuously differentiable
submanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of
the risk functions on these sets of global minima satisfy an appropriate
\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in
[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic
gradient descent method for non-convex objective functions. J. Mach. Learn.
Res. 21(136): 1--48, 2020] to establish convergence of the GD optimization
method with random initializations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Riekert_A/0/1/0/all/0/1"&gt;Adrian Riekert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Learning and Testing Decision Tree. (arXiv:2108.04587v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2108.04587</id>
        <link href="http://arxiv.org/abs/2108.04587"/>
        <updated>2021-08-11T01:55:21.479Z</updated>
        <summary type="html"><![CDATA[In this paper, we study learning and testing decision tree of size and depth
that are significantly smaller than the number of attributes $n$.

Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms
with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that
distinguish between functions that are decision trees of size $s$ from
functions that are $\epsilon$-far from any decision tree of size
$\phi(s,1/\epsilon)$, for some function $\phi > s$. The best known result is
the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that
gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give
a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$.

Moreover, we study the testability of depth-$d$ decision tree and give a {\it
distribution free} tester that distinguishes between depth-$d$ decision tree
and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In
particular, for decision trees of size $s$, the above result holds in the
distribution-free model when the tree depth is $O(\log(s/\epsilon))$.

We also give other new results in learning and testing of size-$s$ decision
trees and depth-$d$ decision trees that follow from results in the literature
and some results we prove in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1"&gt;Nader H. Bshouty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1"&gt;Catherine A. Haddad-Zaknoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04814</id>
        <link href="http://arxiv.org/abs/2108.04814"/>
        <updated>2021-08-11T01:55:21.452Z</updated>
        <summary type="html"><![CDATA[While self-supervised monocular depth estimation in driving scenarios has
achieved comparable performance to supervised approaches, violations of the
static world assumption can still lead to erroneous depth predictions of
traffic participants, posing a potential safety issue. In this paper, we
present R4Dyn, a novel set of techniques to use cost-efficient radar data on
top of a self-supervised depth estimation framework. In particular, we show how
radar can be used during training as weak supervision signal, as well as an
extra input to enhance the estimation robustness at inference time. Since
automotive radars are readily available, this allows to collect training data
from a variety of existing vehicles. Moreover, by filtering and expanding the
signal to make it compatible with learning-based approaches, we address radar
inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome
a major limitation of self-supervised depth estimation, i.e. the prediction of
traffic participants. We substantially improve the estimation on dynamic
objects, such as cars by 37% on the challenging nuScenes dataset, hence
demonstrating that radar is a valuable additional sensor for monocular depth
estimation in autonomous vehicles. Additionally, we plan on making the code
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1"&gt;Stefano Gasperini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1"&gt;Patrick Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1"&gt;Vinzenz Dallabetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1"&gt;Benjamin Busam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Clustering Reconstruction in Semi-Adversarial Models. (arXiv:2108.04729v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2108.04729</id>
        <link href="http://arxiv.org/abs/2108.04729"/>
        <updated>2021-08-11T01:55:21.447Z</updated>
        <summary type="html"><![CDATA[Correlation Clustering is an important clustering problem with many
applications. We study the reconstruction version of this problem in which one
is seeking to reconstruct a latent clustering that has been corrupted by random
noise and adversarial modifications.

Concerning the latter, we study a standard "post-adversarial" model, in which
adversarial modifications come after the noise, and also introduce and analyze
a "pre-adversarial" model in which adversarial modifications come before the
noise. Given an input coming from such a semi-adversarial generative model, the
goal is to reconstruct almost perfectly and with high probability the latent
clustering.

We focus on the case where the hidden clusters have equal size and show the
following. In the pre-adversarial setting, spectral algorithms are optimal, in
the sense that they reconstruct all the way to the information-theoretic
threshold beyond which no reconstruction is possible. In contrast, in the
post-adversarial setting their ability to restore the hidden clusters stops
before the threshold, but the gap is optimally filled by SDP-based algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chierichetti_F/0/1/0/all/0/1"&gt;Flavio Chierichetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1"&gt;Alessandro Panconesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_G/0/1/0/all/0/1"&gt;Giuseppe Re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trevisan_L/0/1/0/all/0/1"&gt;Luca Trevisan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04584</id>
        <link href="http://arxiv.org/abs/2108.04584"/>
        <updated>2021-08-11T01:55:21.440Z</updated>
        <summary type="html"><![CDATA[Scene understanding is crucial for autonomous systems which intend to operate
in the real world. Single task vision networks extract information only based
on some aspects of the scene. In multi-task learning (MTL), on the other hand,
these single tasks are jointly learned, thereby providing an opportunity for
tasks to share information and obtain a more comprehensive understanding. To
this end, we develop UniNet, a unified scene understanding network that
accurately and efficiently infers vital vision tasks including object
detection, semantic segmentation, instance segmentation, monocular depth
estimation, and monocular instance depth prediction. As these tasks look at
different semantic and geometric information, they can either complement or
conflict with each other. Therefore, understanding inter-task relationships can
provide useful cues to enable complementary information sharing. We evaluate
the task relationships in UniNet through the lens of adversarial attacks based
on the notion that they can exploit learned biases and task interactions in the
neural network. Extensive experiments on the Cityscapes dataset, using
untargeted and targeted attacks reveal that semantic tasks strongly interact
amongst themselves, and the same holds for geometric tasks. Additionally, we
show that the relationship between semantic and geometric tasks is asymmetric
and their interaction becomes weaker as we move towards higher-level
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1"&gt;NareshKumar Gurulingan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Algorithms for Precision Medicine. (arXiv:2108.04782v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.04782</id>
        <link href="http://arxiv.org/abs/2108.04782"/>
        <updated>2021-08-11T01:55:21.435Z</updated>
        <summary type="html"><![CDATA[The Oxford English Dictionary defines precision medicine as "medical care
designed to optimize efficiency or therapeutic benefit for particular groups of
patients, especially by using genetic or molecular profiling." It is not an
entirely new idea: physicians from ancient times have recognized that medical
treatment needs to consider individual variations in patient characteristics.
However, the modern precision medicine movement has been enabled by a
confluence of events: scientific advances in fields such as genetics and
pharmacology, technological advances in mobile devices and wearable sensors,
and methodological advances in computing and data sciences.

This chapter is about bandit algorithms: an area of data science of special
relevance to precision medicine. With their roots in the seminal work of
Bellman, Robbins, Lai and others, bandit algorithms have come to occupy a
central place in modern data science ( Lattimore and Szepesvari, 2020). Bandit
algorithms can be used in any situation where treatment decisions need to be
made to optimize some health outcome. Since precision medicine focuses on the
use of patient characteristics to guide treatment, contextual bandit algorithms
are especially useful since they are designed to take such information into
account. The role of bandit algorithms in areas of precision medicine such as
mobile health and digital phenotyping has been reviewed before (Tewari and
Murphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit
algorithms have continued to find uses in mobile health and several new topics
have emerged in the research on bandit algorithms. This chapter is written for
quantitative researchers in fields such as statistics, machine learning, and
operations research who might be interested in knowing more about the
algorithmic and mathematical details of bandit algorithms that have been used
in mobile health.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziping Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ambuj Tewari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowdsourced Databases and Sui Generis Rights. (arXiv:2108.04727v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2108.04727</id>
        <link href="http://arxiv.org/abs/2108.04727"/>
        <updated>2021-08-11T01:55:21.429Z</updated>
        <summary type="html"><![CDATA[In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alo Sim&amp;#xf5;es de Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreu_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alo Faria Abreu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage. (arXiv:2108.04725v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04725</id>
        <link href="http://arxiv.org/abs/2108.04725"/>
        <updated>2021-08-11T01:55:21.423Z</updated>
        <summary type="html"><![CDATA[Collaborative training of neural networks leverages distributed data by
exchanging gradient information between different clients. Although training
data entirely resides with the clients, recent work shows that training data
can be reconstructed from such exchanged gradient information. To enhance
privacy, gradient perturbation techniques have been proposed. However, they
come at the cost of reduced model performance, increased convergence time, or
increased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing
mODulE that can be used as generic extension for arbitrary model architectures.
We propose a simple yet effective realization of PRECODE using variational
modeling. The stochastic sampling induced by variational modeling effectively
prevents privacy leakage from gradients and in turn preserves privacy of data
owners. We evaluate PRECODE using state of the art gradient inversion attacks
on two different model architectures trained on three datasets. In contrast to
commonly used defense mechanisms, we find that our proposed modification
consistently reduces the attack success rate to 0% while having almost no
negative impact on model training and final performance. As a result, PRECODE
reveals a promising path towards privacy enhancing model extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scheliga_D/0/1/0/all/0/1"&gt;Daniel Scheliga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1"&gt;Patrick M&amp;#xe4;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeland_M/0/1/0/all/0/1"&gt;Marco Seeland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandits with Partially Observable Confounded Data. (arXiv:2006.06731v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06731</id>
        <link href="http://arxiv.org/abs/2006.06731"/>
        <updated>2021-08-11T01:55:21.407Z</updated>
        <summary type="html"><![CDATA[We study linear contextual bandits with access to a large, confounded,
offline dataset that was sampled from some fixed policy. We show that this
problem is closely related to a variant of the bandit problem with side
information. We construct a linear bandit algorithm that takes advantage of the
projected information, and prove regret bounds. Our results demonstrate the
ability to take advantage of confounded offline data. Particularly, we prove
regret bounds that improve current bounds by a factor related to the visible
dimensionality of the contexts in the data. Our results indicate that
confounded offline data can significantly improve online learning algorithms.
Finally, we demonstrate various characteristics of our approach through
synthetic simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1"&gt;Guy Tennenholtz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benefits of Implicit Regularization from SGD in Least Squares Problems. (arXiv:2108.04552v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04552</id>
        <link href="http://arxiv.org/abs/2108.04552"/>
        <updated>2021-08-11T01:55:21.400Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice, which has been hypothesized to play an important role in
the generalization of modern machine learning approaches. In this work, we seek
to understand these issues in the simpler setting of linear regression
(including both underparameterized and overparameterized regimes), where our
goal is to make sharp instance-based comparisons of the implicit regularization
afforded by (unregularized) average SGD with the explicit regularization of
ridge regression. For a broad class of least squares problem instances (that
are natural in high-dimensional settings), we show: (1) for every problem
instance and for every ridge parameter, (unregularized) SGD, when provided with
logarithmically more samples than that provided to the ridge algorithm,
generalizes no worse than the ridge solution (provided SGD uses a tuned
constant stepsize); (2) conversely, there exist instances (in this wide problem
class) where optimally-tuned ridge regression requires quadratically more
samples than SGD in order to have the same generalization performance. Taken
together, our results show that, up to the logarithmic factors, the
generalization performance of SGD is always no worse than that of ridge
regression in a wide range of overparameterized problems, and, in fact, could
be much better for some problem instances. More generally, our results show how
algorithmic regularization has important consequences even in simpler
(overparameterized) convex settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Difan Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingfeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1"&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1"&gt;Dean P. Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning by Reinforcement Learning. (arXiv:2108.04763v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.04763</id>
        <link href="http://arxiv.org/abs/2108.04763"/>
        <updated>2021-08-11T01:55:21.394Z</updated>
        <summary type="html"><![CDATA[Imitation Learning algorithms learn a policy from demonstrations of expert
behavior. Somewhat counterintuitively, we show that, for deterministic experts,
imitation learning can be done by reduction to reinforcement learning, which is
commonly considered more difficult. We conduct experiments which confirm that
our reduction works well in practice for a continuous control task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1"&gt;Kamil Ciosek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABC-FL: Anomalous and Benign client Classification in Federated Learning. (arXiv:2108.04551v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04551</id>
        <link href="http://arxiv.org/abs/2108.04551"/>
        <updated>2021-08-11T01:55:21.389Z</updated>
        <summary type="html"><![CDATA[Federated Learning is a distributed machine learning framework designed for
data privacy preservation i.e., local data remain private throughout the entire
training and testing procedure. Federated Learning is gaining popularity
because it allows one to use machine learning techniques while preserving
privacy. However, it inherits the vulnerabilities and susceptibilities raised
in deep learning techniques. For instance, Federated Learning is particularly
vulnerable to data poisoning attacks that may deteriorate its performance and
integrity due to its distributed nature and inaccessibility to the raw data. In
addition, it is extremely difficult to correctly identify malicious clients due
to the non-Independently and/or Identically Distributed (non-IID) data. The
real-world data can be complex and diverse, making them hardly distinguishable
from the malicious data without direct access to the raw data. Prior research
has focused on detecting malicious clients while treating only the clients
having IID data as benign. In this study, we propose a method that detects and
classifies anomalous clients from benign clients when benign ones have non-IID
data. Our proposed method leverages feature dimension reduction, dynamic
clustering, and cosine similarity-based clipping. The experimental results
validates that our proposed method not only classifies the malicious clients
but also alleviates their negative influences from the entire procedure. Our
findings may be used in future studies to effectively eliminate anomalous
clients when building a model with diverse data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hyejun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Joonyong Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1"&gt;Tai Myung Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Composite Optimization with Compression. (arXiv:2108.04448v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04448</id>
        <link href="http://arxiv.org/abs/2108.04448"/>
        <updated>2021-08-11T01:55:21.383Z</updated>
        <summary type="html"><![CDATA[Decentralized optimization and communication compression have exhibited their
great potential in accelerating distributed machine learning by mitigating the
communication bottleneck in practice. While existing decentralized algorithms
with communication compression mostly focus on the problems with only smooth
components, we study the decentralized stochastic composite optimization
problem with a potentially non-smooth component. A \underline{Prox}imal
gradient \underline{L}in\underline{EA}r convergent \underline{D}ecentralized
algorithm with compression, Prox-LEAD, is proposed with rigorous theoretical
analyses in the general stochastic setting and the finite-sum setting. Our
theorems indicate that Prox-LEAD works with arbitrary compression precision,
and it tremendously reduces the communication cost almost for free. The
superiorities of the proposed algorithms are demonstrated through the
comparison with state-of-the-art algorithms in terms of convergence
complexities and numerical experiments. Our algorithmic framework also
generally enlightens the compressed communication on other primal-dual
algorithms by reducing the impact of inexact iterations, which might be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kun Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04543</id>
        <link href="http://arxiv.org/abs/2108.04543"/>
        <updated>2021-08-11T01:55:21.367Z</updated>
        <summary type="html"><![CDATA[In this article, we perform a review of the state-of-the-art of hybrid
machine learning in medical imaging. We start with a short summary of the
general developments of the past in machine learning and how general and
specialized approaches have been in competition in the past decades. A
particular focus will be the theoretical and experimental evidence pro and
contra hybrid modelling. Next, we inspect several new developments regarding
hybrid machine learning with a particular focus on so-called known operator
learning and how hybrid approaches gain more and more momentum across
essentially all applications in medical imaging and medical image analysis. As
we will point out by numerous examples, hybrid models are taking over in image
reconstruction and analysis. Even domains such as physical simulation and
scanner and acquisition design are being addressed using machine learning grey
box modelling approaches. Towards the end of the article, we will investigate a
few future directions and point out relevant areas in which hybrid modelling,
meta learning, and other domains will likely be able to drive the
state-of-the-art ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1"&gt;Andreas Maier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1"&gt;Harald K&amp;#xf6;stler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1"&gt;Marco Heisig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1"&gt;Patrick Krauss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Seung Hee Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.14408</id>
        <link href="http://arxiv.org/abs/2005.14408"/>
        <updated>2021-08-11T01:55:21.360Z</updated>
        <summary type="html"><![CDATA[Named Entity Recognition (NER) and Entity Linking (EL) play an essential role
in voice assistant interaction, but are challenging due to the special
difficulties associated with spoken user queries. In this paper, we propose a
novel architecture that jointly solves the NER and EL tasks by combining them
in a joint reranking module. We show that our proposed framework improves NER
accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features
used also lead to better accuracies in other natural language understanding
tasks, such as domain classification and semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1"&gt;Deepak Muralidharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1"&gt;Joel Ruben Antony Moniz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Sida Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1"&gt;Justine Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1"&gt;Stephen Pulman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1"&gt;Atish Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ray Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yinying Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1"&gt;Vivek Kaul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1"&gt;Mubarak Seyed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1"&gt;Gang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1"&gt;Nan Dun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yidan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1"&gt;Andy O&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1"&gt;Pooja Chitkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1"&gt;Alkesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1"&gt;Kushal Tayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Roger Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1"&gt;Peter Grasch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1"&gt;Jason D. Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration. (arXiv:2108.04343v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.04343</id>
        <link href="http://arxiv.org/abs/2108.04343"/>
        <updated>2021-08-11T01:55:21.352Z</updated>
        <summary type="html"><![CDATA[Big Data are rapidly produced from various heterogeneous data sources. They
are of different types (text, image, video or audio) and have different levels
of reliability and completeness. One of the most interesting architectures that
deal with the large amount of emerging data at high velocity is called the
lambda architecture. In fact, it combines two different processing layers
namely batch and speed layers, each providing specific views of data while
ensuring robustness, fast and scalable data processing. However, most papers
dealing with the lambda architecture are focusing one single type of data
generally produced by a single data source. Besides, the layers of the
architecture are implemented independently, or, at best, are combined to
perform basic processing without assessing either the data reliability or
completeness. Therefore, inspired by the lambda architecture, we propose in
this paper a generic multimodal architecture that combines both batch and
streaming processing in order to build a complete, global and accurate insight
in near-real-time based on the knowledge extracted from multiple heterogeneous
Big Data sources. Our architecture uses batch processing to analyze the data
structures and contents, build the learning models and calculate the
reliability index of the involved sources, while the streaming processing uses
the built-in models of the batch layer to immediately process incoming data and
rapidly provide results. We validate our architecture in the context of urban
traffic management systems in order to detect congestions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1"&gt;Siham Yousfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1"&gt;Maryem Rhanoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiadmi_D/0/1/0/all/0/1"&gt;Dalila Chiadmi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey. (arXiv:2108.04462v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04462</id>
        <link href="http://arxiv.org/abs/2108.04462"/>
        <updated>2021-08-11T01:55:21.345Z</updated>
        <summary type="html"><![CDATA[Recent technology development brings the booming of numerous new
Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand
delivery, express systems and warehousing. In DDS, a service loop is an
elemental structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
humans or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two individual stages: 1)
Dispatching, which is to form service loops from demand/supply distributions,
and 2)Routing, which is to decide specific serving orders within the
constructed loops. Generating high-quality strategies in both stages is
important to develop DDS but faces several challenging. Meanwhile, deep
reinforcement learning (DRL) has been developed rapidly in recent years. It is
a powerful tool to solve these problems since DRL can learn a parametric model
without relying on too many problem-based assumptions and optimize long-term
effect by learning sequential decisions. In this survey, we first define DDS,
then highlight common applications and important decision/control problems
within. For each problem, we comprehensively introduce the existing DRL
solutions, and further summarize them in
\textit{https://github.com/tsinghua-fib-lab/DDS\_Survey}. We also introduce
open simulation environments for development and evaluation of DDS
applications. Finally, we analyze remaining challenges and discuss further
research opportunities in DRL solutions for DDS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Zefang Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1"&gt;Tao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Depeng/0/1/0/all/0/1"&gt;Depeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04392</id>
        <link href="http://arxiv.org/abs/2108.04392"/>
        <updated>2021-08-11T01:55:21.338Z</updated>
        <summary type="html"><![CDATA[Differentiable Neural Architecture Search is one of the most popular Neural
Architecture Search (NAS) methods for its search efficiency and simplicity,
accomplished by jointly optimizing the model weight and architecture parameters
in a weight-sharing supernet via gradient-based algorithms. At the end of the
search phase, the operations with the largest architecture parameters will be
selected to form the final architecture, with the implicit assumption that the
values of architecture parameters reflect the operation strength. While much
has been discussed about the supernet's optimization, the architecture
selection process has received little attention. We provide empirical and
theoretical analysis to show that the magnitude of architecture parameters does
not necessarily indicate how much the operation contributes to the supernet's
performance. We propose an alternative perturbation-based architecture
selection that directly measures each operation's influence on the supernet. We
re-evaluate several differentiable NAS methods with the proposed architecture
selection and find that it is able to extract significantly improved
architectures from the underlying supernets consistently. Furthermore, we find
that several failure modes of DARTS can be greatly alleviated with the proposed
selection method, indicating that much of the poor generalization observed in
DARTS can be attributed to the failure of magnitude-based architecture
selection rather than entirely the optimization of its supernet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruochen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Minhao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth. (arXiv:2108.04482v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04482</id>
        <link href="http://arxiv.org/abs/2108.04482"/>
        <updated>2021-08-11T01:55:21.322Z</updated>
        <summary type="html"><![CDATA[Several decades ago the Proximal Point Algorithm (PPA) started to gain much
attraction for both abstract operator theory and the numerical optimization
communities. Even in modern applications, researchers still use proximal
minimization theory to design scalable algorithms that overcome nonsmoothness
in high dimensional models. Several remarkable references as
\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local
relations between the convergence rate of PPA and the regularity of the
objective function. However, without taking into account the concrete
computational effort paid for computing each PPA iteration, any iteration
complexity remains abstract and purely informative. In this manuscript we aim
to evaluate the computational complexity of practical PPA in terms of
(proximal) gradient/subgradient iterations, which might allow a fair
positioning of the famous PPA numerical performance in the class of first order
methods. First, we derive nonasymptotic iteration complexity estimates of exact
and inexact PPA to minimize convex functions under $\gamma-$Holderian growth:
$\BigO{\log(1/\epsilon)}$ (for $\gamma \in [1,2]$) and
$\BigO{1/\epsilon^{\gamma - 2}}$ (for $\gamma > 2$). In particular, we recover
well-known results on exact PPA: finite convergence for sharp minima and linear
convergence for quadratic growth, even under presence of inexactness. Second,
assuming that an usual (proximal) gradient/subgradient method subroutine is
employed to compute inexact PPA iteration, we show novel computational
complexity bounds on a restarted variant of the inexact PPA, available when no
information on the growth of the objective function is known. In the numerical
experiments we confirm the practical performance and implementability of our
schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1"&gt;Andrei Patrascu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1"&gt;Paul Irofti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04345</id>
        <link href="http://arxiv.org/abs/2108.04345"/>
        <updated>2021-08-11T01:55:21.313Z</updated>
        <summary type="html"><![CDATA[Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1"&gt;Hamza Rasaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1"&gt;Hassan Rivaz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Knowledge Tracing via Adversarial Training. (arXiv:2108.04430v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.04430</id>
        <link href="http://arxiv.org/abs/2108.04430"/>
        <updated>2021-08-11T01:55:21.306Z</updated>
        <summary type="html"><![CDATA[We study the problem of knowledge tracing (KT) where the goal is to trace the
students' knowledge mastery over time so as to make predictions on their future
performance. Owing to the good representation capacity of deep neural networks
(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs
to improve the performance of KT. However, we empirically reveal that the DNNs
based KT models may run the risk of overfitting, especially on small datasets,
leading to limited generalization. In this paper, by leveraging the current
advances in adversarial training (AT), we propose an efficient AT based KT
method (ATKT) to enhance KT model's generalization and thus push the limit of
KT. Specifically, we first construct adversarial perturbations and add them on
the original interaction embeddings as adversarial examples. The original and
adversarial examples are further used to jointly train the KT model, forcing it
is not only to be robust to the adversarial examples, but also to enhance the
generalization over the original ones. To better implement AT, we then present
an efficient attentive-LSTM model as KT backbone, where the key is a proposed
knowledge hidden state attention module that adaptively aggregates information
from previous knowledge hidden states while simultaneously highlighting the
importance of current knowledge hidden state to make a more accurate
prediction. Extensive experiments on four public benchmark datasets demonstrate
that our ATKT achieves new state-of-the-art performance. Code is available at:
\color{blue} {\url{https://github.com/xiaopengguo/ATKT}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaopeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhijie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1"&gt;Mingyu Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1"&gt;Maojing Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jun Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication. (arXiv:2108.04436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04436</id>
        <link href="http://arxiv.org/abs/2108.04436"/>
        <updated>2021-08-11T01:55:21.300Z</updated>
        <summary type="html"><![CDATA[Radio-frequency fingerprints~(RFFs) are promising solutions for realizing
low-cost physical layer authentication. Machine learning-based methods have
been proposed for RFF extraction and discrimination. However, most existing
methods are designed for the closed-set scenario where the set of devices is
remains unchanged. These methods can not be generalized to the RFF
discrimination of unknown devices. To enable the discrimination of RFF from
both known and unknown devices, we propose a new end-to-end deep learning
framework for extracting RFFs from raw received signals. The proposed framework
comprises a novel preprocessing module, called neural synchronization~(NS),
which incorporates the data-driven learning with signal processing priors as an
inductive bias from communication-model based processing. Compared to
traditional carrier synchronization techniques, which are static, this module
estimates offsets by two learnable deep neural networks jointly trained by the
RFF extractor. Additionally, a hypersphere representation is proposed to
further improve the discrimination of RFF. Theoretical analysis shows that such
a data-and-model framework can better optimize the mutual information between
device identity and the RFF, which naturally leads to better performance.
Experimental results verify that the proposed RFF significantly outperforms
purely data-driven DNN-design and existing handcrafted RFF methods in terms of
both discrimination and network generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Renjie Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiabao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Aiqun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1"&gt;Derrick Wing Kwan Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1"&gt;A. Lee Swindlehurst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisit the Fundamental Theorem of Linear Algebra. (arXiv:2108.04432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04432</id>
        <link href="http://arxiv.org/abs/2108.04432"/>
        <updated>2021-08-11T01:55:21.293Z</updated>
        <summary type="html"><![CDATA[This survey is meant to provide an introduction to the fundamental theorem of
linear algebra and the theories behind them. Our goal is to give a rigorous
introduction to the readers with prior exposure to linear algebra.
Specifically, we provide some details and proofs of some results from (Strang,
1993). We then describe the fundamental theorem of linear algebra from
different views and find the properties and relationships behind the views. The
fundamental theorem of linear algebra is essential in many fields, such as
electrical engineering, computer science, machine learning, and deep learning.
This survey is primarily a summary of purpose, significance of important
theories behind it.

The sole aim of this survey is to give a self-contained introduction to
concepts and mathematical tools in theory behind the fundamental theorem of
linear algebra and rigorous analysis in order to seamlessly introduce its
properties in four subspaces in subsequent sections. However, we clearly
realize our inability to cover all the useful and interesting results and given
the paucity of scope to present this discussion, e.g., the separated analysis
of the (orthogonal) projection matrices. We refer the reader to literature in
the field of linear algebra for a more detailed introduction to the related
fields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen
and Bau III, 1997; Strang, 2019, 2021).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularized Sequential Latent Variable Models with Adversarial Neural Networks. (arXiv:2108.04496v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04496</id>
        <link href="http://arxiv.org/abs/2108.04496"/>
        <updated>2021-08-11T01:55:21.273Z</updated>
        <summary type="html"><![CDATA[The recurrent neural networks (RNN) with richly distributed internal states
and flexible non-linear transition functions, have overtaken the dynamic
Bayesian networks such as the hidden Markov models (HMMs) in the task of
modeling highly structured sequential data. These data, such as from speech and
handwriting, often contain complex relationships between the underlaying
variational factors and the observed data. The standard RNN model has very
limited randomness or variability in its structure, coming from the output
conditional probability model. This paper will present different ways of using
high level latent random variables in RNN to model the variability in the
sequential data, and the training method of such RNN model under the VAE
(Variational Autoencoder) principle. We will explore possible ways of using
adversarial method to train a variational RNN model. Contrary to competing
approaches, our approach has theoretical optimum in the model training and
provides better model training stability. Our approach also improves the
posterior approximation in the variational inference network by a separated
adversarial training step. Numerical results simulated from TIMIT speech data
show that reconstruction loss and evidence lower bound converge to the same
level and adversarial training loss converges to 0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1"&gt;Ming Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04358</id>
        <link href="http://arxiv.org/abs/2108.04358"/>
        <updated>2021-08-11T01:55:21.268Z</updated>
        <summary type="html"><![CDATA[Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1"&gt;Ipsita Sutradhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mahziba Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Mehedi Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1"&gt;Malabika Sarker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04351</id>
        <link href="http://arxiv.org/abs/2108.04351"/>
        <updated>2021-08-11T01:55:21.262Z</updated>
        <summary type="html"><![CDATA[This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04238</id>
        <link href="http://arxiv.org/abs/2108.04238"/>
        <updated>2021-08-11T01:55:21.256Z</updated>
        <summary type="html"><![CDATA[Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is "counter-facted", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Haocheng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Caleb Chen Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Enhanced Dynamic Mode Decomposition. (arXiv:2108.04433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04433</id>
        <link href="http://arxiv.org/abs/2108.04433"/>
        <updated>2021-08-11T01:55:21.250Z</updated>
        <summary type="html"><![CDATA[Koopman operator theory shows how nonlinear dynamical systems can be
represented as an infinite-dimensional, linear operator acting on a Hilbert
space of observables of the system. However, determining the relevant modes and
eigenvalues of this infinite-dimensional operator can be difficult. The
extended dynamic mode decomposition (EDMD) is one such method for generating
approximations to Koopman spectra and modes, but the EDMD method faces its own
set of challenges due to the need of user defined observables. To address this
issue, we explore the use of convolutional autoencoder networks to
simultaneously find optimal families of observables which also generate both
accurate embeddings of the flow into a space of observables and immersions of
the observables back into flow coordinates. This network results in a global
transformation of the flow and affords future state prediction via EDMD and the
decoder network. We call this method deep learning dynamic mode decomposition
(DLDMD). The method is tested on canonical nonlinear data sets and is shown to
produce results that outperform a standard DMD approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1"&gt;Christopher W. Curtis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1"&gt;Daniel Jay Alford-Lago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issan_O/0/1/0/all/0/1"&gt;Opal Issan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04409</id>
        <link href="http://arxiv.org/abs/2108.04409"/>
        <updated>2021-08-11T01:55:21.244Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small per-
turbations on the input images. Researchers have been devoted to promoting the
research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise at- tack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiaoyang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huilin Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Wancheng Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaRNN: Adaptive Learning and Forecasting of Time Series. (arXiv:2108.04443v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04443</id>
        <link href="http://arxiv.org/abs/2108.04443"/>
        <updated>2021-08-11T01:55:21.221Z</updated>
        <summary type="html"><![CDATA[Time series has wide applications in the real world and is known to be
difficult to forecast. Since its statistical properties change over time, its
distribution also changes temporally, which will cause severe distribution
shift problem to existing methods. However, it remains unexplored to model the
time series in the distribution perspective. In this paper, we term this as
Temporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to
tackle the TCS problem by building an adaptive model that generalizes well on
the unseen test data. AdaRNN is sequentially composed of two novel algorithms.
First, we propose Temporal Distribution Characterization to better characterize
the distribution information in the TS. Second, we propose Temporal
Distribution Matching to reduce the distribution mismatch in TS to learn the
adaptive TS model. AdaRNN is a general framework with flexible distribution
distances integrated. Experiments on human activity recognition, air quality
prediction, and financial analysis show that AdaRNN outperforms the latest
methods by a classification accuracy of 2.6% and significantly reduces the RMSE
by 9.0%. We also show that the temporal distribution matching algorithm can be
extended in Transformer structure to boost its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wenjie Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Sinno Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04536</id>
        <link href="http://arxiv.org/abs/2108.04536"/>
        <updated>2021-08-11T01:55:21.215Z</updated>
        <summary type="html"><![CDATA[The task of skeleton-based action recognition remains a core challenge in
human-centred scene understanding due to the multiple granularities and large
variation in human motion. Existing approaches typically employ a single neural
representation for different motion patterns, which has difficulty in capturing
fine-grained action classes given limited training data. To address the
aforementioned problems, we propose a novel multi-granular spatio-temporal
graph network for skeleton-based action classification that jointly models the
coarse- and fine-grained skeleton motion patterns. To this end, we develop a
dual-head graph network consisting of two interleaved branches, which enables
us to extract features at two spatio-temporal resolutions in an effective and
efficient manner. Moreover, our network utilises a cross-head communication
strategy to mutually enhance the representations of both heads. We conducted
extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU
RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance
on all the benchmarks, which validates the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tailin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Desen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yu Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Principal Component Analysis in High Dimensional CP Models. (arXiv:2108.04428v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.04428</id>
        <link href="http://arxiv.org/abs/2108.04428"/>
        <updated>2021-08-11T01:55:21.207Z</updated>
        <summary type="html"><![CDATA[The CP decomposition for high dimensional non-orthogonal spike tensors is an
important problem with broad applications across many disciplines. However,
previous works with theoretical guarantee typically assume restrictive
incoherence conditions on the basis vectors for the CP components. In this
paper, we propose new computationally efficient composite PCA and concurrent
orthogonalization algorithms for tensor CP decomposition with theoretical
guarantees under mild incoherence conditions. The composite PCA applies the
principal component or singular value decompositions twice, first to a matrix
unfolding of the tensor data to obtain singular vectors and then to the matrix
folding of the singular vectors obtained in the first step. It can be used as
an initialization for any iterative optimization schemes for the tensor CP
decomposition. The concurrent orthogonalization algorithm iteratively estimates
the basis vector in each mode of the tensor by simultaneously applying
projections to the orthogonal complements of the spaces generated by others CP
components in other modes. It is designed to improve the alternating least
squares estimator and other forms of the high order orthogonal iteration for
tensors with low or moderately high CP ranks. Our theoretical investigation
provides estimation accuracy and statistical convergence rates for the two
proposed algorithms. Our implementations on synthetic data demonstrate
significant practical superiority of our approach over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yuefeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cun-Hui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Machine Learning: Methods, Challenges and Directions. (arXiv:2108.04417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04417</id>
        <link href="http://arxiv.org/abs/2108.04417"/>
        <updated>2021-08-11T01:55:21.191Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is increasingly being adopted in a wide variety of
application domains. Usually, a well-performing ML model, especially, emerging
deep neural network model, relies on a large volume of training data and
high-powered computational resources. The need for a vast volume of available
data raises serious privacy concerns because of the risk of leakage of highly
privacy-sensitive information and the evolving regulatory environments that
increasingly restrict access to and use of privacy-sensitive data. Furthermore,
a trained ML model may also be vulnerable to adversarial attacks such as
membership/property inference attacks and model inversion attacks. Hence,
well-designed privacy-preserving ML (PPML) solutions are crucial and have
attracted increasing research interest from academia and industry. More and
more efforts of PPML are proposed via integrating privacy-preserving techniques
into ML algorithms, fusing privacy-preserving approaches into ML pipeline, or
designing various privacy-preserving architectures for existing ML systems. In
particular, existing PPML arts cross-cut ML, system, security, and privacy;
hence, there is a critical need to understand state-of-art studies, related
challenges, and a roadmap for future research. This paper systematically
reviews and summarizes existing privacy-preserving approaches and proposes a
PGU model to guide evaluation for various PPML solutions through elaborately
decomposing their privacy-preserving functionalities. The PGU model is designed
as the triad of Phase, Guarantee, and technical Utility. Furthermore, we also
discuss the unique characteristics and challenges of PPML and outline possible
directions of future work that benefit a wide range of research communities
among ML, distributed systems, security, and privacy areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Runhua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1"&gt;Nathalie Baracaldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1"&gt;James Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04359</id>
        <link href="http://arxiv.org/abs/2108.04359"/>
        <updated>2021-08-11T01:55:21.186Z</updated>
        <summary type="html"><![CDATA[The performance of many medical image analysis tasks are strongly associated
with image data quality. When developing modern deep learning algorithms,
rather than relying on subjective (human-based) image quality assessment (IQA),
task amenability potentially provides an objective measure of task-specific
image quality. To predict task amenability, an IQA agent is trained using
reinforcement learning (RL) with a simultaneously optimised task predictor,
such as a classification or segmentation neural network. In this work, we
develop transfer learning or adaptation strategies to increase the adaptability
of both the IQA agent and the task predictor so that they are less dependent on
high-quality, expert-labelled training data. The proposed transfer learning
strategy re-formulates the original RL problem for task amenability in a
meta-reinforcement learning (meta-RL) framework. The resulting algorithm
facilitates efficient adaptation of the agent to different definitions of image
quality, each with its own Markov decision process environment including
different images, labels and an adaptable task predictor. Our work demonstrates
that the IQA agents pre-trained on non-expert task labels can be adapted to
predict task amenability as defined by expert task labels, using only a small
set of expert labels. Using 6644 clinical ultrasound images from 249 prostate
cancer patients, our results for image classification and segmentation tasks
show that the proposed IQA method can be adapted using data with as few as
respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve
comparable IQA and task performance, which would otherwise require a training
dataset with 100% expert labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1"&gt;Shaheer U. Saeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yunguan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1"&gt;Vasilis Stavrinides&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1"&gt;Zachary M. C. Baum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1"&gt;Mirabela Rusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Richard E. Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1"&gt;Geoffrey A. Sonn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1"&gt;J. Alison Noble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1"&gt;Dean C. Barratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04423</id>
        <link href="http://arxiv.org/abs/2108.04423"/>
        <updated>2021-08-11T01:55:21.164Z</updated>
        <summary type="html"><![CDATA[Deep learning models achieve strong performance for radiology image
classification, but their practical application is bottlenecked by the need for
large labeled training datasets. Semi-supervised learning (SSL) approaches
leverage small labeled datasets alongside larger unlabeled datasets and offer
potential for reducing labeling cost. In this work, we introduce NoTeacher, a
novel consistency-based SSL framework which incorporates probabilistic
graphical models. Unlike Mean Teacher which maintains a teacher network updated
via a temporal ensemble, NoTeacher employs two independent networks, thereby
eliminating the need for a teacher network. We demonstrate how NoTeacher can be
customized to handle a range of challenges in radiology image classification.
Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni
and multi-label classification, and class distribution mismatch between labeled
and unlabeled portions of the training data. In realistic empirical evaluations
on three public benchmark datasets spanning the workhorse modalities of
radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the
fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher
outperforms established SSL methods with minimal hyperparameter tuning, and has
implications as a principled and practical option for semisupervised learning
in radiology applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1"&gt;Balagopal Unnikrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cuong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1"&gt;Shafa Balaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1"&gt;Chuan Sheng Foo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1"&gt;Pavitra Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04384</id>
        <link href="http://arxiv.org/abs/2108.04384"/>
        <updated>2021-08-11T01:55:21.158Z</updated>
        <summary type="html"><![CDATA[For the past ten years, CNN has reigned supreme in the world of computer
vision, but recently, Transformer is on the rise. However, the quadratic
computational cost of self-attention has become a severe problem of practice.
There has been much research on architectures without CNN and self-attention in
this context. In particular, MLP-Mixer is a simple idea designed using MLPs and
hit an accuracy comparable to the Vision Transformer. However, the only
inductive bias in this architecture is the embedding of tokens. Thus, there is
still a possibility to build a non-convolutional inductive bias into the
architecture itself, and we built in an inductive bias using two simple ideas.
A way is to divide the token-mixing block vertically and horizontally. Another
way is to make spatial correlations denser among some channels of token-mixing.
With this approach, we were able to improve the accuracy of the MLP-Mixer while
reducing its parameters and computational complexity. Compared to other
MLP-based models, the proposed model, named RaftMLP has a good balance of
computational complexity, the number of parameters, and actual memory usage. In
addition, our work indicates that MLP-based models have the potential to
replace CNNs by adopting inductive bias. The source code in PyTorch version is
available at \url{https://github.com/okojoalg/raft-mlp}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1"&gt;Yuki Tatsunami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1"&gt;Masato Taki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04349</id>
        <link href="http://arxiv.org/abs/2108.04349"/>
        <updated>2021-08-11T01:55:21.151Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity-aware Web APIs Recommendation with Compatibility Guarantee. (arXiv:2108.04389v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.04389</id>
        <link href="http://arxiv.org/abs/2108.04389"/>
        <updated>2021-08-11T01:55:21.139Z</updated>
        <summary type="html"><![CDATA[With the ever-increasing prevalence of web APIs (Application Programming
Interfaces) in enabling smart software developments, finding and composing a
list of existing web APIs that can corporately fulfil the software developers'
functional needs have become a promising way to develop a successful mobile
app, economically and conveniently. However, the big volume and diversity of
candidate web APIs put additional burden on the app developers' web APIs
selection decision-makings, since it is often a challenging task to
simultaneously guarantee the diversity and compatibility of the finally
selected a set of web APIs. Considering this challenge, a Diversity-aware and
Compatibility-driven web APIs Recommendation approach, namely DivCAR, is put
forward in this paper. First, to achieve diversity, DivCAR employs random walk
sampling technique on a pre-built correlation graph to generate diverse
correlation subgraphs. Afterwards, with the diverse correlation subgraphs, we
model the compatible web APIs recommendation problem to be a minimum group
Steiner tree search problem. Through solving the minimum group Steiner tree
search problem, manifold sets of compatible and diverse web APIs ranked are
returned to the app developers. At last, we design and enact a set of
experiments on a real-world dataset crawled from www.programmableWeb.com.
Experimental results validate the effectiveness and efficiency of our proposed
DivCAR approach in balancing the web APIs recommendation diversity and
compatibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonga_W/0/1/0/all/0/1"&gt;Wenwen Gonga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuyun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1"&gt;Yucong Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yawei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1"&gt;Yifei Chena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lianyong Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.04327</id>
        <link href="http://arxiv.org/abs/2108.04327"/>
        <updated>2021-08-11T01:55:21.132Z</updated>
        <summary type="html"><![CDATA[Natural numerical networks are introduced as a new classification algorithm
based on the numerical solution of nonlinear partial differential equations of
forward-backward diffusion type on complete graphs. The proposed natural
numerical network is applied to open important environmental and nature
conservation task, the automated identification of protected habitats by using
satellite images. In the natural numerical network, the forward diffusion
causes the movement of points in a feature space toward each other. The
opposite effect, keeping the points away from each other, is caused by backward
diffusion. This yields the desired classification. The natural numerical
network contains a few parameters that are optimized in the learning phase of
the method. After learning parameters and optimizing the topology of the
network graph, classification necessary for habitat identification is
performed. A relevancy map for each habitat is introduced as a tool for
validating the classification and finding new Natura 2000 habitat appearances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1"&gt;Karol Mikula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1"&gt;Michal Kollar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1"&gt;Aneta A. Ozvat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1"&gt;Martin Ambroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1"&gt;Lucia Cahojova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1"&gt;Ivan Jarolimek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1"&gt;Jozef Sibik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1"&gt;Maria Sibikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04357</id>
        <link href="http://arxiv.org/abs/2108.04357"/>
        <updated>2021-08-11T01:55:21.119Z</updated>
        <summary type="html"><![CDATA[Touchless computer interaction has become an important consideration during
the COVID-19 pandemic period. Despite progress in machine learning and computer
vision that allows for advanced gesture recognition, an integrated collection
of such open-source methods and a user-customisable approach to utilising them
in a low-cost solution for touchless interaction in existing software is still
missing. In this paper, we introduce the MotionInput v2.0 application. This
application utilises published open-source libraries and additional gesture
definitions developed to take the video stream from a standard RGB webcam as
input. It then maps human motion gestures to input operations for existing
applications and games. The user can choose their own preferred way of
interacting from a series of motion types, including single and bi-modal hand
gesturing, full-body repetitive or extremities-based exercises, head and facial
movements, eye tracking, and combinations of the above. We also introduce a
series of bespoke gesture recognition classifications as DirectInput triggers,
including gestures for idle states, auto calibration, depth capture from a 2D
RGB webcam stream and tracking of facial motions such as mouth motions,
winking, and head direction with rotation. Three use case areas assisted the
development of the modules: creativity software, office and clinical software,
and gaming software. A collection of open-source libraries has been integrated
and provide a layer of modular gesture mapping on top of existing mouse and
keyboard controls in Windows via DirectX. With ease of access to webcams
integrated into most laptops and desktop computers, touchless computing becomes
more available with MotionInput v2.0, in a federated and locally processed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1"&gt;Ashild Kummen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1"&gt;Ali Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1"&gt;Teodora Ganeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qianying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1"&gt;Robert Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1"&gt;Chenuka Ratwatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yang Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1"&gt;Lu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1"&gt;Emil Almazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1"&gt;Sheena Visram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1"&gt;Andrew Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1"&gt;Neil J Sebire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1"&gt;Lee Stott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1"&gt;Yvonne Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1"&gt;Graham Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1"&gt;Dean Mohamedally&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.04816</id>
        <link href="http://arxiv.org/abs/2108.04816"/>
        <updated>2021-08-11T01:55:21.091Z</updated>
        <summary type="html"><![CDATA[Public response to COVID-19 vaccines is the key success factor to control the
COVID-19 pandemic. To understand the public response, there is a need to
explore public opinion. Traditional surveys are expensive and time-consuming,
address limited health topics, and obtain small-scale data. Twitter can provide
a great opportunity to understand public opinion regarding COVID-19 vaccines.
The current study proposes an approach using computational and human coding
methods to collect and analyze a large number of tweets to provide a wider
perspective on the COVID-19 vaccine. This study identifies the sentiment of
tweets and their temporal trend, discovers major topics, compares topics of
negative and non-negative tweets, and discloses top topics of negative and
non-negative tweets. Our findings show that the negative sentiment regarding
the COVID-19 vaccine had a decreasing trend between November 2020 and February
2021. We found Twitter users have discussed a wide range of topics from
vaccination sites to the 2020 U.S. election between November 2020 and February
2021. The findings show that there was a significant difference between
negative and non-negative tweets regarding the weight of most topics. Our
results also indicate that the negative and non-negative tweets had different
topic priorities and focuses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1"&gt;Amir Karami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Michael Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1"&gt;Bailey Goldschmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1"&gt;Hannah R. Boyajieff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1"&gt;Mahdi M. Najafabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Zero Resource Speech Challenge 2021: Spoken language modelling. (arXiv:2104.14700v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14700</id>
        <link href="http://arxiv.org/abs/2104.14700"/>
        <updated>2021-08-11T01:55:21.085Z</updated>
        <summary type="html"><![CDATA[We present the Zero Resource Speech Challenge 2021, which asks participants
to learn a language model directly from audio, without any text or labels. The
challenge is based on the Libri-light dataset, which provides up to 60k hours
of audio from English audio books without any associated text. We provide a
pipeline baseline system consisting on an encoder based on contrastive
predictive coding (CPC), a quantizer ($k$-means) and a standard language model
(BERT or LSTM). The metrics evaluate the learned representations at the
acoustic (ABX discrimination), lexical (spot-the-word), syntactic
(acceptability judgment) and semantic levels (similarity judgment). We present
an overview of the eight submitted systems from four groups and discuss the
main results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1"&gt;Ewan Dunbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1"&gt;Mathieu Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1"&gt;Nicolas Hamilakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tu Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1"&gt;Maureen de Seyssel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roze_P/0/1/0/all/0/1"&gt;Patricia Roz&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1"&gt;Morgane Rivi&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1"&gt;Eugene Kharitonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04344</id>
        <link href="http://arxiv.org/abs/2108.04344"/>
        <updated>2021-08-11T01:55:21.078Z</updated>
        <summary type="html"><![CDATA[Due to the limited availability and high cost of the reverse
transcription-polymerase chain reaction (RT-PCR) test, many studies have
proposed machine learning techniques for detecting COVID-19 from medical
imaging. The purpose of this study is to systematically review, assess, and
synthesize research articles that have used different machine learning
techniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.
A structured literature search was conducted in the relevant bibliographic
databases to ensure that the survey solely centered on reproducible and
high-quality research. We selected papers based on our inclusion criteria. In
this survey, we reviewed $98$ articles that fulfilled our inclusion criteria.
We have surveyed a complete pipeline of chest imaging analysis techniques
related to COVID-19, including data collection, pre-processing, feature
extraction, classification, and visualization. We have considered CT scans and
X-rays as both are widely used to describe the latest developments in medical
imaging to detect COVID-19. This survey provides researchers with valuable
insights into different machine learning techniques and their performance in
the detection and diagnosis of COVID-19 from chest imaging. At the end, the
challenges and limitations in detecting COVID-19 using machine learning
techniques and the future direction of research are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1"&gt;Aishwarza Panday&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Muhammad Ashad Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1"&gt;Nihad Karim Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Headed Span-Based Projective Dependency Parsing. (arXiv:2108.04750v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04750</id>
        <link href="http://arxiv.org/abs/2108.04750"/>
        <updated>2021-08-11T01:55:21.072Z</updated>
        <summary type="html"><![CDATA[We propose a headed span-based method for projective dependency parsing. In a
projective tree, the subtree rooted at each word occurs in a contiguous
sequence (i.e., span) in the surface order, we call the span-headword pair
\textit{headed span}. In this view, a projective tree can be regarded as a
collection of headed spans. It is similar to the case in constituency parsing
since a constituency tree can be regarded as a collection of constituent spans.
Span-based methods decompose the score of a constituency tree sorely into the
score of constituent spans and use the CYK algorithm for global training and
exact inference, obtaining state-of-the-art results in constituency parsing.
Inspired by them, we decompose the score of a dependency tree into the score of
headed spans. We use neural networks to score headed spans and design a novel
$O(n^3)$ dynamic programming algorithm to enable global training and exact
inference. We evaluate our method on PTB, CTB, and UD, achieving
state-of-the-art or comparable results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Songlin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00502</id>
        <link href="http://arxiv.org/abs/2010.00502"/>
        <updated>2021-08-11T01:55:21.055Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition. (arXiv:2012.12007v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12007</id>
        <link href="http://arxiv.org/abs/2012.12007"/>
        <updated>2021-08-11T01:55:21.049Z</updated>
        <summary type="html"><![CDATA[Humor recognition has been widely studied as a text classification problem
using data-driven approaches. However, most existing work does not examine the
actual joke mechanism to understand humor. We break down any joke into two
distinct components: the set-up and the punchline, and further explore the
special relationship between them. Inspired by the incongruity theory of humor,
we model the set-up as the part developing semantic uncertainty, and the
punchline disrupting audience expectations. With increasingly powerful language
models, we were able to feed the set-up along with the punchline into the GPT-2
language model, and calculate the uncertainty and surprisal values of the
jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found
that these two features have better capabilities of telling jokes from
non-jokes, compared with existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yubo Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junze Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1"&gt;Pearl Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04812</id>
        <link href="http://arxiv.org/abs/2108.04812"/>
        <updated>2021-08-11T01:55:21.042Z</updated>
        <summary type="html"><![CDATA[We study continual learning for natural language instruction generation, by
observing human users' instruction execution. We focus on a collaborative
scenario, where the system both acts and delegates tasks to human users using
natural language. We compare user execution of generated instructions to the
original system intent as an indication to the system's success communicating
its intent. We show how to use this signal to improve the system's ability to
generate instructions via contextual bandit learning. In interaction with real
users, our system demonstrates dramatic improvements in its ability to generate
language over time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1"&gt;Noriyuki Kojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1"&gt;Alane Suhr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1"&gt;Yoav Artzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.04378</id>
        <link href="http://arxiv.org/abs/2108.04378"/>
        <updated>2021-08-11T01:55:21.037Z</updated>
        <summary type="html"><![CDATA[Several studies have reported the inability of Transformer models to
generalize compositionally, a key type of generalization in many NLP tasks such
as semantic parsing. In this paper we explore the design space of Transformer
models showing that the inductive biases given to the model by several design
decisions significantly impact compositional generalization. Through this
exploration, we identified Transformer configurations that generalize
compositionally significantly better than previously reported in the literature
in a diverse set of compositional tasks, and that achieve state-of-the-art
results in a semantic parsing compositional generalization benchmark (COGS),
and a string edit operation composition benchmark (PCFG).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onta%7Bn%7D%7Bo%7Dn_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1"&gt;Vaclav Cvicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1"&gt;Zachary Fisher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hope Speech detection in under-resourced Kannada language. (arXiv:2108.04616v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04616</id>
        <link href="http://arxiv.org/abs/2108.04616"/>
        <updated>2021-08-11T01:55:21.030Z</updated>
        <summary type="html"><![CDATA[Numerous methods have been developed to monitor the spread of negativity in
modern years by eliminating vulgar, offensive, and fierce comments from social
media platforms. However, there are relatively lesser amounts of study that
converges on embracing positivity, reinforcing supportive and reassuring
content in online forums. Consequently, we propose creating an English-Kannada
Hope speech dataset, KanHope and comparing several experiments to benchmark the
dataset. The dataset consists of 6,176 user-generated comments in code mixed
Kannada scraped from YouTube and manually annotated as bearing hope speech or
Not-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model
that uses the English translation of KanHope for additional training to promote
hope speech detection. The approach achieves a weighted F1-score of 0.756,
bettering other models. Henceforth, KanHope aims to instigate research in
Kannada while broadly promoting researchers to take a pragmatic approach
towards online content that encourages, positive, and supportive.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1"&gt;Adeep Hande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1"&gt;Ruba Priyadharshini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1"&gt;Anbukkarasi Sampath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1"&gt;Kingston Pal Thamburaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1"&gt;Prabakaran Chandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04539</id>
        <link href="http://arxiv.org/abs/2108.04539"/>
        <updated>2021-08-11T01:55:21.011Z</updated>
        <summary type="html"><![CDATA[Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical
cases where order information of text blocks is not available. Further
experiments with a varying number of training examples demonstrate the high
training efficiency of our approach. Our code will be open to the public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1"&gt;Teakgyu Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1"&gt;Mingi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Wonseok Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1"&gt;Daehyun Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sungrae Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04267</id>
        <link href="http://arxiv.org/abs/2108.04267"/>
        <updated>2021-08-11T01:55:21.005Z</updated>
        <summary type="html"><![CDATA[The neuroimage analysis community has neglected the automated segmentation of
the olfactory bulb (OB) despite its crucial role in olfactory function. The
lack of an automatic processing method for the OB can be explained by its
challenging properties. Nonetheless, recent advances in MRI acquisition
techniques and resolution have allowed raters to generate more reliable manual
annotations. Furthermore, the high accuracy of deep learning methods for
solving semantic segmentation problems provides us with an option to reliably
assess even small structures. In this work, we introduce a novel, fast, and
fully automated deep learning pipeline to accurately segment OB tissue on
sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we
designed a three-stage pipeline: (1) Localization of a region containing both
OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized
region through four independent AttFastSurferCNN - a novel deep learning
architecture with a self-attention mechanism to improve modeling of contextual
information, and (3) Ensemble of the predicted label maps. The OB pipeline
exhibits high performance in terms of boundary delineation, OB localization,
and volume estimation across a wide range of ages in 203 participants of the
Rhineland Study. Moreover, it also generalizes to scans of an independent
dataset never encountered during training, the Human Connectome Project (HCP),
with different acquisition parameters and demographics, evaluated in 30 cases
at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.
We extensively validated our pipeline not only with respect to segmentation
accuracy but also to known OB volume effects, where it can sensitively
replicate age effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1"&gt;Santiago Estrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Ran Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1"&gt;Kersten Diers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Weiyi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1"&gt;Philipp Ehses&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1"&gt;Tony St&amp;#xf6;cker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1"&gt;Monique M.B Breteler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1"&gt;Martin Reuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system. (arXiv:2105.00260v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00260</id>
        <link href="http://arxiv.org/abs/2105.00260"/>
        <updated>2021-08-11T01:55:20.998Z</updated>
        <summary type="html"><![CDATA[People convey information extremely effectively through spoken interaction
using multiple channels of information transmission: the lexical channel of
what is said, and the non-lexical channel of how it is said. We propose
studying human perception of spoken communication as a means to better
understand how information is encoded across these channels, focusing on the
question 'What characteristics of communicative context affect listener's
expectations of speech?'. To investigate this, we present a novel behavioural
task testing whether listeners can discriminate between the true utterance in a
dialogue and utterances sampled from other contexts with the same lexical
content. We characterize how perception - and subsequent discriminative
capability - is affected by different degrees of additional contextual
information across both the lexical and non-lexical channel of speech. Results
demonstrate that people can effectively discriminate between different prosodic
realisations, that non-lexical context is informative, and that this channel
provides more salient information than the lexical channel, highlighting the
importance of the non-lexical channel in spoken interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1"&gt;Sarenne Wallbridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1"&gt;Peter Bell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Catherine Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies. (arXiv:2108.04674v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04674</id>
        <link href="http://arxiv.org/abs/2108.04674"/>
        <updated>2021-08-11T01:55:20.988Z</updated>
        <summary type="html"><![CDATA[In this paper, we give an overview of commonsense reasoning in natural
language processing, which requires a deeper understanding of the contexts and
usually involves inference over implicit external knowledge. We first review
some popular commonsense knowledge bases and commonsense reasoning benchmarks,
but give more emphasis on the methodologies, including recent approaches that
aim at solving some general natural language problems that take advantage of
external knowledge bases. Finally, we discuss some future directions in pushing
the boundary of commonsense reasoning in natural language processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yubo Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1"&gt;Pearl Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01139</id>
        <link href="http://arxiv.org/abs/2108.01139"/>
        <updated>2021-08-11T01:55:20.980Z</updated>
        <summary type="html"><![CDATA[EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1"&gt;Andrei-Marius Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1"&gt;Vasile Pais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1"&gt;Dan Tufis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews. (arXiv:2108.04366v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04366</id>
        <link href="http://arxiv.org/abs/2108.04366"/>
        <updated>2021-08-11T01:55:20.962Z</updated>
        <summary type="html"><![CDATA[Comparing research papers is a conventional method to demonstrate progress in
experimental research. We present COMPARE, a taxonomy and a dataset of
comparison discussions in peer reviews of research papers in the domain of
experimental deep learning. From a thorough observation of a large set of
review sentences, we build a taxonomy of categories in comparison discussions
and present a detailed annotation scheme to analyze this. Overall, we annotate
117 reviews covering 1,800 sentences. We experiment with various methods to
identify comparison sentences in peer reviews and report a maximum F1 Score of
0.49. We also pretrain two language models specifically on ML, NLP, and CV
paper abstracts and reviews to learn informative representations of peer
reviews. The annotated dataset and the pretrained models are available at
https://github.com/shruti-singh/COMPARE .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Shruti Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04324</id>
        <link href="http://arxiv.org/abs/2108.04324"/>
        <updated>2021-08-11T01:55:20.955Z</updated>
        <summary type="html"><![CDATA[Storytelling is an open-ended task that entails creative thinking and
requires a constant flow of ideas. Natural language generation (NLG) for
storytelling is especially challenging because it requires the generated text
to follow an overall theme while remaining creative and diverse to engage the
reader. In this work, we introduce a system and a web-based demo, FairyTailor,
for human-in-the-loop visual story co-creation. Users can create a cohesive
children's fairytale by weaving generated texts and retrieved images with their
input. FairyTailor adds another modality and modifies the text generation
process to produce a coherent and creative sequence of text and images. To our
knowledge, this is the first dynamic tool for multimodal story generation that
allows interactive co-formation of both texts and images. It allows users to
give feedback on co-created stories and share their results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1"&gt;Eden Bensaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1"&gt;Mauro Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1"&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1"&gt;Hendrik Strobelt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity. (arXiv:2108.04289v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2108.04289</id>
        <link href="http://arxiv.org/abs/2108.04289"/>
        <updated>2021-08-11T01:55:20.948Z</updated>
        <summary type="html"><![CDATA[Analysing correlations between streams of events is an important problem. It
arises for example in Neurosciences, when the connectivity of neurons should be
inferred from spike trains that record neurons' individual spiking activity.
While recently some approaches for inferring delayed synaptic connections have
been proposed, they are limited in the types of connectivities and delays they
are able to handle, or require computation-intensive procedures. This paper
proposes a faster and more flexible approach for analysing such delayed
correlated activity: a statistical approach for the Analysis of Connectivity in
spiking Events (ACE), based on the idea of hypothesis testing. It first
computes for any pair of a source and a target neuron the inter-spike delays
between subsequent source- and target-spikes. Then, it derives a null model for
the distribution of inter-spike delays for \emph{uncorrelated}~neurons.
Finally, it compares the observed distribution of inter-spike delays to this
null model and infers pairwise connectivity based on the Pearson's Chi-squared
test statistic. Thus, ACE is capable to detect connections with a priori
unknown, non-discrete (and potentially large) inter-spike delays, which might
vary between pairs of neurons. Since ACE works incrementally, it has potential
for being used in online processing. In our experiments, we visualise the
advantages of ACE in varying experimental scenarios (except for one special
case) and in a state-of-the-art dataset which has been generated for
neuro-scientific research under most realistic conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Krempl/0/1/0/all/0/1"&gt;Krempl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Georg/0/1/0/all/0/1"&gt;Georg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kottke/0/1/0/all/0/1"&gt;Kottke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Daniel/0/1/0/all/0/1"&gt;Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Minh_P/0/1/0/all/0/1"&gt;Pham Minh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Tuan/0/1/0/all/0/1"&gt;Tuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Intent Detection via Multi-Strategy Rebalancing. (arXiv:2108.04445v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04445</id>
        <link href="http://arxiv.org/abs/2108.04445"/>
        <updated>2021-08-11T01:55:20.941Z</updated>
        <summary type="html"><![CDATA[Conventional Intent Detection (ID) models are usually trained offline, which
relies on a fixed dataset and a predefined set of intent classes. However, in
real-world applications, online systems usually involve continually emerging
new user intents, which pose a great challenge to the offline training
paradigm. Recently, lifelong learning has received increasing attention and is
considered to be the most promising solution to this challenge. In this paper,
we propose Lifelong Intent Detection (LID), which continually trains an ID
model on new data to learn newly emerging intents while avoiding
catastrophically forgetting old data. Nevertheless, we find that existing
lifelong learning methods usually suffer from a serious imbalance between old
and new data in the LID task. Therefore, we propose a novel lifelong learning
method, Multi-Strategy Rebalancing (MSR), which consists of cosine
normalization, hierarchical knowledge distillation, and inter-class margin loss
to alleviate the multiple negative effects of the imbalance problem.
Experimental results demonstrate the effectiveness of our method, which
significantly outperforms previous state-of-the-art lifelong learning methods
on the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingbin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaoyan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shizhu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Audio Captioning using Transfer Learning and Reconstruction Latent Space Similarity Regularization. (arXiv:2108.04692v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04692</id>
        <link href="http://arxiv.org/abs/2108.04692"/>
        <updated>2021-08-11T01:55:20.930Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the use of Transfer Learning using Pretrained Audio
Neural Networks (PANNs), and propose an architecture that is able to better
leverage the acoustic features provided by PANNs for the Automated Audio
Captioning Task. We also introduce a novel self-supervised objective,
Reconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module
supplements the training of the model by minimizing the similarity between the
encoder and decoder embedding. The combination of both methods allows us to
surpass state of the art results by a significant margin on the Clotho dataset
across several metrics and benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1"&gt;Andrew Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1"&gt;Eng Siong Chng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00502</id>
        <link href="http://arxiv.org/abs/2010.00502"/>
        <updated>2021-08-11T01:55:20.915Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1"&gt;Gautam Kishore Shahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04556</id>
        <link href="http://arxiv.org/abs/2108.04556"/>
        <updated>2021-08-11T01:55:20.907Z</updated>
        <summary type="html"><![CDATA[Pre-trained models for programming languages have proven their significant
values in various code-related tasks, such as code search, code clone
detection, and code translation. Currently, most pre-trained models treat a
code snippet as a sequence of tokens or only focus on the data flow between
code identifiers. However, rich code syntax and hierarchy are ignored which can
provide important structure information and semantic rules of codes to help
enhance code representations. In addition, although the BERT-based code
pre-trained models achieve high performance on many downstream tasks, the
native derived sequence representations of BERT are proven to be of
low-quality, it performs poorly on code matching and similarity tasks. To
address these problems, we propose CLSEBERT, a Constrastive Learning Framework
for Syntax Enhanced Code Pre-Trained Model, to deal with various code
intelligence tasks. In the pre-training stage, we consider the code syntax and
hierarchy contained in the Abstract Syntax Tree (AST) and leverage the
constrastive learning to learn noise-invariant code representations. Besides
the masked language modeling (MLM), we also introduce two novel pre-training
objectives. One is to predict the edges between nodes in the abstract syntax
tree, and the other is to predict the types of code tokens. Through extensive
experiments on four code intelligence tasks, we successfully show the
effectiveness of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pingyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1"&gt;Meng Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yadao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04718</id>
        <link href="http://arxiv.org/abs/2108.04718"/>
        <updated>2021-08-11T01:55:20.898Z</updated>
        <summary type="html"><![CDATA[In neural machine translation (NMT), we search for the mode of the model
distribution to form predictions. The mode as well as other high probability
translations found by beam search have been shown to often be inadequate in a
number of ways. This prevents practitioners from improving translation quality
through better search, as these idiosyncratic translations end up being
selected by the decoding algorithm, a problem known as the beam search curse.
Recently, a sampling-based approximation to minimum Bayes risk (MBR) decoding
has been proposed as an alternative decision rule for NMT that would likely not
suffer from the same problems. We analyse this approximation and establish that
it has no equivalent to the beam search curse, i.e. better search always leads
to better translations. We also design different approximations aimed at
decoupling the cost of exploration from the cost of robust estimation of
expected utility. This allows for exploration of much larger hypothesis spaces,
which we show to be beneficial. We also show that it can be beneficial to make
use of strategies like beam search and nucleus sampling to construct hypothesis
spaces efficiently. We show on three language pairs (English into and from
German, Romanian, and Nepali) that MBR can improve upon beam search with
moderate computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1"&gt;Bryan Eikema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1"&gt;Wilker Aziz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks. (arXiv:2108.04240v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2108.04240</id>
        <link href="http://arxiv.org/abs/2108.04240"/>
        <updated>2021-08-11T01:55:20.890Z</updated>
        <summary type="html"><![CDATA[The Influenza virus can be considered as one of the most severe viruses that
can infect multiple species with often fatal consequences to the hosts. The
Hemagglutinin (HA) gene of the virus can be a target for antiviral drug
development realised through accurate identification of its sub-types and
possible the targeted hosts. This paper focuses on accurately predicting if an
Influenza type A virus can infect specific hosts, and more specifically, Human,
Avian and Swine hosts, using only the protein sequence of the HA gene. In more
detail, we propose encoding the protein sequences into numerical signals using
the Hydrophobicity Index and subsequently utilising a Convolutional Neural
Network-based predictive model. The Influenza HA protein sequences used in the
proposed work are obtained from the Influenza Research Database (IRD).
Specifically, complete and unique HA protein sequences were used for avian,
human and swine hosts. The data obtained for this work was 17999 human-host
proteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this
set of collected proteins, the proposed method yields as much as 10% higher
accuracy for an individual class (namely, Avian) and 5% higher overall accuracy
than in an earlier study. It is also observed that the accuracy for each class
in this work is more balanced than what was presented in this earlier study. As
the results show, the proposed model can distinguish HA protein sequences with
high accuracy whenever the virus under investigation can infect Human, Avian or
Swine hosts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Chrysostomou_C/0/1/0/all/0/1"&gt;Charalambos Chrysostomou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Alexandrou_F/0/1/0/all/0/1"&gt;Floris Alexandrou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nicolaou_M/0/1/0/all/0/1"&gt;Mihalis A. Nicolaou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Seker_H/0/1/0/all/0/1"&gt;Huseyin Seker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04235</id>
        <link href="http://arxiv.org/abs/2108.04235"/>
        <updated>2021-08-11T01:55:20.882Z</updated>
        <summary type="html"><![CDATA[Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuting Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1"&gt;Gang Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04657</id>
        <link href="http://arxiv.org/abs/2108.04657"/>
        <updated>2021-08-11T01:55:20.866Z</updated>
        <summary type="html"><![CDATA[Multi-head attention, a collection of several attention mechanisms that
independently attend to different parts of the input, is the key ingredient in
the Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a
large proportion of the heads in a Transformer's multi-head attention mechanism
can be safely pruned away without significantly harming the performance of the
model; such pruning leads to models that are noticeably smaller and faster in
practice. Our work introduces a new head pruning technique that we term
differentiable subset pruning. Intuitively, our method learns per-head
importance variables and then enforces a user-specified hard constraint on the
number of unpruned heads. The importance variables are learned via stochastic
gradient descent. We conduct experiments on natural language inference and
machine translation; we show that differentiable subset pruning performs
comparably or better than Voita et al. (2019) while offering the same exact
control over the number of heads as Michel et al. (2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiaoda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. (arXiv:2108.04690v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04690</id>
        <link href="http://arxiv.org/abs/2108.04690"/>
        <updated>2021-08-11T01:55:20.567Z</updated>
        <summary type="html"><![CDATA[Recommendation for new users, also called user cold start, has been a
well-recognized challenge for online recommender systems. Most existing methods
view the crux as the lack of initial data. However, in this paper, we argue
that there are neglected problems: 1) New users' behaviour follows much
different distributions from regular users. 2) Although personalized features
are involved, heavily imbalanced samples prevent the model from balancing
new/regular user distributions, as if the personalized features are
overwhelmed. We name the problem as the ``submergence" of personalization. To
tackle this problem, we propose a novel module: Personalized COld Start MOdules
(POSO). Considering from a model architecture perspective, POSO personalizes
existing modules by introducing multiple user-group-specialized sub-modules.
Then, it fuses their outputs by personalized gates, resulting in comprehensive
representations. In such way, POSO projects imbalanced features to even
modules. POSO can be flexibly integrated into many existing modules and
effectively improves their performance with negligible computational overheads.
The proposed method shows remarkable advantage in industrial scenario. It has
been deployed on the large-scale recommender system of Kwai, and improves new
user Watch Time by a large margin (+7.75%). Moreover, POSO can be further
generalized to regular users, inactive users and returning users (+2%-3% on
Watch Time), as well as item cold start (+3.8% on Watch Time). Its
effectiveness has also been verified on public dataset (MovieLens 20M). We
believe such practical experience can be well generalized to other scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shangfeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haobin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhichen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianying Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Honghuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+PinghuaGong/0/1/0/all/0/1"&gt;PinghuaGong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04452</id>
        <link href="http://arxiv.org/abs/2108.04452"/>
        <updated>2021-08-11T01:55:20.557Z</updated>
        <summary type="html"><![CDATA["High Quality Related Search Query Suggestions" task aims at recommending
search queries which are real, accurate, diverse, relevant and engaging.
Obtaining large amounts of query-quality human annotations is expensive. Prior
work on supervised query suggestion models suffered from selection and exposure
bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),
leading to low quality suggestions. Reinforcement Learning techniques employed
to reformulate a query using terms from search results, have limited
scalability to large-scale industry applications. To recommend high quality
related search queries, we train a Deep Reinforcement Learning model to predict
the query a user would enter next. The reward signal is composed of long-term
session-based user feedback, syntactic relatedness and estimated naturalness of
generated query. Over the baseline supervised model, our proposed approach
achieves a significant relative improvement in terms of recommendation
diversity (3%), down-stream user-engagement (4.2%) and per-sentence word
repetitions (82%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1"&gt;Praveen Kumar Bodigutla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fully Hyperbolic Graph Convolution Network for Recommendation. (arXiv:2108.04607v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04607</id>
        <link href="http://arxiv.org/abs/2108.04607"/>
        <updated>2021-08-11T01:55:20.547Z</updated>
        <summary type="html"><![CDATA[Recently, Graph Convolution Network (GCN) based methods have achieved
outstanding performance for recommendation. These methods embed users and items
in Euclidean space, and perform graph convolution on user-item interaction
graphs. However, real-world datasets usually exhibit tree-like hierarchical
structures, which make Euclidean space less effective in capturing user-item
relationship. In contrast, hyperbolic space, as a continuous analogue of a
tree-graph, provides a promising alternative. In this paper, we propose a fully
hyperbolic GCN model for recommendation, where all operations are performed in
hyperbolic space. Utilizing the advantage of hyperbolic space, our method is
able to embed users/items with less distortion and capture user-item
interaction relationship more accurately. Extensive experiments on public
benchmark datasets show that our method outperforms both Euclidean and
hyperbolic counterparts and requires far lower embedding dimensionality to
achieve comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fenyu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04655</id>
        <link href="http://arxiv.org/abs/2108.04655"/>
        <updated>2021-08-11T01:55:20.511Z</updated>
        <summary type="html"><![CDATA[Collaborative Metric Learning (CML) recently emerged as a powerful paradigm
for recommendation based on implicit feedback collaborative filtering. However,
standard CML methods learn fixed user and item representations, which fails to
capture the complex interests of users. Existing extensions of CML also either
ignore the heterogeneity of user-item relations, i.e. that a user can
simultaneously like very different items, or the latent item-item relations,
i.e. that a user's preference for an item depends, not only on its intrinsic
characteristics, but also on items they previously interacted with. In this
paper, we present a hierarchical CML model that jointly captures latent
user-item and item-item relations from implicit data. Our approach is inspired
by translation mechanisms from knowledge graph embedding and leverages
memory-based attention networks. We empirically show the relevance of this
joint relational modeling, by outperforming existing CML models on
recommendation tasks on several real-world datasets. Our experiments also
emphasize the limits of current CML relational models on very sparse datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Viet-Anh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1"&gt;Guillaume Salha-Galvan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1"&gt;Romain Hennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1"&gt;Manuel Moussallam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04603</id>
        <link href="http://arxiv.org/abs/2108.04603"/>
        <updated>2021-08-11T01:55:20.491Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel model for recognizing images with composite
attribute-object concepts, notably for composite concepts that are unseen
during model training. We aim to explore the three key properties required by
the task --- relation-aware, consistent, and decoupled --- to learn rich and
robust features for primitive concepts that compose attribute-object pairs. To
this end, we propose the Blocked Message Passing Network (BMP-Net). The model
consists of two modules. The concept module generates semantically meaningful
features for primitive concepts, whereas the visual module extracts visual
features for attributes and objects from input images. A message passing
mechanism is used in the concept module to capture the relations between
primitive concepts. Furthermore, to prevent the model from being biased towards
seen composite concepts and reduce the entanglement between attributes and
objects, we propose a blocking mechanism that equalizes the information
available to the model for both seen and unseen concepts. Extensive experiments
and ablation studies on two benchmarks show the efficacy of the proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1"&gt;Yongkang Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1"&gt;Mohan Kankanhalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. (arXiv:2108.04468v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04468</id>
        <link href="http://arxiv.org/abs/2108.04468"/>
        <updated>2021-08-11T01:55:20.482Z</updated>
        <summary type="html"><![CDATA[Click-Through Rate (CTR) prediction is one of the core tasks in recommender
systems (RS). It predicts a personalized click probability for each user-item
pair. Recently, researchers have found that the performance of CTR model can be
improved greatly by taking user behavior sequence into consideration,
especially long-term user behavior sequence. The report on an e-commerce
website shows that 23\% of users have more than 1000 clicks during the past 5
months. Though there are numerous works focus on modeling sequential user
behaviors, few works can handle long-term user behavior sequence due to the
strict inference time constraint in real world system. Two-stage methods are
proposed to push the limit for better performance. At the first stage, an
auxiliary task is designed to retrieve the top-$k$ similar items from long-term
user behavior sequence. At the second stage, the classical attention mechanism
is conducted between the candidate item and $k$ items selected in the first
stage. However, information gap happens between retrieval stage and the main
CTR task. This goal divergence can greatly diminishing the performance gain of
long-term user sequence. In this paper, inspired by Reformer, we propose a
locality-sensitive hashing (LSH) method called ETA (End-to-end Target
Attention) which can greatly reduce the training and inference cost and make
the end-to-end training with long-term user behavior sequence possible. Both
offline and online experiments confirm the effectiveness of our model. We
deploy ETA into a large-scale real world E-commerce system and achieve extra
3.1\% improvements on GMV (Gross Merchandise Value) compared to a two-stage
long user sequence CTR model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1"&gt;Changhua Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1"&gt;Shanshan Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1"&gt;Junfeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1"&gt;Wenwu Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Localized Graph Collaborative Filtering. (arXiv:2108.04475v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04475</id>
        <link href="http://arxiv.org/abs/2108.04475"/>
        <updated>2021-08-11T01:55:20.455Z</updated>
        <summary type="html"><![CDATA[User-item interactions in recommendations can be naturally de-noted as a
user-item bipartite graph. Given the success of graph neural networks (GNNs) in
graph representation learning, GNN-based C methods have been proposed to
advance recommender systems. These methods often make recommendations based on
the learned user and item embeddings. However, we found that they do not
perform well wit sparse user-item graphs which are quite common in real-world
recommendations. Therefore, in this work, we introduce a novel perspective to
build GNN-based CF methods for recommendations which leads to the proposed
framework Localized Graph Collaborative Filtering (LGCF). One key advantage of
LGCF is that it does not need to learn embeddings for each user and item, which
is challenging in sparse scenarios.

Alternatively, LGCF aims at encoding useful CF information into a localized
graph and making recommendations based on such graph. Extensive experiments on
various datasets validate the effectiveness of LGCF especially in sparse
scenarios. Furthermore, empirical results demonstrate that LGCF provides
complementary information to the embedding-based CF model which can be utilized
to boost recommendation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaozhuo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mingzheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Wei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Open Framework for Analyzing and Modeling XR Network Traffic. (arXiv:2108.04577v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.04577</id>
        <link href="http://arxiv.org/abs/2108.04577"/>
        <updated>2021-08-11T01:55:20.435Z</updated>
        <summary type="html"><![CDATA[Thanks to recent advancements in the technology, eXtended Reality (XR)
applications are gaining a lot of momentum, and they will surely become
increasingly popular in the next decade. These new applications, however,
require a step forward also in terms of models to simulate and analyze this
type of traffic sources in modern communication networks, in order to guarantee
to the users state of the art performance and Quality of Experience (QoE).
Recognizing this need, in this work, we present a novel open-source traffic
model, which researchers can use as a starting point both for improvements of
the model itself and for the design of optimized algorithms for the
transmission of these peculiar data flows. Along with the mathematical model
and the code, we also share with the community the traces that we gathered for
our study, collected from freely available applications such as Minecraft VR,
Google Earth VR, and Virus Popper. Finally, we propose a roadmap for the
construction of an end-to-end framework that fills this gap in the current
state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lecci_M/0/1/0/all/0/1"&gt;Mattia Lecci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drago_M/0/1/0/all/0/1"&gt;Matteo Drago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1"&gt;Andrea Zanella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1"&gt;Michele Zorzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04294</id>
        <link href="http://arxiv.org/abs/2108.04294"/>
        <updated>2021-08-11T01:55:20.416Z</updated>
        <summary type="html"><![CDATA[Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1"&gt;Alejandro Pardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1"&gt;Fabian Caba Heilbron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1"&gt;Juan Le&amp;#xf3;n Alc&amp;#xe1;zar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1"&gt;Ali Thabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13689</id>
        <link href="http://arxiv.org/abs/2103.13689"/>
        <updated>2021-08-11T01:55:20.299Z</updated>
        <summary type="html"><![CDATA[Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1"&gt;Xianbo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Shunquan Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiwu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Neural Network Approach for Crop Selection and Yield Prediction in Bangladesh. (arXiv:2108.03320v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03320</id>
        <link href="http://arxiv.org/abs/2108.03320"/>
        <updated>2021-08-10T02:00:12.259Z</updated>
        <summary type="html"><![CDATA[Agriculture is the essential ingredients to mankind which is a major source
of livelihood. Agriculture work in Bangladesh is mostly done in old ways which
directly affects our economy. In addition, institutions of agriculture are
working with manual data which cannot provide a proper solution for crop
selection and yield prediction. This paper shows the best way of crop selection
and yield prediction in minimum cost and effort. Artificial Neural Network is
considered robust tools for modeling and prediction. This algorithm aims to get
better output and prediction, as well as, support vector machine, Logistic
Regression, and random forest algorithm is also considered in this study for
comparing the accuracy and error rate. Moreover, all of these algorithms used
here are just to see how well they performed for a dataset which is over 0.3
million. We have collected 46 parameters such as maximum and minimum
temperature, average rainfall, humidity, climate, weather, and types of land,
types of chemical fertilizer, types of soil, soil structure, soil composition,
soil moisture, soil consistency, soil reaction and soil texture for applying
into this prediction process. In this paper, we have suggested using the deep
neural network for agricultural crop selection and yield prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tanhim Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chisty_T/0/1/0/all/0/1"&gt;Tanjir Alam Chisty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1"&gt;Amitabha Chakrabarty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score Matching Model for Unbounded Data Score. (arXiv:2106.05527v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05527</id>
        <link href="http://arxiv.org/abs/2106.05527"/>
        <updated>2021-08-10T02:00:12.224Z</updated>
        <summary type="html"><![CDATA[Recent advance in diffusion models incorporates the Stochastic Differential
Equation (SDE), which brings the state-of-the art performance on image
generation tasks. This paper improves such diffusion models by analyzing the
model at the zero diffusion time. In real datasets, the score function diverges
as the diffusion time ($t$) decreases to zero, and this observation leads an
argument that the score estimation fails at $t=0$ with any neural network
structure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that
resolves the score diverging problem with an easily applicable modification to
any diffusion models. Additionally, we introduce a new SDE that overcomes the
theoretic and practical limitations of Variance Exploding SDE. On top of that,
the introduced Soft Truncation method improves the sample quality by mitigating
the loss scale issue that happens at $t=0$. We further provide a theoretic
result of the proposed method to uncover the behind mechanism of the diffusion
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongjun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1"&gt;Seungjae Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kyungwoo Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wanmo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1"&gt;Il-Chul Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network for DrawiNg Networks, (DNN)^2. (arXiv:2108.03632v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03632</id>
        <link href="http://arxiv.org/abs/2108.03632"/>
        <updated>2021-08-10T02:00:12.211Z</updated>
        <summary type="html"><![CDATA[By leveraging recent progress of stochastic gradient descent methods, several
works have shown that graphs could be efficiently laid out through the
optimization of a tailored objective function. In the meantime, Deep Learning
(DL) techniques achieved great performances in many applications. We
demonstrate that it is possible to use DL techniques to learn a graph-to-layout
sequence of operations thanks to a graph-related objective function. In this
paper, we present a novel graph drawing framework called (DNN)^2: Deep Neural
Network for DrawiNg Networks. Our method uses Graph Convolution Networks to
learn a model. Learning is achieved by optimizing a graph topology related loss
function that evaluates (DNN)^2 generated layouts during training. Once
trained, the (DNN)^ model is able to quickly lay any input graph out. We
experiment (DNN)^2 and statistically compare it to optimization-based and
regular graph layout algorithms. The results show that (DNN)^2 performs well
and are encouraging as the Deep Learning approach to Graph Drawing is novel and
many leads for future works are identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giovannangeli_L/0/1/0/all/0/1"&gt;Loann Giovannangeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lalanne_F/0/1/0/all/0/1"&gt;Frederic Lalanne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auber_D/0/1/0/all/0/1"&gt;David Auber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1"&gt;Romain Giot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourqui_R/0/1/0/all/0/1"&gt;Romain Bourqui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations. (arXiv:2108.03561v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03561</id>
        <link href="http://arxiv.org/abs/2108.03561"/>
        <updated>2021-08-10T02:00:12.192Z</updated>
        <summary type="html"><![CDATA[We present a supervised learning method to learn the propagator map of a
dynamical system from partial and noisy observations. In our computationally
cheap and easy-to-implement framework a neural network consisting of random
feature maps is trained sequentially by incoming observations within a data
assimilation procedure. By employing Takens' embedding theorem, the network is
trained on delay coordinates. We show that the combination of random feature
maps and data assimilation, called RAFDA, outperforms standard random feature
maps for which the dynamics is learned using batch data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gottwald_G/0/1/0/all/0/1"&gt;Georg A. Gottwald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1"&gt;Sebastian Reich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Linear Policies for Robust Bipedal Locomotion on Terrains with Varying Slopes. (arXiv:2104.01662v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01662</id>
        <link href="http://arxiv.org/abs/2104.01662"/>
        <updated>2021-08-10T02:00:12.186Z</updated>
        <summary type="html"><![CDATA[In this paper, with a view toward deployment of light-weight control
frameworks for bipedal walking robots, we realize end-foot trajectories that
are shaped by a single linear feedback policy. We learn this policy via a
model-free and a gradient-free learning algorithm, Augmented Random Search
(ARS), in the two robot platforms Rabbit and Digit. Our contributions are
two-fold: a) By using torso and support plane orientation as inputs, we achieve
robust walking on slopes of up to 20 degrees in simulation. b) We demonstrate
additional behaviors like walking backwards, stepping-in-place, and recovery
from external pushes of up to 120 N. The end result is a robust and a fast
feedback control law for bipedal walking on terrains with varying slopes.
Towards the end, we also provide preliminary results of hardware transfer to
Digit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_L/0/1/0/all/0/1"&gt;Lokesh Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_U/0/1/0/all/0/1"&gt;Utkarsh A. Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castillo_G/0/1/0/all/0/1"&gt;Guillermo A. Castillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hereid_A/0/1/0/all/0/1"&gt;Ayonga Hereid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolathaya_S/0/1/0/all/0/1"&gt;Shishir Kolathaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03357</id>
        <link href="http://arxiv.org/abs/2108.03357"/>
        <updated>2021-08-10T02:00:12.178Z</updated>
        <summary type="html"><![CDATA[Traditional recommendation systems are faced with two long-standing
obstacles, namely, data sparsity and cold-start problems, which promote the
emergence and development of Cross-Domain Recommendation (CDR). The core idea
of CDR is to leverage information collected from other domains to alleviate the
two problems in one domain. Over the last decade, many efforts have been
engaged for cross-domain recommendation. Recently, with the development of deep
learning and neural networks, a large number of methods have emerged. However,
there is a limited number of systematic surveys on CDR, especially regarding
the latest proposed methods as well as the recommendation scenarios and
recommendation tasks they address. In this survey paper, we first proposed a
two-level taxonomy of cross-domain recommendation which classifies different
recommendation scenarios and recommendation tasks. We then introduce and
summarize existing cross-domain recommendation approaches under different
recommendation scenarios in a structured manner. We also organize datasets
commonly used. We conclude this survey by providing several potential research
directions about this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1"&gt;Tianzi Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanmin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haobing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiadi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (arXiv:2107.09804v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09804</id>
        <link href="http://arxiv.org/abs/2107.09804"/>
        <updated>2021-08-10T02:00:12.171Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) classifiers are powerful tools that drive a broad
spectrum of important applications, from image recognition to autonomous
vehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks
that affect virtually all state-of-the-art models. These attacks make small
imperceptible modifications to inputs that are sufficient to induce the DNNs to
produce the wrong classification.

In this paper we propose a novel, lightweight adversarial correction and/or
detection mechanism for image classifiers that relies on undervolting (running
a chip at a voltage that is slightly below its safe margin). We propose using
controlled undervolting of the chip running the inference process in order to
introduce a limited number of compute errors. We show that these errors disrupt
the adversarial input in a way that can be used either to correct the
classification or detect the input as adversarial. We evaluate the proposed
solution in an FPGA design and through software simulation. We evaluate 10
attacks and show average detection rates of 77% and 90% on two popular DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.03576</id>
        <link href="http://arxiv.org/abs/2108.03576"/>
        <updated>2021-08-10T02:00:12.164Z</updated>
        <summary type="html"><![CDATA[The online estimation of rhythmic information, such as beat positions,
downbeat positions, and meter, is critical for many real-time music
applications. Musical rhythm comprises complex hierarchical relationships
across time, rendering its analysis intrinsically challenging and at times
subjective. Furthermore, systems which attempt to estimate rhythmic information
in real-time must be causal and must produce estimates quickly and efficiently.
In this work, we introduce an online system for joint beat, downbeat, and meter
tracking, which utilizes causal convolutional and recurrent layers, followed by
a pair of sequential Monte Carlo particle filters applied during inference. The
proposed system does not need to be primed with a time signature in order to
perform downbeat tracking, and is instead able to estimate meter and adjust the
predictions over time. Additionally, we propose an information gate strategy to
significantly decrease the computational cost of particle filtering during the
inference step, making the system much faster than previous sampling-based
methods. Experiments on the GTZAN dataset, which is unseen during training,
show that the system outperforms various online beat and downbeat tracking
systems and achieves comparable performance to a baseline offline joint method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1"&gt;Mojtaba Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1"&gt;Frank Cwitkowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhiyao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concept Drift Detection with Variable Interaction Networks. (arXiv:2108.03273v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03273</id>
        <link href="http://arxiv.org/abs/2108.03273"/>
        <updated>2021-08-10T02:00:12.130Z</updated>
        <summary type="html"><![CDATA[The current development of today's production industry towards seamless
sensor-based monitoring is paving the way for concepts such as Predictive
Maintenance. By this means, the condition of plants and products in future
production lines will be continuously analyzed with the objective to predict
any kind of breakdown and trigger preventing actions proactively. Such
ambitious predictions are commonly performed with support of machine learning
algorithms. In this work, we utilize these algorithms to model complex systems,
such as production plants, by focusing on their variable interactions. The core
of this contribution is a sliding window based algorithm, designed to detect
changes of the identified interactions, which might indicate beginning
malfunctions in the context of a monitored production plant. Besides a detailed
description of the algorithm, we present results from experiments with a
synthetic dynamical system, simulating stable and drifting system behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zenisek_J/0/1/0/all/0/1"&gt;Jan Zenisek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfartsberger_J/0/1/0/all/0/1"&gt;Josef Wolfartsberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wild_N/0/1/0/all/0/1"&gt;Norbert Wild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1"&gt;Michael Affenzeller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-08-10T02:00:12.123Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2108.03465</id>
        <link href="http://arxiv.org/abs/2108.03465"/>
        <updated>2021-08-10T02:00:12.097Z</updated>
        <summary type="html"><![CDATA[With the rapid spread of the novel coronavirus (COVID-19) across the globe
and its continuous mutation, it is of pivotal importance to design a system to
identify different known (and unknown) variants of SARS-CoV-2. Identifying
particular variants helps to understand and model their spread patterns, design
effective mitigation strategies, and prevent future outbreaks. It also plays a
crucial role in studying the efficacy of known vaccines against each variant
and modeling the likelihood of breakthrough infections. It is well known that
the spike protein contains most of the information/variation pertaining to
coronavirus variants.

In this paper, we use spike sequences to classify different variants of the
coronavirus in humans. We show that preserving the order of the amino acids
helps the underlying classifiers to achieve better performance. We also show
that we can train our model to outperform the baseline algorithms using only a
small number of training samples ($1\%$ of the data). Finally, we show the
importance of the different amino acids which play a key role in identifying
variants and how they coincide with those reported by the USA's Centers for
Disease Control and Prevention (CDC).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sarwan Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1"&gt;Bikram Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1"&gt;Naimat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1"&gt;Alexander Zelikovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1"&gt;Murray Patterson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1"&gt;Imdadullah Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Graph Dimension with Cross-validated Eigenvalues. (arXiv:2108.03336v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2108.03336</id>
        <link href="http://arxiv.org/abs/2108.03336"/>
        <updated>2021-08-10T02:00:12.075Z</updated>
        <summary type="html"><![CDATA[In applied multivariate statistics, estimating the number of latent
dimensions or the number of clusters is a fundamental and recurring problem.
One common diagnostic is the scree plot, which shows the largest eigenvalues of
the data matrix; the user searches for a "gap" or "elbow" in the decreasing
eigenvalues; unfortunately, these patterns can hide beneath the bias of the
sample eigenvalues. This methodological problem is conceptually difficult
because, in many situations, there is only enough signal to detect a subset of
the $k$ population dimensions/eigenvectors. In this situation, one could argue
that the correct choice of $k$ is the number of detectable dimensions. We
alleviate these problems with cross-validated eigenvalues. Under a large class
of random graph models, without any parametric assumptions, we provide a
p-value for each sample eigenvector. It tests the null hypothesis that this
sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent
dimensions. This approach naturally adapts to problems where some dimensions
are not statistically detectable. In scenarios where all $k$ dimensions can be
estimated, we prove that our procedure consistently estimates $k$. In
simulations and a data example, the proposed estimator compares favorably to
alternative approaches in both computational and statistical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roch_S/0/1/0/all/0/1"&gt;Sebastien Roch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1"&gt;Karl Rohe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shuqi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03649</id>
        <link href="http://arxiv.org/abs/2108.03649"/>
        <updated>2021-08-10T02:00:12.067Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to joint depth and normal estimation for
time-of-flight (ToF) sensors. Our model learns to predict the high-quality
depth and normal maps jointly from ToF raw sensor data. To achieve this, we
meticulously constructed the first large-scale dataset (named ToF-100) with
paired raw ToF data and ground-truth high-resolution depth maps provided by an
industrial depth camera. In addition, we also design a simple but effective
framework for joint depth and normal estimation, applying a robust Chamfer loss
via jittering to improve the performance of our model. Our experiments
demonstrate that our proposed method can efficiently reconstruct
high-resolution depth and normal maps and significantly outperforms
state-of-the-art approaches. Our code and data will be available at
\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Rongrong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1"&gt;Na Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters. (arXiv:2108.03645v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.03645</id>
        <link href="http://arxiv.org/abs/2108.03645"/>
        <updated>2021-08-10T02:00:12.058Z</updated>
        <summary type="html"><![CDATA[Efficient GPU resource scheduling is essential to maximize resource
utilization and save training costs for the increasing amount of deep learning
workloads in shared GPU clusters. Existing GPU schedulers largely rely on
static policies to leverage the performance characteristics of deep learning
jobs. However, they can hardly reach optimal efficiency due to the lack of
elasticity. To address the problem, we propose ONES, an ONline Evolutionary
Scheduler for elastic batch size orchestration. ONES automatically manages the
elasticity of each job based on the training batch size, so as to maximize GPU
utilization and improve scheduling efficiency. It determines the batch size for
each job through an online evolutionary search that can continuously optimize
the scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on
TACC's Longhorn supercomputers. The results show that ONES can outperform the
prior deep learning schedulers with a significantly shorter average job
completion time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1"&gt;Zhengda Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shenggui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02346</id>
        <link href="http://arxiv.org/abs/2012.02346"/>
        <updated>2021-08-10T02:00:12.030Z</updated>
        <summary type="html"><![CDATA[A point cloud serves as a representation of the surface of a
three-dimensional (3D) shape. Deep generative models have been adapted to model
their variations typically using a map from a ball-like set of latent
variables. However, previous approaches did not pay much attention to the
topological structure of a point cloud, despite that a continuous map cannot
express the varying numbers of holes and intersections. Moreover, a point cloud
is often composed of multiple subparts, and it is also difficult to express. In
this study, we propose ChartPointFlow, a flow-based generative model with
multiple latent labels for 3D point clouds. Each label is assigned to points in
an unsupervised manner. Then, a map conditioned on a label is assigned to a
continuous subset of a point cloud, similar to a chart of a manifold. This
enables our proposed model to preserve the topological structure with clear
boundaries, whereas previous approaches tend to generate blurry point clouds
and fail to generate holes. The experimental results demonstrate that
ChartPointFlow achieves state-of-the-art performance in terms of generation and
reconstruction compared with other point cloud generators. Moreover,
ChartPointFlow divides an object into semantic subparts using charts, and it
demonstrates superior performance in case of unsupervised segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1"&gt;Takumi Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takashi Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1"&gt;Kuniaki Uehara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03272</id>
        <link href="http://arxiv.org/abs/2108.03272"/>
        <updated>2021-08-10T02:00:12.020Z</updated>
        <summary type="html"><![CDATA[Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset will be publicly available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1"&gt;Michael Lingelbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1"&gt;Cem Gokmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1"&gt;Gokul Dharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1"&gt;Tanish Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1"&gt;Andrey Kurenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1"&gt;Hyowon Gweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of statistical learning techniques as applied to inexpensive pediatric Obstructive Sleep Apnea data. (arXiv:2002.07873v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07873</id>
        <link href="http://arxiv.org/abs/2002.07873"/>
        <updated>2021-08-10T02:00:12.012Z</updated>
        <summary type="html"><![CDATA[Pediatric obstructive sleep apnea affects an estimated 1-5% of
elementary-school aged children and can lead to other detrimental health
problems. Swift diagnosis and treatment are critical to a child's growth and
development, but the variability of symptoms and the complexity of the
available data make this a challenge. We take a first step in streamlining the
process by focusing on inexpensive data from questionnaires and craniofacial
measurements. We apply correlation networks, the Mapper algorithm from
topological data analysis, and singular value decomposition in a process of
exploratory data analysis. We then apply a variety of supervised and
unsupervised learning techniques from statistics, machine learning, and
topology, ranging from support vector machines to Bayesian classifiers and
manifold learning. Finally, we analyze the results of each of these methods and
discuss the implications for a multi-data-sourced algorithm moving forward.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Winn_E/0/1/0/all/0/1"&gt;Emily T. Winn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Vazquez_M/0/1/0/all/0/1"&gt;Marilyn Vazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Loliencar_P/0/1/0/all/0/1"&gt;Prachi Loliencar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Taipale_K/0/1/0/all/0/1"&gt;Kaisa Taipale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Heo_G/0/1/0/all/0/1"&gt;Giseon Heo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kinematics clustering enables head impact subtyping for better traumatic brain injury prediction. (arXiv:2108.03498v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.03498</id>
        <link href="http://arxiv.org/abs/2108.03498"/>
        <updated>2021-08-10T02:00:12.004Z</updated>
        <summary type="html"><![CDATA[Traumatic brain injury can be caused by various types of head impacts.
However, due to different kinematic characteristics, many brain injury risk
estimation models are not generalizable across the variety of impacts that
humans may sustain. The current definitions of head impact subtypes are based
on impact sources (e.g., football, traffic accident), which may not reflect the
intrinsic kinematic similarities of impacts across the impact sources. To
investigate the potential new definitions of impact subtypes based on
kinematics, 3,161 head impacts from various sources including simulation,
college football, mixed martial arts, and car racing were collected. We applied
the K-means clustering to cluster the impacts on 16 standardized temporal
features from head rotation kinematics. Then, we developed subtype-specific
ridge regression models for cumulative strain damage (using the threshold of
15%), which significantly improved the estimation accuracy compared with the
baseline method which mixed impacts from different sources and developed one
model (R^2 from 0.7 to 0.9). To investigate the effect of kinematic features,
we presented the top three critical features (maximum resultant angular
acceleration, maximum angular acceleration along the z-axis, maximum linear
acceleration along the y-axis) based on regression accuracy and used logistic
regression to find the critical points for each feature that partitioned the
subtypes. This study enables researchers to define head impact subtypes in a
data-driven manner, which leads to more generalizable brain injury risk
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuzhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cecchi_N/0/1/0/all/0/1"&gt;Nicholas J. Cecchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gevaert_O/0/1/0/all/0/1"&gt;Olivier Gevaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zeineh_M/0/1/0/all/0/1"&gt;Michael M. Zeineh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grant_G/0/1/0/all/0/1"&gt;Gerald A. Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Camarillo_D/0/1/0/all/0/1"&gt;David B. Camarillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03489</id>
        <link href="http://arxiv.org/abs/2108.03489"/>
        <updated>2021-08-10T02:00:11.986Z</updated>
        <summary type="html"><![CDATA[We investigate the impact of aliasing on generalization in Deep Convolutional
Networks and show that data augmentation schemes alone are unable to prevent it
due to structural limitations in widely used architectures. Drawing insights
from frequency analysis theory, we take a closer look at ResNet and
EfficientNet architectures and review the trade-off between aliasing and
information loss in each of their major components. We show how to mitigate
aliasing by inserting non-trainable low-pass filters at key locations,
particularly where networks lack the capacity to learn them. These simple
architectural changes lead to substantial improvements in generalization on
i.i.d. and even more on out-of-distribution conditions, such as image
classification under natural corruptions on ImageNet-C [11] and few-shot
learning on Meta-Dataset [26]. State-of-the art results are achieved on both
datasets without introducing additional trainable parameters and using the
default hyper-parameters of open source codebases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1"&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1"&gt;Ross Goroshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Neuroimaging Analysis using Federated Learning with Homomorphic Encryption. (arXiv:2108.03437v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.03437</id>
        <link href="http://arxiv.org/abs/2108.03437"/>
        <updated>2021-08-10T02:00:11.980Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) enables distributed computation of machine learning
models over various disparate, remote data sources, without requiring to
transfer any individual data to a centralized location. This results in an
improved generalizability of models and efficient scaling of computation as
more sources and larger datasets are added to the federation. Nevertheless,
recent membership attacks show that private or sensitive personal data can
sometimes be leaked or inferred when model parameters or summary statistics are
shared with a central site, requiring improved security solutions. In this
work, we propose a framework for secure FL using fully-homomorphic encryption
(FHE). Specifically, we use the CKKS construction, an approximate, floating
point compatible scheme that benefits from ciphertext packing and rescaling. In
our evaluation on large-scale brain MRI datasets, we use our proposed secure FL
framework to train a deep learning model to predict a person's age from
distributed MRI scans, a common benchmarking task, and demonstrate that there
is no degradation in the learning performance between the encrypted and
non-encrypted federated models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1"&gt;Dimitris Stripelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleem_H/0/1/0/all/0/1"&gt;Hamza Saleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghai_T/0/1/0/all/0/1"&gt;Tanmay Ghai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhinagar_N/0/1/0/all/0/1"&gt;Nikhil Dhinagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Umang Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1"&gt;Chrysovalantis Anastasiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1"&gt;Greg Ver Steeg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Srivatsan Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naveed_M/0/1/0/all/0/1"&gt;Muhammad Naveed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1"&gt;Paul M. Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1"&gt;Jose Luis Ambite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchored-STFT and GNAA: An extension of STFT in conjunction with an adversarial data augmentation technique for the decoding of neural signals. (arXiv:2011.14694v4 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14694</id>
        <link href="http://arxiv.org/abs/2011.14694"/>
        <updated>2021-08-10T02:00:11.972Z</updated>
        <summary type="html"><![CDATA[Brain-computer interfaces (BCIs) enable communication between humans and
machines by translating brain activity into control commands.
Electroencephalography (EEG) signals are one of the most used brain signals in
non-invasive BCI applications but are often contaminated with noise. Therefore,
it is possible that meaningful patterns for classifying EEG signals are deeply
hidden. State-of-the-art deep-learning algorithms are successful in learning
hidden, meaningful patterns. However, the quality and the quantity of the
presented inputs is pivotal. Here, we propose a novel feature extraction method
called anchored Short Time Fourier Transform (anchored-STFT), which is an
advanced version of STFT, as it minimizes the trade-off between temporal and
spectral resolution presented by STFT. In addition, we propose a novel
augmentation method, called gradient norm adversarial augmentation (GNAA). GNAA
is not only an augmentation method but is also used to harness adversarial
inputs in EEG data, which not only improves the classification accuracy but
also enhances the robustness of the classifier. In addition, we also propose a
new CNN architecture, namely Skip-Net, for the classification of EEG signals.
The proposed pipeline outperforms all state-of-the-art methods and yields an
average classification accuracy of 90.7 % and 89.54 % on BCI competition II
dataset III and BCI competition IV dataset 2b, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ali_O/0/1/0/all/0/1"&gt;Omair Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1"&gt;Muhammad Saif-ur-Rehman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Dyck_S/0/1/0/all/0/1"&gt;Susanne Dyck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Glasmachers_T/0/1/0/all/0/1"&gt;Tobias Glasmachers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Iossifidis_I/0/1/0/all/0/1"&gt;Ioannis Iossifidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Klaes_C/0/1/0/all/0/1"&gt;Christian Klaes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Matters in Learning from Offline Human Demonstrations for Robot Manipulation. (arXiv:2108.03298v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03298</id>
        <link href="http://arxiv.org/abs/2108.03298"/>
        <updated>2021-08-10T02:00:11.964Z</updated>
        <summary type="html"><![CDATA[Imitating human demonstrations is a promising approach to endow robots with
various manipulation capabilities. While recent advances have been made in
imitation learning and batch (offline) reinforcement learning, a lack of
open-source human datasets and reproducible learning methods make assessing the
state of the field difficult. In this paper, we conduct an extensive study of
six offline learning algorithms for robot manipulation on five simulated and
three real-world multi-stage manipulation tasks of varying complexity, and with
datasets of varying quality. Our study analyzes the most critical challenges
when learning from offline human data for manipulation. Based on the study, we
derive a series of lessons including the sensitivity to different algorithmic
design choices, the dependence on the quality of the demonstrations, and the
variability based on the stopping criteria due to the different objectives in
training and evaluation. We also highlight opportunities for learning from
human datasets, such as the ability to learn proficient policies on
challenging, multi-stage tasks beyond the scope of current reinforcement
learning methods, and the ability to easily scale to natural, real-world
manipulation scenarios where only raw sensory signals are available. We have
open-sourced our datasets and all algorithm implementations to facilitate
future research and fair comparisons in learning from human demonstration data.
Codebase, datasets, trained models, and more available at
https://arise-initiative.github.io/robomimic-web/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1"&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Josiah Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasiriany_S/0/1/0/all/0/1"&gt;Soroush Nasiriany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1"&gt;Rohun Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03478</id>
        <link href="http://arxiv.org/abs/2108.03478"/>
        <updated>2021-08-10T02:00:11.956Z</updated>
        <summary type="html"><![CDATA[Deep learning is providing very positive results in areas related to
conversational interfaces, such as speech recognition, but its potential
benefit for dialog management has still not been fully studied. In this paper,
we perform an assessment of different configurations for deep-learned dialog
management with three dialog corpora from different application domains and
varying in size, dimensionality and possible system responses. Our results have
allowed us to identify several aspects that can have an impact on accuracy,
including the approaches used for feature extraction, input representation,
context consideration and the hyper-parameters of the deep neural networks
employed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1"&gt;Luk&amp;#xe1;&amp;#x161; Mat&amp;#x11b;j&amp;#x16f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1"&gt;David Griol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1"&gt;Zoraida Callejas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Manuel Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1"&gt;Araceli Sanchis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10684</id>
        <link href="http://arxiv.org/abs/2009.10684"/>
        <updated>2021-08-10T02:00:11.949Z</updated>
        <summary type="html"><![CDATA[Despite efforts to distinguish three different evaluation setups (Bekoulis et
al., 2018), numerous end-to-end Relation Extraction (RE) articles present
unreliable performance comparison to previous work. In this paper, we first
identify several patterns of invalid comparisons in published papers and
describe them to avoid their propagation. We then propose a small empirical
study to quantify the impact of the most common mistake and evaluate it leads
to overestimating the final RE performance by around 5% on ACE05. We also seize
this opportunity to study the unexplored ablations of two recent developments:
the use of language model pretraining (specifically BERT) and span-level NER.
This meta-analysis emphasizes the need for rigor in the report of both the
evaluation setting and the datasets statistics and we call for unifying the
evaluation setting in end-to-end RE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1"&gt;Bruno Taill&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1"&gt;Vincent Guigue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMOTified-GAN for class imbalanced pattern classification problems. (arXiv:2108.03235v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03235</id>
        <link href="http://arxiv.org/abs/2108.03235"/>
        <updated>2021-08-10T02:00:11.931Z</updated>
        <summary type="html"><![CDATA[Class imbalance in a dataset is a major problem for classifiers that results
in poor prediction with a high true positive rate (TPR) but a low true negative
rate (TNR) for a majority positive training dataset. Generally, the
pre-processing technique of oversampling of minority class(es) are used to
overcome this deficiency. Our focus is on using the hybridization of Generative
Adversarial Network (GAN) and Synthetic Minority Over-Sampling Technique
(SMOTE) to address class imbalanced problems. We propose a novel two-phase
oversampling approach that has the synergy of SMOTE and GAN. The initial data
of minority class(es) generated by SMOTE is further enhanced by GAN that
produces better quality samples. We named it SMOTified-GAN as GAN works on
pre-sampled minority data produced by SMOTE rather than randomly generating the
samples itself. The experimental results prove the sample quality of minority
class(es) has been improved in a variety of tested benchmark datasets. Its
performance is improved by up to 9\% from the next best algorithm tested on
F1-score measurements. Its time complexity is also reasonable which is around
$O(N^2d^2T)$ for a sequential algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Anuraganand Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Prabhat Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohitash Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations. (arXiv:2108.03712v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.03712</id>
        <link href="http://arxiv.org/abs/2108.03712"/>
        <updated>2021-08-10T02:00:11.923Z</updated>
        <summary type="html"><![CDATA[This paper tackles the data-driven approximation of unknown dynamical systems
using Koopman-operator methods. Given a dictionary of functions, these methods
approximate the projection of the action of the operator on the
finite-dimensional subspace spanned by the dictionary. We propose the Tunable
Symmetric Subspace Decomposition algorithm to refine the dictionary, balancing
its expressiveness and accuracy. Expressiveness corresponds to the ability of
the dictionary to describe the evolution of as many observables as possible and
accuracy corresponds to the ability to correctly predict their evolution. Based
on the observation that Koopman-invariant subspaces give rise to exact
predictions, we reason that prediction accuracy is a function of the degree of
invariance of the subspace generated by the dictionary and provide a
data-driven measure to measure invariance proximity. The proposed algorithm
iteratively prunes the initial functional space to identify a refined
dictionary of functions that satisfies the desired level of accuracy while
retaining as much of the original expressiveness as possible. We provide a full
characterization of the algorithm properties and show that it generalizes both
Extended Dynamic Mode Decomposition and Symmetric Subspace Decomposition.
Simulations on planar systems show the effectiveness of the proposed methods in
producing Koopman approximations of tunable accuracy that capture relevant
information about the dynamical system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Haseli_M/0/1/0/all/0/1"&gt;Masih Haseli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cortes_J/0/1/0/all/0/1"&gt;Jorge Cort&amp;#xe9;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03579</id>
        <link href="http://arxiv.org/abs/2108.03579"/>
        <updated>2021-08-10T02:00:11.916Z</updated>
        <summary type="html"><![CDATA[The goals of this paper are two-fold. The first goal is to serve as an
expository tutorial on the working of deep learning models which emphasizes
geometrical intuition about the reasons for success of deep learning. The
second goal is to complement the current results on the expressive power of
deep learning models and their loss surfaces with novel insights and results.
In particular, we describe how deep neural networks carve out manifolds
especially when the multiplication neurons are introduced. Multiplication is
used in dot products and the attention mechanism and it is employed in capsule
networks and self-attention based transformers. We also describe how random
polynomial, random matrix, spin glass and computational complexity perspectives
on the loss surfaces are interconnected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1"&gt;Simant Dube&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering Large Data Sets with Incremental Estimation of Low-density Separating Hyperplanes. (arXiv:2108.03442v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.03442</id>
        <link href="http://arxiv.org/abs/2108.03442"/>
        <updated>2021-08-10T02:00:11.910Z</updated>
        <summary type="html"><![CDATA[An efficient method for obtaining low-density hyperplane separators in the
unsupervised context is proposed. Low density separators can be used to obtain
a partition of a set of data based on their allocations to the different sides
of the separators. The proposed method is based on applying stochastic gradient
descent to the integrated density on the hyperplane with respect to a
convolution of the underlying distribution and a smoothing kernel. In the case
where the bandwidth of the smoothing kernel is decreased towards zero, the bias
of these updates with respect to the true underlying density tends to zero, and
convergence to a minimiser of the density on the hyperplane can be obtained. A
post-processing of the partition induced by a collection of low-density
hyperplanes yields an efficient and accurate clustering method which is capable
of automatically selecting an appropriate number of clusters. Experiments with
the proposed approach show that it is highly competitive in terms of both speed
and accuracy when compared with relevant benchmarks. Code to implement the
proposed approach is available in the form of an R package from
https://github.com/DavidHofmeyr/iMDH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hofmeyr_D/0/1/0/all/0/1"&gt;David P. Hofmeyr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14483</id>
        <link href="http://arxiv.org/abs/2107.14483"/>
        <updated>2021-08-10T02:00:11.904Z</updated>
        <summary type="html"><![CDATA[Learning generalizable manipulation skills is central for robots to achieve
task automation in environments with endless scene and object variations.
However, existing robot learning environments are limited in both scale and
diversity of 3D assets (especially of articulated objects), making it difficult
to train and evaluate the generalization ability of agents over novel objects.
In this work, we focus on object-level generalization and propose SAPIEN
Manipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale
learning-from-demonstrations benchmark for articulated object manipulation with
3D visual input (point cloud and RGB-D image). ManiSkill supports object-level
variations by utilizing a rich and diverse set of articulated objects, and each
task is carefully designed for learning manipulations on a single category of
objects. We equip ManiSkill with a large number of high-quality demonstrations
to facilitate learning-from-demonstrations approaches and perform evaluations
on baseline algorithms. We believe that ManiSkill can encourage the robot
learning community to explore more on learning generalizable object
manipulation skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1"&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Derek Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Stone Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhiwei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning. (arXiv:2108.03978v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.03978</id>
        <link href="http://arxiv.org/abs/2108.03978"/>
        <updated>2021-08-10T02:00:11.903Z</updated>
        <summary type="html"><![CDATA[Digital hardware is verified by comparing its behavior against a reference
model on a range of randomly generated input signals. The random generation of
the inputs hopes to achieve sufficient coverage of the different parts of the
design. However, such coverage is often difficult to achieve, amounting to
large verification efforts and delays. An alternative is to use Reinforcement
Learning (RL) to generate the inputs by learning to prioritize those inputs
which can more efficiently explore the design under test. In this work, we
present VeRLPy an open-source library to allow RL-driven verification with
limited additional engineering overhead. This contributes to two broad
movements within the EDA community of (a) moving to open-source toolchains and
(b) reducing barriers for development with Python support. We also demonstrate
the use of VeRLPy for a few designs and establish its value over randomly
generated input signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shibu_A/0/1/0/all/0/1"&gt;Aebel Joe Shibu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1"&gt;Sadhana S&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Shilpa N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;Pratyush Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00591</id>
        <link href="http://arxiv.org/abs/2105.00591"/>
        <updated>2021-08-10T02:00:11.903Z</updated>
        <summary type="html"><![CDATA[In the past few years, mobile deep-learning deployment progressed by leaps
and bounds, but solutions still struggle to accommodate its severe and
fluctuating operational restrictions, which include bandwidth, latency,
computation, and energy. In this work, we help to bridge that gap, introducing
the first configurable solution for object detection that manages the triple
communication-computation-accuracy trade-off with a single set of weights. Our
solution shows state-of-the-art results on COCO-2017, adding only a minor
penalty on the base EfficientDet-D2 architecture. Our design is robust to the
choice of base architecture and compressor and should adapt well for future
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1"&gt;Juliano S. Assine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1"&gt;J. C. S. Santos Filho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1"&gt;Eduardo Valle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Global Labels in Few-Shot Classification and How to Infer Them. (arXiv:2108.04055v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04055</id>
        <link href="http://arxiv.org/abs/2108.04055"/>
        <updated>2021-08-10T02:00:11.902Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) is a central problem in meta-learning, where learners
must quickly adapt to new tasks given limited training data. Surprisingly,
recent works have outperformed meta-learning methods tailored to FSL by casting
it as standard supervised learning to jointly classify all classes shared
across tasks. However, this approach violates the standard FSL setting by
requiring global labels shared across tasks, which are often unavailable in
practice. In this paper, we show why solving FSL via standard classification is
theoretically advantageous. This motivates us to propose Meta Label Learning
(MeLa), a novel algorithm that infers global labels and obtains robust few-shot
models via standard classification. Empirically, we demonstrate that MeLa
outperforms meta-learning competitors and is comparable to the oracle setting
where ground truth labels are given. We provide extensive ablation studies to
highlight the key properties of the proposed strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1"&gt;Carlo Ciliberto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.08543</id>
        <link href="http://arxiv.org/abs/1903.08543"/>
        <updated>2021-08-10T02:00:11.902Z</updated>
        <summary type="html"><![CDATA[Using a model heat engine, we show that neural network-based reinforcement
learning can identify thermodynamic trajectories of maximal efficiency. We
consider both gradient and gradient-free reinforcement learning. We use an
evolutionary learning algorithm to evolve a population of neural networks,
subject to a directive to maximize the efficiency of a trajectory composed of a
set of elementary thermodynamic processes; the resulting networks learn to
carry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given
an additional irreversible process, this evolutionary scheme learns a
previously unknown thermodynamic cycle. Gradient-based reinforcement learning
is able to learn the Stirling cycle, whereas an evolutionary approach achieves
the optimal Carnot cycle. Our results show how the reinforcement learning
strategies developed for game playing can be applied to solve physical problems
conditioned upon path-extensive order parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1"&gt;Chris Beeler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1"&gt;Uladzimir Yahorau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1"&gt;Rory Coles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Kyle Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1"&gt;Stephen Whitelam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1"&gt;Isaac Tamblyn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maxmin Q-learning: Controlling the Estimation Bias of Q-learning. (arXiv:2002.06487v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06487</id>
        <link href="http://arxiv.org/abs/2002.06487"/>
        <updated>2021-08-10T02:00:11.901Z</updated>
        <summary type="html"><![CDATA[Q-learning suffers from overestimation bias, because it approximates the
maximum action value using the maximum estimated action value. Algorithms have
been proposed to reduce overestimation bias, but we lack an understanding of
how bias interacts with performance, and the extent to which existing
algorithms mitigate bias. In this paper, we 1) highlight that the effect of
overestimation bias on learning efficiency is environment-dependent; 2) propose
a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides
a parameter to flexibly control bias; 3) show theoretically that there exists a
parameter choice for Maxmin Q-learning that leads to unbiased estimation with a
lower approximation variance than Q-learning; and 4) prove the convergence of
our algorithm in the tabular case, as well as convergence of several previous
Q-learning variants, using a novel Generalized Q-learning framework. We
empirically verify that our algorithm better controls estimation bias in toy
environments, and that it achieves superior performance on several benchmark
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1"&gt;Qingfeng Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yangchen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1"&gt;Alona Fyshe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1"&gt;Martha White&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alternating linear scheme in a Bayesian framework for low-rank tensor approximation. (arXiv:2012.11228v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11228</id>
        <link href="http://arxiv.org/abs/2012.11228"/>
        <updated>2021-08-10T02:00:11.901Z</updated>
        <summary type="html"><![CDATA[Multiway data often naturally occurs in a tensorial format which can be
approximately represented by a low-rank tensor decomposition. This is useful
because complexity can be significantly reduced and the treatment of
large-scale data sets can be facilitated. In this paper, we find a low-rank
representation for a given tensor by solving a Bayesian inference problem. This
is achieved by dividing the overall inference problem into sub-problems where
we sequentially infer the posterior distribution of one tensor decomposition
component at a time. This leads to a probabilistic interpretation of the
well-known iterative algorithm alternating linear scheme (ALS). In this way,
the consideration of measurement noise is enabled, as well as the incorporation
of application-specific prior knowledge and the uncertainty quantification of
the low-rank tensor estimate. To compute the low-rank tensor estimate from the
posterior distributions of the tensor decomposition components, we present an
algorithm that performs the unscented transform in tensor train format.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menzen_C/0/1/0/all/0/1"&gt;Clara Menzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1"&gt;Manon Kok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batselier_K/0/1/0/all/0/1"&gt;Kim Batselier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Uncertainty for Improved Static Malware Detection Under Extreme False Positive Constraints. (arXiv:2108.04081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04081</id>
        <link href="http://arxiv.org/abs/2108.04081"/>
        <updated>2021-08-10T02:00:11.900Z</updated>
        <summary type="html"><![CDATA[The detection of malware is a critical task for the protection of computing
environments. This task often requires extremely low false positive rates (FPR)
of 0.01% or even lower, for which modern machine learning has no readily
available tools. We introduce the first broad investigation of the use of
uncertainty for malware detection across multiple datasets, models, and feature
types. We show how ensembling and Bayesian treatments of machine learning
methods for static malware detection allow for improved identification of model
errors, uncovering of new malware families, and predictive performance under
extreme false positive constraints. In particular, we improve the true positive
rate (TPR) at an actual realized FPR of 1e-5 from an expected 0.69 for previous
methods to 0.80 on the best performing model class on the Sophos industry scale
dataset. We additionally demonstrate how previous works have used an evaluation
protocol that can lead to misleading results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Andre T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1"&gt;Edward Raff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1"&gt;Charles Nicholas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holt_J/0/1/0/all/0/1"&gt;James Holt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09128</id>
        <link href="http://arxiv.org/abs/2011.09128"/>
        <updated>2021-08-10T02:00:11.899Z</updated>
        <summary type="html"><![CDATA[We present a multigrid-in-channels (MGIC) approach that tackles the quadratic
growth of the number of parameters with respect to the number of channels in
standard convolutional neural networks (CNNs). Thereby our approach addresses
the redundancy in CNNs that is also exposed by the recent success of
lightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard
CNNs with fewer parameters; however, the number of weights still scales
quadratically with the CNN's width. Our MGIC architectures replace each CNN
block with an MGIC counterpart that utilizes a hierarchy of nested grouped
convolutions of small group size to address this.

Hence, our proposed architectures scale linearly with respect to the
network's width while retaining full coupling of the channels as in standard
CNNs.

Our extensive experiments on image classification, segmentation, and point
cloud classification show that applying this strategy to different
architectures like ResNet and MobileNetV3 reduces the number of parameters
while obtaining similar or better accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1"&gt;Jonathan Ephrath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1"&gt;Lars Ruthotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Level Set Learning for Function Approximation on Sparse Data with Applications to Parametric Differential Equations. (arXiv:2104.14072v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14072</id>
        <link href="http://arxiv.org/abs/2104.14072"/>
        <updated>2021-08-10T02:00:11.899Z</updated>
        <summary type="html"><![CDATA[A dimension reduction method based on the "Nonlinear Level set Learning"
(NLL) approach is presented for the pointwise prediction of functions which
have been sparsely sampled. Leveraging geometric information provided by the
Implicit Function Theorem, the proposed algorithm effectively reduces the input
dimension to the theoretical lower bound with minor accuracy loss, providing a
one-dimensional representation of the function which can be used for regression
and sensitivity analysis. Experiments and applications are presented which
compare this modified NLL with the original NLL and the Active Subspaces (AS)
method. While accommodating sparse input data, the proposed algorithm is shown
to train quickly and provide a much more accurate and informative reduction
than either AS or the original NLL on two example functions with
high-dimensional domains, as well as two state-dependent quantities depending
on the solutions to parametric differential equations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gruber_A/0/1/0/all/0/1"&gt;Anthony Gruber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gunzburger_M/0/1/0/all/0/1"&gt;Max Gunzburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lili Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yuankai Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03786</id>
        <link href="http://arxiv.org/abs/2108.03786"/>
        <updated>2021-08-10T02:00:11.880Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel lightweight COVID-19 diagnosis framework using CT
scans. Our system utilises a novel two-stage approach to generate robust and
efficient diagnoses across heterogeneous patient level inputs. We use a
powerful backbone network as a feature extractor to capture discriminative
slice-level features. These features are aggregated by a lightweight network to
obtain a patient level diagnosis. The aggregation network is carefully designed
to have a small number of trainable parameters while also possessing sufficient
capacity to generalise to diverse variations within different CT volumes and to
adapt to noise introduced during the data acquisition. We achieve a significant
performance increase over the baselines when benchmarked on the SPGC COVID-19
Radiomics Dataset, despite having only 2.5 million trainable parameters and
requiring only 0.623 seconds on average to process a single patient's CT volume
using an Nvidia-GeForce RTX 2080 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1"&gt;Harshala Gammulle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1"&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1"&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1"&gt;Simon Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effect of Training Parameters and Mechanisms on Decentralized Federated Learning based on MNIST Dataset. (arXiv:2108.03508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03508</id>
        <link href="http://arxiv.org/abs/2108.03508"/>
        <updated>2021-08-10T02:00:11.873Z</updated>
        <summary type="html"><![CDATA[Federated Learning is an algorithm suited for training models on
decentralized data, but the requirement of a central "server" node is a
bottleneck. In this document, we first introduce the notion of Decentralized
Federated Learning (DFL). We then perform various experiments on different
setups, such as changing model aggregation frequency, switching from
independent and identically distributed (IID) dataset partitioning to non-IID
partitioning with partial global sharing, using different optimization methods
across clients, and breaking models into segments with partial sharing. All
experiments are run on the MNIST handwritten digits dataset. We observe that
those altered training procedures are generally robust, albeit non-optimal. We
also observe failures in training when the variance between model weights is
too large. The open-source experiment code is accessible through
GitHub\footnote{Code was uploaded at
\url{https://github.com/zhzhang2018/DecentralizedFL}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuofan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1"&gt;Kaicheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_C/0/1/0/all/0/1"&gt;Chaouki Abdallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction. (arXiv:2006.03041v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03041</id>
        <link href="http://arxiv.org/abs/2006.03041"/>
        <updated>2021-08-10T02:00:11.865Z</updated>
        <summary type="html"><![CDATA[Asynchronous Q-learning aims to learn the optimal action-value function (or
Q-function) of a Markov decision process (MDP), based on a single trajectory of
Markovian samples induced by a behavior policy. Focusing on a
$\gamma$-discounted MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$, we demonstrate that the $\ell_{\infty}$-based sample complexity
of classical asynchronous Q-learning --- namely, the number of samples needed
to yield an entrywise $\varepsilon$-accurate estimate of the Q-function --- is
at most on the order of $\frac{1}{\mu_{\min}(1-\gamma)^5\varepsilon^2}+
\frac{t_{mix}}{\mu_{\min}(1-\gamma)}$ up to some logarithmic factor, provided
that a proper constant learning rate is adopted. Here, $t_{mix}$ and
$\mu_{\min}$ denote respectively the mixing time and the minimum state-action
occupancy probability of the sample trajectory. The first term of this bound
matches the sample complexity in the synchronous case with independent samples
drawn from the stationary distribution of the trajectory. The second term
reflects the cost taken for the empirical distribution of the Markovian
trajectory to reach a steady state, which is incurred at the very beginning and
becomes amortized as the algorithm runs. Encouragingly, the above bound
improves upon the state-of-the-art result \cite{qu2020finite} by a factor of at
least $|\mathcal{S}||\mathcal{A}|$ for all scenarios, and by a factor of at
least $t_{mix}|\mathcal{S}||\mathcal{A}|$ for any sufficiently small accuracy
level $\varepsilon$. Further, we demonstrate that the scaling on the effective
horizon $\frac{1}{1-\gamma}$ can be improved by means of variance reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yuting Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yuejie Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuantao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Intelligent Healthcare Systems: A Comprehensive Survey. (arXiv:2108.04087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04087</id>
        <link href="http://arxiv.org/abs/2108.04087"/>
        <updated>2021-08-10T02:00:11.864Z</updated>
        <summary type="html"><![CDATA[The rapid increase in the percentage of chronic disease patients along with
the recent pandemic pose immediate threats on healthcare expenditure and
elevate causes of death. This calls for transforming healthcare systems away
from one-on-one patient treatment into intelligent health systems, to improve
services, access and scalability, while reducing costs. Reinforcement Learning
(RL) has witnessed an intrinsic breakthrough in solving a variety of complex
problems for diverse applications and services. Thus, we conduct in this paper
a comprehensive survey of the recent models and techniques of RL that have been
developed/used for supporting Intelligent-healthcare (I-health) systems. This
paper can guide the readers to deeply understand the state-of-the-art regarding
the use of RL in the context of I-health. Specifically, we first present an
overview for the I-health systems challenges, architecture, and how RL can
benefit these systems. We then review the background and mathematical modeling
of different RL, Deep RL (DRL), and multi-agent RL models. After that, we
provide a deep literature review for the applications of RL in I-health
systems. In particular, three main areas have been tackled, i.e., edge
intelligence, smart core network, and dynamic treatment regimes. Finally, we
highlight emerging challenges and outline future research directions in driving
the future success of RL in I-health systems, which opens the door for
exploring some interesting and unsolved problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1"&gt;Alaa Awad Abdellatif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1"&gt;Naram Mhaisen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chkirbene_Z/0/1/0/all/0/1"&gt;Zina Chkirbene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02193</id>
        <link href="http://arxiv.org/abs/2103.02193"/>
        <updated>2021-08-10T02:00:11.864Z</updated>
        <summary type="html"><![CDATA[While recent studies on semi-supervised learning have shown remarkable
progress in leveraging both labeled and unlabeled data, most of them presume a
basic setting of the model is randomly initialized. In this work, we consider
semi-supervised learning and transfer learning jointly, leading to a more
practical and competitive paradigm that can utilize both powerful pre-trained
models from source domain as well as labeled/unlabeled data in the target
domain. To better exploit the value of both pre-trained weights and unlabeled
target examples, we introduce adaptive consistency regularization that consists
of two complementary components: Adaptive Knowledge Consistency (AKC) on the
examples between the source and target model, and Adaptive Representation
Consistency (ARC) on the target model between labeled and unlabeled examples.
Examples involved in the consistency regularization are adaptively selected
according to their potential contributions to the target task. We conduct
extensive experiments on popular benchmarks including CIFAR-10, CUB-200, and
MURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show
that our proposed adaptive consistency regularization outperforms
state-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean
Teacher, and FixMatch. Moreover, our algorithm is orthogonal to existing
methods and thus able to gain additional improvements on top of MixMatch and
FixMatch. Our code is available at
https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1"&gt;Abulikemu Abuduweili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xingjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach. (arXiv:2104.06307v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06307</id>
        <link href="http://arxiv.org/abs/2104.06307"/>
        <updated>2021-08-10T02:00:11.857Z</updated>
        <summary type="html"><![CDATA[Most traditional false data injection attack (FDIA) detection approaches rely
on a key assumption, i.e., the power system can be accurately modeled. However,
the transmission line parameters are dynamic and cannot be accurately known
during operation and thus the involved modeling errors should not be neglected.
In this paper, an illustrative case has revealed that modeling errors in
transmission lines significantly weaken the detection effectiveness of
conventional FDIA approaches. To tackle this issue, we propose an FDIA
detection mechanism from the perspective of transfer learning. Specifically,
the simulated power system is treated as a source domain, which provides
abundant simulated normal and attack data. The real world's running system
whose transmission line parameters are unknown is taken as a target domain
where sufficient real normal data are collected for tracking the latest system
states online. The designed transfer strategy that aims at making full use of
data in hand is divided into two optimization stages. In the first stage, a
deep neural network (DNN) is built by simultaneously optimizing several
well-designed objective terms with both simulated data and real data, and then
it is fine-tuned via real data in the second stage. Several case studies on the
IEEE 14-bus and 118-bus systems verify the effectiveness of the proposed
mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bowen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fanghong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1"&gt;Changyun Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruilong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen-An Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. (arXiv:2010.03110v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03110</id>
        <link href="http://arxiv.org/abs/2010.03110"/>
        <updated>2021-08-10T02:00:11.856Z</updated>
        <summary type="html"><![CDATA[Animals exhibit an innate ability to learn regularities of the world through
interaction. By performing experiments in their environment, they are able to
discern the causal factors of variation and infer how they affect the world's
dynamics. Inspired by this, we attempt to equip reinforcement learning agents
with the ability to perform experiments that facilitate a categorization of the
rolled-out trajectories, and to subsequently infer the causal factors of the
environment in a hierarchical manner. We introduce {\em causal curiosity}, a
novel intrinsic reward, and show that it allows our agents to learn optimal
sequences of actions and discover causal factors in the dynamics of the
environment. The learned behavior allows the agents to infer a binary quantized
representation for the ground-truth causal factors in every environment.
Additionally, we find that these experimental behaviors are semantically
meaningful (e.g., our agents learn to lift blocks to categorize them by
weight), and are learnt in a self-supervised manner with approximately 2.5
times less data than conventional supervised planners. We show that these
behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or
other downstream tasks). Finally, we show that the knowledge of causal factor
representations aids zero-shot learning for more complex tasks. Visit
https://sites.google.com/usc.edu/causal-curiosity/home for website.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1"&gt;Sumedh A. Sontakke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1"&gt;Arash Mehrjou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1"&gt;Laurent Itti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information. (arXiv:2012.00489v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00489</id>
        <link href="http://arxiv.org/abs/2012.00489"/>
        <updated>2021-08-10T02:00:11.856Z</updated>
        <summary type="html"><![CDATA[The movements of individuals within and among cities influence critical
aspects of our society, such as well-being, the spreading of epidemics, and the
quality of the environment. When information about mobility flows is not
available for a particular region of interest, we must rely on mathematical
models to generate them. In this work, we propose the Deep Gravity model, an
effective method to generate flow probabilities that exploits many variables
(e.g., land use, road network, transport, food, health facilities) extracted
from voluntary geographic data, and uses deep neural networks to discover
non-linear relationships between those variables and mobility flows. Our
experiments, conducted on mobility flows in England, Italy, and New York State,
show that Deep Gravity has good geographic generalization capability, achieving
a significant increase in performance (especially in densely populated regions
of interest) with respect to the classic gravity model and models that do not
use deep neural networks or geographic data. We also show how flows generated
by Deep Gravity may be explained in terms of the geographic features using
explainable AI techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simini_F/0/1/0/all/0/1"&gt;Filippo Simini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1"&gt;Gianni Barlacchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1"&gt;Massimiliano Luca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1"&gt;Luca Pappalardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-learning sparse PCA for multimode process monitoring. (arXiv:2108.03449v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03449</id>
        <link href="http://arxiv.org/abs/2108.03449"/>
        <updated>2021-08-10T02:00:11.855Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel sparse principal component analysis algorithm
with self-learning ability for successive modes, where synaptic intelligence is
employed to measure the importance of variables and a regularization term is
added to preserve the learned knowledge of previous modes. Different from
traditional multimode monitoring methods, the monitoring model is updated based
on the current model and new data when a new mode arrives, thus delivering
prominent performance for sequential modes. Besides, the computation and
storage resources are saved in the long run, because it is not necessary to
retrain the model from scratch frequently and store data from previous modes.
More importantly, the model furnishes excellent interpretability owing to the
sparsity of parameters. Finally, a numerical case and a practical pulverizing
system are adopted to illustrate the effectiveness of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Donghua Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Maoyin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03378</id>
        <link href="http://arxiv.org/abs/2108.03378"/>
        <updated>2021-08-10T02:00:11.848Z</updated>
        <summary type="html"><![CDATA[Reconstructing a layout of indoor spaces has been a crucial part of growing
indoor location based services. One of the key challenges in the proliferation
of indoor location based services is the unavailability of indoor spatial maps
due to the complex nature of capturing an indoor space model (e.g., floor plan)
of an existing building. In this paper, we propose a system to automatically
generate floor plans that can recognize rooms from the point-clouds obtained
through smartphones like Google's Tango. In particular, we propose two
approaches - a Recurrent Neural Network based approach using Pointer Network
and a Convolutional Neural Network based approach using Mask-RCNN to identify
rooms (and thereby floor plans) from point-clouds. Experimental results on
different datasets demonstrate approximately 0.80-0.90 Intersection-over-Union
scores, which show that our models can effectively identify the rooms and
regenerate the shapes of the rooms in heterogeneous environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1"&gt;Md. Tareq Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mohammed Eunus Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Graph Neural Networks for Rumor Detection in Online Forums. (arXiv:2108.03548v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.03548</id>
        <link href="http://arxiv.org/abs/2108.03548"/>
        <updated>2021-08-10T02:00:11.827Z</updated>
        <summary type="html"><![CDATA[The widespread adoption of online social networks in daily life has created a
pressing need for effectively classifying user-generated content. This work
presents techniques for classifying linked content spread on forum websites --
specifically, links to news articles or blogs -- using user interaction signals
alone. Importantly, online forums such as Reddit do not have a user-generated
social graph, which is assumed in social network behavioral-based
classification settings. Using Reddit as a case-study, we show how to obtain a
derived social graph, and use this graph, Reddit post sequences, and comment
trees as inputs to a Recurrent Graph Neural Network (R-GNN) encoder. We train
the R-GNN on news link categorization and rumor detection, showing superior
results to recent baselines. Our code is made publicly available at
https://github.com/google-research/social_cascades.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartel_J/0/1/0/all/0/1"&gt;Jacob Bartel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palowitch_J/0/1/0/all/0/1"&gt;John Palowitch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.15007</id>
        <link href="http://arxiv.org/abs/2104.15007"/>
        <updated>2021-08-10T02:00:11.813Z</updated>
        <summary type="html"><![CDATA[The first known case of Coronavirus disease 2019 (COVID-19) was identified in
December 2019. It has spread worldwide, leading to an ongoing pandemic, imposed
restrictions and costs to many countries. Predicting the number of new cases
and deaths during this period can be a useful step in predicting the costs and
facilities required in the future. The purpose of this study is to predict new
cases and deaths rate one, three and seven-day ahead during the next 100 days.
The motivation for predicting every n days (instead of just every day) is the
investigation of the possibility of computational cost reduction and still
achieving reasonable performance. Such a scenario may be encountered real-time
forecasting of time series. Six different deep learning methods are examined on
the data adopted from the WHO website. Three methods are LSTM, Convolutional
LSTM, and GRU. The bidirectional extension is then considered for each method
to forecast the rate of new cases and new deaths in Australia and Iran
countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1"&gt;Nooshin Ayoobi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1"&gt;Danial Sharifrazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;Juan M. Gorriz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1"&gt;Hossein Moosaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1"&gt;Abdoulmohammad Gholamzadeh Chofreh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1"&gt;Feybi Ariani Goni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1"&gt;Jiri Jaromir Klemes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1"&gt;Amir Mosavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN). (arXiv:2108.03400v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03400</id>
        <link href="http://arxiv.org/abs/2108.03400"/>
        <updated>2021-08-10T02:00:11.811Z</updated>
        <summary type="html"><![CDATA[GNNs have been proven to perform highly effective in various node-level,
edge-level, and graph-level prediction tasks in several domains. Existing
approaches mainly focus on static graphs. However, many graphs change over time
with their edge may disappear, or node or edge attribute may alter from one
time to the other. It is essential to consider such evolution in representation
learning of nodes in time varying graphs. In this paper, we propose a Temporal
Multilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding
approach for dynamic graph that incorporates the interdependence of temporal
relations into embedding computation. We evaluate the performance of TMP-GNN on
two different representations of temporal multilayered graphs. The performance
is assessed against the most popular GNNs on node-level prediction tasks. Then,
we incorporate TMP-GNN into a deep learning framework to estimate missing data
and compare the performance with their corresponding competent GNNs from our
former experiment, and a baseline method. Experimental results on four
real-world datasets yield up to 58% of lower ROC AUC for pairwise node
classification task, and 96% of lower MAE in missing feature estimation,
particularly for graphs with a relatively high number of nodes and lower mean
degree of connectivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Najafi_B/0/1/0/all/0/1"&gt;Bahareh Najafi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1"&gt;Saeedeh Parsaeefard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1"&gt;Alberto Leon-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03233</id>
        <link href="http://arxiv.org/abs/2108.03233"/>
        <updated>2021-08-10T02:00:11.805Z</updated>
        <summary type="html"><![CDATA[Incorporating boundaries of the imaging object as a priori information to
imaging algorithms can significantly improve the performance of electromagnetic
medical imaging systems. To avoid overly complicating the system by using
different sensors and the adverse effect of the subject's movement, a
learning-based method is proposed to estimate the boundary (external contour)
of the imaged object using the same electromagnetic imaging data. While imaging
techniques may discard the reflection coefficients for being dominant and
uninformative for imaging, these parameters are made use of for boundary
detection. The learned model is verified through independent clinical human
trials by using a head imaging system with a 16-element antenna array that
works across the band 0.7-1.6 GHz. The evaluation demonstrated that the model
achieves average dissimilarity of 0.012 in Hu-moment while detecting head
boundary. The model enables fast scan and image creation while eliminating the
need for additional devices for accurate boundary estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1"&gt;A. Al-Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1"&gt;A. Stancombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1"&gt;A. Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1"&gt;A. Abbosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collapsing the Decision Tree: the Concurrent Data Predictor. (arXiv:2108.03887v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03887</id>
        <link href="http://arxiv.org/abs/2108.03887"/>
        <updated>2021-08-10T02:00:11.799Z</updated>
        <summary type="html"><![CDATA[A family of concurrent data predictors is derived from the decision tree
classifier by removing the limitation of sequentially evaluating attributes. By
evaluating attributes concurrently, the decision tree collapses into a flat
structure. Experiments indicate improvements of the prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alb_C/0/1/0/all/0/1"&gt;Cristian Alb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Symbolic Regression: Transformation of Symbolic Regression into a Real-valued Optimization Problem. (arXiv:2108.03274v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03274</id>
        <link href="http://arxiv.org/abs/2108.03274"/>
        <updated>2021-08-10T02:00:11.797Z</updated>
        <summary type="html"><![CDATA[The typical methods for symbolic regression produce rather abrupt changes in
solution candidates. In this work, we have tried to transform symbolic
regression from an optimization problem, with a landscape that is so rugged
that typical analysis methods do not produce meaningful results, to one that
can be compared to typical and very smooth real-valued problems. While the
ruggedness might not interfere with the performance of optimization, it
restricts the possibilities of analysis. Here, we have explored different
aspects of a transformation and propose a simple procedure to create
real-valued optimization problems from symbolic regression problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pitzer_E/0/1/0/all/0/1"&gt;Erik Pitzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12023</id>
        <link href="http://arxiv.org/abs/2106.12023"/>
        <updated>2021-08-10T02:00:11.789Z</updated>
        <summary type="html"><![CDATA[This report describes the technical details of our submission to the
EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action
Recognition. The EPIC-Kitchens dataset is more difficult than other video
domain adaptation datasets due to multi-tasks with more modalities. Firstly, to
participate in the challenge, we employ a transformer to capture the spatial
information from each modality. Secondly, we employ a temporal attention module
to model temporal-wise inter-dependency. Thirdly, we employ the adversarial
domain adaptation network to learn the general features between labeled source
and unlabeled target domain. Finally, we incorporate multiple modalities to
improve the performance by a three-stream network with late fusion. Our network
achieves the comparable performance with the state-of-the-art baseline T$A^3$N
and outperforms the baseline on top-1 accuracy for verb class and top-5
accuracies for all three tasks which are verb, noun and action. Under the team
name xy9, our submission achieved 5th place in terms of top-1 accuracy for verb
class and all top-5 accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1"&gt;Tao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jointly Attacking Graph Neural Network and its Explanations. (arXiv:2108.03388v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03388</id>
        <link href="http://arxiv.org/abs/2108.03388"/>
        <updated>2021-08-10T02:00:11.782Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have boosted the performance for many
graph-related tasks. Despite the great success, recent studies have shown that
GNNs are highly vulnerable to adversarial attacks, where adversaries can
mislead the GNNs' prediction by modifying graphs. On the other hand, the
explanation of GNNs (GNNExplainer) provides a better understanding of a trained
GNN model by generating a small subgraph and features that are most influential
for its prediction. In this paper, we first perform empirical studies to
validate that GNNExplainer can act as an inspection tool and have the potential
to detect the adversarial perturbations for graphs. This finding motivates us
to further initiate a new problem investigation: Whether a graph neural network
and its explanations can be jointly attacked by modifying graphs with malicious
desires? It is challenging to answer this question since the goals of
adversarial attacks and bypassing the GNNExplainer essentially contradict each
other. In this work, we give a confirmative answer to this question by
proposing a novel attack framework (GEAttack), which can attack both a GNN
model and its explanations by simultaneously exploiting their vulnerabilities.
Extensive experiments on two explainers (GNNExplainer and PGExplainer) under
various real-world datasets demonstrate the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1"&gt;Wenqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Wei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Han Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xianfeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilevel Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04395</id>
        <link href="http://arxiv.org/abs/2007.04395"/>
        <updated>2021-08-10T02:00:11.775Z</updated>
        <summary type="html"><![CDATA[While the celebrated graph neural networks yield effective representations
for individual nodes of a graph, there has been relatively less success in
extending to the task of graph similarity learning. Recent work on graph
similarity learning has considered either global-level graph-graph interactions
or low-level node-node interactions, however ignoring the rich cross-level
interactions (e.g., between each node of one graph and the other whole graph).
In this paper, we propose a multi-level graph matching network (MGMN) framework
for computing the graph similarity between any pair of graph-structured objects
in an end-to-end fashion. In particular, the proposed MGMN consists of a
node-graph matching network for effectively learning cross-level interactions
between each node of one graph and the other whole graph, and a siamese graph
neural network to learn global-level interactions between two input graphs.
Furthermore, to compensate for the lack of standard benchmark datasets, we have
created and collected a set of datasets for both the graph-graph classification
and graph-graph regression tasks with different sizes in order to evaluate the
effectiveness and robustness of our models. Comprehensive experiments
demonstrate that MGMN consistently outperforms state-of-the-art baseline models
on both the graph-graph classification and graph-graph regression tasks.
Compared with previous work, MGMN also exhibits stronger robustness as the
sizes of the two input graphs increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiang Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Saizhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fangli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alex X. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chunming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning. (arXiv:2108.03353v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.03353</id>
        <link href="http://arxiv.org/abs/2108.03353"/>
        <updated>2021-08-10T02:00:11.763Z</updated>
        <summary type="html"><![CDATA[Mobile User Interface Summarization generates succinct language descriptions
of mobile screens for conveying important contents and functionalities of the
screen, which can be useful for many language-based application scenarios. We
present Screen2Words, a novel screen summarization approach that automatically
encapsulates essential information of a UI screen into a coherent language
phrase. Summarizing mobile screens requires a holistic understanding of the
multi-modal data of mobile UIs, including text, image, structures as well as UI
semantics, motivating our multi-modal learning approach. We collected and
analyzed a large-scale screen summarization dataset annotated by human workers.
Our dataset contains more than 112k language summarization across $\sim$22k
unique UI screens. We then experimented with a set of deep models with
different configurations. Our evaluation of these models with both automatic
accuracy metrics and human rating shows that our approach can generate
high-quality summaries for mobile screens. We demonstrate potential use cases
of Screen2Words and open-source our dataset and model to lay the foundations
for further bridging language and user interfaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bryan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhourong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grossman_T/0/1/0/all/0/1"&gt;Tovi Grossman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03354</id>
        <link href="http://arxiv.org/abs/2108.03354"/>
        <updated>2021-08-10T02:00:11.757Z</updated>
        <summary type="html"><![CDATA[The research on human emotion under multimedia stimulation based on
physiological signals is an emerging field, and important progress has been
achieved for emotion recognition based on multi-modal signals. However, it is
challenging to make full use of the complementarity among
spatial-spectral-temporal domain features for emotion recognition, as well as
model the heterogeneity and correlation among multi-modal signals. In this
paper, we propose a novel two-stream heterogeneous graph recurrent neural
network, named HetEmotionNet, fusing multi-modal physiological signals for
emotion recognition. Specifically, HetEmotionNet consists of the
spatial-temporal stream and the spatial-spectral stream, which can fuse
spatial-spectral-temporal domain features in a unified framework. Each stream
is composed of the graph transformer network for modeling the heterogeneity,
the graph convolutional network for modeling the correlation, and the gated
recurrent unit for capturing the temporal domain or spectral domain dependency.
Extensive experiments on two real-world datasets demonstrate that our proposed
model achieves better performance than state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Ziyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youfang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiangheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Caijie Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data. (arXiv:2108.03288v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03288</id>
        <link href="http://arxiv.org/abs/2108.03288"/>
        <updated>2021-08-10T02:00:11.749Z</updated>
        <summary type="html"><![CDATA[Time-series data are one of the fundamental types of raw data representation
used in data-driven techniques. In machine condition monitoring, time-series
vibration data are overly used in data mining for deep neural networks.
Typically, vibration data is converted into images for classification using
Deep Neural Networks (DNNs), and scalograms are the most effective form of
image representation. However, the DNN classifiers require huge labeled
training samples to reach their optimum performance. So, many forms of data
augmentation techniques are applied to the classifiers to compensate for the
lack of training samples. However, the scalograms are graphical representations
where the existing augmentation techniques suffer because they either change
the graphical meaning or have too much noise in the samples that change the
physical meaning. In this study, a data augmentation technique named ensemble
augmentation is proposed to overcome this limitation. This augmentation method
uses the power of white noise added in ensembles to the original samples to
generate real-like samples. After averaging the signal with ensembles, a new
signal is obtained that contains the characteristics of the original signal.
The parameters for the ensemble augmentation are validated using a simulated
signal. The proposed method is evaluated using 10 class bearing vibration data
using three state-of-the-art Transfer Learning (TL) models, namely,
Inception-V3, MobileNet-V2, and ResNet50. Augmented samples are generated in
two increments: the first increment generates the same number of fake samples
as the training samples, and in the second increment, the number of samples is
increased gradually. The outputs from the proposed method are compared with no
augmentation, augmentations using deep convolution generative adversarial
network (DCGAN), and several geometric transformation-based augmentations...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faysal_A/0/1/0/all/0/1"&gt;Atik Faysal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keng_N/0/1/0/all/0/1"&gt;Ngui Wai Keng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1"&gt;M. H. Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative Pre-Conditioning for Expediting the Gradient-Descent Method: The Distributed Linear Least-Squares Problem. (arXiv:2008.02856v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02856</id>
        <link href="http://arxiv.org/abs/2008.02856"/>
        <updated>2021-08-10T02:00:11.712Z</updated>
        <summary type="html"><![CDATA[This paper considers the multi-agent linear least-squares problem in a
server-agent network. In this problem, the system comprises multiple agents,
each having a set of local data points, that are connected to a server. The
goal for the agents is to compute a linear mathematical model that optimally
fits the collective data points held by all the agents, without sharing their
individual local data points. This goal can be achieved, in principle, using
the server-agent variant of the traditional iterative gradient-descent method.
The gradient-descent method converges linearly to a solution, and its rate of
convergence is lower bounded by the conditioning of the agents' collective data
points. If the data points are ill-conditioned, the gradient-descent method may
require a large number of iterations to converge.

We propose an iterative pre-conditioning technique that mitigates the
deleterious effect of the conditioning of data points on the rate of
convergence of the gradient-descent method. We rigorously show that the
resulting pre-conditioned gradient-descent method, with the proposed iterative
pre-conditioning, achieves superlinear convergence when the least-squares
problem has a unique solution. In general, the convergence is linear with
improved rate of convergence in comparison to the traditional gradient-descent
method and the state-of-the-art accelerated gradient-descent methods. We
further illustrate the improved rate of convergence of our proposed algorithm
through experiments on different real-world least-squares problems in both
noise-free and noisy computation environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1"&gt;Kushal Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nirupam Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1"&gt;Nikhil Chopra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whittle Index for A Class of Restless Bandits with Imperfect Observations. (arXiv:2108.03812v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03812</id>
        <link href="http://arxiv.org/abs/2108.03812"/>
        <updated>2021-08-10T02:00:11.708Z</updated>
        <summary type="html"><![CDATA[We consider a class of restless bandit problems that finds a broad
application area in stochastic optimization, reinforcement learning and
operations research. In our model, there are $N$ independent $2$-state Markov
processes that may be observed and accessed for accruing rewards. The
observation is error-prone, i.e., both false alarm and miss detection may
happen. Furthermore, the user can only choose a subset of $M~(M<N)$ processes
to observe at each discrete time. If a process in state~$1$ is correctly
observed, then it will offer some reward. Due to the partial and imperfect
observation model, the system is formulated as a restless multi-armed bandit
problem with an information state space of uncountable cardinality. Restless
bandit problems with finite state spaces are PSPACE-HARD in general. In this
paper, we establish a low-complexity algorithm that achieves a strong
performance for this class of restless bandits. Under certain conditions, we
theoretically prove the existence (indexability) of Whittle index and its
equivalence to our algorithm. When those conditions do not hold, we show by
numerical experiments the near-optimal performance of our algorithm in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Keqin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Ting Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Linear Models Co-supervised by Deep Neural Networks. (arXiv:2108.04035v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04035</id>
        <link href="http://arxiv.org/abs/2108.04035"/>
        <updated>2021-08-10T02:00:11.707Z</updated>
        <summary type="html"><![CDATA[Deep neural network (DNN) models have achieved phenomenal success for
applications in many domains, ranging from academic research in science and
engineering to industry and business. The modeling power of DNN is believed to
have come from the complexity and over-parameterization of the model, which on
the other hand has been criticized for the lack of interpretation. Although
certainly not true for every application, in some applications, especially in
economics, social science, healthcare industry, and administrative decision
making, scientists or practitioners are resistant to use predictions made by a
black-box system for multiple reasons. One reason is that a major purpose of a
study can be to make discoveries based upon the prediction function, e.g., to
reveal the relationships between measurements. Another reason can be that the
training dataset is not large enough to make researchers feel completely sure
about a purely data-driven result. Being able to examine and interpret the
prediction function will enable researchers to connect the result with existing
knowledge or gain insights about new directions to explore. Although classic
statistical models are much more explainable, their accuracy often falls
considerably below DNN. In this paper, we propose an approach to fill the gap
between relatively simple explainable models and DNN such that we can more
flexibly tune the trade-off between interpretability and accuracy. Our main
idea is a mixture of discriminative models that is trained with the guidance
from a DNN. Although mixtures of discriminative models have been studied
before, our way of generating the mixture is quite different.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1"&gt;Beomseok Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Under Feature Exemptions: Counterfactual and Observational Measures. (arXiv:2006.07986v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07986</id>
        <link href="http://arxiv.org/abs/2006.07986"/>
        <updated>2021-08-10T02:00:11.707Z</updated>
        <summary type="html"><![CDATA[With the growing use of ML in highly consequential domains, quantifying
disparity with respect to protected attributes, e.g., gender, race, etc., is
important. While quantifying disparity is essential, sometimes the needs of an
occupation may require the use of certain features that are critical in a way
that any disparity that can be explained by them might need to be exempted.
E.g., in hiring a software engineer for a safety-critical application,
coding-skills may be weighed strongly, whereas name, zip code, or reference
letters may be used only to the extent that they do not add disparity. In this
work, we propose an information-theoretic decomposition of the total disparity
(a quantification inspired from counterfactual fairness) into two components: a
non-exempt component which quantifies the part that cannot be accounted for by
the critical features, and an exempt component that quantifies the remaining
disparity. This decomposition allows one to check if the disparity arose purely
due to the critical features (inspired from the business necessity defense of
disparate impact law) and also enables selective removal of the non-exempt
component if desired. We arrive at this decomposition through canonical
examples that lead to a set of desirable properties (axioms) that a measure of
non-exempt disparity should satisfy. Our proposed measure satisfies all of
them. Our quantification bridges ideas of causality, Simpson's paradox, and a
body of work from information theory called Partial Information Decomposition.
We also obtain an impossibility result showing that no observational measure
can satisfy all the desirable properties, leading us to relax our goals and
examine observational measures that satisfy only some of them. We perform case
studies to show how one can audit/train models while reducing non-exempt
disparity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Sanghamitra Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1"&gt;Praveen Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1"&gt;Piotr Mardziel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1"&gt;Anupam Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_P/0/1/0/all/0/1"&gt;Pulkit Grover&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Multiobjective Minimax Optimization and Applications. (arXiv:2108.03837v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03837</id>
        <link href="http://arxiv.org/abs/2108.03837"/>
        <updated>2021-08-10T02:00:11.706Z</updated>
        <summary type="html"><![CDATA[We introduce a simple but general online learning framework, in which at
every round, an adaptive adversary introduces a new game, consisting of an
action space for the learner, an action space for the adversary, and a vector
valued objective function that is convex-concave in every coordinate. The
learner and the adversary then play in this game. The learner's goal is to play
so as to minimize the maximum coordinate of the cumulative vector-valued loss.
The resulting one-shot game is not convex-concave, and so the minimax theorem
does not apply. Nevertheless, we give a simple algorithm that can compete with
the setting in which the adversary must announce their action first, with
optimally diminishing regret.

We demonstrate the power of our simple framework by using it to derive
optimal bounds and algorithms across a variety of domains. This includes no
regret learning: we can recover optimal algorithms and bounds for minimizing
external regret, internal regret, adaptive regret, multigroup regret,
subsequence regret, and a notion of regret in the sleeping experts setting.
Next, we use it to derive a variant of Blackwell's Approachability Theorem,
which we term "Fast Polytope Approachability". Finally, we are able to recover
recently derived algorithms and bounds for online adversarial multicalibration
and related notions (mean-conditioned moment multicalibration, and prediction
interval multivalidity).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noarov_G/0/1/0/all/0/1"&gt;Georgy Noarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_M/0/1/0/all/0/1"&gt;Mallesh Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14353</id>
        <link href="http://arxiv.org/abs/2012.14353"/>
        <updated>2021-08-10T02:00:11.706Z</updated>
        <summary type="html"><![CDATA[The exponential growths of social media and micro-blogging sites not only
provide platforms for empowering freedom of expressions and individual voices,
but also enables people to express anti-social behaviour like online
harassment, cyberbullying, and hate speech. Numerous works have been proposed
to utilize textual data for social and anti-social behaviour analysis, by
predicting the contexts mostly for highly-resourced languages like English.
However, some languages are under-resourced, e.g., South Asian languages like
Bengali, that lack computational resources for accurate natural language
processing (NLP). In this paper, we propose an explainable approach for hate
speech detection from the under-resourced Bengali language, which we called
DeepHateExplainer. Bengali texts are first comprehensively preprocessed, before
classifying them into political, personal, geopolitical, and religious hates
using a neural ensemble method of transformer-based neural architectures (i.e.,
monolingual Bangla BERT-base, multilingual BERT-cased/uncased, and
XLM-RoBERTa). Important(most and least) terms are then identified using
sensitivity analysis and layer-wise relevance propagation(LRP), before
providing human-interpretable explanations. Finally, we compute
comprehensiveness and sufficiency scores to measure the quality of explanations
w.r.t faithfulness. Evaluations against machine learning~(linear and tree-based
models) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word
embeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,
personal, geopolitical, and religious hates, respectively, outperforming both
ML and DNN baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"&gt;Sumon Kanti Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tanhim Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1"&gt;Sagor Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1"&gt;Mehadi Hasan Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1"&gt;Kabir Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Azam Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1"&gt;Stefan Decker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03457</id>
        <link href="http://arxiv.org/abs/2108.03457"/>
        <updated>2021-08-10T02:00:11.705Z</updated>
        <summary type="html"><![CDATA[Existing vision systems for autonomous driving or robots are sensitive to
waterdrops adhered to windows or camera lenses. Most recent waterdrop removal
approaches take a single image as input and often fail to recover the missing
content behind waterdrops faithfully. Thus, we propose a learning-based model
for waterdrop removal with stereo images. To better detect and remove
waterdrops from stereo images, we propose a novel row-wise dilated attention
module to enlarge attention's receptive field for effective information
propagation between the two stereo images. In addition, we propose an attention
consistency loss between the ground-truth disparity map and attention scores to
enhance the left-right consistency in stereo images. Because of related
datasets' unavailability, we collect a real-world dataset that contains stereo
images with and without waterdrops. Extensive experiments on our dataset
suggest that our model outperforms state-of-the-art methods both quantitatively
and qualitatively. Our source code and the stereo waterdrop dataset are
available at
\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zifan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1"&gt;Na Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1"&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-08-10T02:00:11.705Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) utilizes data or knowledge from one or more source
domains to facilitate the learning in a target domain. It is particularly
useful when the target domain has very few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source
domain data/knowledge undesirably reduces the learning performance in the
target domain, has been a long-standing and challenging problem in TL. Various
approaches have been proposed in the literature to handle it. However, there
does not exist a systematic survey on the formulation of NT, the factors
leading to NT, and the algorithms that mitigate NT. This paper fills this gap,
by first introducing the definition of NT and its factors, then reviewing about
fifty representative approaches for overcoming NT, according to four
categories: secure transfer, domain similarity estimation, distant transfer,
and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong
learning, and adversarial attacks, are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems. (arXiv:2108.03514v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.03514</id>
        <link href="http://arxiv.org/abs/2108.03514"/>
        <updated>2021-08-10T02:00:11.704Z</updated>
        <summary type="html"><![CDATA[The core network architecture of telecommunication systems has undergone a
paradigm shift in the fifth-generation (5G)networks. 5G networks have
transitioned to software-defined infrastructures, thereby reducing their
dependence on hardware-based network functions. New technologies, like network
function virtualization and software-defined networking, have been incorporated
in the 5G core network (5GCN) architecture to enable this transition. This has
resulted in significant improvements in efficiency, performance, and robustness
of the networks. However, this has also made the core network more vulnerable,
as software systems are generally easier to compromise than hardware systems.
In this article, we present a comprehensive security analysis framework for the
5GCN. The novelty of this approach lies in the creation and analysis of attack
graphs of the software-defined and virtualized 5GCN through machine learning.
This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate
that these possible exploits of 5GCN vulnerabilities generate five novel
attacks on the 5G Authentication and Key Agreement protocol. We combine the
attacks at the network, protocol, and the application layers to generate
complex attack vectors. In a case study, we use these attack vectors to find
four novel security loopholes in WhatsApp running on a 5G network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_T/0/1/0/all/0/1"&gt;Tanujay Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aaraj_N/0/1/0/all/0/1"&gt;Najwa Aaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Niraj K. Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05690</id>
        <link href="http://arxiv.org/abs/2010.05690"/>
        <updated>2021-08-10T02:00:11.704Z</updated>
        <summary type="html"><![CDATA[The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1"&gt;Lalith Bharadwaj B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1"&gt;Rohit Boddeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Sai Vardhan K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1"&gt;Madhu G&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03578</id>
        <link href="http://arxiv.org/abs/2108.03578"/>
        <updated>2021-08-10T02:00:11.703Z</updated>
        <summary type="html"><![CDATA[Although current state-of-the-art language models have achieved impressive
results in numerous natural language processing tasks, still they could not
solve the problem of producing repetitive, dull and sometimes inconsistent text
in open-ended text generation. Studies often attribute this problem to the
maximum likelihood training objective, and propose alternative approaches by
using stochastic decoding methods or altering the training objective. However,
there is still a lack of consistent evaluation metrics to directly compare the
efficacy of these solutions. In this work, we study different evaluation
metrics that have been proposed to evaluate quality, diversity and consistency
of machine-generated text. From there, we propose a practical pipeline to
evaluate language models in open-ended generation task, and research on how to
improve the model's performance in all dimensions by leveraging different
auxiliary training objectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.03706</id>
        <link href="http://arxiv.org/abs/2108.03706"/>
        <updated>2021-08-10T02:00:11.702Z</updated>
        <summary type="html"><![CDATA[The recent emergence of reinforcement learning has created a demand for
robust statistical inference methods for the parameter estimates computed using
these algorithms. Existing methods for statistical inference in online learning
are restricted to settings involving independently sampled observations, while
existing statistical inference methods in reinforcement learning (RL) are
limited to the batch setting. The online bootstrap is a flexible and efficient
approach for statistical inference in linear stochastic approximation
algorithms, but its efficacy in settings involving Markov noise, such as RL,
has yet to be explored. In this paper, we study the use of the online bootstrap
method for statistical inference in RL. In particular, we focus on the temporal
difference (TD) learning and Gradient TD (GTD) learning algorithms, which are
themselves special instances of linear stochastic approximation under Markov
noise. The method is shown to be distributionally consistent for statistical
inference in policy evaluation, and numerical experiments are included to
demonstrate the effectiveness of this algorithm at statistical inference tasks
across a range of real RL environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ramprasad_P/0/1/0/all/0/1"&gt;Pratik Ramprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuantong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1"&gt;Will Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03322</id>
        <link href="http://arxiv.org/abs/2108.03322"/>
        <updated>2021-08-10T02:00:11.701Z</updated>
        <summary type="html"><![CDATA[Pre-trained transformers have recently clinched top spots in the gamut of
natural language tasks and pioneered solutions to software engineering tasks.
Even information retrieval has not been immune to the charm of the transformer,
though their large size and cost is generally a barrier to deployment. While
there has been much work in streamlining, caching, and modifying transformer
architectures for production, here we explore a new direction: distilling a
large pre-trained translation model into a lightweight bi-encoder which can be
efficiently cached and queried. We argue from a probabilistic perspective that
sequence-to-sequence models are a conceptually ideal---albeit highly
impractical---retriever. We derive a new distillation objective, implementing
it as a data augmentation scheme. Using natural language source code search as
a case study for cross-domain search, we demonstrate the validity of this idea
by significantly improving upon the current leader of the CodeSearchNet
challenge, a recent natural language code search benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1"&gt;Colin B. Clement&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bob and Alice Go to a Bar: Reasoning About Future With Probabilistic Programs. (arXiv:2108.03834v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03834</id>
        <link href="http://arxiv.org/abs/2108.03834"/>
        <updated>2021-08-10T02:00:11.700Z</updated>
        <summary type="html"><![CDATA[Agent preferences should be specified stochastically rather than
deterministically. Planning as inference with stochastic preferences naturally
describes agent behaviors, does not require introducing rewards and exponential
weighing of behaviors, and allows to reason about agents using the solid
foundation of Bayesian statistics. Stochastic conditioning is the formalism
behind agents with stochastic preferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolpin_D/0/1/0/all/0/1"&gt;David Tolpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobkin_T/0/1/0/all/0/1"&gt;Tomer Dobkin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised learning of anomalous diffusion data. (arXiv:2108.03411v1 [cond-mat.stat-mech])]]></title>
        <id>http://arxiv.org/abs/2108.03411</id>
        <link href="http://arxiv.org/abs/2108.03411"/>
        <updated>2021-08-10T02:00:11.679Z</updated>
        <summary type="html"><![CDATA[The characterization of diffusion processes is a keystone in our
understanding of a variety of physical phenomena. Many of these deviate from
Brownian motion, giving rise to anomalous diffusion. Various theoretical models
exists nowadays to describe such processes, but their application to
experimental setups is often challenging, due to the stochastic nature of the
phenomena and the difficulty to harness reliable data. The latter often
consists on short and noisy trajectories, which are hard to characterize with
usual statistical approaches. In recent years, we have witnessed an impressive
effort to bridge theory and experiments by means of supervised machine learning
techniques, with astonishing results. In this work, we explore the use of
unsupervised methods in anomalous diffusion data. We show that the main
diffusion characteristics can be learnt without the need of any labelling of
the data. We use such method to discriminate between anomalous diffusion models
and extract their physical parameters. Moreover, we explore the feasibility of
finding novel types of diffusion, in this case represented by compositions of
existing diffusion models. At last, we showcase the use of the method in
experimental data and demonstrate its advantages for cases where supervised
learning is not applicable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1"&gt;Gorka Mu&amp;#xf1;oz-Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Corominas_G/0/1/0/all/0/1"&gt;Guillem Guig&amp;#xf3; i Corominas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1"&gt;Maciej Lewenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Last Iterate Convergence in Overparameterized GANs. (arXiv:2108.03491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03491</id>
        <link href="http://arxiv.org/abs/2108.03491"/>
        <updated>2021-08-10T02:00:11.672Z</updated>
        <summary type="html"><![CDATA[In this work, we showed that the Implicit Update and Predictive Methods
dynamics introduced in prior work satisfy last iterate convergence to a
neighborhood around the optimum in overparameterized GANs, where the size of
the neighborhood shrinks with the width of the neural network. This is in
contrast to prior results, which only guaranteed average iterate convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_E/0/1/0/all/0/1"&gt;Elbert Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs. (arXiv:2108.03348v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03348</id>
        <link href="http://arxiv.org/abs/2108.03348"/>
        <updated>2021-08-10T02:00:11.665Z</updated>
        <summary type="html"><![CDATA[Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty in
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. This simple addition allows us to use
global self-attention, the key element of transformers, directly for graphs and
comes with the benefit of long-range interaction among nodes. Moreover, the
edge channels allow the structural information to evolve from layer to layer,
and prediction tasks on edges can be derived directly from these channels. In
addition to that, we introduce positional encodings based on Singular Value
Decomposition which can improve the performance of EGT. Our framework, which
relies on global node feature aggregation, achieves better performance compared
to Graph Convolutional Networks (GCN), which rely on local feature aggregation
within a neighborhood. We verify the performance of EGT in a supervised
learning setting on a wide range of experiments on benchmark datasets. Our
findings indicate that convolutional aggregation is not an essential inductive
bias for graphs and global self-attention can serve as a flexible and adaptive
alternative to graph convolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1"&gt;Md Shamim Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1"&gt;Mohammed J. Zaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1"&gt;Dharmashankar Subramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-10T02:00:11.652Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00591</id>
        <link href="http://arxiv.org/abs/2105.00591"/>
        <updated>2021-08-10T02:00:11.645Z</updated>
        <summary type="html"><![CDATA[In the past few years, mobile deep-learning deployment progressed by leaps
and bounds, but solutions still struggle to accommodate its severe and
fluctuating operational restrictions, which include bandwidth, latency,
computation, and energy. In this work, we help to bridge that gap, introducing
the first configurable solution for object detection that manages the triple
communication-computation-accuracy trade-off with a single set of weights. Our
solution shows state-of-the-art results on COCO-2017, adding only a minor
penalty on the base EfficientDet-D2 architecture. Our design is robust to the
choice of base architecture and compressor and should adapt well for future
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1"&gt;Juliano S. Assine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1"&gt;J. C. S. Santos Filho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1"&gt;Eduardo Valle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Representation for Electric Vehicle Charging Station Operations using Reinforcement Learning. (arXiv:2108.03236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03236</id>
        <link href="http://arxiv.org/abs/2108.03236"/>
        <updated>2021-08-10T02:00:11.596Z</updated>
        <summary type="html"><![CDATA[Effectively operating electrical vehicle charging station (EVCS) is crucial
for enabling the rapid transition of electrified transportation. To solve this
problem using reinforcement learning (RL), the dimension of state/action spaces
scales with the number of EVs and is thus very large and time-varying. This
dimensionality issue affects the efficiency and convergence properties of
generic RL algorithms. We develop aggregation schemes that are based on the
emergency of EV charging, namely the laxity value. A least-laxity first (LLF)
rule is adopted to consider only the total charging power of the EVCS which
ensures the feasibility of individual EV schedules. In addition, we propose an
equivalent state aggregation that can guarantee to attain the same optimal
policy. Based on the proposed representation, policy gradient method is used to
find the best parameters for the linear Gaussian policy . Numerical results
have validated the performance improvement of the proposed representation
approaches in attaining higher rewards and more effective policies as compared
to existing approximation based approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_K/0/1/0/all/0/1"&gt;Kyung-bin Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00689</id>
        <link href="http://arxiv.org/abs/2107.00689"/>
        <updated>2021-08-10T02:00:11.466Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel approach to map-based navigation system for
unmanned aircraft. The proposed system attempts label-to-label matching, not
image-to-image matching, between aerial images and a map database. By using
semantic segmentation, the ground objects are labelled and the configuration of
the objects is used to find the corresponding location in the map database. The
use of the deep learning technique as a tool for extracting high-level features
reduces the image-based localization problem to a pattern matching problem.
This paper proposes a pattern matching algorithm which does not require
altitude information or a camera model to estimate the absolute horizontal
position. The feasibility analysis with simulated images shows the proposed
map-based navigation can be realized with the proposed pattern matching
algorithm and it is able to provide positions given the labelled objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Youngjoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review. (arXiv:2105.04881v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04881</id>
        <link href="http://arxiv.org/abs/2105.04881"/>
        <updated>2021-08-10T02:00:11.465Z</updated>
        <summary type="html"><![CDATA[Multiple Sclerosis (MS) is a type of brain disease which causes visual,
sensory, and motor problems for people with a detrimental effect on the
functioning of the nervous system. In order to diagnose MS, multiple screening
methods have been proposed so far; among them, magnetic resonance imaging (MRI)
has received considerable attention among physicians. MRI modalities provide
physicians with fundamental information about the structure and function of the
brain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS
using MRI is time-consuming, tedious, and prone to manual errors. Hence,
computer aided diagnosis systems (CADS) based on artificial intelligence (AI)
methods have been proposed in recent years for accurate diagnosis of MS using
MRI neuroimaging modalities. In the AI field, automated MS diagnosis is being
conducted using (i) conventional machine learning and (ii) deep learning (DL)
techniques. The conventional machine learning approach is based on feature
extraction and selection by trial and error. In DL, these steps are performed
by the DL model itself. In this paper, a complete review of automated MS
diagnosis methods performed using DL techniques with MRI neuroimaging
modalities are discussed. Also, each work is thoroughly reviewed and discussed.
Finally, the most important challenges and future directions in the automated
MS diagnosis using DL techniques coupled with MRI modalities are presented in
detail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khodatars_M/0/1/0/all/0/1"&gt;Marjane Khodatars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jafari_M/0/1/0/all/0/1"&gt;Mahboobeh Jafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moridian_P/0/1/0/all/0/1"&gt;Parisa Moridian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rezaei_M/0/1/0/all/0/1"&gt;Mitra Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khozeimeh_F/0/1/0/all/0/1"&gt;Fahime Khozeimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;Juan Manuel Gorriz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heras_J/0/1/0/all/0/1"&gt;J&amp;#xf3;nathan Heras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Panahiazar_M/0/1/0/all/0/1"&gt;Maryam Panahiazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1"&gt;U. Rajendra Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11769</id>
        <link href="http://arxiv.org/abs/2107.11769"/>
        <updated>2021-08-10T02:00:11.465Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep learning on supervised point cloud semantic
segmentation, obtaining large-scale point-by-point manual annotations is still
a significant challenge. To reduce the huge annotation burden, we propose a
Region-based and Diversity-aware Active Learning (ReDAL), a general framework
for many deep learning approaches, aiming to automatically select only
informative and diverse sub-scene regions for label acquisition. Observing that
only a small portion of annotated regions are sufficient for 3D scene
understanding with deep learning, we use softmax entropy, color discontinuity,
and structural complexity to measure the information of sub-scene regions. A
diversity-aware selection algorithm is also developed to avoid redundant
annotations resulting from selecting informative but similar regions in a
querying batch. Extensive experiments show that our method highly outperforms
previous active learning strategies, and we achieve the performance of 90%
fully supervised learning, while less than 15% and 5% annotations are required
on S3DIS and SemanticKITTI datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Ping-Chia Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12666</id>
        <link href="http://arxiv.org/abs/2107.12666"/>
        <updated>2021-08-10T02:00:11.462Z</updated>
        <summary type="html"><![CDATA[Text-to-image person re-identification (ReID) aims to search for images
containing a person of interest using textual descriptions. However, due to the
significant modality gap and the large intra-class variance in textual
descriptions, text-to-image ReID remains a challenging problem. Accordingly, in
this paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the
above problems. First, we propose a novel method that automatically extracts
semantically aligned part-level features from the two modalities. Second, we
design a multi-view non-local network that captures the relationships between
body parts, thereby establishing better correspondences between body parts and
noun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use
of textual descriptions for other images of the same identity to provide extra
supervision, thereby effectively reducing the intra-class variance in textual
features. Finally, to expedite future research in text-to-image ReID, we build
a new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN
outperforms state-of-the-art approaches by significant margins. Both the new
ICFG-PEDES database and the SSAN code are available at
https://github.com/zifyloo/SSAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zefeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Changxing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhiyin Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12960</id>
        <link href="http://arxiv.org/abs/2107.12960"/>
        <updated>2021-08-10T02:00:11.461Z</updated>
        <summary type="html"><![CDATA[Effectively tackling the problem of temporal action localization (TAL)
necessitates a visual representation that jointly pursues two confounding
goals, i.e., fine-grained discrimination for temporal localization and
sufficient visual invariance for action classification. We address this
challenge by enriching both the local and global contexts in the popular
two-stage temporal localization framework, where action proposals are first
generated followed by action classification and temporal boundary regression.
Our proposed model, dubbed ContextLoc, can be divided into three sub-networks:
L-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained
modeling of snippet-level features, which is formulated as a
query-and-retrieval process. G-Net enriches the global context via higher-level
modeling of the video-level representation. In addition, we introduce a novel
context adaptation module to adapt the global context to different proposals.
P-Net further models the context-aware inter-proposal relations. We explore two
existing models to be the P-Net in our experiments. The efficacy of our
proposed method is validated by experimental results on the THUMOS14 (54.3\% at
tIoU@0.5) and ActivityNet v1.3 (56.01\% at tIoU@0.5) datasets, which
outperforms recent states of the art. Code is available at
https://github.com/buxiangzhiren/ContextLoc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zixin Zhu&lt;/a&gt; (Xi&amp;#x27;an jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt; (University of Illinois at Chicago), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt; (Xi&amp;#x27;an Jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt; (Xi&amp;#x27;an Jiaotong University), &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt; (Wormpex AI Research)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14572</id>
        <link href="http://arxiv.org/abs/2107.14572"/>
        <updated>2021-08-10T02:00:11.461Z</updated>
        <summary type="html"><![CDATA[Nowadays, customer's demands for E-commerce are more diversified, which
introduces more complications to the product retrieval industry. Previous
methods are either subject to single-modal input or perform supervised
image-level product retrieval, thus fail to accommodate real-life scenarios
where enormous weakly annotated multi-modal data are present. In this paper, we
investigate a more realistic setting that aims to perform weakly-supervised
multi-modal instance-level product retrieval among fine-grained product
categories. To promote the study of this challenging task, we contribute
Product1M, one of the largest multi-modal cosmetic datasets for real-world
instance-level retrieval. Notably, Product1M contains over 1 million
image-caption pairs and consists of two sample types, i.e., single-product and
multi-product samples, which encompass a wide variety of cosmetics brands. In
addition to the great diversity, Product1M enjoys several appealing
characteristics including fine-grained categories, complex combinations, and
fuzzy correspondence that well mimic the real-world scenes. Moreover, we
propose a novel model named Cross-modal contrAstive Product Transformer for
instance-level prodUct REtrieval (CAPTURE), that excels in capturing the
potential synergy between multi-modal inputs via a hybrid-stream transformer in
a self-supervised manner.CAPTURE generates discriminative instance features via
masked multi-modal learning as well as cross-modal contrastive pretraining and
it outperforms several SOTA cross-modal baselines. Extensive ablation studies
well demonstrate the effectiveness and the generalization capacity of our
model. Dataset and codes are available at https:
//github.com/zhanxlin/Product1M.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xunlin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yangxin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Minlong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03894</id>
        <link href="http://arxiv.org/abs/2108.03894"/>
        <updated>2021-08-10T02:00:11.460Z</updated>
        <summary type="html"><![CDATA[We introduce FIFA, a fast approximate inference method for action
segmentation and alignment. Unlike previous approaches, FIFA does not rely on
expensive dynamic programming for inference. Instead, it uses an approximate
differentiable energy function that can be minimized using gradient-descent.
FIFA is a general approach that can replace exact inference improving its speed
by more than 5 times while maintaining its performance. FIFA is an anytime
inference algorithm that provides a better speed vs. accuracy trade-off
compared to exact inference. We apply FIFA on top of state-of-the-art
approaches for weakly supervised action segmentation and alignment as well as
fully supervised action segmentation. FIFA achieves state-of-the-art results on
most metrics on two action segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1"&gt;Yaser Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1"&gt;Yazan Abu Farha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1"&gt;Fabien Despinoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1"&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some thoughts on catastrophic forgetting and how to learn an algorithm. (arXiv:2108.03940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03940</id>
        <link href="http://arxiv.org/abs/2108.03940"/>
        <updated>2021-08-10T02:00:11.460Z</updated>
        <summary type="html"><![CDATA[The work of McCloskey and Cohen popularized the concept of catastrophic
interference. They used a neural network that tried to learn addition using two
groups of examples as two different tasks. In their case, learning the second
task rapidly deteriorated the acquired knowledge about the previous one. This
could be a symptom of a fundamental problem: addition is an algorithmic task
that should not be learned through pattern recognition. We propose to use a
neural network with a different architecture that can be trained to recover the
correct algorithm for the addition of binary numbers. We test it in the setting
proposed by McCloskey and Cohen and training on random examples one by one. The
neural network not only does not suffer from catastrophic forgetting but it
improves its predictive power on unseen pairs of numbers as training
progresses. This work emphasizes the importance that neural network
architecture has for the emergence of catastrophic forgetting and introduces a
neural network that is able to learn an algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1"&gt;Miguel Ruiz-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimizing Labeling Effort for Tree Skeleton Segmentation using an Automated Iterative Training Methodology. (arXiv:2010.08296v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08296</id>
        <link href="http://arxiv.org/abs/2010.08296"/>
        <updated>2021-08-10T02:00:11.455Z</updated>
        <summary type="html"><![CDATA[Training of convolutional neural networks for semantic segmentation requires
accurate pixel-wise labeling which requires large amounts of human effort. The
human-in-the-loop method reduces labeling effort; however, it requires human
intervention for each image. This paper describes a general iterative training
methodology for semantic segmentation, Automating-the-Loop. This aims to
replicate the manual adjustments of the human-in-the-loop method with an
automated process, hence, drastically reducing labeling effort. Using the
application of detecting partially occluded apple tree segmentation, we compare
manually labeled annotations, self-training, human-in-the-loop, and
Automating-the-Loop methods in both the quality of the trained convolutional
neural networks, and the effort needed to create them. The convolutional neural
network (U-Net) performance is analyzed using traditional metrics and a new
metric, Complete Grid Scan, which promotes connectivity and low noise. It is
shown that in our application, the new Automating-the-Loop method greatly
reduces the labeling effort while producing comparable performance to both
human-in-the-loop and complete manual labeling methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granland_K/0/1/0/all/0/1"&gt;Keenan Granland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newbury_R/0/1/0/all/0/1"&gt;Rhys Newbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1"&gt;David Ting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12746</id>
        <link href="http://arxiv.org/abs/2107.12746"/>
        <updated>2021-08-10T02:00:11.455Z</updated>
        <summary type="html"><![CDATA[Localizing individuals in crowds is more in accordance with the practical
demands of subsequent high-level crowd analysis tasks than simply counting.
However, existing localization based methods relying on intermediate
representations (\textit{i.e.}, density maps or pseudo boxes) serving as
learning targets are counter-intuitive and error-prone. In this paper, we
propose a purely point-based framework for joint crowd counting and individual
localization. For this framework, instead of merely reporting the absolute
counting error at image level, we propose a new metric, called density
Normalized Average Precision (nAP), to provide more comprehensive and more
precise performance evaluation. Moreover, we design an intuitive solution under
this framework, which is called Point to Point Network (P2PNet). P2PNet
discards superfluous steps and directly predicts a set of point proposals to
represent heads in an image, being consistent with the human annotation
results. By thorough analysis, we reveal the key step towards implementing such
a novel idea is to assign optimal learning targets for these proposals.
Therefore, we propose to conduct this crucial association in an one-to-one
matching manner using the Hungarian algorithm. The P2PNet not only
significantly surpasses state-of-the-art methods on popular counting
benchmarks, but also achieves promising localization accuracy. The codes will
be available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06534</id>
        <link href="http://arxiv.org/abs/2104.06534"/>
        <updated>2021-08-10T02:00:11.454Z</updated>
        <summary type="html"><![CDATA[Existing thermal-to-visible face verification approaches expect the thermal
and visible face images to be of similar resolution. This is unlikely in
real-world long-range surveillance systems, since humans are distant from the
cameras. To address this issue, we introduce the task of thermal-to-visible
face verification from low-resolution thermal images. Furthermore, we propose
Axial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution
visible images for matching. In the proposed approach we augment the GAN
framework with axial-attention layers which leverage the recent advances in
transformers for modelling long-range dependencies. We demonstrate the
effectiveness of the proposed method by evaluating on two different
thermal-visible face datasets. When compared to related state-of-the-art works,
our results show significant improvements in both image quality and face
verification performance, and are also much more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1"&gt;Rakhil Immidisetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shuowen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-08-10T02:00:11.452Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-08-10T02:00:11.449Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection using Edge Computing in Video Surveillance System: Review. (arXiv:2107.02778v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02778</id>
        <link href="http://arxiv.org/abs/2107.02778"/>
        <updated>2021-08-10T02:00:11.439Z</updated>
        <summary type="html"><![CDATA[The current concept of Smart Cities influences urban planners and researchers
to provide modern, secured and sustainable infrastructure and give a decent
quality of life to its residents. To fulfill this need video surveillance
cameras have been deployed to enhance the safety and well-being of the
citizens. Despite technical developments in modern science, abnormal event
detection in surveillance video systems is challenging and requires exhaustive
human efforts. In this paper, we surveyed various methodologies developed to
detect anomalies in intelligent video surveillance. Firstly, we revisit the
surveys on anomaly detection in the last decade. We then present a systematic
categorization of methodologies developed for ease of understanding.
Considering the notion of anomaly depends on context, we identify different
objects-of-interest and publicly available datasets in anomaly detection. Since
anomaly detection is considered a time-critical application of computer vision,
our emphasis is on anomaly detection using edge devices and approaches
explicitly designed for them. Further, we discuss the challenges and
opportunities involved in anomaly detection at the edge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patrikar_D/0/1/0/all/0/1"&gt;Devashree R. Patrikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1"&gt;Mayur Rajram Parate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-view: Diagnosis of COVID-19 using Chest CT. (arXiv:2108.03799v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03799</id>
        <link href="http://arxiv.org/abs/2108.03799"/>
        <updated>2021-08-10T02:00:11.405Z</updated>
        <summary type="html"><![CDATA[Significant work has been done towards deep learning (DL) models for
automatic lung and lesion segmentation and classification of COVID-19 on chest
CT data. However, comprehensive visualization systems focused on supporting the
dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a
visualization application specially tailored for radiologists to diagnose
COVID-19 from chest CT data. The system incorporates a complete pipeline of
automatic lungs segmentation, localization/ isolation of lung abnormalities,
followed by visualization, visual and DL analysis, and
measurement/quantification tools. Our system combines the traditional 2D
workflow of radiologists with newer 2D and 3D visualization techniques with DL
support for a more comprehensive diagnosis. COVID-view incorporates a novel DL
model for classifying the patients into positive/negative COVID-19 cases, which
acts as a reading aid for the radiologist using COVID-view and provides the
attention heatmap as an explainable DL for the model output. We designed and
evaluated COVID-view through suggestions, close feedback and conducting case
studies of real-world patient data by expert radiologists who have substantial
experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and
other forms of lung infections. We present requirements and task analysis for
the diagnosis of COVID-19 that motivate our design choices and results in a
practical system which is capable of handling real-world patient cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1"&gt;Shreeraj Jadhav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1"&gt;Gaofeng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zawin_M/0/1/0/all/0/1"&gt;Marlene Zawin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufman_A/0/1/0/all/0/1"&gt;Arie E. Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistence Curves: A canonical framework for summarizing persistence diagrams. (arXiv:1904.07768v4 [cs.CG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.07768</id>
        <link href="http://arxiv.org/abs/1904.07768"/>
        <updated>2021-08-10T02:00:11.405Z</updated>
        <summary type="html"><![CDATA[Persistence diagrams are one of the main tools in the field of Topological
Data Analysis (TDA). They contain fruitful information about the shape of data.
The use of machine learning algorithms on the space of persistence diagrams
proves to be challenging as the space lacks an inner product. For that reason,
transforming these diagrams in a way that is compatible with machine learning
is an important topic currently researched in TDA. In this paper, our main
contribution consists of three components. First, we develop a general and
unifying framework of vectorizing diagrams that we call the \textit{Persistence
Curves} (PCs), and show that several well-known summaries, such as Persistence
Landscapes, fall under the PC framework. Second, we propose several new
summaries based on PC framework and provide a theoretical foundation for their
stability analysis. Finally, we apply proposed PCs to two
applications---texture classification and determining the parameters of a
discrete dynamical system; their performances are competitive with other TDA
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1"&gt;Yu-Min Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawson_A/0/1/0/all/0/1"&gt;Austin Lawson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation. (arXiv:2103.11661v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11661</id>
        <link href="http://arxiv.org/abs/2103.11661"/>
        <updated>2021-08-10T02:00:11.405Z</updated>
        <summary type="html"><![CDATA[Many unsupervised domain adaptation (UDA) methods exploit domain adversarial
training to align the features to reduce domain gap, where a feature extractor
is trained to fool a domain discriminator in order to have aligned feature
distributions. The discrimination capability of the domain classifier w.r.t the
increasingly aligned feature distributions deteriorates as training goes on,
thus cannot effectively further drive the training of feature extractor. In
this work, we propose an efficient optimization strategy named Re-enforceable
Adversarial Domain Adaptation (RADA) which aims to re-energize the domain
discriminator during the training by using dynamic domain labels. Particularly,
we relabel the well aligned target domain samples as source domain samples on
the fly. Such relabeling makes the less separable distributions more separable,
and thus leads to a more powerful domain classifier w.r.t. the new data
distributions, which in turn further drives feature alignment. Extensive
experiments on multiple UDA benchmarks demonstrate the effectiveness and
superiority of our RADA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12048</id>
        <link href="http://arxiv.org/abs/2107.12048"/>
        <updated>2021-08-10T02:00:11.405Z</updated>
        <summary type="html"><![CDATA[Decentralized federated learning (DFL) is a powerful framework of distributed
machine learning and decentralized stochastic gradient descent (SGD) is a
driving engine for DFL. The performance of decentralized SGD is jointly
influenced by communication-efficiency and convergence rate. In this paper, we
propose a general decentralized federated learning framework to strike a
balance between communication-efficiency and convergence performance. The
proposed framework performs both multiple local updates and multiple inter-node
communications periodically, unifying traditional decentralized SGD methods. We
establish strong convergence guarantees for the proposed DFL algorithm without
the assumption of convex objective function. The balance of communication and
computation rounds is essential to optimize decentralized federated learning
under constrained communication and computation resources. For further
improving communication-efficiency of DFL, compressed communication is applied
to DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL
exhibits linear convergence for strongly convex objectives. Experiment results
based on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over
traditional decentralized SGD methods and show that C-DFL further enhances
communication-efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenyi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Adaptive Transfer Learning for Multicenter Glaucoma Classification in Fundus Retina Images. (arXiv:2105.03068v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03068</id>
        <link href="http://arxiv.org/abs/2105.03068"/>
        <updated>2021-08-10T02:00:11.404Z</updated>
        <summary type="html"><![CDATA[The early diagnosis and screening of glaucoma are important for patients to
receive treatment in time and maintain eyesight. Nowadays, deep learning (DL)
based models have been successfully used for computer-aided diagnosis (CAD) of
glaucoma from retina fundus images. However, a DL model pre-trained using a
dataset from one hospital center may have poor performance on a dataset from
another new hospital center and therefore its applications in the real scene
are limited. In this paper, we propose a self-adaptive transfer learning (SATL)
strategy to fill the domain gap between multicenter datasets. Specifically, the
encoder of a DL model that is pre-trained on the source domain is used to
initialize the encoder of a reconstruction model. Then, the reconstruction
model is trained using only unlabeled image data from the target domain, which
makes the encoder in the model adapt itself to extract useful high-level
features both for target domain images encoding and glaucoma classification,
simultaneously. Experimental results demonstrate that the proposed SATL
strategy is effective in the domain adaptation task between one private and two
public glaucoma diagnosis datasets, i.e. pri-RFG, REFUGE, and LAG. Moreover,
the proposed strategy is completely independent of the source domain data,
which meets the real scene application and the privacy protection policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1"&gt;Yiming Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Tong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1"&gt;Juan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qian_D/0/1/0/all/0/1"&gt;Dahong Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-08-10T02:00:11.403Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) utilizes data or knowledge from one or more source
domains to facilitate the learning in a target domain. It is particularly
useful when the target domain has very few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source
domain data/knowledge undesirably reduces the learning performance in the
target domain, has been a long-standing and challenging problem in TL. Various
approaches have been proposed in the literature to handle it. However, there
does not exist a systematic survey on the formulation of NT, the factors
leading to NT, and the algorithms that mitigate NT. This paper fills this gap,
by first introducing the definition of NT and its factors, then reviewing about
fifty representative approaches for overcoming NT, according to four
categories: secure transfer, domain similarity estimation, distant transfer,
and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong
learning, and adversarial attacks, are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05248</id>
        <link href="http://arxiv.org/abs/2103.05248"/>
        <updated>2021-08-10T02:00:11.403Z</updated>
        <summary type="html"><![CDATA[Recent studies unveil the vulnerabilities of deep ranking models, where an
imperceptible perturbation can trigger dramatic changes in the ranking result.
While previous attempts focus on manipulating absolute ranks of certain
candidates, the possibility of adjusting their relative order remains
under-explored. In this paper, we formulate a new adversarial attack against
deep ranking systems, i.e., the Order Attack, which covertly alters the
relative order among a selected set of candidates according to an
attacker-specified permutation, with limited interference to other unrelated
candidates. Specifically, it is formulated as a triplet-style loss imposing an
inequality chain reflecting the specified permutation. However, direct
optimization of such white-box objective is infeasible in a real-world attack
scenario due to various black-box limitations. To cope with them, we propose a
Short-range Ranking Correlation metric as a surrogate objective for black-box
Order Attack to approximate the white-box method. The Order Attack is evaluated
on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box
and black-box threat models. The black-box attack is also successfully
implemented on a major e-commerce platform. Comprehensive experimental
evaluations demonstrate the effectiveness of the proposed methods, revealing a
new type of ranking model vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-10T02:00:11.403Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stream Convolutional Networks for Multi-frame Face Anti-spoofing. (arXiv:2108.04032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04032</id>
        <link href="http://arxiv.org/abs/2108.04032"/>
        <updated>2021-08-10T02:00:11.402Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing is an important task to protect the security of face
recognition. Most of previous work either struggle to capture discriminative
and generalizable feature or rely on auxiliary information which is unavailable
for most of industrial product. Inspired by the video classification work, we
propose an efficient two-stream model to capture the key differences between
live and spoof faces, which takes multi-frames and RGB difference as input
respectively. Feature pyramid modules with two opposite fusion directions and
pyramid pooling modules are applied to enhance feature representation. We
evaluate the proposed method on the datasets of Siw, Oulu-NPU, CASIA-MFSD and
Replay-Attack. The results show that our model achieves the state-of-the-art
results on most of datasets' protocol with much less parameter size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Cheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xiya Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03815</id>
        <link href="http://arxiv.org/abs/2108.03815"/>
        <updated>2021-08-10T02:00:11.401Z</updated>
        <summary type="html"><![CDATA[To mitigate the inspector's workload and improve the quality of the product,
computer vision-based anomaly detection (AD) techniques are gradually deployed
in real-world industrial scenarios. Recent anomaly analysis benchmarks progress
to generative models. The aim is to model the defect-free distribution so that
anomalies can be classified as out-of-distribution samples. Nevertheless, there
are two disturbing factors that need researchers and deployers to prioritize:
(i) the simplistic prior latent distribution inducing limited expressive
capability; (ii) the collapsed mutual-dependent features resulting in poor
generalization. In this paper, we propose a novel Patch-wise Wasserstein
AutoEncoder (P-WAE) architecture to alleviate those challenges. In particular,
a patch-wise variational inference model coupled with solving the jigsaw puzzle
is designed, which is a simple yet effective way to increase the expressiveness
and complexity of the latent manifold. This alleviates the blurry
reconstruction problem. In addition, the Hilbert-Schmidt Independence Criterion
(HSIC) bottleneck is introduced to constrain the over-regularization
representation. Comprehensive experiments, conducted on the MVTec AD dataset,
demonstrate the superior performance of our propo]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yurong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Multi-Scale Loss Optimization for Object Detection. (arXiv:2108.04014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04014</id>
        <link href="http://arxiv.org/abs/2108.04014"/>
        <updated>2021-08-10T02:00:11.401Z</updated>
        <summary type="html"><![CDATA[With the continuous improvement of the performance of object detectors via
advanced model architectures, imbalance problems in the training process have
received more attention. It is a common paradigm in object detection frameworks
to perform multi-scale detection. However, each scale is treated equally during
training. In this paper, we carefully study the objective imbalance of
multi-scale detector training. We argue that the loss in each scale level is
neither equally important nor independent. Different from the existing
solutions of setting multi-task weights, we dynamically optimize the loss
weight of each scale level in the training process. Specifically, we propose an
Adaptive Variance Weighting (AVW) to balance multi-scale loss according to the
statistical variance. Then we develop a novel Reinforcement Learning
Optimization (RLO) to decide the weighting scheme probabilistically during
training. The proposed dynamic methods make better utilization of multi-scale
training loss without extra computational complexity and learnable parameters
for backpropagation. Experiments show that our approaches can consistently
boost the performance over various baseline detectors on Pascal VOC and MS COCO
benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yihao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juntao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Peng Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Paint Transformer: Feed Forward Neural Painting with Stroke Prediction. (arXiv:2108.03798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03798</id>
        <link href="http://arxiv.org/abs/2108.03798"/>
        <updated>2021-08-10T02:00:11.400Z</updated>
        <summary type="html"><![CDATA[Neural painting refers to the procedure of producing a series of strokes for
a given image and non-photo-realistically recreating it using neural networks.
While reinforcement learning (RL) based agents can generate a stroke sequence
step by step for this task, it is not easy to train a stable RL agent. On the
other hand, stroke optimization methods search for a set of stroke parameters
iteratively in a large search space; such low efficiency significantly limits
their prevalence and practicality. Different from previous methods, in this
paper, we formulate the task as a set prediction problem and propose a novel
Transformer-based framework, dubbed Paint Transformer, to predict the
parameters of a stroke set with a feed forward network. This way, our model can
generate a set of strokes in parallel and obtain the final painting of size 512
* 512 in near real time. More importantly, since there is no dataset available
for training the Paint Transformer, we devise a self-training pipeline such
that it can be trained without any off-the-shelf dataset while still achieving
excellent generalization capability. Experiments demonstrate that our method
achieves better painting performance than previous ones with cheaper training
and inference costs. Codes and models are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruifeng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-based Differentiable Depth Sensor Simulation. (arXiv:2103.16563v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16563</id>
        <link href="http://arxiv.org/abs/2103.16563"/>
        <updated>2021-08-10T02:00:11.400Z</updated>
        <summary type="html"><![CDATA[Gradient-based algorithms are crucial to modern computer-vision and graphics
applications, enabling learning-based optimization and inverse problems. For
example, photorealistic differentiable rendering pipelines for color images
have been proven highly valuable to applications aiming to map 2D and 3D
domains. However, to the best of our knowledge, no effort has been made so far
towards extending these gradient-based methods to the generation of depth
(2.5D) images, as simulating structured-light depth sensors implies solving
complex light transport and stereo-matching problems. In this paper, we
introduce a novel end-to-end differentiable simulation pipeline for the
generation of realistic 2.5D scans, built on physics-based 3D rendering and
custom block-matching algorithms. Each module can be differentiated w.r.t
sensor and scene parameters; e.g., to automatically tune the simulation for new
devices over some provided scans or to leverage the pipeline as a 3D-to-2.5D
transformer within larger computer-vision applications. Applied to the training
of deep-learning methods for various depth-based recognition tasks
(classification, pose estimation, semantic segmentation), our simulation
greatly improves the performance of the resulting models on real scans, thereby
demonstrating the fidelity and value of its synthetic depth data compared to
previous static simulations and learning-based domain adaptation schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1"&gt;Benjamin Planche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajat Vikram Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03893</id>
        <link href="http://arxiv.org/abs/2108.03893"/>
        <updated>2021-08-10T02:00:11.398Z</updated>
        <summary type="html"><![CDATA[Self-supervised deep learning-based 3D scene understanding methods can
overcome the difficulty of acquiring the densely labeled ground-truth and have
made a lot of advances. However, occlusions and moving objects are still some
of the major limitations. In this paper, we explore the learnable occlusion
aware optical flow guided self-supervised depth and camera pose estimation by
an adaptive cross weighted loss to address the above limitations. Firstly, we
explore to train the learnable occlusion mask fused optical flow network by an
occlusion-aware photometric loss with the temporally supplemental information
and backward-forward consistency of adjacent views. And then, we design an
adaptive cross-weighted loss between the depth-pose and optical flow loss of
the geometric and photometric error to distinguish the moving objects which
violate the static scene assumption. Our method shows promising results on
KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good
generalization ability under a variety of challenging scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiaojiao Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15481</id>
        <link href="http://arxiv.org/abs/2106.15481"/>
        <updated>2021-08-10T02:00:11.346Z</updated>
        <summary type="html"><![CDATA[Finding the similarities and differences between groups of datasets is a
fundamental analysis task. For high-dimensional data, dimensionality reduction
(DR) methods are often used to find the characteristics of each group. However,
existing DR methods provide limited capability and flexibility for such
comparative analysis as each method is designed only for a narrow analysis
target, such as identifying factors that most differentiate groups. This paper
presents an interactive DR framework where we integrate our new DR method,
called ULCA (unified linear comparative analysis), with an interactive visual
interface. ULCA unifies two DR schemes, discriminant analysis and contrastive
learning, to support various comparative analysis tasks. To provide flexibility
for comparative analysis, we develop an optimization algorithm that enables
analysts to interactively refine ULCA results. Additionally, the interactive
visualization interface facilitates interpretation and refinement of the ULCA
results. We evaluate ULCA and the optimization algorithm to show their
efficiency as well as present multiple case studies using real-world datasets
to demonstrate the usefulness of this framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1"&gt;Takanori Fujiwara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xinhai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kwan-Liu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks. (arXiv:2108.04029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04029</id>
        <link href="http://arxiv.org/abs/2108.04029"/>
        <updated>2021-08-10T02:00:11.345Z</updated>
        <summary type="html"><![CDATA[Nowadays Deep Learning became widely used in many economic, technical and
scientific areas of human interest. It is clear that efficiency of solutions
based on Deep Neural Networks should consider not only quality metric for the
target task, but also latency and constraints of target platform design should
be taken into account. In this paper we present novel hardware-friendly
Tensor-Train decomposition implementation for Convolutional Neural Networks
together with Tensor Yard - one-shot training algorithm which optimizes an
order of decomposition of network layers. These ideas allow to accelerate
ResNet models on Ascend 310 NPU devices without significant loss of accuracy.
For example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1
ImageNet accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taskynov_A/0/1/0/all/0/1"&gt;Anuar Taskynov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korviakov_V/0/1/0/all/0/1"&gt;Vladimir Korviakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1"&gt;Ivan Mazurenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yepan Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03541</id>
        <link href="http://arxiv.org/abs/2108.03541"/>
        <updated>2021-08-10T02:00:11.344Z</updated>
        <summary type="html"><![CDATA[Images tell powerful stories but cannot always be trusted. Matching images
back to trusted sources (attribution) enables users to make a more informed
judgment of the images they encounter online. We propose a robust image hashing
algorithm to perform such matching. Our hash is sensitive to manipulation of
subtle, salient visual details that can substantially change the story told by
an image. Yet the hash is invariant to benign transformations (changes in
quality, codecs, sizes, shapes, etc.) experienced by images during online
redistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph
Attention for Image Attribution Network); a robust image hashing model inspired
by recent successes of Transformers in the visual domain. OSCAR-Net constructs
a scene graph representation that attends to fine-grained changes of every
object's visual appearance and their spatial relationships. The network is
trained via contrastive learning on a dataset of original and manipulated
images yielding a state of the art image hash for content fingerprinting that
scales to millions of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1"&gt;Eric Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tu Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1"&gt;Vishy Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1"&gt;John Collomosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Biological Variables and Social Determinants to Predict Malaria and Anemia among Children in Senegal. (arXiv:2108.03601v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03601</id>
        <link href="http://arxiv.org/abs/2108.03601"/>
        <updated>2021-08-10T02:00:11.344Z</updated>
        <summary type="html"><![CDATA[Integrating machine learning techniques in healthcare becomes very common
nowadays, and it contributes positively to improving clinical care and health
decisions planning. Anemia and malaria are two life-threatening diseases in
Africa that affect the red blood cells and reduce hemoglobin production. This
paper focuses on analyzing child health data in Senegal using four machine
learning algorithms in Python: KNN, Random Forests, SVM, and Na\"ive Bayes. Our
task aims to investigate large-scale data from The Demographic and Health
Survey (DHS) and to find out hidden information for anemia and malaria. We
present two classification models for the two blood disorders using biological
variables and social determinants. The findings of this research will
contribute to improving child healthcare in Senegal by eradicating anemia and
malaria, and decreasing the child mortality rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sow_B/0/1/0/all/0/1"&gt;Boubacar Sow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suguri_H/0/1/0/all/0/1"&gt;Hiroki Suguri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukhtar_H/0/1/0/all/0/1"&gt;Hamid Mukhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_H/0/1/0/all/0/1"&gt;Hafiz Farooq Ahmad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Evolution for Neural Architecture Search. (arXiv:2012.06400v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06400</id>
        <link href="http://arxiv.org/abs/2012.06400"/>
        <updated>2021-08-10T02:00:11.344Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) methods rely on a search strategy for
deciding which architectures to evaluate next and a performance estimation
strategy for assessing their performance (e.g., using full evaluations,
multi-fidelity evaluations, or the one-shot model). In this paper, we focus on
the search strategy. We introduce the simple yet powerful evolutionary
algorithm of differential evolution to the NAS community. Using the simplest
performance evaluation strategy of full evaluations, we comprehensively compare
this search strategy to regularized evolution and Bayesian optimization and
demonstrate that it yields improved and more robust results for 13 tabular NAS
benchmarks based on NAS-Bench-101, NAS-Bench-1Shot1, NAS-Bench-201 and NAS-HPO
bench.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1"&gt;Noor Awad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1"&gt;Neeratyoy Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Monocular Hand Pose Estimation on Embedded Systems. (arXiv:2102.07067v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07067</id>
        <link href="http://arxiv.org/abs/2102.07067"/>
        <updated>2021-08-10T02:00:11.337Z</updated>
        <summary type="html"><![CDATA[Hand pose estimation is a fundamental task in many human-robot
interaction-related applications. However, previous approaches suffer from
unsatisfying hand landmark predictions in real-world scenes and high
computation burden. In this paper, we propose a fast and accurate framework for
hand pose estimation, dubbed as "FastHand". Using a lightweight encoder-decoder
network architecture, FastHand fulfills the requirements of practical
applications running on embedded devices. The encoder consists of deep layers
with a small number of parameters, while the decoder makes use of spatial
location information to obtain more accurate results. The evaluation took place
on two publicly available datasets demonstrating the improved performance of
the proposed pipeline compared to other state-of-the-art approaches. FastHand
offers high accuracy scores while reaching a speed of 25 frames per second on
an NVIDIA Jetson TX2 graphics processing unit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1"&gt;Shan An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiajie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dong Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Haogang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsintotas_K/0/1/0/all/0/1"&gt;Konstantinos A. Tsintotas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition. (arXiv:2108.03764v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03764</id>
        <link href="http://arxiv.org/abs/2108.03764"/>
        <updated>2021-08-10T02:00:11.335Z</updated>
        <summary type="html"><![CDATA[Face recognition networks encode information about sensitive attributes while
being trained for identity classification. Such encoding has two major issues:
(a) it makes the face representations susceptible to privacy leakage (b) it
appears to contribute to bias in face recognition. However, existing bias
mitigation approaches generally require end-to-end training and are unable to
achieve high verification accuracy. Therefore, we present a descriptor-based
adversarial de-biasing approach called `Protected Attribute Suppression System
(PASS)'. PASS can be trained on top of descriptors obtained from any previously
trained high-performing network to classify identities and simultaneously
reduce encoding of sensitive attributes. This eliminates the need for
end-to-end training. As a component of PASS, we present a novel discriminator
training strategy that discourages a network from encoding protected attribute
information. We show the efficacy of PASS to reduce gender and skintone
information in descriptors from SOTA face recognition networks like Arcface. As
a result, PASS descriptors outperform existing baselines in reducing gender and
skintone bias on the IJB-C dataset, while maintaining a high verification
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1"&gt;Prithviraj Dhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1"&gt;Joshua Gleason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Aniket Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1"&gt;Carlos D. Castillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1"&gt;Rama Chellappa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04024</id>
        <link href="http://arxiv.org/abs/2108.04024"/>
        <updated>2021-08-10T02:00:11.335Z</updated>
        <summary type="html"><![CDATA[We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1"&gt;Cristian Rodriguez-Opazo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1"&gt;Damien Teney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering Algorithms to Analyze the Road Traffic Crashes. (arXiv:2108.03490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03490</id>
        <link href="http://arxiv.org/abs/2108.03490"/>
        <updated>2021-08-10T02:00:11.316Z</updated>
        <summary type="html"><![CDATA[Selecting an appropriate clustering method as well as an optimal number of
clusters in road accident data is at times confusing and difficult. This paper
analyzes shortcomings of different existing techniques applied to cluster
accident-prone areas and recommends using Density-Based Spatial Clustering of
Applications with Noise (DBSCAN) and Ordering Points To Identify the Clustering
Structure (OPTICS) to overcome them. Comparative performance analysis based on
real-life data on the recorded cases of road accidents in North Carolina also
show more effectiveness and efficiency achieved by these algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mahnaz Rafia Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenny_I/0/1/0/all/0/1"&gt;Israt Jahan Jenny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nayon_M/0/1/0/all/0/1"&gt;Moniruzzaman Nayon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Md. Rajibul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amiruzzaman_M/0/1/0/all/0/1"&gt;Md Amiruzzaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_Al_Wadud_M/0/1/0/all/0/1"&gt;M. Abdullah-Al-Wadud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03506</id>
        <link href="http://arxiv.org/abs/2108.03506"/>
        <updated>2021-08-10T02:00:11.315Z</updated>
        <summary type="html"><![CDATA[The vulnerability of the Lottery Ticket Hypothesis has not been studied from
the purview of Membership Inference Attacks. Through this work, we are the
first to empirically show that the lottery ticket networks are equally
vulnerable to membership inference attacks. A Membership Inference Attack (MIA)
is the process of determining whether a data sample belongs to a training set
of a trained model or not. Membership Inference Attacks could leak critical
information about the training data that can be used for targeted attacks.
Recent deep learning models often have very large memory footprints and a high
computational cost associated with training and drawing inferences. Lottery
Ticket Hypothesis is used to prune the networks to find smaller sub-networks
that at least match the performance of the original model in terms of test
accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and
ImageNet datasets to perform image classification tasks and observe that the
attack accuracies are similar. We also see that the attack accuracy varies
directly according to the number of classes in the dataset and the sparsity of
the network. We demonstrate that these attacks are transferable across models
with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1"&gt;Aadesh Bagmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1"&gt;Shishira R Maiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1"&gt;Shruti Bidwalka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Amol Deshpande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14613</id>
        <link href="http://arxiv.org/abs/2107.14613"/>
        <updated>2021-08-10T02:00:11.308Z</updated>
        <summary type="html"><![CDATA[We present a study of the manners by which Domain information has been
incorporated when building models with Neural Networks. Integrating space data
is uniquely important to the development of Knowledge understanding model, as
well as other fields that aid in understanding information by utilizing the
human-machine interface and Reinforcement Learning. On numerous such occasions,
machine-based model development may profit essentially from the human
information on the world encoded in an adequately exact structure. This paper
inspects expansive ways to affect encode such information as sensible and
mathematical limitations and portrays methods and results that came to a couple
of subcategories under all of those methodologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1"&gt;Aryan Karn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Ashutosh Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blind Source Separation in Polyphonic Music Recordings Using Deep Neural Networks Trained via Policy Gradients. (arXiv:2107.04235v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04235</id>
        <link href="http://arxiv.org/abs/2107.04235"/>
        <updated>2021-08-10T02:00:11.290Z</updated>
        <summary type="html"><![CDATA[We propose a method for the blind separation of sounds of musical instruments
in audio signals. We describe the individual tones via a parametric model,
training a dictionary to capture the relative amplitudes of the harmonics. The
model parameters are predicted via a U-Net, which is a type of deep neural
network. The network is trained without ground truth information, based on the
difference between the model prediction and the individual time frames of the
short-time Fourier transform. Since some of the model parameters do not yield a
useful backpropagation gradient, we model them stochastically and employ the
policy gradient instead. To provide phase information and account for
inaccuracies in the dictionary-based representation, we also let the network
output a direct prediction, which we then use to resynthesize the audio signals
for the individual instruments. Due to the flexibility of the neural network,
inharmonicity can be incorporated seamlessly and no preprocessing of the input
spectra is required. Our algorithm yields high-quality separation results with
particularly low interference on a variety of different audio samples, both
acoustic and synthetic, provided that the sample contains enough data for the
training and that the spectral characteristics of the musical instruments are
sufficiently stable to be approximated by the dictionary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schulze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1"&gt;Johannes Leuschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1"&gt;Emily J. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08142</id>
        <link href="http://arxiv.org/abs/2107.08142"/>
        <updated>2021-08-10T02:00:11.290Z</updated>
        <summary type="html"><![CDATA[Despite the numerous successes of machine learning over the past decade
(image recognition, decision-making, NLP, image synthesis), self-driving
technology has not yet followed the same trend. In this paper, we study the
history, composition, and development bottlenecks of the modern self-driving
stack. We argue that the slow progress is caused by approaches that require too
much hand-engineering, an over-reliance on road testing, and high fleet
deployment costs. We observe that the classical stack has several bottlenecks
that preclude the necessary scale needed to capture the long tail of rare
events. To resolve these problems, we outline the principles of Autonomy 2.0,
an ML-first approach to self-driving, as a viable alternative to the currently
adopted state-of-the-art. This approach is based on (i) a fully differentiable
AV stack trainable from human demonstrations, (ii) closed-loop data-driven
reactive simulation, and (iii) large-scale, low-cost data collections as
critical solutions towards scalability issues. We outline the general
architecture, survey promising works in this direction and propose key
challenges to be addressed by the community in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Ashesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03894</id>
        <link href="http://arxiv.org/abs/2108.03894"/>
        <updated>2021-08-10T02:00:11.289Z</updated>
        <summary type="html"><![CDATA[We introduce FIFA, a fast approximate inference method for action
segmentation and alignment. Unlike previous approaches, FIFA does not rely on
expensive dynamic programming for inference. Instead, it uses an approximate
differentiable energy function that can be minimized using gradient-descent.
FIFA is a general approach that can replace exact inference improving its speed
by more than 5 times while maintaining its performance. FIFA is an anytime
inference algorithm that provides a better speed vs. accuracy trade-off
compared to exact inference. We apply FIFA on top of state-of-the-art
approaches for weakly supervised action segmentation and alignment as well as
fully supervised action segmentation. FIFA achieves state-of-the-art results on
most metrics on two action segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1"&gt;Yaser Souri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1"&gt;Yazan Abu Farha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1"&gt;Fabien Despinoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1"&gt;Gianpiero Francesca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1"&gt;Juergen Gall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex sparse regularization for deep neural networks and its optimality. (arXiv:2003.11769v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11769</id>
        <link href="http://arxiv.org/abs/2003.11769"/>
        <updated>2021-08-10T02:00:11.288Z</updated>
        <summary type="html"><![CDATA[Recent theoretical studies proved that deep neural network (DNN) estimators
obtained by minimizing empirical risk with a certain sparsity constraint can
attain optimal convergence rates for regression and classification problems.
However, the sparsity constraint requires to know certain properties of the
true model, which are not available in practice. Moreover, computation is
difficult due to the discrete nature of the sparsity constraint. In this paper,
we propose a novel penalized estimation method for sparse DNNs, which resolves
the aforementioned problems existing in the sparsity constraint. We establish
an oracle inequality for the excess risk of the proposed sparse-penalized DNN
estimator and derive convergence rates for several learning tasks. In
particular, we prove that the sparse-penalized estimator can adaptively attain
minimax convergence rates for various nonparametric regression problems. For
computation, we develop an efficient gradient-based optimization algorithm that
guarantees the monotonic reduction of the objective function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ohn_I/0/1/0/all/0/1"&gt;Ilsang Ohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yongdai Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03788</id>
        <link href="http://arxiv.org/abs/2108.03788"/>
        <updated>2021-08-10T02:00:11.285Z</updated>
        <summary type="html"><![CDATA[This paper presents a three-tier modality alignment approach to learning
text-image joint embedding, coined as JEMA, for cross-modal retrieval of
cooking recipes and food images. The first tier improves recipe text embedding
by optimizing the LSTM networks with term extraction and ranking enhanced
sequence patterns, and optimizes the image embedding by combining the
ResNeXt-101 image encoder with the category embedding using wideResNet-50 with
word2vec. The second tier modality alignment optimizes the textual-visual joint
embedding loss function using a double batch-hard triplet loss with soft-margin
optimization. The third modality alignment incorporates two types of
cross-modality alignments as the auxiliary loss regularizations to further
reduce the alignment errors in the joint learning of the two modality-specific
embedding functions. The category-based cross-modal alignment aims to align the
image category with the recipe category as a loss regularization to the joint
embedding. The cross-modal discriminator-based alignment aims to add the
visual-textual embedding distribution alignment to further regularize the joint
embedding loss. Extensive experiments with the one-million recipes benchmark
dataset Recipe1M demonstrate that the proposed JEMA approach outperforms the
state-of-the-art cross-modal embedding methods for both image-to-recipe and
recipe-to-image retrievals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark. (arXiv:2108.03830v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03830</id>
        <link href="http://arxiv.org/abs/2108.03830"/>
        <updated>2021-08-10T02:00:11.285Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation aims at predicting depth from a single image or
video. Recently, self-supervised methods draw much attention, due to their free
of depth annotations and impressive performance on several daytime benchmarks,
such as KITTI and Cityscapes. However, they produce weird outputs in more
challenging nighttime scenarios because of low visibility and varying
illuminations, which bring weak textures and break brightness-consistency
assumption, respectively. To address these problems, in this paper we propose a
novel framework with several improvements: (1) we introduce Priors-Based
Regularization to learn distribution knowledge from unpaired depth maps and
prevent model from being incorrectly trained; (2) we leverage
Mapping-Consistent Image Enhancement module to enhance image visibility and
contrast while maintaining brightness consistency; and (3) we present
Statistics-Based Mask strategy to tune the number of removed pixels within
textureless regions, using dynamic statistics. Experimental results demonstrate
the effectiveness of each component. Meanwhile, our framework achieves
remarkable improvements and state-of-the-art results on two nighttime datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhiqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Baobei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03823</id>
        <link href="http://arxiv.org/abs/2108.03823"/>
        <updated>2021-08-10T02:00:11.284Z</updated>
        <summary type="html"><![CDATA[To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yurong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03917</id>
        <link href="http://arxiv.org/abs/2108.03917"/>
        <updated>2021-08-10T02:00:11.282Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1"&gt;Radu Alexandru Rosu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1"&gt;Peer Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1"&gt;Jan Quenzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views. (arXiv:2108.03743v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03743</id>
        <link href="http://arxiv.org/abs/2108.03743"/>
        <updated>2021-08-10T02:00:11.280Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning of global features for 3D shape analysis is an
important research challenge because it avoids manual effort for supervised
information collection. In this paper, we propose a view-based deep learning
model called Hierarchical View Predictor (HVP) to learn 3D shape features from
unordered views in an unsupervised manner. To mine highly discriminative
information from unordered views, HVP performs a novel hierarchical view
prediction over a view pair, and aggregates the knowledge learned from the
predictions in all view pairs into a global feature. In a view pair, we pose
hierarchical view prediction as the task of hierarchically predicting a set of
image patches in a current view from its complementary set of patches, and in
addition, completing the current view and its opposite from any one of the two
sets of patches. Hierarchical prediction, in patches to patches, patches to
view and view to view, facilitates HVP to effectively learn the structure of 3D
shapes from the correlation between patches in the same view and the
correlation between a pair of complementary views. In addition, the employed
implicit aggregation over all view pairs enables HVP to learn global features
from unordered views. Our results show that HVP can outperform state-of-the-art
methods under large-scale 3D shape benchmarks in shape classification and
retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1"&gt;Matthias Zwicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning. (arXiv:2108.03704v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03704</id>
        <link href="http://arxiv.org/abs/2108.03704"/>
        <updated>2021-08-10T02:00:11.279Z</updated>
        <summary type="html"><![CDATA[We introduce the task of open-vocabulary visual instance search (OVIS). Given
an arbitrary textual search query, Open-vocabulary Visual Instance Search
(OVIS) aims to return a ranked list of visual instances, i.e., image patches,
that satisfies the search intent from an image database. The term "open
vocabulary" means that there are neither restrictions to the visual instance to
be searched nor restrictions to the word that can be used to compose the
textual search query. We propose to address such a search challenge via
visual-semantic aligned representation learning (ViSA). ViSA leverages massive
image-caption pairs as weak image-level (not instance-level) supervision to
learn a rich cross-modal semantic space where the representations of visual
instances (not images) and those of textual queries are aligned, thus allowing
us to measure the similarities between any visual instance and an arbitrary
textual query. To evaluate the performance of ViSA, we build two datasets named
OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through
extensive experiments on the two datasets, we demonstrate ViSA's ability to
search for visual instances in images not available during training given a
wide range of textual queries including those composed of uncommon words.
Experimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under
the most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kevin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-08-10T02:00:11.276Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Hierarchical Graph Reasoning with Semantic Coherence for Video-and-Language Inference. (arXiv:2107.12270v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12270</id>
        <link href="http://arxiv.org/abs/2107.12270"/>
        <updated>2021-08-10T02:00:11.272Z</updated>
        <summary type="html"><![CDATA[Video-and-Language Inference is a recently proposed task for joint
video-and-language understanding. This new task requires a model to draw
inference on whether a natural language statement entails or contradicts a
given video clip. In this paper, we study how to address three critical
challenges for this task: judging the global correctness of the statement
involved multiple semantic meanings, joint reasoning over video and subtitles,
and modeling long-range relationships and complex social interactions. First,
we propose an adaptive hierarchical graph network that achieves in-depth
understanding of the video over complex interactions. Specifically, it performs
joint reasoning over video and subtitles in three hierarchies, where the graph
structure is adaptively adjusted according to the semantic structures of the
statement. Secondly, we introduce semantic coherence learning to explicitly
encourage the semantic coherence of the adaptive hierarchical graph network
from three hierarchies. The semantic coherence learning can further improve the
alignment between vision and linguistics, and the coherence across a sequence
of video segments. Experimental results show that our method significantly
outperforms the baseline by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juncheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haochen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.15007</id>
        <link href="http://arxiv.org/abs/2104.15007"/>
        <updated>2021-08-10T02:00:11.223Z</updated>
        <summary type="html"><![CDATA[The first known case of Coronavirus disease 2019 (COVID-19) was identified in
December 2019. It has spread worldwide, leading to an ongoing pandemic, imposed
restrictions and costs to many countries. Predicting the number of new cases
and deaths during this period can be a useful step in predicting the costs and
facilities required in the future. The purpose of this study is to predict new
cases and deaths rate one, three and seven-day ahead during the next 100 days.
The motivation for predicting every n days (instead of just every day) is the
investigation of the possibility of computational cost reduction and still
achieving reasonable performance. Such a scenario may be encountered real-time
forecasting of time series. Six different deep learning methods are examined on
the data adopted from the WHO website. Three methods are LSTM, Convolutional
LSTM, and GRU. The bidirectional extension is then considered for each method
to forecast the rate of new cases and new deaths in Australia and Iran
countries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1"&gt;Nooshin Ayoobi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1"&gt;Danial Sharifrazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;Juan M. Gorriz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1"&gt;Hossein Moosaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1"&gt;Abdoulmohammad Gholamzadeh Chofreh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1"&gt;Feybi Ariani Goni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1"&gt;Jiri Jaromir Klemes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1"&gt;Amir Mosavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint AP Probing and Scheduling: A Contextual Bandit Approach. (arXiv:2108.03297v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03297</id>
        <link href="http://arxiv.org/abs/2108.03297"/>
        <updated>2021-08-10T02:00:11.215Z</updated>
        <summary type="html"><![CDATA[We consider a set of APs with unknown data rates that cooperatively serve a
mobile client. The data rate of each link is i.i.d. sampled from a distribution
that is unknown a priori. In contrast to traditional link scheduling problems
under uncertainty, we assume that in each time step, the device can probe a
subset of links before deciding which one to use. We model this problem as a
contextual bandit problem with probing (CBwP) and present an efficient
algorithm. We further establish the regret of our algorithm for links with
Bernoulli data rates. Our CBwP model is a novel extension of the classic
contextual bandit model and can potentially be applied to a large class of
sequential decision-making problems that involve joint probing and play under
uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tianyi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Ding Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_P/0/1/0/all/0/1"&gt;Parth H. Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zizhan Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-08-10T02:00:11.146Z</updated>
        <summary type="html"><![CDATA[Anomaly detection is a critical problem in the manufacturing industry. In
many applications, images of objects to be analyzed are captured from multiple
perspectives which can be exploited to improve the robustness of anomaly
detection. In this work, we build upon the deep support vector data description
algorithm and address multi-perspective anomaly detection using three different
fusion techniques, i.e., early fusion, late fusion, and late fusion with
multiple decoders. We employ different augmentation techniques with a denoising
process to deal with scarce one-class data, which further improves the
performance (ROC AUC $= 80\%$). Furthermore, we introduce the dices dataset,
which consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g., drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed
{multi-perspective} approach exceeds the state-of-the-art {single-perspective
anomaly detection on both the MNIST and dices datasets}. To the best of our
knowledge, this is the first work that focuses on addressing multi-perspective
anomaly detection in images by jointly using different perspectives together
with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00602</id>
        <link href="http://arxiv.org/abs/2108.00602"/>
        <updated>2021-08-10T02:00:11.146Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the task of hallucinating an authentic
high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage
Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed
Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)
the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates
facial geometry priors for low-resolution (LR) faces and (2) acquires
non-occluded HR face images under the guidance of the estimated priors. Our
multi-stage hallucination network super-resolves and inpaints occluded LR faces
in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts
significantly. Specifically, we design a novel cross-modal transformer module
for facial priors estimation, in which an input face and its landmark features
are formulated as queries and keys, respectively. Such a design encourages
joint feature learning across the input facial and landmark features, and deep
feature correspondences will be discovered by attention. Thus, facial
appearance features and facial geometry priors are learned in a mutual
promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves
visually pleasing HR faces, reaching superior performance in downstream tasks,
i.e., face alignment, face parsing, face recognition and expression
classification, compared with other state-of-the-art (SotA) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaobo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02960</id>
        <link href="http://arxiv.org/abs/2107.02960"/>
        <updated>2021-08-10T02:00:11.137Z</updated>
        <summary type="html"><![CDATA[We introduce the first Neural Architecture Search (NAS) method to find a
better transformer architecture for image recognition. Recently, transformers
without CNN-based backbones are found to achieve impressive performance for
image recognition. However, the transformer is designed for NLP tasks and thus
could be sub-optimal when directly used for image recognition. In order to
improve the visual representation ability for transformers, we propose a new
search space and searching algorithm. Specifically, we introduce a locality
module that models the local correlations in images explicitly with fewer
computational cost. With the locality module, our search space is defined to
let the search algorithm freely trade off between global and local information
as well as optimizing the low-level design choice in each module. To tackle the
problem caused by huge search space, a hierarchical neural architecture search
method is proposed to search the optimal vision transformer from two levels
separately with the evolutionary algorithm. Extensive experiments on the
ImageNet dataset demonstrate that our method can find more discriminative and
efficient transformer variants than the ResNet family (e.g., ResNet101) and the
baseline ViT for image classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peixia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baopu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1"&gt;Lei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1"&gt;Junjie yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the computational demands underlying visual reasoning. (arXiv:2108.03603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03603</id>
        <link href="http://arxiv.org/abs/2108.03603"/>
        <updated>2021-08-10T02:00:11.133Z</updated>
        <summary type="html"><![CDATA[Visual understanding requires comprehending complex visual relations between
objects within a scene. Here, we seek to characterize the computational demands
for abstract visual reasoning. We do this by systematically assessing the
ability of modern deep convolutional neural networks (CNNs) to learn to solve
the Synthetic Visual Reasoning Test (SVRT) challenge, a collection of
twenty-three visual reasoning problems. Our analysis leads to a novel taxonomy
of visual reasoning tasks, which can be primarily explained by both the type of
relations (same-different vs. spatial-relation judgments) and the number of
relations used to compose the underlying rules. Prior cognitive neuroscience
work suggests that attention plays a key role in human's visual reasoning
ability. To test this, we extended the CNNs with spatial and feature-based
attention mechanisms. In a second series of experiments, we evaluated the
ability of these attention networks to learn to solve the SVRT challenge and
found the resulting architectures to be much more efficient at solving the
hardest of these visual reasoning tasks. Most importantly, the corresponding
improvements on individual tasks partially explained the taxonomy. Overall,
this work advances our understanding of visual reasoning and yields testable
Neuroscience predictions regarding the need for feature-based vs. spatial
attention in visual reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaishnav_M/0/1/0/all/0/1"&gt;Mohit Vaishnav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1"&gt;Remi Cadene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1"&gt;Drew Linsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1"&gt;Thomas Serre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer. (arXiv:2108.03647v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03647</id>
        <link href="http://arxiv.org/abs/2108.03647"/>
        <updated>2021-08-10T02:00:11.126Z</updated>
        <summary type="html"><![CDATA[Fast arbitrary neural style transfer has attracted widespread attention from
academic, industrial and art communities due to its flexibility in enabling
various applications. Existing solutions either attentively fuse deep style
feature into deep content feature without considering feature distributions, or
adaptively normalize deep content feature according to the style such that
their global statistics are matched. Although effective, leaving shallow
feature unexplored and without locally considering feature statistics, they are
prone to unnatural output with unpleasing local distortions. To alleviate this
problem, in this paper, we propose a novel attention and normalization module,
named Adaptive Attention Normalization (AdaAttN), to adaptively perform
attentive normalization on per-point basis. Specifically, spatial attention
score is learnt from both shallow and deep features of content and style
images. Then per-point weighted statistics are calculated by regarding a style
feature point as a distribution of attention-weighted output of all style
feature points. Finally, the content feature is normalized so that they
demonstrate the same local feature statistics as the calculated per-point
weighted style feature statistics. Besides, a novel local feature loss is
derived based on AdaAttN to enhance local visual quality. We also extend
AdaAttN to be ready for video style transfer with slight modifications.
Experiments demonstrate that our method achieves state-of-the-art arbitrary
image/video style transfer. Codes and models are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meiling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhengxing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces. (arXiv:2108.03952v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03952</id>
        <link href="http://arxiv.org/abs/2108.03952"/>
        <updated>2021-08-10T02:00:11.119Z</updated>
        <summary type="html"><![CDATA[Multi-agent control problems constitute an interesting area of application
for deep reinforcement learning models with continuous action spaces. Such
real-world applications, however, typically come with critical safety
constraints that must not be violated. In order to ensure safety, we enhance
the well-known multi-agent deep deterministic policy gradient (MADDPG)
framework by adding a safety layer to the deep policy network. %which
automatically corrects invalid actions. In particular, we extend the idea of
linearizing the single-step transition dynamics, as was done for single-agent
systems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. We
additionally propose to circumvent infeasibility problems in the action
correction step using soft constraints (Kerrigan & Maciejowski, 2000). Results
from the theory of exact penalty functions can be used to guarantee constraint
satisfaction of the soft constraints under mild assumptions. We empirically
find that the soft formulation achieves a dramatic decrease in constraint
violations, making safety available even during the learning procedure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheebaelhamd_Z/0/1/0/all/0/1"&gt;Ziyad Sheebaelhamd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisis_K/0/1/0/all/0/1"&gt;Konstantinos Zisis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nisioti_A/0/1/0/all/0/1"&gt;Athina Nisioti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gkouletsos_D/0/1/0/all/0/1"&gt;Dimitris Gkouletsos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1"&gt;Dario Pavllo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas Kohler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption of Battery Electric Vehicles. (arXiv:2108.04036v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04036</id>
        <link href="http://arxiv.org/abs/2108.04036"/>
        <updated>2021-08-10T02:00:11.119Z</updated>
        <summary type="html"><![CDATA[Recently, there has been an increasing interest in the roll-out of electric
vehicles (EVs) in the global automotive market. Compared to conventional
internal combustion engine vehicles (ICEVs), EVs can not only help users reduce
monetary costs in their daily commuting, but also can effectively help mitigate
the increasing level of traffic emissions produced in cities. Among many
others, battery electric vehicles (BEVs) exclusively use chemical energy stored
in their battery packs for propulsion. Hence, it becomes important to
understand how much energy can be consumed by such vehicles in various traffic
scenarios towards effective energy management. To address this challenge, we
propose a novel framework in this paper by leveraging the federated learning
approaches for modelling energy consumption for BEVs (Fed-BEV). More
specifically, a group of BEVs involved in the Fed-BEV framework can learn from
each other to jointly enhance their energy consumption model. We present the
design of the proposed system architecture and implementation details in a
co-simulation environment. Finally, comparative studies and simulation results
are discussed to illustrate the efficacy of our proposed framework for accurate
energy modelling of BEVs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12619</id>
        <link href="http://arxiv.org/abs/2107.12619"/>
        <updated>2021-08-10T02:00:11.114Z</updated>
        <summary type="html"><![CDATA[Recently, the problem of inaccurate learning targets in crowd counting draws
increasing attention. Inspired by a few pioneering work, we solve this problem
by trying to predict the indices of pre-defined interval bins of counts instead
of the count values themselves. However, an inappropriate interval setting
might make the count error contributions from different intervals extremely
imbalanced, leading to inferior counting performance. Therefore, we propose a
novel count interval partition criterion called Uniform Error Partition (UEP),
which always keeps the expected counting error contributions equal for all
intervals to minimize the prediction risk. Then to mitigate the inevitably
introduced discretization errors in the count quantization process, we propose
another criterion called Mean Count Proxies (MCP). The MCP criterion selects
the best count proxy for each interval to represent its count value during
inference, making the overall expected discretization error of an image nearly
negligible. As far as we are aware, this work is the first to delve into such a
classification task and ends up with a promising solution for count interval
partition. Following the above two theoretically demonstrated criterions, we
propose a simple yet effective model termed Uniform Error Partition Network
(UEPNet), which achieves state-of-the-art performance on several challenging
datasets. The codes will be available at:
https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boshen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xuyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Hyperparameters in Stochastic Gradient Descent with Momentum. (arXiv:2108.03947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03947</id>
        <link href="http://arxiv.org/abs/2108.03947"/>
        <updated>2021-08-10T02:00:11.113Z</updated>
        <summary type="html"><![CDATA[Following the same routine as [SSJ20], we continue to present the theoretical
analysis for stochastic gradient descent with momentum (SGD with momentum) in
this paper. Differently, for SGD with momentum, we demonstrate it is the two
hyperparameters together, the learning rate and the momentum coefficient, that
play the significant role for the linear rate of convergence in non-convex
optimization. Our analysis is based on the use of a hyperparameters-dependent
stochastic differential equation (hp-dependent SDE) that serves as a continuous
surrogate for SGD with momentum. Similarly, we establish the linear convergence
for the continuous-time formulation of SGD with momentum and obtain an explicit
expression for the optimal linear rate by analyzing the spectrum of the
Kramers-Fokker-Planck operator. By comparison, we demonstrate how the optimal
linear rate of convergence and the final gap for SGD only about the learning
rate varies with the momentum coefficient increasing from zero to one when the
momentum is introduced. Then, we propose a mathematical interpretation why the
SGD with momentum converges faster and more robust about the learning rate than
the standard SGD in practice. Finally, we show the Nesterov momentum under the
existence of noise has no essential difference with the standard momentum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Bin Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13229</id>
        <link href="http://arxiv.org/abs/2106.13229"/>
        <updated>2021-08-10T02:00:11.109Z</updated>
        <summary type="html"><![CDATA[The ability to plan into the future while utilizing only raw high-dimensional
observations, such as images, can provide autonomous agents with broad
capabilities. Visual model-based reinforcement learning (RL) methods that plan
future actions directly have shown impressive results on tasks that require
only short-horizon reasoning, however, these methods struggle on temporally
extended tasks. We argue that it is easier to solve long-horizon tasks by
planning sequences of states rather than just actions, as the effects of
actions greatly compound over time and are harder to optimize. To achieve this,
we draw on the idea of collocation, which has shown good results on
long-horizon tasks in optimal control literature, and adapt it to the
image-based setting by utilizing learned latent state space models. The
resulting latent collocation method (LatCo) optimizes trajectories of latent
states, which improves over previously proposed shooting methods for visual
model-based RL on tasks with sparse rewards and long-term goals. Videos and
code at https://orybkin.github.io/latco/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chuning Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1"&gt;Anusha Nagabandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction. (arXiv:2012.06170v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06170</id>
        <link href="http://arxiv.org/abs/2012.06170"/>
        <updated>2021-08-10T02:00:11.097Z</updated>
        <summary type="html"><![CDATA[We propose the ViNet architecture for audio-visual saliency prediction. ViNet
is a fully convolutional encoder-decoder architecture. The encoder uses visual
features from a network trained for action recognition, and the decoder infers
a saliency map via trilinear interpolation and 3D convolutions, combining
features from multiple hierarchies. The overall architecture of ViNet is
conceptually simple; it is causal and runs in real-time (60 fps). ViNet does
not use audio as input and still outperforms the state-of-the-art audio-visual
saliency prediction models on nine different datasets (three visual-only and
six audio-visual datasets). ViNet also surpasses human performance on the CC,
SIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first
network to do so. We also explore a variation of ViNet architecture by
augmenting audio features into the decoder. To our surprise, upon sufficient
training, the network becomes agnostic to the input audio and provides the same
output irrespective of the input. Interestingly, we also observe similar
behaviour in the previous state-of-the-art models \cite{tsiami2020stavis} for
audio-visual saliency prediction. Our findings contrast with previous works on
deep learning-based audio-visual saliency prediction, suggesting a clear avenue
for future explorations incorporating audio in a more effective manner. The
code and pre-trained models are available at
https://github.com/samyak0210/ViNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Samyak Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yarlagadda_P/0/1/0/all/0/1"&gt;Pradeep Yarlagadda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyoti_S/0/1/0/all/0/1"&gt;Shreyank Jyoti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1"&gt;Shyamgopal Karthik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1"&gt;Ramanathan Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1"&gt;Vineet Gandhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03613</id>
        <link href="http://arxiv.org/abs/2108.03613"/>
        <updated>2021-08-10T02:00:11.072Z</updated>
        <summary type="html"><![CDATA[Incremental learning of semantic segmentation has emerged as a promising
strategy for visual scene interpretation in the open- world setting. However,
it remains challenging to acquire novel classes in an online fashion for the
segmentation task, mainly due to its continuously-evolving semantic label
space, partial pixelwise ground-truth annotations, and constrained data
availability. To ad- dress this, we propose an incremental learning strategy
that can fast adapt deep segmentation models without catastrophic forgetting,
using a streaming input data with pixel annotations on the novel classes only.
To this end, we develop a uni ed learning strategy based on the
Expectation-Maximization (EM) framework, which integrates an iterative
relabeling strategy that lls in the missing labels and a rehearsal-based
incremental learning step that balances the stability-plasticity of the model.
Moreover, our EM algorithm adopts an adaptive sampling method to select
informative train- ing data and a class-balancing training strategy in the
incremental model updates, both improving the e cacy of model learning. We
validate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the
results demonstrate its superior performance over the existing incremental
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shipeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiale Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiangwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Visual Design Principles in Art and Architecture through Deep Convolutional Neural Networks. (arXiv:2108.04048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04048</id>
        <link href="http://arxiv.org/abs/2108.04048"/>
        <updated>2021-08-10T02:00:11.064Z</updated>
        <summary type="html"><![CDATA[Visual design is associated with the use of some basic design elements and
principles. Those are applied by the designers in the various disciplines for
aesthetic purposes, relying on an intuitive and subjective process. Thus,
numerical analysis of design visuals and disclosure of the aesthetic value
embedded in them are considered as hard. However, it has become possible with
emerging artificial intelligence technologies. This research aims at a neural
network model, which recognizes and classifies the design principles over
different domains. The domains include artwork produced since the late 20th
century; professional photos; and facade pictures of contemporary buildings.
The data collection and curation processes, including the production of
computationally-based synthetic dataset, is genuine. The proposed model learns
from the knowledge of myriads of original designs, by capturing the underlying
shared patterns. It is expected to consolidate design processes by providing an
aesthetic evaluation of the visual compositions with objectivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_G/0/1/0/all/0/1"&gt;Gozdenur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cekmis_A/0/1/0/all/0/1"&gt;Asli Cekmis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yesilkaynak_V/0/1/0/all/0/1"&gt;Vahit Bugra Yesilkaynak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Latent Semantic Graph for Video Captioning. (arXiv:2108.03662v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03662</id>
        <link href="http://arxiv.org/abs/2108.03662"/>
        <updated>2021-08-10T02:00:11.055Z</updated>
        <summary type="html"><![CDATA[Video captioning aims to automatically generate natural language sentences
that can describe the visual contents of a given video. Existing generative
models like encoder-decoder frameworks cannot explicitly explore the
object-level interactions and frame-level information from complex
spatio-temporal data to generate semantic-rich captions. Our main contribution
is to identify three key problems in a joint framework for future video
summarization tasks. 1) Enhanced Object Proposal: we propose a novel
Conditional Graph that can fuse spatio-temporal information into latent object
proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to
dynamically extract visual words with higher semantic levels. 3) Sentence
Validation: A novel Discriminative Language Validator is proposed to verify
generated captions so that key semantic concepts can be effectively preserved.
Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant
improvements over state-of-the-art approaches on all metrics, especially for
BLEU-4 and CIDEr. Our code is available at
https://github.com/baiyang4/D-LSG-Video-Caption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingzhang Hul Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1"&gt;Maurice Pagnucco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yu Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSGR: Pixel-wise Sparse Graph Reasoning for COVID-19 Pneumonia Segmentation in CT Images. (arXiv:2108.03809v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03809</id>
        <link href="http://arxiv.org/abs/2108.03809"/>
        <updated>2021-08-10T02:00:11.048Z</updated>
        <summary type="html"><![CDATA[Automated and accurate segmentation of the infected regions in computed
tomography (CT) images is critical for the prediction of the pathological stage
and treatment response of COVID-19. Several deep convolutional neural networks
(DCNNs) have been designed for this task, whose performance, however, tends to
be suppressed by their limited local receptive fields and insufficient global
reasoning ability. In this paper, we propose a pixel-wise sparse graph
reasoning (PSGR) module and insert it into a segmentation network to enhance
the modeling of long-range dependencies for COVID-19 infected region
segmentation in CT images. In the PSGR module, a graph is first constructed by
projecting each pixel on a node based on the features produced by the
segmentation backbone, and then converted into a sparsely-connected graph by
keeping only K strongest connections to each uncertain pixel. The long-range
information reasoning is performed on the sparsely-connected graph to generate
enhanced features. The advantages of this module are two-fold: (1) the
pixel-wise mapping strategy not only avoids imprecise pixel-to-node projections
but also preserves the inherent information of each pixel for global reasoning;
and (2) the sparsely-connected graph construction results in effective
information retrieval and reduction of the noise propagation. The proposed
solution has been evaluated against four widely-used segmentation models on
three public datasets. The results show that the segmentation model equipped
with our PSGR module can effectively segment COVID-19 infected regions in CT
images, outperforming all other competing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;Haozhe Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoteng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1"&gt;Guixiang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Liang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Vessel Navigation Visually Aided by Autonomous Unmanned Aerial Vehicles in Congested Harbors and Waterways. (arXiv:2108.03862v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03862</id>
        <link href="http://arxiv.org/abs/2108.03862"/>
        <updated>2021-08-10T02:00:11.038Z</updated>
        <summary type="html"><![CDATA[In the maritime sector, safe vessel navigation is of great importance,
particularly in congested harbors and waterways. The focus of this work is to
estimate the distance between an object of interest and potential obstacles
using a companion UAV. The proposed approach fuses GPS data with long-range
aerial images. First, we employ semantic segmentation DNN for discriminating
the vessel of interest, water, and potential solid objects using raw image
data. The network is trained with both real and images generated and
automatically labeled from a realistic AirSim simulation environment. Then, the
distances between the extracted vessel and non-water obstacle blobs are
computed using a novel GSD estimation algorithm. To the best of our knowledge,
this work is the first attempt to detect and estimate distances to unknown
objects from long-range visual data captured with conventional RGB cameras and
auxiliary absolute positioning systems (e.g. GPS). The simulation results
illustrate the accuracy and efficacy of the proposed method for visually aided
navigation of vessels assisted by UAV.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sejersen_J/0/1/0/all/0/1"&gt;Jonas le Fevre Sejersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1"&gt;Rui Pimentel de Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1"&gt;Erdal Kayacan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12619</id>
        <link href="http://arxiv.org/abs/2107.12619"/>
        <updated>2021-08-10T02:00:11.008Z</updated>
        <summary type="html"><![CDATA[Recently, the problem of inaccurate learning targets in crowd counting draws
increasing attention. Inspired by a few pioneering work, we solve this problem
by trying to predict the indices of pre-defined interval bins of counts instead
of the count values themselves. However, an inappropriate interval setting
might make the count error contributions from different intervals extremely
imbalanced, leading to inferior counting performance. Therefore, we propose a
novel count interval partition criterion called Uniform Error Partition (UEP),
which always keeps the expected counting error contributions equal for all
intervals to minimize the prediction risk. Then to mitigate the inevitably
introduced discretization errors in the count quantization process, we propose
another criterion called Mean Count Proxies (MCP). The MCP criterion selects
the best count proxy for each interval to represent its count value during
inference, making the overall expected discretization error of an image nearly
negligible. As far as we are aware, this work is the first to delve into such a
classification task and ends up with a promising solution for count interval
partition. Following the above two theoretically demonstrated criterions, we
propose a simple yet effective model termed Uniform Error Partition Network
(UEPNet), which achieves state-of-the-art performance on several challenging
datasets. The codes will be available at:
https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qingyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Boshen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xuyi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiayi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14724</id>
        <link href="http://arxiv.org/abs/2107.14724"/>
        <updated>2021-08-10T02:00:11.000Z</updated>
        <summary type="html"><![CDATA[Domain adaptation is critical for success when confronting with the lack of
annotations in a new domain. As the huge time consumption of labeling process
on 3D point cloud, domain adaptation for 3D semantic segmentation is of great
expectation. With the rise of multi-modal datasets, large amount of 2D images
are accessible besides 3D point clouds. In light of this, we propose to further
leverage 2D data for 3D domain adaptation by intra and inter domain cross modal
learning. As for intra-domain cross modal learning, most existing works sample
the dense 2D pixel-wise features into the same size with sparse 3D point-wise
features, resulting in the abandon of numerous useful 2D features. To address
this problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)
to increase the sufficiency of multi-modality information interaction for
domain adaptation. For inter-domain cross modal learning, we further advance
Cross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains
different semantic content aiming to promote high-level modal complementarity.
We evaluate our model under various multi-modality domain adaptation settings
including day-to-night, country-to-country and dataset-to-dataset, brings large
improvements over both uni-modal and multi-modal domain adaptation methods on
all settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Duo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yulan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistenty. (arXiv:2106.05616v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05616</id>
        <link href="http://arxiv.org/abs/2106.05616"/>
        <updated>2021-08-10T02:00:10.993Z</updated>
        <summary type="html"><![CDATA[Recovering 3D human pose from 2D joints is still a challenging problem,
especially without any 3D annotation, video information, or multi-view
information. In this paper, we present an unsupervised GAN-based model
consisting of multiple weight-sharing generators to estimate a 3D human pose
from a single image without 3D annotations. In our model, we introduce
single-view-multi-angle consistency (SVMAC) to significantly improve the
estimation performance. With 2D joint locations as input, our model estimates a
3D pose and a camera simultaneously. During training, the estimated 3D pose is
rotated by random angles and the estimated camera projects the rotated 3D poses
back to 2D. The 2D reprojections will be fed into weight-sharing generators to
estimate the corresponding 3D poses and cameras, which are then mixed to impose
SVMAC constraints to self-supervise the training process. The experimental
results show that our method outperforms the state-of-the-art unsupervised
methods by 2.6% on Human 3.6M and 15.0% on MPI-INF-3DHP. Moreover, qualitative
results on MPII and LSP show that our method can generalize well to unknown
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yicheng Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiahui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yongqi Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MPI: Multi-receptive and Parallel Integration for Salient Object Detection. (arXiv:2108.03618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03618</id>
        <link href="http://arxiv.org/abs/2108.03618"/>
        <updated>2021-08-10T02:00:10.985Z</updated>
        <summary type="html"><![CDATA[The semantic representation of deep features is essential for image context
understanding, and effective fusion of features with different semantic
representations can significantly improve the model's performance on salient
object detection. In this paper, a novel method called MPI is proposed for
salient object detection. Firstly, a multi-receptive enhancement module (MRE)
is designed to effectively expand the receptive fields of features from
different layers and generate features with different receptive fields. MRE can
enhance the semantic representation and improve the model's perception of the
image context, which enables the model to locate the salient object accurately.
Secondly, in order to reduce the reuse of redundant information in the complex
top-down fusion method and weaken the differences between semantic features, a
relatively simple but effective parallel fusion strategy (PFS) is proposed. It
allows multi-scale features to better interact with each other, thus improving
the overall performance of the model. Experimental results on multiple datasets
demonstrate that the proposed method outperforms state-of-the-art methods under
different evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Han Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1"&gt;Jun Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Ningzhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Dong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network. (arXiv:2108.03824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03824</id>
        <link href="http://arxiv.org/abs/2108.03824"/>
        <updated>2021-08-10T02:00:10.977Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel recurrent multi-view stereo network based
on long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet.
We firstly introduce an intra-view aggregation module to adaptively extract
image features by using context-aware convolution and multi-scale aggregation,
which efficiently improves the performance on challenging regions, such as thin
objects and large low-textured surfaces. To overcome the difficulty of varying
occlusion in complex scenes, we propose an inter-view cost volume aggregation
module for adaptive pixel-wise view aggregation, which is able to preserve
better-matched pairs among all views. The two proposed adaptive aggregation
modules are lightweight, effective and complementary regarding improving the
accuracy and completeness of 3D reconstruction. Instead of conventional 3D
CNNs, we utilize a hybrid network with recurrent structure for cost volume
regularization, which allows high-resolution reconstruction and finer
hypothetical plane sweep. The proposed network is trained end-to-end and
achieves excellent performance on various datasets. It ranks $1^{st}$ among all
submissions on Tanks and Temples benchmark and achieves competitive results on
DTU dataset, which exhibits strong generalizability and robustness.
Implementation of our method is available at
https://github.com/QT-Zhu/AA-RMVSNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zizhuang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qingtian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1"&gt;Chen Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yisong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04767</id>
        <link href="http://arxiv.org/abs/2107.04767"/>
        <updated>2021-08-10T02:00:10.969Z</updated>
        <summary type="html"><![CDATA[Intelligent resident surveillance is one of the most essential smart
community services. The increasing demand for security needs surveillance
systems to be able to detect anomalies in surveillance scenes. Employing
high-capacity computational devices for intelligent surveillance in residential
societies is costly and not feasible. Therefore, we propose anomaly detection
for intelligent surveillance using CPU-only edge devices. A modular framework
to capture object-level inferences and tracking is developed. To cope with
partial occlusions, posture deformations, and complex scenes, we employed
feature encoding and trajectory association governed by two metrices
complementing to each other. The elements of an anomaly detection framework are
optimized to run on CPU-only edge devices with sufficient frames per second
(FPS). The experimental results indicate the proposed method is feasible and
achieves satisfactory results in real-life scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1"&gt;Mayur R. Parate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1"&gt;Kishor M. Bhurchandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1"&gt;Ashwin G. Kothari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RECALL: Replay-based Continual Learning in Semantic Segmentation. (arXiv:2108.03673v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03673</id>
        <link href="http://arxiv.org/abs/2108.03673"/>
        <updated>2021-08-10T02:00:10.961Z</updated>
        <summary type="html"><![CDATA[Deep networks allow to obtain outstanding results in semantic segmentation,
however they need to be trained in a single shot with a large amount of data.
Continual learning settings where new classes are learned in incremental steps
and previous training data is no longer available are challenging due to the
catastrophic forgetting phenomenon. Existing approaches typically fail when
several incremental steps are performed or in presence of a distribution shift
of the background class. We tackle these issues by recreating no longer
available data for the old classes and outlining a content inpainting scheme on
the background class. We propose two sources for replay data. The first resorts
to a generative adversarial network to sample from the class space of past
learning steps. The second relies on web-crawled data to retrieve images
containing examples of old classes from online databases. In both scenarios no
samples of past steps are stored, thus avoiding privacy concerns. Replay data
are then blended with new samples during the incremental steps. Our approach,
RECALL, outperforms state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1"&gt;Andrea Maracani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1"&gt;Umberto Michieli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1"&gt;Marco Toldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1"&gt;Pietro Zanuttigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03555</id>
        <link href="http://arxiv.org/abs/2108.03555"/>
        <updated>2021-08-10T02:00:10.940Z</updated>
        <summary type="html"><![CDATA[Background: Accurate diagnosis of skull base tumors is essential for
providing personalized surgical treatment strategies. Intraoperative diagnosis
can be challenging due to tumor diversity and lack of intraoperative pathology
resources.

Objective: To develop an independent and parallel intraoperative pathology
workflow that can provide rapid and accurate skull base tumor diagnoses using
label-free optical imaging and artificial intelligence (AI).

Method: We used a fiber laser-based, label-free, non-consumptive,
high-resolution microscopy method ($<$ 60 sec per 1 $\times$ 1 mm$^\text{2}$),
called stimulated Raman histology (SRH), to image a consecutive, multicenter
cohort of skull base tumor patients. SRH images were then used to train a
convolutional neural network (CNN) model using three representation learning
strategies: cross-entropy, self-supervised contrastive learning, and supervised
contrastive learning. Our trained CNN models were tested on a held-out,
multicenter SRH dataset.

Results: SRH was able to image the diagnostic features of both benign and
malignant skull base tumors. Of the three representation learning strategies,
supervised contrastive learning most effectively learned the distinctive and
diagnostic SRH image features for each of the skull base tumor types. In our
multicenter testing set, cross-entropy achieved an overall diagnostic accuracy
of 91.5%, self-supervised contrastive learning 83.9%, and supervised
contrastive learning 96.6%. Our trained model was able to identify tumor-normal
margins and detect regions of microscopic tumor infiltration in whole-slide SRH
images.

Conclusion: SRH with AI models trained using contrastive representation
learning can provide rapid and accurate intraoperative diagnosis of skull base
tumors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Cheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Abhishek Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1"&gt;Joseph Linzey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Rushikesh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sung Jik Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Sudharsan Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1"&gt;Daniel Alber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1"&gt;Akhil Kondepudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1"&gt;Esteban Urias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1"&gt;Balaji Pandian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1"&gt;Wajd Al-Holou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1"&gt;Steve Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1"&gt;B. Gregory Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1"&gt;Jason Heth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1"&gt;Chris Freudiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1"&gt;Siri Khalsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1"&gt;Donato Pacione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1"&gt;John G. Golfinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1"&gt;Sandra Camelo-Piragua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1"&gt;Daniel A. Orringer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1"&gt;Todd Hollon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Foveated Reconstruction to Preserve Perceived Image Statistics. (arXiv:2108.03499v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2108.03499</id>
        <link href="http://arxiv.org/abs/2108.03499"/>
        <updated>2021-08-10T02:00:10.930Z</updated>
        <summary type="html"><![CDATA[Foveated image reconstruction recovers full image from a sparse set of
samples distributed according to the human visual system's retinal sensitivity
that rapidly drops with eccentricity. Recently, the use of Generative
Adversarial Networks was shown to be a promising solution for such a task as
they can successfully hallucinate missing image information. Like for other
supervised learning approaches, also for this one, the definition of the loss
function and training strategy heavily influences the output quality. In this
work, we pose the question of how to efficiently guide the training of foveated
reconstruction techniques such that they are fully aware of the human visual
system's capabilities and limitations, and therefore, reconstruct visually
important image features. Due to the nature of GAN-based solutions, we
concentrate on the human's sensitivity to hallucination for different input
sample densities. We present new psychophysical experiments, a dataset, and a
procedure for training foveated image reconstruction. The strategy provides
flexibility to the generator network by penalizing only perceptually important
deviations in the output. As a result, the method aims to preserve perceived
image statistics rather than natural image statistics. We evaluate our strategy
and compare it to alternative solutions using a newly trained objective metric
and user experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Surace_L/0/1/0/all/0/1"&gt;Luca Surace&lt;/a&gt; (Universit&amp;#xe0; della Svizzera italiana), &lt;a href="http://arxiv.org/find/cs/1/au:+Wernikowski_M/0/1/0/all/0/1"&gt;Marek Wernikowski&lt;/a&gt; (West Pomeranian University of Technology), &lt;a href="http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1"&gt;Okan Tursun&lt;/a&gt; (Universit&amp;#xe0; della Svizzera italiana), &lt;a href="http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1"&gt;Karol Myszkowski&lt;/a&gt; (Max Planck Institute for Informatics), &lt;a href="http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1"&gt;Rados&amp;#x142;aw Mantiuk&lt;/a&gt; (West Pomeranian University of Technology), &lt;a href="http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1"&gt;Piotr Didyk&lt;/a&gt; (Universit&amp;#xe0; della Svizzera italiana)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Deepfake Detection. (arXiv:2106.10705v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10705</id>
        <link href="http://arxiv.org/abs/2106.10705"/>
        <updated>2021-08-10T02:00:10.920Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose to utilize Automated Machine Learning to adaptively
search a neural architecture for deepfake detection. This is the first time to
employ automated machine learning for deepfake detection. Based on our explored
search space, our proposed method achieves competitive prediction accuracy
compared to previous methods. To improve the generalizability of our method,
especially when training data and testing data are manipulated by different
methods, we propose a simple yet effective strategy in our network learning
process: making it to estimate potential manipulation regions besides
predicting the real/fake labels. Unlike previous works manually design neural
networks, our method can relieve us from the high labor cost in network
construction. More than that, compared to previous works, our method depends
much less on prior knowledge, e.g., which manipulation method is utilized or
where exactly the fake image is manipulated. Extensive experimental results on
two benchmark datasets demonstrate the effectiveness of our proposed method for
deepfake detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuewei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1"&gt;Liangli Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03857</id>
        <link href="http://arxiv.org/abs/2108.03857"/>
        <updated>2021-08-10T02:00:10.910Z</updated>
        <summary type="html"><![CDATA["Art is the lie that enables us to realize the truth." - Pablo Picasso. For
centuries, humans have dedicated themselves to producing arts to convey their
imagination. The advancement in technology and deep learning in particular, has
caught the attention of many researchers trying to investigate whether art
generation is possible by computers and algorithms. Using generative
adversarial networks (GANs), applications such as synthesizing photorealistic
human faces and creating captions automatically from images were realized. This
survey takes a comprehensive look at the recent works using GANs for generating
visual arts, music, and literary text. A performance comparison and description
of the various GAN architecture are also presented. Finally, some of the key
challenges in art generation using GANs are highlighted along with
recommendations for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1"&gt;Sakib Shahriar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs. (arXiv:2108.00887v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00887</id>
        <link href="http://arxiv.org/abs/2108.00887"/>
        <updated>2021-08-10T02:00:10.898Z</updated>
        <summary type="html"><![CDATA[Along with climate change, more frequent extreme events, such as flooding and
tropical cyclones, threaten the livelihoods and wellbeing of poor and
vulnerable populations. One of the most immediate needs of people affected by a
disaster is finding shelter. While the proliferation of data on disasters is
already helping to save lives, identifying damages in buildings, assessing
shelter needs, and finding appropriate places to establish emergency shelters
or settlements require a wide range of data to be combined rapidly. To address
this gap and make a headway in comprehensive assessments, this paper proposes a
machine learning workflow that aims to fuse and rapidly analyse multimodal
data. This workflow is built around open and online data to ensure scalability
and broad accessibility. Based on a database of 19 characteristics for more
than 200 disasters worldwide, a fusion approach at the decision level was used.
This technique allows the collected multimodal data to share a common semantic
space that facilitates the prediction of individual variables. Each fused
numerical vector was fed into an unsupervised clustering algorithm called
Self-Organizing-Maps (SOM). The trained SOM serves as a predictor for future
cases, allowing predicting consequences such as total deaths, total people
affected, and total damage, and provides specific recommendations for
assessments in the shelter and housing sector. To achieve such prediction, a
satellite image from before the disaster and the geographic and demographic
conditions are shown to the trained model, which achieved a prediction accuracy
of 62 %]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_K/0/1/0/all/0/1"&gt;Karla Saldana Ochoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comes_T/0/1/0/all/0/1"&gt;Tina Comes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03913</id>
        <link href="http://arxiv.org/abs/2108.03913"/>
        <updated>2021-08-10T02:00:10.876Z</updated>
        <summary type="html"><![CDATA[Fundamental machine learning theory shows that different samples contribute
unequally both in learning and testing processes. Contemporary studies on DNN
imply that such sample di?erence is rooted on the distribution of intrinsic
pattern information, namely sample regularity. Motivated by the recent
discovery on network memorization and generalization, we proposed a pair of
sample regularity measures for both processes with a formulation-consistent
representation. Specifically, cumulative binary training/generalizing loss
(CBTL/CBGL), the cumulative number of correct classi?cations of the
training/testing sample within training stage, is proposed to quantize the
stability in memorization-generalization process; while
forgetting/mal-generalizing events, i.e., the mis-classification of previously
learned or generalized sample, are utilized to represent the uncertainty of
sample regularity with respect to optimization dynamics. Experiments validated
the effectiveness and robustness of the proposed approaches for mini-batch SGD
optimization. Further applications on training/testing sample selection show
the proposed measures sharing the uni?ed computing procedure could benefit for
both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaoning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yuanqi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuehu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image reconstruction in light-sheet microscopy: spatially varying deconvolution and mixed noise. (arXiv:2108.03642v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.03642</id>
        <link href="http://arxiv.org/abs/2108.03642"/>
        <updated>2021-08-10T02:00:10.868Z</updated>
        <summary type="html"><![CDATA[We study the problem of deconvolution for light-sheet microscopy, where the
data is corrupted by spatially varying blur and a combination of Poisson and
Gaussian noise. The spatial variation of the point spread function (PSF) of a
light-sheet microscope is determined by the interaction between the excitation
sheet and the detection objective PSF. First, we introduce a model of the image
formation process that incorporates this interaction, therefore capturing the
main characteristics of this imaging modality. Then, we formulate a variational
model that accounts for the combination of Poisson and Gaussian noise through a
data fidelity term consisting of the infimal convolution of the single noise
fidelities, first introduced in L. Calatroni et al. "Infimal convolution of
data discrepancies for mixed noise removal", SIAM Journal on Imaging Sciences
10.3 (2017), 1196-1233. We establish convergence rates in a Bregman distance
under a source condition for the infimal convolution fidelity and a discrepancy
principle for choosing the value of the regularisation parameter. The inverse
problem is solved by applying the primal-dual hybrid gradient (PDHG) algorithm
in a novel way. Finally, numerical experiments performed on both simulated and
real data show superior reconstruction results in comparison with other
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Toader_B/0/1/0/all/0/1"&gt;Bogdan Toader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Boulanger_J/0/1/0/all/0/1"&gt;Jerome Boulanger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Korolev_Y/0/1/0/all/0/1"&gt;Yury Korolev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lenz_M/0/1/0/all/0/1"&gt;Martin O. Lenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Manton_J/0/1/0/all/0/1"&gt;James Manton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Schonlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Muresan_L/0/1/0/all/0/1"&gt;Leila Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DistillPose: Lightweight Camera Localization Using Auxiliary Learning. (arXiv:2108.03819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03819</id>
        <link href="http://arxiv.org/abs/2108.03819"/>
        <updated>2021-08-10T02:00:10.862Z</updated>
        <summary type="html"><![CDATA[We propose a lightweight retrieval-based pipeline to predict 6DOF camera
poses from RGB images. Our pipeline uses a convolutional neural network (CNN)
to encode a query image as a feature vector. A nearest neighbor lookup finds
the pose-wise nearest database image. A siamese convolutional neural network
regresses the relative pose from the nearest neighboring database image to the
query image. The relative pose is then applied to the nearest neighboring
absolute pose to obtain the query image's final absolute pose prediction. Our
model is a distilled version of NN-Net that reduces its parameters by 98.87%,
information retrieval feature vector size by 87.5%, and inference time by
89.18% without a significant decrease in localization accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abouelnaga_Y/0/1/0/all/0/1"&gt;Yehya Abouelnaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1"&gt;Mai Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1"&gt;Slobodan Ilic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EigenGAN: Layer-Wise Eigen-Learning for GANs. (arXiv:2104.12476v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12476</id>
        <link href="http://arxiv.org/abs/2104.12476"/>
        <updated>2021-08-10T02:00:10.852Z</updated>
        <summary type="html"><![CDATA[Recent studies on Generative Adversarial Network (GAN) reveal that different
layers of a generative CNN hold different semantics of the synthesized images.
However, few GAN models have explicit dimensions to control the semantic
attributes represented in a specific layer. This paper proposes EigenGAN which
is able to unsupervisedly mine interpretable and controllable dimensions from
different generator layers. Specifically, EigenGAN embeds one linear subspace
with orthogonal basis into each generator layer. Via generative adversarial
training to learn a target distribution, these layer-wise subspaces
automatically discover a set of "eigen-dimensions" at each layer corresponding
to a set of semantic attributes or interpretable variations. By traversing the
coefficient of a specific eigen-dimension, the generator can produce samples
with continuous changes corresponding to a specific semantic attribute. Taking
the human face for example, EigenGAN can discover controllable dimensions for
high-level concepts such as pose and gender in the subspace of deep layers, as
well as low-level concepts such as hue and color in the subspace of shallow
layers. Moreover, in the linear case, we theoretically prove that our algorithm
derives the principal components as PCA does. Codes can be found in
https://github.com/LynnHo/EigenGAN-Tensorflow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhenliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1"&gt;Meina Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary-aware Graph Reasoning for Semantic Segmentation. (arXiv:2108.03791v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03791</id>
        <link href="http://arxiv.org/abs/2108.03791"/>
        <updated>2021-08-10T02:00:10.846Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Boundary-aware Graph Reasoning (BGR) module to
learn long-range contextual features for semantic segmentation. Rather than
directly construct the graph based on the backbone features, our BGR module
explores a reasonable way to combine segmentation erroneous regions with the
graph construction scenario. Motivated by the fact that most hard-to-segment
pixels broadly distribute on boundary regions, our BGR module uses the boundary
score map as prior knowledge to intensify the graph node connections and
thereby guide the graph reasoning focus on boundary regions. In addition, we
employ an efficient graph convolution implementation to reduce the
computational cost, which benefits the integration of our BGR module into
current segmentation backbones. Extensive experiments on three challenging
segmentation benchmarks demonstrate the effectiveness of our proposed BGR
module for semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoteng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;Haozhe Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Liang Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Saliency-Associated Object Tracking. (arXiv:2108.03637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03637</id>
        <link href="http://arxiv.org/abs/2108.03637"/>
        <updated>2021-08-10T02:00:10.790Z</updated>
        <summary type="html"><![CDATA[Most existing trackers based on deep learning perform tracking in a holistic
strategy, which aims to learn deep representations of the whole target for
localizing the target. It is arduous for such methods to track targets with
various appearance variations. To address this limitation, another type of
methods adopts a part-based tracking strategy which divides the target into
equal patches and tracks all these patches in parallel. The target state is
inferred by summarizing the tracking results of these patches. A potential
limitation of such trackers is that not all patches are equally informative for
tracking. Some patches that are not discriminative may have adverse effects. In
this paper, we propose to track the salient local parts of the target that are
discriminative for tracking. In particular, we propose a fine-grained saliency
mining module to capture the local saliencies. Further, we design a
saliency-association modeling module to associate the captured saliencies
together to learn effective correlation representations between the exemplar
and the search image for state estimation. Extensive experiments on five
diverse datasets demonstrate that the proposed method performs favorably
against state-of-the-art trackers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zikun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1"&gt;Wenjie Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongpeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhenyu He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection. (arXiv:2108.03619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03619</id>
        <link href="http://arxiv.org/abs/2108.03619"/>
        <updated>2021-08-10T02:00:10.765Z</updated>
        <summary type="html"><![CDATA[In video understanding, most cross-modal knowledge distillation (KD) methods
are tailored for classification tasks, focusing on the discriminative
representation of the trimmed videos. However, action detection requires not
only categorizing actions, but also localizing them in untrimmed videos.
Therefore, transferring knowledge pertaining to temporal relations is critical
for this task which is missing in the previous cross-modal KD frameworks. To
this end, we aim at learning an augmented RGB representation for action
detection, taking advantage of additional modalities at training time through
KD. We propose a KD framework consisting of two levels of distillation. On one
hand, atomic-level distillation encourages the RGB student to learn the
sub-representation of the actions from the teacher in a contrastive manner. On
the other hand, sequence-level distillation encourages the student to learn the
temporal knowledge from the teacher, which consists of transferring the Global
Contextual Relations and the Action Boundary Saliency. The result is an
Augmented-RGB stream that can achieve competitive performance as the two-stream
network while using only RGB at inference time. Extensive experimental analysis
shows that our proposed distillation framework is generic and outperforms other
popular cross-modal distillation methods in action detection task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1"&gt;Rui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Srijan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1"&gt;Francois Bremond&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching. (arXiv:2108.03746v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03746</id>
        <link href="http://arxiv.org/abs/2108.03746"/>
        <updated>2021-08-10T02:00:10.765Z</updated>
        <summary type="html"><![CDATA[Learning to generate 3D point clouds without 3D supervision is an important
but challenging problem. Current solutions leverage various differentiable
renderers to project the generated 3D point clouds onto a 2D image plane, and
train deep neural networks using the per-pixel difference with 2D ground truth
images. However, these solutions are still struggling to fully recover fine
structures of 3D shapes, such as thin tubes or planes. To resolve this issue,
we propose an unsupervised approach for 3D point cloud generation with fine
structures. Specifically, we cast 3D point cloud learning as a 2D projection
matching problem. Rather than using entire 2D silhouette images as a regular
pixel supervision, we introduce structure adaptive sampling to randomly sample
2D points within the silhouettes as an irregular point supervision, which
alleviates the consistency issue of sampling from different view angles. Our
method pushes the neural network to generate a 3D point cloud whose 2D
projections match the irregular point supervision from different view angles.
Our 2D projection matching approach enables the neural network to learn more
accurate structure information than using the per-pixel difference, especially
for fine and thin 3D structures. Our method can recover fine 3D structures from
2D silhouette images at different resolutions, and is robust to different
sampling methods and point number in irregular point supervision. Our method
outperforms others under widely used benchmarks. Our code, data and models are
available at https://github.com/chenchao15/2D\_projection\_matching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chao_C/0/1/0/all/0/1"&gt;Chen Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhizhong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu-Shen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1"&gt;Matthias Zwicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03786</id>
        <link href="http://arxiv.org/abs/2108.03786"/>
        <updated>2021-08-10T02:00:10.757Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel lightweight COVID-19 diagnosis framework using CT
scans. Our system utilises a novel two-stage approach to generate robust and
efficient diagnoses across heterogeneous patient level inputs. We use a
powerful backbone network as a feature extractor to capture discriminative
slice-level features. These features are aggregated by a lightweight network to
obtain a patient level diagnosis. The aggregation network is carefully designed
to have a small number of trainable parameters while also possessing sufficient
capacity to generalise to diverse variations within different CT volumes and to
adapt to noise introduced during the data acquisition. We achieve a significant
performance increase over the baselines when benchmarked on the SPGC COVID-19
Radiomics Dataset, despite having only 2.5 million trainable parameters and
requiring only 0.623 seconds on average to process a single patient's CT volume
using an Nvidia-GeForce RTX 2080 GPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1"&gt;Harshala Gammulle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1"&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1"&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1"&gt;Simon Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1"&gt;Clinton Fookes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution. (arXiv:2108.03918v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03918</id>
        <link href="http://arxiv.org/abs/2108.03918"/>
        <updated>2021-08-10T02:00:10.749Z</updated>
        <summary type="html"><![CDATA[Camera arrays provide spatial and angular information within a single
snapshot. With refocusing methods, focal planes can be altered after exposure.
In this letter, we propose a light field refocusing method to improve the
imaging quality of camera arrays. In our method, the disparity is first
estimated. Then, the unfocused region (bokeh) is rendered by using a
depth-based anisotropic filter. Finally, the refocused image is produced by a
reconstruction-based superresolution approach where the bokeh image is used as
a regularization term. Our method can selectively refocus images with focused
region being superresolved and bokeh being aesthetically rendered. Our method
also enables postadjustment of depth of field. We conduct experiments on both
public and self-developed datasets. Our method achieves superior visual
performance with acceptable computational cost as compared to other
state-of-the-art methods. Code is available at
https://github.com/YingqianWang/Selective-LF-Refocusing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jungang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yulan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1"&gt;Wei An&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alignment of Tractography Streamlines using Deformation Transfer via Parallel Transport. (arXiv:2108.03697v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03697</id>
        <link href="http://arxiv.org/abs/2108.03697"/>
        <updated>2021-08-10T02:00:10.746Z</updated>
        <summary type="html"><![CDATA[We present a geometric framework for aligning white matter fiber tracts. By
registering fiber tracts between brains, one expects to see overlap of
anatomical structures that often provide meaningful comparisons across
subjects. However, the geometry of white matter tracts is highly heterogeneous,
and finding direct tract-correspondence across multiple individuals remains a
challenging problem. We present a novel deformation metric between tracts that
allows one to compare tracts while simultaneously obtaining a registration. To
accomplish this, fiber tracts are represented by an intrinsic mean along with
the deformation fields represented by tangent vectors from the mean. In this
setting, one can determine a parallel transport between tracts and then
register corresponding tangent vectors. We present the results of bundle
alignment on a population of 43 healthy adult subjects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lizarraga_A/0/1/0/all/0/1"&gt;Andrew Lizarraga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;David Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubicki_A/0/1/0/all/0/1"&gt;Antoni Kubicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahib_A/0/1/0/all/0/1"&gt;Ashish Sahib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1"&gt;Elvis Nunez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narr_K/0/1/0/all/0/1"&gt;Katherine Narr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shantanu H. Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rain Removal and Illumination Enhancement Done in One Go. (arXiv:2108.03873v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03873</id>
        <link href="http://arxiv.org/abs/2108.03873"/>
        <updated>2021-08-10T02:00:10.743Z</updated>
        <summary type="html"><![CDATA[Rain removal plays an important role in the restoration of degraded images.
Recently, data-driven methods have achieved remarkable success. However, these
approaches neglect that the appearance of rain is often accompanied by low
light conditions, which will further degrade the image quality. Therefore, it
is very indispensable to jointly remove the rain and enhance the light for
real-world rain image restoration. In this paper, we aim to address this
problem from two aspects. First, we proposed a novel entangled network, namely
EMNet, which can remove the rain and enhance illumination in one go.
Specifically, two encoder-decoder networks interact complementary information
through entanglement structure, and parallel rain removal and illumination
enhancement. Considering that the encoder-decoder structure is unreliable in
preserving spatial details, we employ a detail recovery network to restore the
desired fine texture. Second, we present a new synthetic dataset, namely
DarkRain, to boost the development of rain image restoration algorithms in
practical scenarios. DarkRain not only contains different degrees of rain, but
also considers different lighting conditions, and more realistically simulates
the rainfall in the real world. EMNet is extensively evaluated on the proposed
benchmark and achieves state-of-the-art results. In addition, after a simple
transformation, our method outshines existing methods in both rain removal and
low-light image enhancement. The source code and dataset will be made publicly
available later.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yecong Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuanshuo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1"&gt;Mingwen Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton-Contrastive 3D Action Representation Learning. (arXiv:2108.03656v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03656</id>
        <link href="http://arxiv.org/abs/2108.03656"/>
        <updated>2021-08-10T02:00:10.683Z</updated>
        <summary type="html"><![CDATA[This paper strives for self-supervised learning of a feature space suitable
for skeleton-based action recognition. Our proposal is built upon learning
invariances to input skeleton representations and various skeleton
augmentations via a noise contrastive estimation. In particular, we propose
inter-skeleton contrastive learning, which learns from multiple different input
skeleton representations in a cross-contrastive manner. In addition, we
contribute several skeleton-specific spatial and temporal augmentations which
further encourage the model to learn the spatio-temporal dynamics of skeleton
data. By learning similarities between different skeleton representations as
well as augmented views of the same sequence, the network is encouraged to
learn higher-level semantics of the skeleton data than when only using the
augmented views. Our approach achieves state-of-the-art performance for
self-supervised learning from skeleton data on the challenging PKU and NTU
datasets with multiple downstream tasks, including action recognition, action
retrieval and semi-supervised learning. Code is available at
https://github.com/fmthoker/skeleton-contrast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1"&gt;Fida Mohammad Thoker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1"&gt;Hazel Doughty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13731</id>
        <link href="http://arxiv.org/abs/2106.13731"/>
        <updated>2021-08-10T02:00:10.680Z</updated>
        <summary type="html"><![CDATA[As optimizers are critical to the performances of neural networks, every year
a large number of papers innovating on the subject are published. However,
while most of these publications provide incremental improvements to existing
algorithms, they tend to be presented as new optimizers rather than composable
algorithms. Thus, many worthwhile improvements are rarely seen out of their
initial publication. Taking advantage of this untapped potential, we introduce
Ranger21, a new optimizer which combines AdamW with eight components, carefully
selected after reviewing and testing ideas from the literature. We found that
the resulting optimizer provides significantly improved validation accuracy and
training speed, smoother training curves, and is even able to train a ResNet50
on ImageNet2012 without Batch Normalization layers. A problem on which AdamW
stays systematically stuck in a bad initial state.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1"&gt;Less Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1"&gt;Nestor Demeure&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Quantum Potentials by Deep Neural Network and Metropolis Sampling. (arXiv:2106.03126v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03126</id>
        <link href="http://arxiv.org/abs/2106.03126"/>
        <updated>2021-08-10T02:00:10.585Z</updated>
        <summary type="html"><![CDATA[The hybridizations of machine learning and quantum physics have caused
essential impacts to the methodology in both fields. Inspired by quantum
potential neural network, we here propose to solve the potential in the
Schrodinger equation provided the eigenstate, by combining Metropolis sampling
with deep neural network, which we dub as Metropolis potential neural network
(MPNN). A loss function is proposed to explicitly involve the energy in the
optimization for its accurate evaluation. Benchmarking on the harmonic
oscillator and hydrogen atom, MPNN shows excellent accuracy and stability on
predicting not just the potential to satisfy the Schrodinger equation, but also
the eigen-energy. Our proposal could be potentially applied to the ab-initio
simulations, and to inversely solving other partial differential equations in
physics and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hong_R/0/1/0/all/0/1"&gt;Rui Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Peng-Fei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Xi_B/0/1/0/all/0/1"&gt;Bin Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ji_A/0/1/0/all/0/1"&gt;An-Chun Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1"&gt;Shi-Ju Ran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to "Improve" Prediction Using Behavior Modification. (arXiv:2008.12138v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12138</id>
        <link href="http://arxiv.org/abs/2008.12138"/>
        <updated>2021-08-10T02:00:10.491Z</updated>
        <summary type="html"><![CDATA[Many internet platforms that collect behavioral big data use it to predict
user behavior for internal purposes and for their business customers (e.g.,
advertisers, insurers, security forces, governments, political consulting
firms) who utilize the predictions for personalization, targeting, and other
decision-making. Improving predictive accuracy is therefore extremely valuable.
Data science researchers design algorithms, models, and approaches to improve
prediction. Prediction is also improved with larger and richer data. Beyond
improving algorithms and data, platforms can stealthily achieve better
prediction accuracy by "pushing" users' behaviors towards their predicted
values, using behavior modification techniques, thereby demonstrating more
certain predictions. Such apparent "improved" prediction can unintentionally
result from employing reinforcement learning algorithms that combine prediction
and behavior modification. This strategy is absent from the machine learning
and statistics literature. Investigating its properties requires integrating
causal with predictive notation. To this end, we incorporate Pearl's causal
do(.) operator into the predictive vocabulary. We then decompose the expected
prediction error given behavior modification, and identify the components
impacting predictive power. Our derivation elucidates implications of such
behavior modification to data scientists, platforms, their customers, and the
humans whose behavior is manipulated. Behavior modification can make users'
behavior more predictable and even more homogeneous; yet this apparent
predictability might not generalize when customers use predictions in practice.
Outcomes pushed towards their predictions can be at odds with customers'
intentions, and harmful to manipulated users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmueli_G/0/1/0/all/0/1"&gt;Galit Shmueli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tafti_A/0/1/0/all/0/1"&gt;Ali Tafti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06534</id>
        <link href="http://arxiv.org/abs/2104.06534"/>
        <updated>2021-08-10T02:00:10.485Z</updated>
        <summary type="html"><![CDATA[Existing thermal-to-visible face verification approaches expect the thermal
and visible face images to be of similar resolution. This is unlikely in
real-world long-range surveillance systems, since humans are distant from the
cameras. To address this issue, we introduce the task of thermal-to-visible
face verification from low-resolution thermal images. Furthermore, we propose
Axial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution
visible images for matching. In the proposed approach we augment the GAN
framework with axial-attention layers which leverage the recent advances in
transformers for modelling long-range dependencies. We demonstrate the
effectiveness of the proposed method by evaluating on two different
thermal-visible face datasets. When compared to related state-of-the-art works,
our results show significant improvements in both image quality and face
verification performance, and are also much more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1"&gt;Rakhil Immidisetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shuowen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complex Transformer: A Framework for Modeling Complex-Valued Sequence. (arXiv:1910.10202v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10202</id>
        <link href="http://arxiv.org/abs/1910.10202"/>
        <updated>2021-08-10T02:00:10.477Z</updated>
        <summary type="html"><![CDATA[While deep learning has received a surge of interest in a variety of fields
in recent years, major deep learning models barely use complex numbers.
However, speech, signal and audio data are naturally complex-valued after
Fourier Transform, and studies have shown a potentially richer representation
of complex nets. In this paper, we propose a Complex Transformer, which
incorporates the transformer model as a backbone for sequence modeling; we also
develop attention and encoder-decoder network operating for complex input. The
model achieves state-of-the-art performance on the MusicNet dataset and an
In-phase Quadrature (IQ) signal dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Martin Q. Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dongyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00959</id>
        <link href="http://arxiv.org/abs/2005.00959"/>
        <updated>2021-08-10T02:00:10.460Z</updated>
        <summary type="html"><![CDATA[Ill-posed linear inverse problems appear in many scientific setups, and are
typically addressed by solving optimization problems, which are composed of
data fidelity and prior terms. Recently, several works have considered a
back-projection (BP) based fidelity term as an alternative to the common least
squares (LS), and demonstrated excellent results for popular inverse problems.
These works have also empirically shown that using the BP term, rather than the
LS term, requires fewer iterations of optimization algorithms. In this paper,
we examine the convergence rate of the projected gradient descent (PGD)
algorithm for the BP objective. Our analysis allows to identify an inherent
source for its faster convergence compared to using the LS objective, while
making only mild assumptions. We also analyze the more general proximal
gradient method under a relaxed contraction condition on the proximal mapping
of the prior. This analysis further highlights the advantage of BP when the
linear measurement operator is badly conditioned. Numerical experiments with
both $\ell_1$-norm and GAN-based priors corroborate our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1"&gt;Tom Tirer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning. (arXiv:2108.03803v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03803</id>
        <link href="http://arxiv.org/abs/2108.03803"/>
        <updated>2021-08-10T02:00:10.454Z</updated>
        <summary type="html"><![CDATA[Recent studies in multi-agent communicative reinforcement learning (MACRL)
demonstrate that multi-agent coordination can be significantly improved when
communication between agents is allowed. Meanwhile, advances in adversarial
machine learning (ML) have shown that ML and reinforcement learning (RL) models
are vulnerable to a variety of attacks that significantly degrade the
performance of learned behaviours. However, despite the obvious and growing
importance, the combination of adversarial ML and MACRL remains largely
uninvestigated. In this paper, we make the first step towards conducting
message attacks on MACRL methods. In our formulation, one agent in the
cooperating group is taken over by an adversary and can send malicious messages
to disrupt a deployed MACRL-based coordinated strategy during the deployment
phase. We further our study by developing a defence method via message
reconstruction. Finally, we address the resulting arms race, i.e., we consider
the ability of the malicious agent to adapt to the changing and improving
defensive communicative policies of the benign agents. Specifically, we model
the adversarial MACRL problem as a two-player zero-sum game and then utilize
Policy-Space Response Oracle to achieve communication robustness. Empirically,
we demonstrate that MACRL methods are vulnerable to message attacks while our
defence method the game-theoretic framework can effectively improve the
robustness of MACRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wanqi Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1"&gt;Wei Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabinovich_Z/0/1/0/all/0/1"&gt;Zinovi Rabinovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obraztsova_S/0/1/0/all/0/1"&gt;Svetlana Obraztsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1"&gt;Chai Kiat Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning Local Reduced Density Matrices for Many-body Hamiltonian Estimation. (arXiv:2012.03019v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03019</id>
        <link href="http://arxiv.org/abs/2012.03019"/>
        <updated>2021-08-10T02:00:10.447Z</updated>
        <summary type="html"><![CDATA[Human experts cannot efficiently access the physical information of quantum
many-body states by simply "reading" the coefficients, but have to reply on the
previous knowledge such as order parameters and quantum measurements. In this
work, we demonstrate that convolutional neural network (CNN) can learn from the
coefficients of local reduced density matrices to estimate the physical
parameters of the many-body Hamiltonians, such as coupling strengths and
magnetic fields, provided the states as the ground states. We propose QubismNet
that consists of two main parts: the Qubism map that visualizes the ground
states (or the purified reduced density matrices) as images, and a CNN that
maps the images to the target physical parameters. By assuming certain
constraints on the training set for the sake of balance, QubismNet exhibits
impressive powers of learning and generalization on several quantum spin
models. While the training samples are restricted to the states from certain
ranges of the parameters, QubismNet can accurately estimate the parameters of
the states beyond such training regions. For instance, our results show that
QubismNet can estimate the magnetic fields near the critical point by learning
from the states away from the critical vicinity. Our work illuminates a
data-driven way to infer the Hamiltonians that give the designed ground states,
and therefore would benefit the existing and future generalizations of quantum
technologies such as Hamiltonian-based quantum simulations and state
tomography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinran Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Z. C. Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1"&gt;Shi-Ju Ran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03995</id>
        <link href="http://arxiv.org/abs/2103.03995"/>
        <updated>2021-08-10T02:00:10.441Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are widely used in image recognition.
Numerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have
been proposed by increasing the number of layers, to improve the performance of
CNNs. However, performance deteriorates beyond a certain number of layers.
Hence, hyperparameter optimisation is a more efficient way to improve CNNs. To
validate this concept, a new algorithm based on simplified swarm optimisation
is proposed to optimise the hyperparameters of the simplest CNN model, which is
LeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and
Cifar10 datasets showed that the accuracy of the proposed algorithm is higher
than the original LeNet model and PSO-LeNet and that it has a high potential to
be extended to more complicated models, such as AlexNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1"&gt;Wei-Chang Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi-Ping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yun-Chia Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Chyh-Ming Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Probabilistic Model for Short-Term Solar Power Forecasting Using Natural Gradient Boosting. (arXiv:2108.04058v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.04058</id>
        <link href="http://arxiv.org/abs/2108.04058"/>
        <updated>2021-08-10T02:00:10.434Z</updated>
        <summary type="html"><![CDATA[The stochastic nature of photovoltaic (PV) power has led both academia and
industry to a large amount of research work aiming at the development of
accurate PV power forecasting models. However, most of those models are based
on machine learning algorithms and are considered as black boxes which do not
provide any insight or explanation about their predictions. Therefore, their
direct implementation in environments, where transparency is required, and the
trust associated with their predictions may be questioned. To this end, we
propose a two stage probabilistic forecasting framework able to generate highly
accurate, reliable, and sharp forecasts yet offering full transparency on both
the point forecasts and the prediction intervals (PIs). In the first stage, we
exploit natural gradient boosting (NGBoost) for yielding probabilistic
forecasts while in the second stage, we calculate the Shapley additive
explanation (SHAP) values in order to fully understand why a prediction was
made. To highlight the performance and the applicability of the proposed
framework, real data from two PV parks located in Southern Germany are
employed. Initially, the natural gradient boosting is thoroughly compared with
two state-of-the-art algorithms, namely Gaussian process and lower upper bound
estimation, in a wide range of forecasting metrics. Secondly, a detailed
analysis of the model's complex nonlinear relationships and interaction effects
between the various features is presented. The latter allows us to interpret
the model, identify some learned physical properties, explain individual
predictions, reduce the computational requirements for the training without
jeopardizing the model accuracy, detect possible bugs, and gain trust in the
model. Finally, we conclude that the model was able to develop nonlinear
relationships following human logic and intuition based on learned physical
properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mitrentsis_G/0/1/0/all/0/1"&gt;Georgios Mitrentsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lens_H/0/1/0/all/0/1"&gt;Hendrik Lens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-learning: Learning from Noisy Labels with Self-supervision. (arXiv:2108.04063v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04063</id>
        <link href="http://arxiv.org/abs/2108.04063"/>
        <updated>2021-08-10T02:00:10.403Z</updated>
        <summary type="html"><![CDATA[Noisy labels, resulting from mistakes in manual labeling or webly data
collecting for supervised learning, can cause neural networks to overfit the
misleading information and degrade the generalization performance.
Self-supervised learning works in the absence of labels and thus eliminates the
negative impact of noisy labels. Motivated by co-training with both supervised
learning view and self-supervised learning view, we propose a simple yet
effective method called Co-learning for learning with noisy labels. Co-learning
performs supervised learning and self-supervised learning in a cooperative way.
The constraints of intrinsic similarity with the self-supervised module and the
structural similarity with the noisily-supervised module are imposed on a
shared common feature encoder to regularize the network to maximize the
agreement between the two constraints. Co-learning is compared with peer
methods on corrupted data from benchmark datasets fairly, and extensive results
are provided which demonstrate that Co-learning is superior to many
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Cheng Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jun Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan Z. Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can a CNN trained on the Ising model detect the phase transition of the $q$-state Potts model?. (arXiv:2104.03632v3 [cond-mat.dis-nn] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.03632</id>
        <link href="http://arxiv.org/abs/2104.03632"/>
        <updated>2021-08-10T02:00:10.395Z</updated>
        <summary type="html"><![CDATA[Employing a deep convolutional neural network (deep CNN) trained on spin
configurations of the 2D Ising model and the temperatures, we examine whether
the deep CNN can detect the phase transition of the 2D $q$-state Potts model.
To this end, we generate binarized images of spin configurations of the
$q$-state Potts model ($q\ge 3$) by replacing the spin variables
$\{0,1,\dots,\lfloor q/2\rfloor-1\}$ and $\{\lfloor q/2\rfloor,\dots,q-1\}$
with $\{0\}$ and $\{1\}$, respectively. Then, we input these images to the
trained CNN to output the predicted temperatures. The binarized images of the
$q$-state Potts model are entirely different from Ising spin configurations,
particularly at the transition temperature. Moreover, our CNN model is not
trained on the information about whether phases are ordered/disordered but is
naively trained by Ising spin configurations labeled with temperatures at which
they are generated. Nevertheless, the deep CNN can detect the transition point
with high accuracy, regardless of the type of transition. We also find that, in
the high-temperature region, the CNN outputs the temperature based on the
internal energy, whereas, in the low-temperature region, the output depends on
the magnetization and possibly the internal energy as well. However, in the
vicinity of the transition point, the CNN may use more general factors to
detect the transition point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fukushima_K/0/1/0/all/0/1"&gt;Kimihiko Fukushima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sakai_K/0/1/0/all/0/1"&gt;Kazumitsu Sakai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems. (arXiv:2108.04059v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.04059</id>
        <link href="http://arxiv.org/abs/2108.04059"/>
        <updated>2021-08-10T02:00:10.388Z</updated>
        <summary type="html"><![CDATA[Sensing systems powered by energy harvesting have traditionally been designed
to tolerate long periods without energy. As the Internet of Things (IoT)
evolves towards a more transient and opportunistic execution paradigm, reducing
energy storage costs will be key for its economic and ecologic viability.
However, decreasing energy storage in harvesting systems introduces reliability
issues. Transducers only produce intermittent energy at low voltage and current
levels, making guaranteed task completion a challenge. Existing ad hoc methods
overcome this by buffering enough energy either for single tasks, incurring
large data-retention overheads, or for one full application cycle, requiring a
large energy buffer. We present Julienning: an automated method for optimizing
the total energy cost of batteryless applications. Using a custom specification
model, developers can describe transient applications as a set of atomically
executed kernels with explicit data dependencies. Our optimization flow can
partition data- and energy-intensive applications into multiple execution
cycles with bounded energy consumption. By leveraging interkernel data
dependencies, these energy-bounded execution cycles minimize the number of
system activations and nonvolatile data transfers, and thus the total energy
overhead. We validate our methodology with two batteryless cameras running
energy-intensive machine learning applications. Results demonstrate that
compared to ad hoc solutions, our method can reduce the required energy storage
by over 94% while only incurring a 0.12% energy overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Andres Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tretter_A/0/1/0/all/0/1"&gt;Andreas Tretter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_P/0/1/0/all/0/1"&gt;Pascal Alexander Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanmugarajah_P/0/1/0/all/0/1"&gt;Praveenth Sanmugarajah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1"&gt;Luca Benini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1"&gt;Lothar Thiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Limitations of machine learning for building energy prediction. (arXiv:2106.13475v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13475</id>
        <link href="http://arxiv.org/abs/2106.13475"/>
        <updated>2021-08-10T02:00:10.382Z</updated>
        <summary type="html"><![CDATA[Machine learning for building energy prediction has exploded in popularity in
recent years, yet understanding its limitations and potential for improvement
are lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition
was the largest building energy meter machine learning competition ever held
with 4,370 participants who submitted 39,403 predictions. The test data set
included two years of hourly electricity, hot water, chilled water, and steam
readings from 2,380 meters in 1,448 buildings at 16 locations. This paper
analyzes the various sources and types of residual model error from an
aggregation of the competition's top 50 solutions. This analysis reveals the
limitations for machine learning using the standard model inputs of historical
meter, weather, and basic building metadata. The types of error are classified
according to the amount of time errors occur in each instance, abrupt versus
gradual behavior, the magnitude of error, and whether the error existed on
single buildings or several buildings at once from a single location. The
results show machine learning models have errors within a range of
acceptability on 79.1% of the test data. Lower magnitude model errors occur in
16.1% of the test data. These discrepancies can likely be addressed through
additional training data sources or innovations in machine learning. Higher
magnitude errors occur in 4.8% of the test data and are unlikely to be
accurately predicted regardless of innovation. There is a diversity of error
behavior depending on the energy meter type (electricity prediction models have
unacceptable error in under 10% of test data, while hot water is over 60%) and
building use type (public service less than 14%, while technology/science is
just over 46%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1"&gt;Clayton Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1"&gt;Bianca Picchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1"&gt;Jovan Pantelic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combinatorial Bandits under Strategic Manipulations. (arXiv:2102.12722v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12722</id>
        <link href="http://arxiv.org/abs/2102.12722"/>
        <updated>2021-08-10T02:00:10.375Z</updated>
        <summary type="html"><![CDATA[Strategic behavior against sequential learning methods, such as "click
framing" in real recommendation systems, has been widely observed. Motivated by
such behavior we study the problem of combinatorial multi-armed bandits (CMAB)
under strategic manipulations of rewards, where each arm can modify the emitted
reward signals for its own interest. This characterization of the adversarial
behavior is a relaxation of previously well-studied settings such as
adversarial attacks and adversarial corruption. We propose a strategic variant
of the combinatorial UCB algorithm, which has a regret of at most $O(m\log T +
m B_{max})$ under strategic manipulations, where $T$ is the time horizon, $m$
is the number of arms, and $B_{max}$ is the maximum budget of an arm. We
provide lower bounds on the budget for arms to incur certain regret of the
bandit algorithm. Extensive experiments on online worker selection for
crowdsourcing systems, online influence maximization and online recommendations
with both synthetic and real datasets corroborate our theoretical findings on
robustness and regret bounds, in a variety of regimes of manipulation budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Baoxiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Householder Activations for Provable Robustness against Adversarial Attacks. (arXiv:2108.04062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04062</id>
        <link href="http://arxiv.org/abs/2108.04062"/>
        <updated>2021-08-10T02:00:10.368Z</updated>
        <summary type="html"><![CDATA[Training convolutional neural networks (CNNs) with a strict Lipschitz
constraint under the l_{2} norm is useful for provable adversarial robustness,
interpretable gradients and stable training. While 1-Lipschitz CNNs can be
designed by enforcing a 1-Lipschitz constraint on each layer, training such
networks requires each layer to have an orthogonal Jacobian matrix (for all
inputs) to prevent gradients from vanishing during backpropagation. A layer
with this property is said to be Gradient Norm Preserving (GNP). To construct
expressive GNP activation functions, we first prove that the Jacobian of any
GNP piecewise linear function is only allowed to change via Householder
transformations for the function to be continuous. Building on this result, we
introduce a class of nonlinear GNP activations with learnable Householder
transformations called Householder activations. A householder activation
parameterized by the vector $\mathbf{v}$ outputs $(\mathbf{I} -
2\mathbf{v}\mathbf{v}^{T})\mathbf{z}$ for its input $\mathbf{z}$ if
$\mathbf{v}^{T}\mathbf{z} \leq 0$; otherwise it outputs $\mathbf{z}$. Existing
GNP activations such as $\mathrm{MaxMin}$ can be viewed as special cases of
$\mathrm{HH}$ activations for certain settings of these transformations. Thus,
networks with $\mathrm{HH}$ activations have higher expressive power than those
with $\mathrm{MaxMin}$ activations. Although networks with $\mathrm{HH}$
activations have nontrivial provable robustness against adversarial attacks, we
further boost their robustness by (i) introducing a certificate regularization
and (ii) relaxing orthogonalization of the last layer of the network. Our
experiments on CIFAR-10 and CIFAR-100 show that our regularized networks with
$\mathrm{HH}$ activations lead to significant improvements in both the standard
and provable robust accuracy over the prior works (gain of 3.65\% and 4.46\% on
CIFAR-100 respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sahil Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Surbhi Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty quantification for industrial design using dictionaries of reduced order models. (arXiv:2108.04012v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.04012</id>
        <link href="http://arxiv.org/abs/2108.04012"/>
        <updated>2021-08-10T02:00:10.350Z</updated>
        <summary type="html"><![CDATA[We consider the dictionary-based ROM-net (Reduced Order Model) framework [T.
Daniel, F. Casenave, N. Akkari, D. Ryckelynck, Model order reduction assisted
by deep neural networks (ROM-net), Advanced modeling and Simulation in
Engineering Sciences 7 (16), 2020] and summarize the underlying methodologies
and their recent improvements. The main contribution of this work is the
application of the complete workflow to a real-life industrial model of an
elastoviscoplastic high-pressure turbine blade subjected to thermal,
centrifugal and pressure loadings, for the quantification of the uncertainty on
dual quantities (such as the accumulated plastic strain and the stress tensor),
generated by the uncertainty on the temperature loading field. The
dictionary-based ROM-net computes predictions of dual quantities of interest
for 1008 Monte Carlo draws of the temperature loading field in 2 hours and 48
minutes, which corresponds to a speedup greater than 600 with respect to a
reference parallel solver using domain decomposition, with a relative error in
the order of 2%. Another contribution of this work consists in the derivation
of a meta-model to reconstruct the dual quantities of interest over the
complete mesh from their values on the reduced integration points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Daniel_T/0/1/0/all/0/1"&gt;Thomas Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casenave_F/0/1/0/all/0/1"&gt;Fabien Casenave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akkari_N/0/1/0/all/0/1"&gt;Nissrine Akkari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ryckelynck_D/0/1/0/all/0/1"&gt;David Ryckelynck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rey_C/0/1/0/all/0/1"&gt;Christian Rey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving high-dimensional optimal stopping problems using deep learning. (arXiv:1908.01602v3 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.01602</id>
        <link href="http://arxiv.org/abs/1908.01602"/>
        <updated>2021-08-10T02:00:10.343Z</updated>
        <summary type="html"><![CDATA[Nowadays many financial derivatives, such as American or Bermudan options,
are of early exercise type. Often the pricing of early exercise options gives
rise to high-dimensional optimal stopping problems, since the dimension
corresponds to the number of underlying assets. High-dimensional optimal
stopping problems are, however, notoriously difficult to solve due to the
well-known curse of dimensionality. In this work, we propose an algorithm for
solving such problems, which is based on deep learning and computes, in the
context of early exercise option pricing, both approximations of an optimal
exercise strategy and the price of the considered option. The proposed
algorithm can also be applied to optimal stopping problems that arise in other
areas where the underlying stochastic process can be efficiently simulated. We
present numerical results for a large number of example problems, which include
the pricing of many high-dimensional American and Bermudan options, such as
Bermudan max-call options in up to 5000 dimensions. Most of the obtained
results are compared to reference values computed by exploiting the specific
problem design or, where available, to reference values from the literature.
These numerical results suggest that the proposed algorithm is highly effective
in the case of many underlyings, in terms of both accuracy and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1"&gt;Sebastian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheridito_P/0/1/0/all/0/1"&gt;Patrick Cheridito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welti_T/0/1/0/all/0/1"&gt;Timo Welti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Credibility-aware Swarm-Federated Deep Learning Framework in Internet of Vehicles. (arXiv:2108.03981v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.03981</id>
        <link href="http://arxiv.org/abs/2108.03981"/>
        <updated>2021-08-10T02:00:10.336Z</updated>
        <summary type="html"><![CDATA[Federated Deep Learning (FDL) is helping to realize distributed machine
learning in the Internet of Vehicles (IoV). However, FDL's global model needs
multiple clients to upload learning model parameters, thus still existing
unavoidable communication overhead and data privacy risks. The recently
proposed Swarm Learning (SL) provides a decentralized machine-learning approach
uniting edge computing and blockchain-based coordination without the need for a
central coordinator. This paper proposes a Swarm-Federated Deep Learning
framework in the IoV system (IoV-SFDL) that integrates SL into the FDL
framework. The IoV-SFDL organizes vehicles to generate local SL models with
adjacent vehicles based on the blockchain empowered SL, then aggregates the
global FDL model among different SL groups with a proposed credibility weights
prediction algorithm. Extensive experimental results demonstrate that compared
with the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%
reduction in edge-to-global communication overhead while improving about 5.02%
in model performance with the same training iterations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Certification under Instance-targeted Poisoning. (arXiv:2105.08709v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08709</id>
        <link href="http://arxiv.org/abs/2105.08709"/>
        <updated>2021-08-10T02:00:10.328Z</updated>
        <summary type="html"><![CDATA[In this paper, we study PAC learnability and certification of predictions
under instance-targeted poisoning attacks, where the adversary who knows the
test instance may change a fraction of the training set with the goal of
fooling the learner at the test instance. Our first contribution is to
formalize the problem in various settings and to explicitly model subtle
aspects such as the proper or improper nature of the learning, learner's
randomness, and whether (or not) adversary's attack can depend on it. Our main
result shows that when the budget of the adversary scales sublinearly with the
sample complexity, (improper) PAC learnability and certification are
achievable; in contrast, when the adversary's budget grows linearly with the
sample complexity, the adversary can potentially drive up the expected 0-1 loss
to one. We also study distribution-specific PAC learning in the same attack
model and show that proper learning with certification is possible for learning
half spaces under natural distributions. Finally, we empirically study the
robustness of K nearest neighbour, logistic regression, multi-layer perceptron,
and convolutional neural network on real data sets against targeted-poisoning
attacks. Our experimental results show that many models, especially
state-of-the-art neural networks, are indeed vulnerable to these strong
attacks. Interestingly, we observe that methods with high standard accuracy
might be more vulnerable to instance-targeted poisoning attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Ji Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmoody_M/0/1/0/all/0/1"&gt;Mohammad Mahmoody&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Mechanically Driven Full-Field Quantities of Interest with Deep Learning-Based Metamodels. (arXiv:2108.03995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03995</id>
        <link href="http://arxiv.org/abs/2108.03995"/>
        <updated>2021-08-10T02:00:10.319Z</updated>
        <summary type="html"><![CDATA[Using simulation to predict the mechanical behavior of heterogeneous
materials has applications ranging from topology optimization to multi-scale
structural analysis. However, full-fidelity simulation techniques such as
Finite Element Analysis can be prohibitively computationally expensive when
they are used to explore the massive input parameter space of heterogeneous
materials. Therefore, there has been significant recent interest in machine
learning-based models that, once trained, can predict mechanical behavior at a
fraction of the computational cost. Over the past several years, research in
this area has been focused mainly on predicting single Quantities of Interest
(QoIs). However, there has recently been an increased interest in a more
challenging problem: predicting full-field QoI (e.g., displacement/strain
fields, damage fields) for mechanical problems. Due to the added complexity of
full-field information, network architectures that perform well on single QoI
problems may perform poorly in the full-field QoI problem setting. The work
presented in this paper is twofold. First, we made a significant extension to
the Mechanical MNIST dataset designed to enable the investigation of full field
QoI prediction. Specifically, we added Finite Element simulation results of
quasi-static brittle fracture in a heterogeneous material captured with the
phase-field method. Second, we established strong baseline performance for
predicting full-field QoI with MultiRes-WNet architecture. In addition to
presenting the results in this paper, we have released our model implementation
and the Mechanical MNIST Crack Path dataset under open-source licenses. We
anticipate that future researchers will directly use our model architecture on
related datasets and potentially design models that exceed the baseline
performance for predicting full-field QoI established in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1"&gt;S. Mohammadzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lejeune_E/0/1/0/all/0/1"&gt;E. Lejeune&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data. (arXiv:2108.04085v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.04085</id>
        <link href="http://arxiv.org/abs/2108.04085"/>
        <updated>2021-08-10T02:00:10.301Z</updated>
        <summary type="html"><![CDATA[Scientific machine learning has been successfully applied to inverse problems
and PDE discoveries in computational physics. One caveat of current methods
however is the need for large amounts of (clean) data in order to recover full
system responses or underlying physical models. Bayesian methods may be
particularly promising to overcome these challenges as they are naturally less
sensitive to sparse and noisy data. In this paper, we propose to use Bayesian
neural networks (BNN) in order to: 1) Recover the full system states from
measurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian
Monte-Carlo to sample the posterior distribution of a deep and dense BNN, and
show that it is possible to accurately capture physics of varying complexity
without overfitting. 2) Recover the parameters in the underlying partial
differential equation (PDE) governing the physical system. Using the trained
BNN as a surrogate of the system response, we generate datasets of derivatives
potentially comprising the latent PDE of the observed system and perform a
Bayesian linear regression (BLR) between the successive derivatives in space
and time to recover the original PDE parameters. We take advantage of the
confidence intervals on the BNN outputs and introduce the spatial derivative
variance into the BLR likelihood to discard the influence of highly uncertain
surrogate data points, which allows for more accurate parameter discovery. We
demonstrate our approach on a handful of example applied to physics and
non-linear dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bonneville_C/0/1/0/all/0/1"&gt;Christophe Bonneville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Earls_C/0/1/0/all/0/1"&gt;Christopher J. Earls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09903</id>
        <link href="http://arxiv.org/abs/2105.09903"/>
        <updated>2021-08-10T02:00:10.295Z</updated>
        <summary type="html"><![CDATA[Anomaly detection is a critical problem in the manufacturing industry. In
many applications, images of objects to be analyzed are captured from multiple
perspectives which can be exploited to improve the robustness of anomaly
detection. In this work, we build upon the deep support vector data description
algorithm and address multi-perspective anomaly detection using three different
fusion techniques, i.e., early fusion, late fusion, and late fusion with
multiple decoders. We employ different augmentation techniques with a denoising
process to deal with scarce one-class data, which further improves the
performance (ROC AUC $= 80\%$). Furthermore, we introduce the dices dataset,
which consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g., drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed
{multi-perspective} approach exceeds the state-of-the-art {single-perspective
anomaly detection on both the MNIST and dices datasets}. To the best of our
knowledge, this is the first work that focuses on addressing multi-perspective
anomaly detection in images by jointly using different perspectives together
with one single objective function for anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1"&gt;Peter Jakob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1"&gt;Manav Madan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1"&gt;Tobias Schmid-Schirling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing. (arXiv:2108.04074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04074</id>
        <link href="http://arxiv.org/abs/2108.04074"/>
        <updated>2021-08-10T02:00:10.287Z</updated>
        <summary type="html"><![CDATA[Reservoir computers are powerful tools for chaotic time series prediction.
They can be trained to approximate phase space flows and can thus both predict
future values to a high accuracy, as well as reconstruct the general properties
of a chaotic attractor without requiring a model. In this work, we show that
the ability to learn the dynamics of a complex system can be extended to
systems with co-existing attractors, here a 4-dimensional extension of the
well-known Lorenz chaotic system. We demonstrate that a reservoir computer can
infer entirely unexplored parts of the phase space: a properly trained
reservoir computer can predict the existence of attractors that were never
approached during training and therefore are labelled as unseen. We provide
examples where attractor inference is achieved after training solely on a
single noisy trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1"&gt;Daniel J. Gauthier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1"&gt;Ingo Fischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Heterogenous Electronic Health Records Systems via Text-Based Code Embedding. (arXiv:2108.03625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03625</id>
        <link href="http://arxiv.org/abs/2108.03625"/>
        <updated>2021-08-10T02:00:10.279Z</updated>
        <summary type="html"><![CDATA[Substantial increase in the use of Electronic Health Records (EHRs) has
opened new frontiers for predictive healthcare. However, while EHR systems are
nearly ubiquitous, they lack a unified code system for representing medical
concepts. Heterogeneous formats of EHR present a substantial barrier for the
training and deployment of state-of-the-art deep learning models at scale. To
overcome this problem, we introduce Description-based Embedding, DescEmb, a
code-agnostic description-based representation learning framework for
predictive modeling on EHR. DescEmb takes advantage of the flexibility of
neural language understanding models while maintaining a neutral approach that
can be combined with prior frameworks for task-specific representation learning
or predictive modeling. We tested our model's capacity on various experiments
including prediction tasks, transfer learning and pooled learning. DescEmb
shows higher performance in overall experiments compared to code-based
approach, opening the door to a text-based approach in predictive healthcare
research that is not constrained by EHR structure nor special domain knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1"&gt;Kyunghoon Hur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jungwoo Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1"&gt;Wesley Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Hak Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Edward Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00331</id>
        <link href="http://arxiv.org/abs/2108.00331"/>
        <updated>2021-08-10T02:00:10.273Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the problem of Differentially Private Stochastic
Convex Optimization (DP-SCO) and provide excess population risks for some
special classes of functions that are faster than the previous results of
general convex and strongly convex functions. In the first part of the paper,
we study the case where the population risk function satisfies the Tysbakov
Noise Condition (TNC) with some parameter $\theta>1$. Specifically, we first
show that under some mild assumptions on the loss functions, there is an
algorithm whose output could achieve an upper bound of
$\tilde{O}((\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $(\epsilon,
\delta)$-DP when $\theta\geq 2$, here $n$ is the sample size and $d$ is the
dimension of the space. Then we address the inefficiency issue, improve the
upper bounds by $\text{Poly}(\log n)$ factors and extend to the case where
$\theta\geq \bar{\theta}>1$ for some known $\bar{\theta}$. Next we show that
the excess population risk of population functions satisfying TNC with
parameter $\theta>1$ is always lower bounded by
$\Omega((\frac{d}{n\epsilon})^\frac{\theta}{\theta-1}) $ and
$\Omega((\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $\epsilon$-DP and
$(\epsilon, \delta)$-DP, respectively. In the second part, we focus on a
special case where the population risk function is strongly convex. Unlike the
previous studies, here we assume the loss function is {\em non-negative} and
{\em the optimal value of population risk is sufficiently small}. With these
additional assumptions, we propose a new method whose output could achieve an
upper bound of
$O(\frac{d\log\frac{1}{\delta}}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ for any
$\tau\geq 1$ in $(\epsilon,\delta)$-DP model if the sample size $n$ is
sufficiently large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinyan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Deep Learning for Mobile Edge Computing: A Survey on Communication Efficiency and Trustworthiness. (arXiv:2108.03980v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.03980</id>
        <link href="http://arxiv.org/abs/2108.03980"/>
        <updated>2021-08-10T02:00:10.265Z</updated>
        <summary type="html"><![CDATA[A wider coverage and a better solution to latency reduction in 5G
necessitates its combination with mobile edge computing (MEC) technology.
Decentralized deep learning (DDL) as a promising solution to privacy-preserving
data processing for millions of edge smart devices, it leverages federated
learning within the networking of local models, without disclosing a client's
raw data. Especially, in industries such as finance and healthcare where
sensitive data of transactions and personal medical records is cautiously
maintained, DDL facilitates the collaboration among these institutes to improve
the performance of local models, while protecting data privacy of participating
clients. In this survey paper, we demonstrate technical fundamentals of DDL for
benefiting many walks of society through decentralized learning. Furthermore,
we offer a comprehensive overview of recent challenges of DDL and the most
relevant solutions from novel perspectives of communication efficiency and
trustworthiness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1"&gt;Hideya Ochiai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esaki_H/0/1/0/all/0/1"&gt;Hiroshi Esaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03613</id>
        <link href="http://arxiv.org/abs/2108.03613"/>
        <updated>2021-08-10T02:00:10.246Z</updated>
        <summary type="html"><![CDATA[Incremental learning of semantic segmentation has emerged as a promising
strategy for visual scene interpretation in the open- world setting. However,
it remains challenging to acquire novel classes in an online fashion for the
segmentation task, mainly due to its continuously-evolving semantic label
space, partial pixelwise ground-truth annotations, and constrained data
availability. To ad- dress this, we propose an incremental learning strategy
that can fast adapt deep segmentation models without catastrophic forgetting,
using a streaming input data with pixel annotations on the novel classes only.
To this end, we develop a uni ed learning strategy based on the
Expectation-Maximization (EM) framework, which integrates an iterative
relabeling strategy that lls in the missing labels and a rehearsal-based
incremental learning step that balances the stability-plasticity of the model.
Moreover, our EM algorithm adopts an adaptive sampling method to select
informative train- ing data and a class-balancing training strategy in the
incremental model updates, both improving the e cacy of model learning. We
validate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the
results demonstrate its superior performance over the existing incremental
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shipeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiale Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiangwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Songyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xuming He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate. (arXiv:2108.04051v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.04051</id>
        <link href="http://arxiv.org/abs/2108.04051"/>
        <updated>2021-08-10T02:00:10.240Z</updated>
        <summary type="html"><![CDATA[Recently, GAN vocoders have seen rapid progress in speech synthesis, starting
to outperform autoregressive models in perceptual quality with much higher
generation speed. However, autoregressive vocoders are still the common choice
for neural generation of speech signals coded at very low bit rates. In this
paper, we present a GAN vocoder which is able to generate wideband speech
waveforms from parameters coded at 1.6 kbit/s. The proposed model is a modified
version of the StyleMelGAN vocoder that can run in frame-by-frame manner,
making it suitable for streaming applications. The experimental results show
that the proposed model significantly outperforms prior autoregressive vocoders
like LPCNet for very low bit rate speech coding, with computational complexity
of about 5 GMACs, providing a new state of the art in this domain. Moreover,
this streamwise adversarial vocoder delivers quality competitive to advanced
speech codecs such as EVS at 5.9 kbit/s on clean speech, which motivates
further usage of feed-forward fully-convolutional models for low bit rate
speech coding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mustafa_A/0/1/0/all/0/1"&gt;Ahmed Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buthe_J/0/1/0/all/0/1"&gt;Jan B&amp;#xfc;the&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Korse_S/0/1/0/all/0/1"&gt;Srikanth Korse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kishan Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuchs_G/0/1/0/all/0/1"&gt;Guillaume Fuchs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pia_N/0/1/0/all/0/1"&gt;Nicola Pia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Majority Voting in Digital Hardware. (arXiv:2108.03979v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03979</id>
        <link href="http://arxiv.org/abs/2108.03979"/>
        <updated>2021-08-10T02:00:10.233Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning methods became increasingly important for a
manifold number of applications. However, they often suffer from high
computational requirements impairing their efficient use in real-time systems,
even when employing dedicated hardware accelerators. Ensemble learning methods
are especially suitable for hardware acceleration since they can be constructed
from individual learners of low complexity and thus offer large parallelization
potential. For classification, the outputs of these learners are typically
combined by majority voting, which often represents the bottleneck of a
hardware accelerator for ensemble inference. In this work, we present a novel
architecture that allows obtaining a majority decision in a number of clock
cycles that is logarithmic in the number of inputs. We show, that for the
example application of handwritten digit recognition a random forest processing
engine employing this majority decision architecture implemented on an FPGA
allows the classification of more than seven million images per second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Baumgartner_S/0/1/0/all/0/1"&gt;Stefan Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huemer_M/0/1/0/all/0/1"&gt;Mario Huemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lunglmayr_M/0/1/0/all/0/1"&gt;Michael Lunglmayr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04031</id>
        <link href="http://arxiv.org/abs/2108.04031"/>
        <updated>2021-08-10T02:00:10.227Z</updated>
        <summary type="html"><![CDATA[In the current deep learning based recommendation system, the embedding
method is generally employed to complete the conversion from the
high-dimensional sparse feature vector to the low-dimensional dense feature
vector. However, as the dimension of the input vector of the embedding layer is
too large, the addition of the embedding layer significantly slows down the
convergence speed of the entire neural network, which is not acceptable in
real-world scenarios. In addition, as the interaction between users and items
increases and the relationship between items becomes more complicated, the
embedding method proposed for sequence data is no longer suitable for graphic
data in the current real environment. Therefore, in this paper, we propose the
Dual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes
two modes, static and dynamic. We first construct the item graph to extract the
graph structure and use random walk of unequal probability to capture the
high-order proximity between the items. Then we generate the graph embedding
vector through the Skip-Gram model, and finally feed the downstream deep neural
network for the recommendation task. The experimental results show that DGEM
can mine the high-order proximity between items and enhance the expression
ability of the recommendation model. Meanwhile it also improves the
recommendation performance by utilizing the time dependent relationship between
items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Rongwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhuyun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Joint Unsupervised Learning of Cluster-Aware Embedding for Heterogeneous Networks. (arXiv:2108.03953v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03953</id>
        <link href="http://arxiv.org/abs/2108.03953"/>
        <updated>2021-08-10T02:00:10.210Z</updated>
        <summary type="html"><![CDATA[Heterogeneous Information Network (HIN) embedding refers to the
low-dimensional projections of the HIN nodes that preserve the HIN structure
and semantics. HIN embedding has emerged as a promising research field for
network analysis as it enables downstream tasks such as clustering and node
classification. In this work, we propose \ours for joint learning of cluster
embeddings as well as cluster-aware HIN embedding. We assume that the connected
nodes are highly likely to fall in the same cluster, and adopt a variational
approach to preserve the information in the pairwise relations in a
cluster-aware manner. In addition, we deploy contrastive modules to
simultaneously utilize the information in multiple meta-paths, thereby
alleviating the meta-path selection problem - a challenge faced by many of the
famous HIN embedding approaches. The HIN embedding, thus learned, not only
improves the clustering performance but also preserves pairwise proximity as
well as the high-order HIN structure. We show the effectiveness of our approach
by comparing it with many competitive baselines on three real-world datasets on
clustering and downstream node classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1"&gt;Rayyan Ahmad Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1"&gt;Martin Kleinsteuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Active Learning for Active Class Selection. (arXiv:2108.03891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03891</id>
        <link href="http://arxiv.org/abs/2108.03891"/>
        <updated>2021-08-10T02:00:10.204Z</updated>
        <summary type="html"><![CDATA[In machine learning, active class selection (ACS) algorithms aim to actively
select a class and ask the oracle to provide an instance for that class to
optimize a classifier's performance while minimizing the number of requests. In
this paper, we propose a new algorithm (PAL-ACS) that transforms the ACS
problem into an active learning task by introducing pseudo instances. These are
used to estimate the usefulness of an upcoming instance for each class using
the performance gain model from probabilistic active learning. Our experimental
evaluation (on synthetic and real data) shows the advantages of our algorithm
compared to state-of-the-art algorithms. It effectively prefers the sampling of
difficult classes and thereby improves the classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kottke_D/0/1/0/all/0/1"&gt;Daniel Kottke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krempl_G/0/1/0/all/0/1"&gt;Georg Krempl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stecklina_M/0/1/0/all/0/1"&gt;Marianne Stecklina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekowski_C/0/1/0/all/0/1"&gt;Cornelius Styp von Rekowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabsch_T/0/1/0/all/0/1"&gt;Tim Sabsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1"&gt;Tuan Pham Minh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deliano_M/0/1/0/all/0/1"&gt;Matthias Deliano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spiliopoulou_M/0/1/0/all/0/1"&gt;Myra Spiliopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03917</id>
        <link href="http://arxiv.org/abs/2108.03917"/>
        <updated>2021-08-10T02:00:10.197Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1"&gt;Radu Alexandru Rosu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1"&gt;Peer Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1"&gt;Jan Quenzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum. (arXiv:2108.04033v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.04033</id>
        <link href="http://arxiv.org/abs/2108.04033"/>
        <updated>2021-08-10T02:00:10.190Z</updated>
        <summary type="html"><![CDATA[In more and more application areas, we are witnessing the emergence of
complex workflows that combine computing, analytics and learning. They often
require a hybrid execution infrastructure with IoT devices interconnected to
cloud/HPC systems (aka Computing Continuum). Such workflows are subject to
complex constraints and requirements in terms of performance, resource usage,
energy consumption and financial costs. This makes it challenging to optimize
their configuration and deployment. We propose a methodology to support the
optimization of real-life applications on the Edge-to-Cloud Continuum. We
implement it as an extension of E2Clab, a previously proposed framework
supporting the complete experimental cycle across the Edge-to-Cloud Continuum.
Our approach relies on a rigorous analysis of possible configurations in a
controlled testbed environment to understand their behaviour and related
performance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a
world-wide plant identification application. Our methodology can be generalized
to other applications in the Edge-to-Cloud Continuum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosendo_D/0/1/0/all/0/1"&gt;Daniel Rosendo&lt;/a&gt; (KerData), &lt;a href="http://arxiv.org/find/cs/1/au:+Costan_A/0/1/0/all/0/1"&gt;Alexandru Costan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniu_G/0/1/0/all/0/1"&gt;Gabriel Antoniu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonin_M/0/1/0/all/0/1"&gt;Matthieu Simonin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lombardo_J/0/1/0/all/0/1"&gt;Jean-Christophe Lombardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joly_A/0/1/0/all/0/1"&gt;Alexis Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valduriez_P/0/1/0/all/0/1"&gt;Patrick Valduriez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03913</id>
        <link href="http://arxiv.org/abs/2108.03913"/>
        <updated>2021-08-10T02:00:10.183Z</updated>
        <summary type="html"><![CDATA[Fundamental machine learning theory shows that different samples contribute
unequally both in learning and testing processes. Contemporary studies on DNN
imply that such sample di?erence is rooted on the distribution of intrinsic
pattern information, namely sample regularity. Motivated by the recent
discovery on network memorization and generalization, we proposed a pair of
sample regularity measures for both processes with a formulation-consistent
representation. Specifically, cumulative binary training/generalizing loss
(CBTL/CBGL), the cumulative number of correct classi?cations of the
training/testing sample within training stage, is proposed to quantize the
stability in memorization-generalization process; while
forgetting/mal-generalizing events, i.e., the mis-classification of previously
learned or generalized sample, are utilized to represent the uncertainty of
sample regularity with respect to optimization dynamics. Experiments validated
the effectiveness and robustness of the proposed approaches for mini-batch SGD
optimization. Further applications on training/testing sample selection show
the proposed measures sharing the uni?ed computing procedure could benefit for
both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaoning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yuanqi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuehu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FederatedNILM: A Distributed and Privacy-preserving Framework for Non-intrusive Load Monitoring based on Federated Deep Learning. (arXiv:2108.03591v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03591</id>
        <link href="http://arxiv.org/abs/2108.03591"/>
        <updated>2021-08-10T02:00:10.177Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM), which usually utilizes machine learning
methods and is effective in disaggregating smart meter readings from the
household-level into appliance-level consumptions, can help to analyze
electricity consumption behaviours of users and enable practical smart energy
and smart grid applications. However, smart meters are privately owned and
distributed, which make real-world applications of NILM challenging. To this
end, this paper develops a distributed and privacy-preserving federated deep
learning framework for NILM (FederatedNILM), which combines federated learning
with a state-of-the-art deep learning architecture to conduct NILM for the
classification of typical states of household appliances. Through extensive
comparative experiments, the effectiveness of the proposed FederatedNILM
framework is demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shuang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fanlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xizhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization. (arXiv:2108.03713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03713</id>
        <link href="http://arxiv.org/abs/2108.03713"/>
        <updated>2021-08-10T02:00:10.170Z</updated>
        <summary type="html"><![CDATA[Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pashazadeh_M/0/1/0/all/0/1"&gt;Mostafa Pashazadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03857</id>
        <link href="http://arxiv.org/abs/2108.03857"/>
        <updated>2021-08-10T02:00:10.162Z</updated>
        <summary type="html"><![CDATA["Art is the lie that enables us to realize the truth." - Pablo Picasso. For
centuries, humans have dedicated themselves to producing arts to convey their
imagination. The advancement in technology and deep learning in particular, has
caught the attention of many researchers trying to investigate whether art
generation is possible by computers and algorithms. Using generative
adversarial networks (GANs), applications such as synthesizing photorealistic
human faces and creating captions automatically from images were realized. This
survey takes a comprehensive look at the recent works using GANs for generating
visual arts, music, and literary text. A performance comparison and description
of the various GAN architecture are also presented. Finally, some of the key
challenges in art generation using GANs are highlighted along with
recommendations for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1"&gt;Sakib Shahriar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge Computing: A Contextual-Bandit Approach. (arXiv:2108.03872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03872</id>
        <link href="http://arxiv.org/abs/2108.03872"/>
        <updated>2021-08-10T02:00:10.155Z</updated>
        <summary type="html"><![CDATA[The advances in deep neural networks (DNN) have significantly enhanced
real-time detection of anomalous data in IoT applications. However, the
complexity-accuracy-delay dilemma persists: complex DNN models offer higher
accuracy, but typical IoT devices can barely afford the computation load, and
the remedy of offloading the load to the cloud incurs long delay. In this
paper, we address this challenge by proposing an adaptive anomaly detection
scheme with hierarchical edge computing (HEC). Specifically, we first construct
multiple anomaly detection DNN models with increasing complexity, and associate
each of them to a corresponding HEC layer. Then, we design an adaptive model
selection scheme that is formulated as a contextual-bandit problem and solved
by using a reinforcement learning policy network. We also incorporate a
parallelism policy training method to accelerate the training process by taking
advantage of distributed models. We build an HEC testbed using real IoT
devices, implement and evaluate our contextual-bandit approach with both
univariate and multivariate IoT datasets. In comparison with both baseline and
state-of-the-art schemes, our adaptive approach strikes the best accuracy-delay
tradeoff on the univariate dataset, and achieves the best accuracy and F1-score
on the multivariate dataset with only negligibly longer delay than the best
(but inflexible) scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_M/0/1/0/all/0/1"&gt;Mao V. Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1"&gt;Tie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1"&gt;Tony Q.S. Quek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio Spectral Enhancement: Leveraging Autoencoders for Low Latency Reconstruction of Long, Lossy Audio Sequences. (arXiv:2108.03703v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.03703</id>
        <link href="http://arxiv.org/abs/2108.03703"/>
        <updated>2021-08-10T02:00:10.107Z</updated>
        <summary type="html"><![CDATA[With active research in audio compression techniques yielding substantial
breakthroughs, spectral reconstruction of low-quality audio waves remains a
less indulged topic. In this paper, we propose a novel approach for
reconstructing higher frequencies from considerably longer sequences of
low-quality MP3 audio waves. Our technique involves inpainting audio
spectrograms with residually stacked autoencoder blocks by manipulating
individual amplitude and phase values in relation to perceptual differences.
Our architecture presents several bottlenecks while preserving the spectral
structure of the audio wave via skip-connections. We also compare several task
metrics and demonstrate our visual guide to loss selection. Moreover, we show
how to leverage differential quantization techniques to reduce the initial
model size by more than half while simultaneously reducing inference time,
which is crucial in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1"&gt;Darshan Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abichandani_H/0/1/0/all/0/1"&gt;Harshavardhan Abichandani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Hyperparameter Optimization for Differentially Private Deep Learning. (arXiv:2108.03888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03888</id>
        <link href="http://arxiv.org/abs/2108.03888"/>
        <updated>2021-08-10T02:00:10.089Z</updated>
        <summary type="html"><![CDATA[Tuning the hyperparameters in the differentially private stochastic gradient
descent (DPSGD) is a fundamental challenge. Unlike the typical SGD, private
datasets cannot be used many times for hyperparameter search in DPSGD; e.g.,
via a grid search. Therefore, there is an essential need for algorithms that,
within a given search space, can find near-optimal hyperparameters for the best
achievable privacy-utility tradeoffs efficiently. We formulate this problem
into a general optimization framework for establishing a desirable
privacy-utility tradeoff, and systematically study three cost-effective
algorithms for being used in the proposed framework: evolutionary, Bayesian,
and reinforcement learning. Our experiments, for hyperparameter tuning in DPSGD
conducted on MNIST and CIFAR-10 datasets, show that these three algorithms
significantly outperform the widely used grid search baseline. As this paper
offers a first-of-a-kind framework for hyperparameter tuning in DPSGD, we
discuss existing challenges and open directions for future studies. As we
believe our work has implications to be utilized in the pipeline of private
deep learning, we open-source our code at
https://github.com/AmanPriyanshu/DP-HyperparamTuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1"&gt;Aman Priyanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1"&gt;Fatemehsadat Mireshghallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1"&gt;Mohammad Malekzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03945</id>
        <link href="http://arxiv.org/abs/2108.03945"/>
        <updated>2021-08-10T02:00:10.082Z</updated>
        <summary type="html"><![CDATA[Analogical proportions are statements of the form "A is to B as C is to D"
that are used for several reasoning and classification tasks in artificial
intelligence and natural language processing (NLP). For instance, there are
analogy based approaches to semantics as well as to morphology. In fact,
symbolic approaches were developed to solve or to detect analogies between
character strings, e.g., the axiomatic approach as well as that based on
Kolmogorov complexity. In this paper, we propose a deep learning approach to
detect morphological analogies, for instance, with reinflexion or conjugation.
We present empirical results that show that our framework is competitive with
the above-mentioned state of the art symbolic approaches. We also explore
empirically its transferability capacity across languages, which highlights
interesting similarities between them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1"&gt;Safa Alsaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1"&gt;Amandine Decker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1"&gt;Puthineath Lay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1"&gt;Esteban Marquer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1"&gt;Pierre-Alexandre Murena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1"&gt;Miguel Couceiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MAF-GNN: Multi-adaptive Spatiotemporal-flow Graph Neural Network for Traffic Speed Forecasting. (arXiv:2108.03594v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03594</id>
        <link href="http://arxiv.org/abs/2108.03594"/>
        <updated>2021-08-10T02:00:10.074Z</updated>
        <summary type="html"><![CDATA[Traffic forecasting is a core element of intelligent traffic monitoring
system. Approaches based on graph neural networks have been widely used in this
task to effectively capture spatial and temporal dependencies of road networks.
However, these approaches can not effectively define the complicated network
topology. Besides, their cascade network structures have limitations in
transmitting distinct features in the time and space dimensions. In this paper,
we propose a Multi-adaptive Spatiotemporal-flow Graph Neural Network (MAF-GNN)
for traffic speed forecasting. MAF-GNN introduces an effective Multi-adaptive
Adjacency Matrices Mechanism to capture multiple latent spatial dependencies
between traffic nodes. Additionally, we propose Spatiotemporal-flow Modules
aiming to further enhance feature propagation in both time and space
dimensions. MAF-GNN achieves better performance than other models on two
real-world datasets of public traffic network, METR-LA and PeMS-Bay,
demonstrating the effectiveness of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yaobin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weitang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhongyi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zixuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1"&gt;Tingyun Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lili Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingwei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust 1-bit Compressive Sensing with Partial Gaussian Circulant Matrices and Generative Priors. (arXiv:2108.03570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03570</id>
        <link href="http://arxiv.org/abs/2108.03570"/>
        <updated>2021-08-10T02:00:10.067Z</updated>
        <summary type="html"><![CDATA[In 1-bit compressive sensing, each measurement is quantized to a single bit,
namely the sign of a linear function of an unknown vector, and the goal is to
accurately recover the vector. While it is most popular to assume a standard
Gaussian sensing matrix for 1-bit compressive sensing, using structured sensing
matrices such as partial Gaussian circulant matrices is of significant
practical importance due to their faster matrix operations. In this paper, we
provide recovery guarantees for a correlation-based optimization algorithm for
robust 1-bit compressive sensing with randomly signed partial Gaussian
circulant matrices and generative models. Under suitable assumptions, we match
guarantees that were previously only known to hold for i.i.d.~Gaussian matrices
that require significantly more computation. We make use of a practical
iterative algorithm, and perform numerical experiments on image datasets to
corroborate our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhaoqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Subhroshekhar Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1"&gt;Jonathan Scarlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07331</id>
        <link href="http://arxiv.org/abs/2107.07331"/>
        <updated>2021-08-10T02:00:10.045Z</updated>
        <summary type="html"><![CDATA[Human Activity Recognition (HAR) based on IMU sensors is a crucial area in
ubiquitous computing. Because of the trend of deploying AI on IoT devices or
smartphones, more researchers are designing different HAR models for embedded
devices. Deployment of models in embedded devices can help enhance the
efficiency of HAR. We propose a multi-level HAR modeling pipeline called
Stage-Logits-Memory Distillation (SMLDist) for constructing deep convolutional
HAR models with embedded hardware support. SMLDist includes stage distillation,
memory distillation, and logits distillation. Stage distillation constrains the
learning direction of the intermediate features. The teacher model teaches the
student models how to explain and store the inner relationship among
high-dimensional features based on Hopfield networks in memory distillation.
Logits distillation builds logits distilled by a smoothed conditional rule to
preserve the probability distribution and enhance the softer target accuracy.
We compare the accuracy, F1 macro score, and energy cost on embedded platforms
of a MobileNet V3 model built by our SMLDist with those of various
state-of-the-art HAR frameworks. The product model has a good balance with
robustness and efficiency. SMLDist can also compress models with a minor
performance loss at an equal compression ratio to other advanced knowledge
distillation methods on seven public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runze Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haiyong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xuechun Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yida Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. (arXiv:2105.10554v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10554</id>
        <link href="http://arxiv.org/abs/2105.10554"/>
        <updated>2021-08-10T02:00:10.037Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNN) analysis engines are vital for real-world
problems that use large graph models. Challenges for a GNN hardware platform
include the ability to (a) host a variety of GNNs, (b) handle high sparsity in
input vertex feature vectors and the graph adjacency matrix and the
accompanying random memory access patterns, and (c) maintain load-balanced
computation in the face of uneven workloads, induced by high sparsity and
power-law vertex degree distributions. This paper proposes GNNIE, an
accelerator designed to run a broad range of GNNs. It tackles workload
imbalance by (i)~splitting vertex feature operands into blocks, (ii)~reordering
and redistributing computations, (iii)~using a novel flexible MAC architecture.
It adopts a graph-specific, degree-aware caching policy that is well suited to
real-world graph characteristics. The policy enhances on-chip data reuse and
avoids random memory access to DRAM.

GNNIE achieves average speedups of 21233x over a CPU and 699x over a GPU over
multiple datasets on graph attention networks (GATs), graph convolutional
networks (GCNs), GraphSAGE, GINConv, and DiffPool. Compared to prior
approaches, GNNIE achieves an average speedup of 35x over HyGCN (which cannot
implement GATs) for GCN, GraphSAGE, and GINConv, and, using 3.4x fewer
processing units, an average speedup of 2.1x over AWB-GCN (which runs only
GCNs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1"&gt;Sudipta Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manasi_S/0/1/0/all/0/1"&gt;Susmita Dey Manasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1"&gt;Kishor Kunal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramprasath_S/0/1/0/all/0/1"&gt;S. Ramprasath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapatnekar_S/0/1/0/all/0/1"&gt;Sachin S. Sapatnekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathfinder: Parallel quasi-Newton variational inference. (arXiv:2108.03782v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.03782</id>
        <link href="http://arxiv.org/abs/2108.03782"/>
        <updated>2021-08-10T02:00:10.028Z</updated>
        <summary type="html"><![CDATA[We introduce Pathfinder, a variational method for approximately sampling from
differentiable log densities. Starting from a random initialization, Pathfinder
locates normal approximations to the target density along a quasi-Newton
optimization path, with local covariance estimated using the inverse Hessian
estimates produced by the optimizer. Pathfinder returns draws from the
approximation with the lowest estimated Kullback-Leibler (KL) divergence to the
true posterior. We evaluate Pathfinder on a wide range of posterior
distributions, demonstrating that its approximate draws are better than those
from automatic differentiation variational inference (ADVI) and comparable to
those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as
measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC
runs, Pathfinder requires one to two orders of magnitude fewer log density and
gradient evaluations, with greater reductions for more challenging posteriors.
Importance resampling over multiple runs of Pathfinder improves the diversity
of approximate draws, reducing 1-Wasserstein distance further and providing a
measure of robustness to optimization failures on plateaus, saddle points, or
in minor modes. The Monte Carlo KL-divergence estimates are embarrassingly
parallelizable in the core Pathfinder algorithm, as are multiple runs in the
resampling version, further increasing Pathfinder's speed advantage with
multiple cores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carpenter_B/0/1/0/all/0/1"&gt;Bob Carpenter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1"&gt;Andrew Gelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1"&gt;Aki Vehtari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EVGen: Adversarial Networks for Learning Electric Vehicle Charging Loads and Hidden Representations. (arXiv:2108.03762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03762</id>
        <link href="http://arxiv.org/abs/2108.03762"/>
        <updated>2021-08-10T02:00:10.019Z</updated>
        <summary type="html"><![CDATA[The nexus between transportation, the power grid, and consumer behavior is
more pronounced than ever before as the race to decarbonize the transportation
sector intensifies. Electrification in the transportation sector has led to
technology shifts and rapid deployment of electric vehicles (EVs). The
potential increase in stochastic and spatially heterogeneous charging load
presents a unique challenge that is not well studied, and will have significant
impacts on grid operations, emissions, and system reliability if not managed
effectively. Realistic scenario generators can help operators prepare, and
machine learning can be leveraged to this end. In this work, we develop
generative adversarial networks (GANs) to learn distributions of electric
vehicle (EV) charging sessions and disentangled representations. We show that
this model structure successfully parameterizes unlabeled temporal and power
patterns without supervision and is able to generate synthetic data conditioned
on these parameters. We benchmark the generation capability of this model with
Gaussian Mixture Models (GMMs), and empirically show that our proposed model
framework is better at capturing charging distributions and temporal dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buechler_R/0/1/0/all/0/1"&gt;Robert Buechler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balogun_E/0/1/0/all/0/1"&gt;Emmanuel Balogun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Arun Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1"&gt;Ram Rajagopal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Future Wireless Networks: A Comprehensive Survey. (arXiv:2102.07572v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07572</id>
        <link href="http://arxiv.org/abs/2102.07572"/>
        <updated>2021-08-10T02:00:09.936Z</updated>
        <summary type="html"><![CDATA[With outstanding features, Machine Learning (ML) has been the backbone of
numerous applications in wireless networks. However, the conventional ML
approaches have been facing many challenges in practical implementation, such
as the lack of labeled data, the constantly changing wireless environments, the
long training process, and the limited capacity of wireless devices. These
challenges, if not addressed, will impede the effectiveness and applicability
of ML in future wireless networks. To address these problems, Transfer Learning
(TL) has recently emerged to be a very promising solution. The core idea of TL
is to leverage and synthesize distilled knowledge from similar tasks as well as
from valuable experiences accumulated from the past to facilitate the learning
of new problems. Doing so, TL techniques can reduce the dependence on labeled
data, improve the learning speed, and enhance the ML methods' robustness to
different wireless environments. This article aims to provide a comprehensive
survey on applications of TL in wireless networks. Particularly, we first
provide an overview of TL including formal definitions, classification, and
various types of TL techniques. We then discuss diverse TL approaches proposed
to address emerging issues in wireless networks. The issues include spectrum
management, localization, signal recognition, security, human activity
recognition and caching, which are all important to next-generation networks
such as 5G and beyond. Finally, we highlight important challenges, open issues,
and future research directions of TL in future wireless networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Cong T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_N/0/1/0/all/0/1"&gt;Nam H. Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saputra_Y/0/1/0/all/0/1"&gt;Yuris Mulya Saputra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1"&gt;Quoc-Viet Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1"&gt;Dusit Niyato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1"&gt;Eryk Dutkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Won-Joo Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training of deep residual networks with stochastic MG/OPT. (arXiv:2108.04052v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.04052</id>
        <link href="http://arxiv.org/abs/2108.04052"/>
        <updated>2021-08-10T02:00:09.906Z</updated>
        <summary type="html"><![CDATA[We train deep residual networks with a stochastic variant of the nonlinear
multigrid method MG/OPT. To build the multilevel hierarchy, we use the
dynamical systems viewpoint specific to residual networks. We report
significant speed-ups and additional robustness for training MNIST on deep
residual networks. Our numerical experiments also indicate that multilevel
training can be used as a pruning technique, as many of the auxiliary networks
have accuracies comparable to the original network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Planta_C/0/1/0/all/0/1"&gt;Cyrill von Planta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1"&gt;Alena Kopanicakova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1"&gt;Rolf Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11136</id>
        <link href="http://arxiv.org/abs/2107.11136"/>
        <updated>2021-08-10T02:00:09.838Z</updated>
        <summary type="html"><![CDATA[As one of the most fundamental problems in machine learning, statistics and
differential privacy, Differentially Private Stochastic Convex Optimization
(DP-SCO) has been extensively studied in recent years. However, most of the
previous work can only handle either regular data distribution or irregular
data in the low dimensional space case. To better understand the challenges
arising from irregular data distribution, in this paper we provide the first
study on the problem of DP-SCO with heavy-tailed data in the high dimensional
space. In the first part we focus on the problem over some polytope constraint
(such as the $\ell_1$-norm ball). We show that if the loss function is smooth
and its gradient has bounded second order moment, it is possible to get a (high
probability) error bound (excess population risk) of $\tilde{O}(\frac{\log
d}{(n\epsilon)^\frac{1}{3}})$ in the $\epsilon$-DP model, where $n$ is the
sample size and $d$ is the dimensionality of the underlying space. Next, for
LASSO, if the data distribution that has bounded fourth-order moments, we
improve the bound to $\tilde{O}(\frac{\log d}{(n\epsilon)^\frac{2}{5}})$ in the
$(\epsilon, \delta)$-DP model. In the second part of the paper, we study sparse
learning with heavy-tailed data. We first revisit the sparse linear model and
propose a truncated DP-IHT method whose output could achieve an error of
$\tilde{O}(\frac{s^{*2}\log d}{n\epsilon})$, where $s^*$ is the sparsity of the
underlying parameter. Then we study a more general problem over the sparsity
({\em i.e.,} $\ell_0$-norm) constraint, and show that it is possible to achieve
an error of $\tilde{O}(\frac{s^{*\frac{3}{2}}\log d}{n\epsilon})$, which is
also near optimal up to a factor of $\tilde{O}{(\sqrt{s^*})}$, if the loss
function is smooth and strongly convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lijie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1"&gt;Shuo Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hanshen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00700</id>
        <link href="http://arxiv.org/abs/2108.00700"/>
        <updated>2021-08-10T02:00:09.826Z</updated>
        <summary type="html"><![CDATA[The activation function is at the heart of a deep neural networks
nonlinearity; the choice of the function has great impact on the success of
training. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)
due to its simplicity and reliability, despite its few drawbacks. While most
previous functions proposed to supplant ReLU have been hand-designed, recent
work on learning the function during training has shown promising results. In
this paper we propose an adaptive piecewise linear activation function, the
Piecewise Linear Unit (PiLU), which can be learned independently for each
dimension of the neural network. We demonstrate how PiLU is a generalised
rectifier unit and note its similarities with the Adaptive Piecewise Linear
Units, namely adaptive and piecewise linear. Across a distribution of 30
experiments, we show that for the same model architecture, hyperparameters, and
pre-processing, PiLU significantly outperforms ReLU: reducing classification
error by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in
the number of neurons. Further work should be dedicated to exploring
generalised piecewise linear units, as well as verifying these results across
other challenging domains and larger problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1"&gt;Jordan Inturrisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1"&gt;Sui Yang Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1"&gt;Abbas Kouzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1"&gt;Riccardo Pagliarella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis. (arXiv:2108.03880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03880</id>
        <link href="http://arxiv.org/abs/2108.03880"/>
        <updated>2021-08-10T02:00:09.795Z</updated>
        <summary type="html"><![CDATA[Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge
of novel deep learning methods, learned MVS has surpassed the accuracy of
classical approaches, but still relies on building a memory intensive dense
cost volume. Novel View Synthesis (NVS) is a parallel line of research and has
recently seen an increase in popularity with Neural Radiance Field (NeRF)
models, which optimize a per scene radiance field. However, NeRF methods do not
generalize to novel scenes and are slow to train and test. We propose to bridge
the gap between these two methodologies with a novel network that can recover
3D scene geometry as a distance function, together with high-resolution color
images. Our method uses only a sparse set of images as input and can generalize
well to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing
approach in order to significantly increase speed. We show on various datasets
that our method reaches comparable accuracy to per-scene optimized methods
while being able to generalize and running significantly faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1"&gt;Radu Alexandru Rosu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video. (arXiv:2108.03825v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03825</id>
        <link href="http://arxiv.org/abs/2108.03825"/>
        <updated>2021-08-10T02:00:09.667Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel task, referred to as Weakly-Supervised
Spatio-Temporal Anomaly Detection (WSSTAD) in surveillance video. Specifically,
given an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e.,
a sequence of bounding boxes at consecutive times) that encloses the abnormal
event, with only coarse video-level annotations as supervision during training.
To address this challenging task, we propose a dual-branch network which takes
as input the proposals with multi-granularities in both spatial-temporal
domains. Each branch employs a relationship reasoning module to capture the
correlation between tubes/videolets, which can provide rich contextual
information and complex entity relationships for the concept learning of
abnormal behaviors. Mutually-guided Progressive Refinement framework is set up
to employ dual-path mutual guidance in a recurrent manner, iteratively sharing
auxiliary supervision information across branches. It impels the learned
concepts of each branch to serve as a guide for its counterpart, which
progressively refines the corresponding branch and the whole framework.
Furthermore, we contribute two datasets, i.e., ST-UCF-Crime and STRA,
consisting of videos containing spatio-temporal abnormal annotations to serve
as the benchmarks for WSSTAD. We conduct extensive qualitative and quantitative
evaluations to demonstrate the effectiveness of the proposed approach and
analyze the key factors that contribute more to handle this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMDet: A Tool for Mitotic Cell Detection in Histopathology Slides. (arXiv:2108.03676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03676</id>
        <link href="http://arxiv.org/abs/2108.03676"/>
        <updated>2021-08-10T02:00:09.481Z</updated>
        <summary type="html"><![CDATA[Breast Cancer is the most prevalent cancer in the world. The World Health
Organization reports that the disease still affects a significant portion of
the developing world citing increased mortality rates in the majority of low to
middle income countries. The most popular protocol pathologists use for
diagnosing breast cancer is the Nottingham grading system which grades the
proliferation of tumors based on 3 major criteria, the most important of them
being mitotic cell count. The way in which pathologists evaluate mitotic cell
count is to subjectively and qualitatively analyze cells present in stained
slides of tissue and make a decision on its mitotic state i.e. is it mitotic or
not?This process is extremely inefficient and tiring for pathologists and so an
efficient, accurate, and fully automated tool to aid with the diagnosis is
extremely desirable. Fortunately, creating such a tool is made significantly
easier with the AutoML tool available from Microsoft Azure, however to the best
of our knowledge the AutoML tool has never been formally evaluated for use in
mitotic cell detection in histopathology images. This paper serves as an
evaluation of the AutoML tool for this purpose and will provide a first look on
how the tool handles this challenging problem. All code is available
athttps://github.com/WaltAFWilliams/AMDet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_W/0/1/0/all/0/1"&gt;Walt Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1"&gt;Jimmy Hall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Nonnegative Matrix Factorization for Blind Hyperspectral Unmixing incorporating Endmember Independence. (arXiv:2003.01041v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01041</id>
        <link href="http://arxiv.org/abs/2003.01041"/>
        <updated>2021-08-10T02:00:09.276Z</updated>
        <summary type="html"><![CDATA[Hyperspectral unmixing (HU) has become an important technique in exploiting
hyperspectral data since it decomposes a mixed pixel into a collection of
endmembers weighted by fractional abundances. The endmembers of a hyperspectral
image (HSI) are more likely to be generated by independent sources and be mixed
in a macroscopic degree before arriving at the sensor element of the imaging
spectrometer as mixed spectra. Over the past few decades, many attempts have
focused on imposing auxiliary constraints on the conventional nonnegative
matrix factorization (NMF) framework in order to effectively unmix these mixed
spectra. As a promising step toward finding an optimum constraint to extract
endmembers, this paper presents a novel blind HU algorithm, referred to as
Kurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which
incorporates a novel constraint based on the statistical independence of the
probability density functions of endmember spectra. Imposing this constraint on
the conventional NMF framework promotes the extraction of independent
endmembers while further enhancing the parts-based representation of data.
Experiments conducted on diverse synthetic HSI datasets (with numerous numbers
of endmembers, spectral bands, pixels, and noise levels) and three standard
real HSI datasets demonstrate the validity of the proposed KbSNMF algorithm
compared to several state-of-the-art NMF-based HU baselines. The proposed
algorithm exhibits superior performance especially in terms of extracting
endmember spectra from hyperspectral data; therefore, it could uplift the
performance of recent deep learning HU methods which utilize the endmember
spectra as supervisory input data for abundance extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ekanayake_E/0/1/0/all/0/1"&gt;E.M.M.B. Ekanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weerasooriya_H/0/1/0/all/0/1"&gt;H.M.H.K. Weerasooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ranasinghe_D/0/1/0/all/0/1"&gt;D.Y.L. Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herath_S/0/1/0/all/0/1"&gt;S. Herath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rathnayake_B/0/1/0/all/0/1"&gt;B. Rathnayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Godaliyadda_G/0/1/0/all/0/1"&gt;G.M.R.I. Godaliyadda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ekanayake_M/0/1/0/all/0/1"&gt;M.P.B. Ekanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herath_H/0/1/0/all/0/1"&gt;H.M.V.R. Herath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems. (arXiv:2008.10581v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10581</id>
        <link href="http://arxiv.org/abs/2008.10581"/>
        <updated>2021-08-10T02:00:09.268Z</updated>
        <summary type="html"><![CDATA[Learning-based methodologies increasingly find applications in
safety-critical domains like autonomous driving and medical robotics. Due to
the rare nature of dangerous events, real-world testing is prohibitively
expensive and unscalable. In this work, we employ a probabilistic approach to
safety evaluation in simulation, where we are concerned with computing the
probability of dangerous events. We develop a novel rare-event simulation
method that combines exploration, exploitation, and optimization techniques to
find failure modes and estimate their rate of occurrence. We provide rigorous
guarantees for the performance of our method in terms of both statistical and
computational efficiency. Finally, we demonstrate the efficacy of our approach
on a variety of scenarios, illustrating its usefulness as a tool for rapid
sensitivity analysis and model comparison that are essential to developing and
testing safety-critical autonomous systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Aman Sinha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OKelly_M/0/1/0/all/0/1"&gt;Matthew O&amp;#x27;Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1"&gt;Russ Tedrake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bifold and Semantic Reasoning for Pedestrian Behavior Prediction. (arXiv:2012.03298v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03298</id>
        <link href="http://arxiv.org/abs/2012.03298"/>
        <updated>2021-08-10T02:00:09.250Z</updated>
        <summary type="html"><![CDATA[Pedestrian behavior prediction is one of the major challenges for intelligent
driving systems. Pedestrians often exhibit complex behaviors influenced by
various contextual elements. To address this problem, we propose BiPed, a
multitask learning framework that simultaneously predicts trajectories and
actions of pedestrians by relying on multimodal data. Our method benefits from
1) a bifold encoding approach where different data modalities are processed
independently allowing them to develop their own representations, and jointly
to produce a representation for all modalities using shared parameters; 2) a
novel interaction modeling technique that relies on categorical semantic
parsing of the scenes to capture interactions between target pedestrians and
their surroundings; and 3) a bifold prediction mechanism that uses both
independent and shared decoding of multimodal representations. Using public
pedestrian behavior benchmark datasets for driving, PIE and JAAD, we highlight
the benefits of the proposed method for behavior prediction and show that our
model achieves state-of-the-art performance and improves trajectory and action
prediction by up to 22% and 9% respectively. We further investigate the
contributions of the proposed reasoning techniques via extensive ablation
studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1"&gt;Amir Rasouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohani_M/0/1/0/all/0/1"&gt;Mohsen Rohani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jun Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution. (arXiv:2108.03920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03920</id>
        <link href="http://arxiv.org/abs/2108.03920"/>
        <updated>2021-08-10T02:00:09.244Z</updated>
        <summary type="html"><![CDATA[High-resolution magnetic resonance images can provide fine-grained anatomical
information, but acquiring such data requires a long scanning time. In this
paper, a framework called the Fused Attentive Generative Adversarial
Networks(FA-GAN) is proposed to generate the super-resolution MR image from
low-resolution magnetic resonance images, which can reduce the scanning time
effectively but with high resolution MR images. In the framework of the FA-GAN,
the local fusion feature block, consisting of different three-pass networks by
using different convolution kernels, is proposed to extract image features at
different scales. And the global feature fusion module, including the channel
attention module, the self-attention module, and the fusion operation, is
designed to enhance the important features of the MR image. Moreover, the
spectral normalization process is introduced to make the discriminator network
stable. 40 sets of 3D magnetic resonance images (each set of images contains
256 slices) are used to train the network, and 10 sets of images are used to
test the proposed method. The experimental results show that the PSNR and SSIM
values of the super-resolution magnetic resonance image generated by the
proposed FA-GAN method are higher than the state-of-the-art reconstruction
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Mingfeng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_M/0/1/0/all/0/1"&gt;Minghao Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Liying Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaocheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jucheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09134</id>
        <link href="http://arxiv.org/abs/2006.09134"/>
        <updated>2021-08-10T02:00:09.237Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) are formulated as minimax game
problems, whereby generators attempt to approach real data distributions by
virtue of adversarial learning against discriminators. The intrinsic problem
complexity poses the challenge to enhance the performance of generative
networks. In this work, we aim to boost model learning from the perspective of
network architectures, by incorporating recent progress on automated
architecture search into GANs. To this end, we propose a fully differentiable
search framework for generative adversarial networks, dubbed alphaGAN. The
searching process is formalized as solving a bi-level minimax optimization
problem, in which the outer-level objective aims for seeking a suitable network
architecture towards pure Nash Equilibrium conditioned on the generator and the
discriminator network parameters optimized with a traditional GAN loss in the
inner level. The entire optimization performs a first-order method by
alternately minimizing the two-level objective in a fully differentiable
manner, enabling architecture search to be completed in an enormous search
space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our
algorithm can obtain high-performing architectures only with 3-GPU hours on a
single GPU in the search space comprised of approximate 2 ? 1011 possible
configurations. We also provide a comprehensive analysis on the behavior of the
searching process and the properties of searched architectures, which would
benefit further research on architectures for generative models. Pretrained
models and codes are available at https://github.com/yuesongtian/AlphaGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuesong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1"&gt;Guinan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weak Supervision Approach to Detecting Visual Anomalies for Automated Testing of Graphics Units. (arXiv:1912.04138v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04138</id>
        <link href="http://arxiv.org/abs/1912.04138"/>
        <updated>2021-08-10T02:00:09.231Z</updated>
        <summary type="html"><![CDATA[We present a deep learning system for testing graphics units by detecting
novel visual corruptions in videos. Unlike previous work in which manual
tagging was required to collect labeled training data, our weak supervision
method is fully automatic and needs no human labelling. This is achieved by
reproducing driver bugs that increase the probability of generating
corruptions, and by making use of ideas and methods from the Multiple Instance
Learning (MIL) setting. In our experiments, we significantly outperform
unsupervised methods such as GAN-based models and discover novel corruptions
undetected by baselines, while adhering to strict requirements on accuracy and
efficiency of our real-time system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Szeskin_A/0/1/0/all/0/1"&gt;Adi Szeskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faivishevsky_L/0/1/0/all/0/1"&gt;Lev Faivishevsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muppalla_A/0/1/0/all/0/1"&gt;Ashwin K Muppalla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armon_A/0/1/0/all/0/1"&gt;Amitai Armon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1"&gt;Tom Hope&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11769</id>
        <link href="http://arxiv.org/abs/2107.11769"/>
        <updated>2021-08-10T02:00:09.223Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep learning on supervised point cloud semantic
segmentation, obtaining large-scale point-by-point manual annotations is still
a significant challenge. To reduce the huge annotation burden, we propose a
Region-based and Diversity-aware Active Learning (ReDAL), a general framework
for many deep learning approaches, aiming to automatically select only
informative and diverse sub-scene regions for label acquisition. Observing that
only a small portion of annotated regions are sufficient for 3D scene
understanding with deep learning, we use softmax entropy, color discontinuity,
and structural complexity to measure the information of sub-scene regions. A
diversity-aware selection algorithm is also developed to avoid redundant
annotations resulting from selecting informative but similar regions in a
querying batch. Extensive experiments show that our method highly outperforms
previous active learning strategies, and we achieve the performance of 90%
fully supervised learning, while less than 15% and 5% annotations are required
on S3DIS and SemanticKITTI datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tsung-Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Ping-Chia Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09124</id>
        <link href="http://arxiv.org/abs/2104.09124"/>
        <updated>2021-08-10T02:00:09.199Z</updated>
        <summary type="html"><![CDATA[While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of
ResNet-101/ResNet-50. Code is available at https://github.
com/Yuting-Gao/DisCo-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mining Latent Classes for Few-shot Segmentation. (arXiv:2103.15402v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15402</id>
        <link href="http://arxiv.org/abs/2103.15402"/>
        <updated>2021-08-10T02:00:09.187Z</updated>
        <summary type="html"><![CDATA[Few-shot segmentation (FSS) aims to segment unseen classes given only a few
annotated samples. Existing methods suffer the problem of feature undermining,
i.e. potential novel classes are treated as background during training phase.
Our method aims to alleviate this problem and enhance the feature embedding on
latent novel classes. In our work, we propose a novel joint-training framework.
Based on conventional episodic training on support-query pairs, we add an
additional mining branch that exploits latent novel classes via transferable
sub-clusters, and a new rectification technique on both background and
foreground categories to enforce more stable prototypes. Over and above that,
our transferable sub-cluster has the ability to leverage extra unlabeled data
for further feature enhancement. Extensive experiments on two FSS benchmarks
demonstrate that our method outperforms previous state-of-the-art by a large
margin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%
fewer parameters and 2.5x faster inference speed. The source code is available
at https://github.com/LiheYoung/MiningFSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lihe Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05690</id>
        <link href="http://arxiv.org/abs/2010.05690"/>
        <updated>2021-08-10T02:00:09.179Z</updated>
        <summary type="html"><![CDATA[The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1"&gt;Lalith Bharadwaj B&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1"&gt;Rohit Boddeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1"&gt;Sai Vardhan K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1"&gt;Madhu G&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09128</id>
        <link href="http://arxiv.org/abs/2011.09128"/>
        <updated>2021-08-10T02:00:09.172Z</updated>
        <summary type="html"><![CDATA[We present a multigrid-in-channels (MGIC) approach that tackles the quadratic
growth of the number of parameters with respect to the number of channels in
standard convolutional neural networks (CNNs). Thereby our approach addresses
the redundancy in CNNs that is also exposed by the recent success of
lightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard
CNNs with fewer parameters; however, the number of weights still scales
quadratically with the CNN's width. Our MGIC architectures replace each CNN
block with an MGIC counterpart that utilizes a hierarchy of nested grouped
convolutions of small group size to address this.

Hence, our proposed architectures scale linearly with respect to the
network's width while retaining full coupling of the channels as in standard
CNNs.

Our extensive experiments on image classification, segmentation, and point
cloud classification show that applying this strategy to different
architectures like ResNet and MobileNetV3 reduces the number of parameters
while obtaining similar or better accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1"&gt;Jonathan Ephrath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1"&gt;Lars Ruthotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Transfer with von Neumann Conditional Divergence. (arXiv:2108.03531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03531</id>
        <link href="http://arxiv.org/abs/2108.03531"/>
        <updated>2021-08-10T02:00:09.165Z</updated>
        <summary type="html"><![CDATA[The similarity of feature representations plays a pivotal role in the success
of domain adaptation and generalization. Feature similarity includes both the
invariance of marginal distributions and the closeness of conditional
distributions given the desired response $y$ (e.g., class labels).
Unfortunately, traditional methods always learn such features without fully
taking into consideration the information in $y$, which in turn may lead to a
mismatch of the conditional distributions or the mix-up of discriminative
structures underlying data distributions. In this work, we introduce the
recently proposed von Neumann conditional divergence to improve the
transferability across multiple domains. We show that this new divergence is
differentiable and eligible to easily quantify the functional dependence
between features and $y$. Given multiple source tasks, we integrate this
divergence to capture discriminative information in $y$ and design novel
learning objectives assuming those source tasks are observed either
simultaneously or sequentially. In both scenarios, we obtain favorable
performance against state-of-the-art methods in terms of smaller generalization
error on new tasks and less catastrophic forgetting on source tasks (in the
sequential setup).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1"&gt;Ammar Shaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shujian Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Image Transformation for Inducing Affect. (arXiv:1707.08148v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1707.08148</id>
        <link href="http://arxiv.org/abs/1707.08148"/>
        <updated>2021-08-10T02:00:09.140Z</updated>
        <summary type="html"><![CDATA[Current image transformation and recoloring algorithms try to introduce
artistic effects in the photographed images, based on user input of target
image(s) or selection of pre-designed filters. These manipulations, although
intended to enhance the impact of an image on the viewer, do not include the
option of image transformation by specifying the affect information. In this
paper we present an automatic image-transformation method that transforms the
source image such that it can induce an emotional affect on the viewer, as
desired by the user. Our proposed novel image emotion transfer algorithm does
not require a user-specified target image. The proposed algorithm uses features
extracted from top layers of deep convolutional neural network and the
user-specified emotion distribution to select multiple target images from an
image database for color transformation, such that the resultant image has
desired emotional impact. Our method can handle more diverse set of photographs
than the previous methods. We conducted a detailed user study showing the
effectiveness of our proposed method. A discussion and reasoning of failure
cases has also been provided, indicating inherent limitation of color-transfer
based methods in the use of emotion assignment.

Project Page: this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1"&gt;Afsheen Rafaqat Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mohsen Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge. (arXiv:2108.04016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04016</id>
        <link href="http://arxiv.org/abs/2108.04016"/>
        <updated>2021-08-10T02:00:09.133Z</updated>
        <summary type="html"><![CDATA[A key factor for assessing the state of the heart after myocardial infarction
(MI) is to measure whether the myocardium segment is viable after reperfusion
or revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is
performed several minutes after injection of the contrast agent, provides high
contrast between viable and nonviable myocardium and is therefore a method of
choice to evaluate the extent of MI. To automatically assess myocardial status,
the results of the EMIDEC challenge that focused on this task are presented in
this paper. The challenge's main objectives were twofold. First, to evaluate if
deep learning methods can distinguish between normal and pathological cases.
Second, to automatically calculate the extent of myocardial infarction. The
publicly available database consists of 150 exams divided into 50 cases with
normal MRI after injection of a contrast agent and 100 cases with myocardial
infarction (and then with a hyperenhanced area on DE-MRI), whatever their
inclusion in the cardiac emergency department. Along with MRI, clinical
characteristics are also provided. The obtained results issued from several
works show that the automatic classification of an exam is a reachable task
(the best method providing an accuracy of 0.92), and the automatic segmentation
of the myocardium is possible. However, the segmentation of the diseased area
needs to be improved, mainly due to the small size of these areas and the lack
of contrast with the surrounding structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1"&gt;Alain Lalande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pommier_T/0/1/0/all/0/1"&gt;Thibaut Pommier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decourselle_T/0/1/0/all/0/1"&gt;Thomas Decourselle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1"&gt;Abdul Qayyum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salomon_M/0/1/0/all/0/1"&gt;Michel Salomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1"&gt;Dominique Ginhac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1"&gt;Youssef Skandarani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1"&gt;Arnaud Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brahim_K/0/1/0/all/0/1"&gt;Khawla Brahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1"&gt;Marleen de Bruijne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1"&gt;Robin Camarasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correia_T/0/1/0/all/0/1"&gt;Teresa M. Correia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xue Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girum_K/0/1/0/all/0/1"&gt;Kibrom B. Girum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennemuth_A/0/1/0/all/0/1"&gt;Anja Hennemuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huellebrand_M/0/1/0/all/0/1"&gt;Markus Huellebrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1"&gt;Raabid Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivantsits_M/0/1/0/all/0/1"&gt;Matthias Ivantsits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1"&gt;Craig Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1"&gt;Rishabh Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jixi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsekos_N/0/1/0/all/0/1"&gt;Nikolaos V. Tsekos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1"&gt;Marta Varela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hannu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuncheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1"&gt;Raphael Couturier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1"&gt;Fabrice Meriaudeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.00414</id>
        <link href="http://arxiv.org/abs/1811.00414"/>
        <updated>2021-08-10T02:00:09.125Z</updated>
        <summary type="html"><![CDATA[A central roadblock to analyzing quantum algorithms on quantum states is the
lack of a comparable input model for classical algorithms. Inspired by recent
work of the author [E. Tang, STOC'19], we introduce such a model, where we
assume we can efficiently perform $\ell^2$-norm samples of input data, a
natural analogue to quantum algorithms that assume efficient state preparation
of classical data. Though this model produces less practical algorithms than
the (stronger) standard model of classical computation, it captures versions of
many of the features and nuances of quantum linear algebra algorithms. With
this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's
quantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]
and nearest-centroid clustering [arXiv:1307.0411]. Since they are only
polynomially slower, these algorithms suggest that the exponential speedups of
their quantum counterparts are simply an artifact of state preparation
assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1"&gt;Ewin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Reconstruction in the Wild with Collaborative Aerial Cameras. (arXiv:2108.03936v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03936</id>
        <link href="http://arxiv.org/abs/2108.03936"/>
        <updated>2021-08-10T02:00:09.116Z</updated>
        <summary type="html"><![CDATA[Aerial vehicles are revolutionizing applications that require capturing the
3D structure of dynamic targets in the wild, such as sports, medicine, and
entertainment. The core challenges in developing a motion-capture system that
operates in outdoors environments are: (1) 3D inference requires multiple
simultaneous viewpoints of the target, (2) occlusion caused by obstacles is
frequent when tracking moving targets, and (3) the camera and vehicle state
estimation is noisy. We present a real-time aerial system for multi-camera
control that can reconstruct human motions in natural environments without the
use of special-purpose markers. We develop a multi-robot coordination scheme
that maintains the optimal flight formation for target reconstruction quality
amongst obstacles. We provide studies evaluating system performance in
simulation, and validate real-world performance using two drones while a target
performs activities such as jogging and playing soccer. Supplementary video:
https://youtu.be/jxt91vx0cns]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1"&gt;Cherie Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_A/0/1/0/all/0/1"&gt;Andrew Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_H/0/1/0/all/0/1"&gt;Harry Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1"&gt;Rohan Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1"&gt;Rogerio Bonatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14483</id>
        <link href="http://arxiv.org/abs/2107.14483"/>
        <updated>2021-08-10T02:00:09.110Z</updated>
        <summary type="html"><![CDATA[Learning generalizable manipulation skills is central for robots to achieve
task automation in environments with endless scene and object variations.
However, existing robot learning environments are limited in both scale and
diversity of 3D assets (especially of articulated objects), making it difficult
to train and evaluate the generalization ability of agents over novel objects.
In this work, we focus on object-level generalization and propose SAPIEN
Manipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale
learning-from-demonstrations benchmark for articulated object manipulation with
3D visual input (point cloud and RGB-D image). ManiSkill supports object-level
variations by utilizing a rich and diverse set of articulated objects, and each
task is carefully designed for learning manipulations on a single category of
objects. We equip ManiSkill with a large number of high-quality demonstrations
to facilitate learning-from-demonstrations approaches and perform evaluations
on baseline algorithms. We believe that ManiSkill can encourage the robot
learning community to explore more on learning generalizable object
manipulation skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1"&gt;Fanbo Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Derek Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Stone Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zhiwei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining a Convolutional Neural Network with Autoencoders to Predict the Survival Chance of COVID-19 Patients. (arXiv:2104.08954v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08954</id>
        <link href="http://arxiv.org/abs/2104.08954"/>
        <updated>2021-08-10T02:00:09.091Z</updated>
        <summary type="html"><![CDATA[COVID-19 has caused many deaths worldwide. The automation of the diagnosis of
this virus is highly desired. Convolutional neural networks (CNNs) have shown
outstanding classification performance on image datasets. To date, it appears
that COVID computer-aided diagnosis systems based on CNNs and clinical
information have not yet been analysed or explored. We propose a novel method,
named the CNN-AE, to predict the survival chance of COVID-19 patients using a
CNN trained with clinical information. Notably, the required resources to
prepare CT images are expensive and limited compared to those required to
collect clinical data, such as blood pressure, liver disease, etc. We evaluated
our method using a publicly available clinical dataset that we collected. The
dataset properties were carefully analysed to extract important features and
compute the correlations of features. A data augmentation procedure based on
autoencoders (AEs) was proposed to balance the dataset. The experimental
results revealed that the average accuracy of the CNN-AE (96.05%) was higher
than that of the CNN (92.49%). To demonstrate the generality of our
augmentation method, we trained some existing mortality risk prediction methods
on our dataset (with and without data augmentation) and compared their
performances. We also evaluated our method using another dataset for further
generality verification. To show that clinical data can be used for COVID-19
survival chance prediction, the CNN-AE was compared with multiple pre-trained
deep models that were tuned based on CT images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khozeimeh_F/0/1/0/all/0/1"&gt;Fahime Khozeimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1"&gt;Danial Sharifrazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izadi_N/0/1/0/all/0/1"&gt;Navid Hoseini Izadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1"&gt;Javad Hassannataj Joloudari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1"&gt;Afshin Shoeibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1"&gt;Juan M. Gorriz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Sadiq Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sani_Z/0/1/0/all/0/1"&gt;Zahra Alizadeh Sani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1"&gt;Hossein Moosaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1"&gt;Abbas Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1"&gt;Saeid Nahavandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1"&gt;Sheikh Mohammed Shariful Islam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine Learning Tool to Determine State of Mind and Emotion. (arXiv:2108.03444v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03444</id>
        <link href="http://arxiv.org/abs/2108.03444"/>
        <updated>2021-08-10T02:00:09.084Z</updated>
        <summary type="html"><![CDATA[This paper investigates the possibility of creating a machine learning tool
that automatically determines the state of mind and emotion of an individual
through a questionnaire, without the aid of a human expert. The state of mind
and emotion is defined in this work as pertaining to preference, feelings, or
opinion that is not based on logic or reason. It is the case when a person
gives out an answer to start by saying, "I feel...". The tool is designed to
mimic the expertise of a psychologist and is built without any formal knowledge
of psychology. The idea is to build the expertise by purely computational
methods through thousands of questions collected from users. It is aimed
towards possibly diagnosing substance addiction, alcoholism, sexual attraction,
HIV status, degree of commitment, activity inclination, etc. First, the paper
presents the related literature and classifies them according to data gathering
methods. Another classification is created according to preference, emotion,
grouping, and rules to achieve a deeper interpretation and better understanding
of the state of mind and emotion. Second, the proposed tool is developed using
an online addiction questionnaire with 10 questions and 292 respondents. In
addition, an initial investigation on the dimension of addiction is presented
through the built machine learning model. Machine learning methods, namely,
artificial neural network (ANN) and support vector machine (SVM), are used to
determine a true or false or degree of state of a respondent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jamisola_R/0/1/0/all/0/1"&gt;Rodrigo S. Jamisola Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks. (arXiv:1909.11799v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11799</id>
        <link href="http://arxiv.org/abs/1909.11799"/>
        <updated>2021-08-10T02:00:09.078Z</updated>
        <summary type="html"><![CDATA[Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perry_R/0/1/0/all/0/1"&gt;Ronan Perry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Adam Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_C/0/1/0/all/0/1"&gt;Chester Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomita_T/0/1/0/all/0/1"&gt;Tyler M. Tomita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Ronak Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arroyo_J/0/1/0/all/0/1"&gt;Jesus Arroyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patsolic_J/0/1/0/all/0/1"&gt;Jesse Patsolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falk_B/0/1/0/all/0/1"&gt;Benjamin Falk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1"&gt;Joshua T. Vogelstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Transformer for Dynamic Scene Graph Generation. (arXiv:2107.12309v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12309</id>
        <link href="http://arxiv.org/abs/2107.12309"/>
        <updated>2021-08-10T02:00:09.068Z</updated>
        <summary type="html"><![CDATA[Dynamic scene graph generation aims at generating a scene graph of the given
video. Compared to the task of scene graph generation from images, it is more
challenging because of the dynamic relationships between objects and the
temporal dependencies between frames allowing for a richer semantic
interpretation. In this paper, we propose Spatial-temporal Transformer
(STTran), a neural network that consists of two core modules: (1) a spatial
encoder that takes an input frame to extract spatial context and reason about
the visual relationships within a frame, and (2) a temporal decoder which takes
the output of the spatial encoder as input in order to capture the temporal
dependencies between frames and infer the dynamic relationships. Furthermore,
STTran is flexible to take varying lengths of videos as input without clipping,
which is especially important for long videos. Our method is validated on the
benchmark dataset Action Genome (AG). The experimental results demonstrate the
superior performance of our method in terms of dynamic scene graphs. Moreover,
a set of ablative studies is conducted and the effect of each proposed module
is justified. Code available at: https://github.com/yrcong/STTran.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1"&gt;Yuren Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wentong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ackermann_H/0/1/0/all/0/1"&gt;Hanno Ackermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Michael Ying Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network. (arXiv:2108.03990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03990</id>
        <link href="http://arxiv.org/abs/2108.03990"/>
        <updated>2021-08-10T02:00:09.062Z</updated>
        <summary type="html"><![CDATA[Salient object detection is the pixel-level dense prediction task which can
highlight the prominent object in the scene. Recently U-Net framework is widely
used, and continuous convolution and pooling operations generate multi-level
features which are complementary with each other. In view of the more
contribution of high-level features for the performance, we propose a triplet
transformer embedding module to enhance them by learning long-range
dependencies across layers. It is the first to use three transformer encoders
with shared weights to enhance multi-level features. By further designing scale
adjustment module to process the input, devising three-stream decoder to
process the output and attaching depth features to color features for the
multi-modal fusion, the proposed triplet transformer embedding network
(TriTransNet) achieves the state-of-the-art performance in RGB-D salient object
detection, and pushes the performance to a new level. Experimental results
demonstrate the effectiveness of the proposed modules and the competition of
TriTransNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Bin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02193</id>
        <link href="http://arxiv.org/abs/2103.02193"/>
        <updated>2021-08-10T02:00:09.040Z</updated>
        <summary type="html"><![CDATA[While recent studies on semi-supervised learning have shown remarkable
progress in leveraging both labeled and unlabeled data, most of them presume a
basic setting of the model is randomly initialized. In this work, we consider
semi-supervised learning and transfer learning jointly, leading to a more
practical and competitive paradigm that can utilize both powerful pre-trained
models from source domain as well as labeled/unlabeled data in the target
domain. To better exploit the value of both pre-trained weights and unlabeled
target examples, we introduce adaptive consistency regularization that consists
of two complementary components: Adaptive Knowledge Consistency (AKC) on the
examples between the source and target model, and Adaptive Representation
Consistency (ARC) on the target model between labeled and unlabeled examples.
Examples involved in the consistency regularization are adaptively selected
according to their potential contributions to the target task. We conduct
extensive experiments on popular benchmarks including CIFAR-10, CUB-200, and
MURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show
that our proposed adaptive consistency regularization outperforms
state-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean
Teacher, and FixMatch. Moreover, our algorithm is orthogonal to existing
methods and thus able to gain additional improvements on top of MixMatch and
FixMatch. Our code is available at
https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1"&gt;Abulikemu Abuduweili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xingjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v4 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15634</id>
        <link href="http://arxiv.org/abs/2007.15634"/>
        <updated>2021-08-10T02:00:09.033Z</updated>
        <summary type="html"><![CDATA[Anomalies are occurrences in a dataset that are in some way unusual and do
not fit the general patterns. The concept of the anomaly is typically
ill-defined and perceived as vague and domain-dependent. Moreover, despite some
250 years of publications on the topic, no comprehensive and concrete overviews
of the different types of anomalies have hitherto been published. By means of
an extensive literature review this study therefore offers the first
theoretically principled and domain-independent typology of data anomalies and
presents a full overview of anomaly types and subtypes. To concretely define
the concept of the anomaly and its different manifestations, the typology
employs five dimensions: data type, cardinality of relationship, anomaly level,
data structure, and data distribution. These fundamental and data-centric
dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of
anomalies. The typology facilitates the evaluation of the functional
capabilities of anomaly detection algorithms, contributes to explainable data
science, and provides insights into relevant topics such as local versus global
anomalies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foorthuis_R/0/1/0/all/0/1"&gt;Ralph Foorthuis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation. (arXiv:2108.04023v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04023</id>
        <link href="http://arxiv.org/abs/2108.04023"/>
        <updated>2021-08-10T02:00:09.026Z</updated>
        <summary type="html"><![CDATA[We present a novel and flexible architecture for point cloud segmentation
with dual-representation iterative learning. In point cloud processing,
different representations have their own pros and cons. Thus, finding suitable
ways to represent point cloud data structure while keeping its own internal
physical property such as permutation and scale-invariant is a fundamental
problem. Therefore, we propose our work, DRINet, which serves as the basic
network structure for dual-representation learning with great flexibility at
feature transferring and less computation cost, especially for large-scale
point clouds. DRINet mainly consists of two modules called Sparse Point-Voxel
Feature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing
these two modules iteratively, features can be propagated between two different
representations. We further propose a novel multi-scale pooling layer for
pointwise locality learning to improve context information propagation. Our
network achieves state-of-the-art results for point cloud classification and
segmentation tasks on several datasets while maintaining high runtime
efficiency. For large-scale outdoor scenarios, our method outperforms
state-of-the-art methods with a real-time inference speed of 62ms per frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Maosheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shuangjie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tongyi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03995</id>
        <link href="http://arxiv.org/abs/2103.03995"/>
        <updated>2021-08-10T02:00:09.016Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are widely used in image recognition.
Numerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have
been proposed by increasing the number of layers, to improve the performance of
CNNs. However, performance deteriorates beyond a certain number of layers.
Hence, hyperparameter optimisation is a more efficient way to improve CNNs. To
validate this concept, a new algorithm based on simplified swarm optimisation
is proposed to optimise the hyperparameters of the simplest CNN model, which is
LeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and
Cifar10 datasets showed that the accuracy of the proposed algorithm is higher
than the original LeNet model and PSO-LeNet and that it has a high potential to
be extended to more complicated models, such as AlexNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1"&gt;Wei-Chang Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yi-Ping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yun-Chia Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Chyh-Ming Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03555</id>
        <link href="http://arxiv.org/abs/2108.03555"/>
        <updated>2021-08-10T02:00:09.008Z</updated>
        <summary type="html"><![CDATA[Background: Accurate diagnosis of skull base tumors is essential for
providing personalized surgical treatment strategies. Intraoperative diagnosis
can be challenging due to tumor diversity and lack of intraoperative pathology
resources.

Objective: To develop an independent and parallel intraoperative pathology
workflow that can provide rapid and accurate skull base tumor diagnoses using
label-free optical imaging and artificial intelligence (AI).

Method: We used a fiber laser-based, label-free, non-consumptive,
high-resolution microscopy method ($<$ 60 sec per 1 $\times$ 1 mm$^\text{2}$),
called stimulated Raman histology (SRH), to image a consecutive, multicenter
cohort of skull base tumor patients. SRH images were then used to train a
convolutional neural network (CNN) model using three representation learning
strategies: cross-entropy, self-supervised contrastive learning, and supervised
contrastive learning. Our trained CNN models were tested on a held-out,
multicenter SRH dataset.

Results: SRH was able to image the diagnostic features of both benign and
malignant skull base tumors. Of the three representation learning strategies,
supervised contrastive learning most effectively learned the distinctive and
diagnostic SRH image features for each of the skull base tumor types. In our
multicenter testing set, cross-entropy achieved an overall diagnostic accuracy
of 91.5%, self-supervised contrastive learning 83.9%, and supervised
contrastive learning 96.6%. Our trained model was able to identify tumor-normal
margins and detect regions of microscopic tumor infiltration in whole-slide SRH
images.

Conclusion: SRH with AI models trained using contrastive representation
learning can provide rapid and accurate intraoperative diagnosis of skull base
tumors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Cheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1"&gt;Abhishek Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1"&gt;Joseph Linzey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Rushikesh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sung Jik Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Sudharsan Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1"&gt;Daniel Alber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1"&gt;Akhil Kondepudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1"&gt;Esteban Urias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1"&gt;Balaji Pandian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1"&gt;Wajd Al-Holou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1"&gt;Steve Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1"&gt;B. Gregory Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1"&gt;Jason Heth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1"&gt;Chris Freudiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1"&gt;Siri Khalsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1"&gt;Donato Pacione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1"&gt;John G. Golfinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1"&gt;Sandra Camelo-Piragua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1"&gt;Daniel A. Orringer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1"&gt;Todd Hollon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00959</id>
        <link href="http://arxiv.org/abs/2005.00959"/>
        <updated>2021-08-10T02:00:08.991Z</updated>
        <summary type="html"><![CDATA[Ill-posed linear inverse problems appear in many scientific setups, and are
typically addressed by solving optimization problems, which are composed of
data fidelity and prior terms. Recently, several works have considered a
back-projection (BP) based fidelity term as an alternative to the common least
squares (LS), and demonstrated excellent results for popular inverse problems.
These works have also empirically shown that using the BP term, rather than the
LS term, requires fewer iterations of optimization algorithms. In this paper,
we examine the convergence rate of the projected gradient descent (PGD)
algorithm for the BP objective. Our analysis allows to identify an inherent
source for its faster convergence compared to using the LS objective, while
making only mild assumptions. We also analyze the more general proximal
gradient method under a relaxed contraction condition on the proximal mapping
of the prior. This analysis further highlights the advantage of BP when the
linear measurement operator is badly conditioned. Numerical experiments with
both $\ell_1$-norm and GAN-based priors corroborate our theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1"&gt;Tom Tirer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a method to anticipate dark matter signals with deep learning at the LHC. (arXiv:2105.12018v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12018</id>
        <link href="http://arxiv.org/abs/2105.12018"/>
        <updated>2021-08-10T02:00:08.984Z</updated>
        <summary type="html"><![CDATA[We study several simplified dark matter (DM) models and their signatures at
the LHC using neural networks. We focus on the usual monojet plus missing
transverse energy channel, but to train the algorithms we organize the data in
2D histograms instead of event-by-event arrays. This results in a large
performance boost to distinguish between standard model (SM) only and SM plus
new physics signals. We use the kinematic monojet features as input data which
allow us to describe families of models with a single data sample. We found
that the neural network performance does not depend on the simulated number of
background events if they are presented as a function of $S/\sqrt{B}$, where
$S$ and $B$ are the number of signal and background events per histogram,
respectively. This provides flexibility to the method, since testing a
particular model in that case only requires knowing the new physics monojet
cross section. Furthermore, we also discuss the network performance under
incorrect assumptions about the true DM nature. Finally, we propose multimodel
classifiers to search and identify new signals in a more general way, for the
next LHC run.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Arganda_E/0/1/0/all/0/1"&gt;Ernesto Arganda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Medina_A/0/1/0/all/0/1"&gt;Anibal D. Medina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Perez_A/0/1/0/all/0/1"&gt;Andres D. Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Szynkman_A/0/1/0/all/0/1"&gt;Alejandro Szynkman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08825</id>
        <link href="http://arxiv.org/abs/2009.08825"/>
        <updated>2021-08-10T02:00:08.977Z</updated>
        <summary type="html"><![CDATA[With the success of deep neural networks, knowledge distillation which guides
the learning of a small student network from a large teacher network is being
actively studied for model compression and transfer learning. However, few
studies have been performed to resolve the poor learning issue of the student
network when the student and teacher model sizes significantly differ. In this
paper, we propose a densely guided knowledge distillation using multiple
teacher assistants that gradually decreases the model size to efficiently
bridge the large gap between the teacher and student networks. To stimulate
more efficient learning of the student network, we guide each teacher assistant
to every other smaller teacher assistants iteratively. Specifically, when
teaching a smaller teacher assistant at the next step, the existing larger
teacher assistants from the previous step are used as well as the teacher
network. Moreover, we design stochastic teaching where, for each mini-batch, a
teacher or teacher assistants are randomly dropped. This acts as a regularizer
to improve the efficiency of teaching of the student network. Thus, the student
can always learn salient distilled knowledge from the multiple sources. We
verified the effectiveness of the proposed method for a classification task
using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant
performance improvements with various backbone architectures such as ResNet,
WideResNet, and VGG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1"&gt;Wonchul Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1"&gt;Jaemin Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Junyong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1"&gt;Wonjun Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Look at the Evaluation Setup of the M5 Forecasting Competition. (arXiv:2108.03588v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03588</id>
        <link href="http://arxiv.org/abs/2108.03588"/>
        <updated>2021-08-10T02:00:08.970Z</updated>
        <summary type="html"><![CDATA[Forecast evaluation plays a key role in how empirical evidence shapes the
development of the discipline. Domain experts are interested in error measures
relevant for their decision making needs. Such measures may produce unreliable
results. Although reliability properties of several metrics have already been
discussed, it has hardly been quantified in an objective way. We propose a
measure named Rank Stability, which evaluates how much the rankings of an
experiment differ in between similar datasets, when the models and errors are
constant. We use this to study the evaluation setup of the M5. We find that the
evaluation setup of the M5 is less reliable than other measures. The main
drivers of instability are hierarchical aggregation and scaling.
Price-weighting reduces the stability of all tested error measures. Scale
normalization of the M5 error measure results in less stability than other
scale-free errors. Hierarchical levels taken separately are less stable with
more aggregation, and their combination is even less stable than individual
levels. We also show positive tradeoffs of retaining aggregation importance
without affecting stability. Aggregation and stability can be linked to the
influence of much debated magic numbers. Many of our findings can be applied to
general hierarchical forecast benchmarking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hewamalage_H/0/1/0/all/0/1"&gt;Hansika Hewamalage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montero_Manso_P/0/1/0/all/0/1"&gt;Pablo Montero-Manso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1"&gt;Christoph Bergmeir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hyndman_R/0/1/0/all/0/1"&gt;Rob J Hyndman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors. (arXiv:2103.12474v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12474</id>
        <link href="http://arxiv.org/abs/2103.12474"/>
        <updated>2021-08-10T02:00:08.964Z</updated>
        <summary type="html"><![CDATA[Predicting the states of dynamic traffic actors into the future is important
for autonomous systems to operate safelyand efficiently. Remarkably, the most
critical scenarios aremuch less frequent and more complex than the
uncriticalones. Therefore, uncritical cases dominate the prediction. In this
paper, we address specifically the challenging scenarios at the long tail of
the dataset distribution. Our analysis shows that the common losses tend to
place challenging cases suboptimally in the embedding space. As a consequence,
we propose to supplement the usual loss with aloss that places challenging
cases closer to each other. This triggers sharing information among challenging
cases andlearning specific predictive features. We show on four public datasets
that this leads to improved performance on the challenging scenarios while the
overall performance stays stable. The approach is agnostic w.r.t. the used
network architecture, input modality or viewpoint, and can be integrated into
existing solutions easily. Code is available at
https://github.com/lmb-freiburg/Contrastive-Future-Trajectory-Prediction]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makansi_O/0/1/0/all/0/1"&gt;Osama Makansi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_O/0/1/0/all/0/1"&gt;&amp;#xd6;zg&amp;#xfc;n Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marrakchi_Y/0/1/0/all/0/1"&gt;Yassine Marrakchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Camera Simulators. (arXiv:2104.05237v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05237</id>
        <link href="http://arxiv.org/abs/2104.05237"/>
        <updated>2021-08-10T02:00:08.957Z</updated>
        <summary type="html"><![CDATA[We present a controllable camera simulator based on deep neural networks to
synthesize raw image data under different camera settings, including exposure
time, ISO, and aperture. The proposed simulator includes an exposure module
that utilizes the principle of modern lens designs for correcting the luminance
level. It also contains a noise module using the noise level function and an
aperture module with adaptive attention to simulate the side effects on noise
and defocus blur. To facilitate the learning of a simulator model, we collect a
dataset of the 10,000 raw images of 450 scenes with different exposure
settings. Quantitative experiments and qualitative comparisons show that our
approach outperforms relevant baselines in raw data synthesize on multiple
cameras. Furthermore, the camera simulator enables various applications,
including large-aperture enhancement, HDR, auto exposure, and data augmentation
for training local feature detectors. Our work represents the first attempt to
simulate a camera sensor's behavior leveraging both the advantage of
traditional raw sensor features and the power of data-driven deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1"&gt;Hao Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zifan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1"&gt;Chenyang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1"&gt;Ka Lung Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.13490</id>
        <link href="http://arxiv.org/abs/2107.13490"/>
        <updated>2021-08-10T02:00:08.937Z</updated>
        <summary type="html"><![CDATA[Quantization is a technique for reducing deep neural networks (DNNs) training
and inference times, which is crucial for training in resource constrained
environments or time critical inference applications. State-of-the-art (SOTA)
quantization approaches focus on post-training quantization, i.e. quantization
of pre-trained DNNs for speeding up inference. Very little work on quantized
training exists, which neither al-lows dynamic intra-epoch precision switches
nor em-ploys an information theory based switching heuristic. Usually, existing
approaches require full precision refinement afterwards and enforce a global
word length across the whole DNN. This leads to suboptimal quantization
mappings and resource usage. Recognizing these limits, we introduce MARViN, a
new quantized training strategy using information theory-based intra-epoch
precision switching, which decides on a per-layer basis which precision should
be used in order to minimize quantization-induced information loss. Note that
any quantization must leave enough precision such that future learning steps do
not suffer from vanishing gradients. We achieve an average speedup of 1.86
compared to a float32 basis while limiting mean accuracy degradation on
AlexNet/ResNet to only -0.075%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1"&gt;Lorenz Kummer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1"&gt;Kevin Sidak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1"&gt;Tabea Reichmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1"&gt;Wilfried Gansterer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Reinforcement Learning in Broad and Non-Parametric Environments. (arXiv:2108.03718v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03718</id>
        <link href="http://arxiv.org/abs/2108.03718"/>
        <updated>2021-08-10T02:00:08.928Z</updated>
        <summary type="html"><![CDATA[Recent state-of-the-art artificial agents lack the ability to adapt rapidly
to new tasks, as they are trained exclusively for specific objectives and
require massive amounts of interaction to learn new skills. Meta-reinforcement
learning (meta-RL) addresses this challenge by leveraging knowledge learned
from training tasks to perform well in previously unseen tasks. However,
current meta-RL approaches limit themselves to narrow parametric task
distributions, ignoring qualitative differences between tasks that occur in the
real world. In this paper, we introduce TIGR, a Task-Inference-based meta-RL
algorithm using Gaussian mixture models (GMM) and gated Recurrent units,
designed for tasks in non-parametric environments. We employ a generative model
involving a GMM to capture the multi-modality of the tasks. We decouple the
policy training from the task-inference learning and efficiently train the
inference mechanism on the basis of an unsupervised reconstruction objective.
We provide a benchmark with qualitatively distinct tasks based on the
half-cheetah environment and demonstrate the superior performance of TIGR
compared to state-of-the-art meta-RL approaches in terms of sample efficiency
(3-10 times faster), asymptotic performance, and applicability in
non-parametric environments with zero-shot adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1"&gt;Zhenshan Bing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knak_L/0/1/0/all/0/1"&gt;Lukas Knak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robin_F/0/1/0/all/0/1"&gt;Fabrice Oliver Robin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1"&gt;Alois Knoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07911</id>
        <link href="http://arxiv.org/abs/2104.07911"/>
        <updated>2021-08-10T02:00:08.921Z</updated>
        <summary type="html"><![CDATA[In the recent decade, high-throughput plant phenotyping techniques, which
combine non-invasive image analysis and machine learning, have been
successfully applied to identify and quantify plant health and diseases.
However, these techniques usually do not consider the progressive nature of
plant stress and often require images showing severe signs of stress to ensure
high confidence detection, thereby reducing the feasibility for early detection
and recovery of plants under stress. To overcome the problem mentioned above,
we propose a deep learning pipeline for the temporal analysis of the visual
changes induced in the plant due to stress and apply it for the specific case
of water stress identification in Chickpea plant shoot images. For this, we
have considered an image dataset of two chickpea varieties JG-62 and Pusa-372,
under three water stress conditions; control, young seedling, and before
flowering, captured over five months. We have employed a variant of the
Long-term Recurrent Convolutional Network (LRCN) to learn spatio-temporal
patterns from the chickpea plant dataset and use them for water stress
classification. Our model has achieved ceiling level classification performance
of 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has
outperformed the state-of-the-art time-invariant technique by at least 14% for
both JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our
LRCN model has demonstrated robustness to noisy input, with a less than 2.5%
dip in average model accuracy and a small standard deviation about the mean for
both species. Lastly, we have performed an ablation study to analyze the
performance of the LRCN model by decreasing the number of temporal session data
used for training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1"&gt;Shiva Azimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1"&gt;Rohan Wadhawan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1"&gt;Tapan K. Gandhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAOD: Budget-Aware Object Detection. (arXiv:1904.05443v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.05443</id>
        <link href="http://arxiv.org/abs/1904.05443"/>
        <updated>2021-08-10T02:00:08.914Z</updated>
        <summary type="html"><![CDATA[We study the problem of object detection from a novel perspective in which
annotation budget constraints are taken into consideration, appropriately
coined Budget Aware Object Detection (BAOD). When provided with a fixed budget,
we propose a strategy for building a diverse and informative dataset that can
be used to optimally train a robust detector. We investigate both optimization
and learning-based methods to sample which images to annotate and what type of
annotation (strongly or weakly supervised) to annotate them with. We adopt a
hybrid supervised learning framework to train the object detector from both
these types of annotation. We conduct a comprehensive empirical study showing
that a handcrafted optimization method outperforms other selection techniques
including random sampling, uncertainty sampling and active learning. By
combining an optimal image/annotation selection scheme with hybrid supervised
learning to solve the BAOD problem, we show that one can achieve the
performance of a strongly supervised detector on PASCAL-VOC 2007 while saving
12.8% of its original annotation budget. Furthermore, when $100\%$ of the
budget is used, it surpasses this performance by 2.0 mAP percentage points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1"&gt;Alejandro Pardo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengmeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1"&gt;Ali Thabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08142</id>
        <link href="http://arxiv.org/abs/2107.08142"/>
        <updated>2021-08-10T02:00:08.895Z</updated>
        <summary type="html"><![CDATA[Despite the numerous successes of machine learning over the past decade
(image recognition, decision-making, NLP, image synthesis), self-driving
technology has not yet followed the same trend. In this paper, we study the
history, composition, and development bottlenecks of the modern self-driving
stack. We argue that the slow progress is caused by approaches that require too
much hand-engineering, an over-reliance on road testing, and high fleet
deployment costs. We observe that the classical stack has several bottlenecks
that preclude the necessary scale needed to capture the long tail of rare
events. To resolve these problems, we outline the principles of Autonomy 2.0,
an ML-first approach to self-driving, as a viable alternative to the currently
adopted state-of-the-art. This approach is based on (i) a fully differentiable
AV stack trainable from human demonstrations, (ii) closed-loop data-driven
reactive simulation, and (iii) large-scale, low-cost data collections as
critical solutions towards scalability issues. We outline the general
architecture, survey promising works in this direction and propose key
challenges to be addressed by the community in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Ashesh Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1"&gt;Luca Del Pero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1"&gt;Hugo Grimmett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1"&gt;Peter Ondruska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Refinement Network for Point Cloud Completion with Self-supervision. (arXiv:2010.08719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08719</id>
        <link href="http://arxiv.org/abs/2010.08719"/>
        <updated>2021-08-10T02:00:08.885Z</updated>
        <summary type="html"><![CDATA[Point clouds are often sparse and incomplete, which imposes difficulties for
real-world applications. Existing shape completion methods tend to generate
rough shapes without fine-grained details. Considering this, we introduce a
two-branch network for shape completion. The first branch is a cascaded shape
completion sub-network to synthesize complete objects, where we propose to use
the partial input together with the coarse output to preserve the object
details during the dense point reconstruction. The second branch is an
auto-encoder to reconstruct the original partial input. The two branches share
a same feature extractor to learn an accurate global feature for shape
completion. Furthermore, we propose two strategies to enable the training of
our network when ground truth data are not available. This is to mitigate the
dependence of existing approaches on large amounts of ground truth training
data that are often difficult to obtain in real-world applications.
Additionally, our proposed strategies are also able to improve the
reconstruction quality for fully supervised learning. We verify our approach in
self-supervised, semi-supervised and fully supervised settings with superior
performances. Quantitative and qualitative results on different datasets
demonstrate that our method achieves more realistic outputs than
state-of-the-art approaches on the point cloud completion task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transductive Few-Shot Classification on the Oblique Manifold. (arXiv:2108.04009v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04009</id>
        <link href="http://arxiv.org/abs/2108.04009"/>
        <updated>2021-08-10T02:00:08.877Z</updated>
        <summary type="html"><![CDATA[Few-shot learning (FSL) attempts to learn with limited data. In this work, we
perform the feature extraction in the Euclidean space and the geodesic distance
metric on the Oblique Manifold (OM). Specially, for better feature extraction,
we propose a non-parametric Region Self-attention with Spatial Pyramid Pooling
(RSSPP), which realizes a trade-off between the generalization and the
discriminative ability of the single image feature. Then, we embed the feature
to OM as a point. Furthermore, we design an Oblique Distance-based Classifier
(ODC) that achieves classification in the tangent spaces which better
approximate OM locally by learnable tangency points. Finally, we introduce a
new method for parameters initialization and a novel loss function in the
transductive settings. Extensive experiments demonstrate the effectiveness of
our algorithm and it outperforms state-of-the-art methods on the popular
benchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011
(CUB).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1"&gt;Guodong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Huimin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhaohui Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuzhao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Annotation for Visual Tracking via Selection and Refinement. (arXiv:2108.03821v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03821</id>
        <link href="http://arxiv.org/abs/2108.03821"/>
        <updated>2021-08-10T02:00:08.871Z</updated>
        <summary type="html"><![CDATA[Deep learning based visual trackers entail offline pre-training on large
volumes of video datasets with accurate bounding box annotations that are
labor-expensive to achieve. We present a new framework to facilitate bounding
box annotations for video sequences, which investigates a
selection-and-refinement strategy to automatically improve the preliminary
annotations generated by tracking algorithms. A temporal assessment network
(T-Assess Net) is proposed which is able to capture the temporal coherence of
target locations and select reliable tracking results by measuring their
quality. Meanwhile, a visual-geometry refinement network (VG-Refine Net) is
also designed to further enhance the selected tracking results by considering
both target appearance and temporal geometry constraints, allowing inaccurate
tracking results to be corrected. The combination of the above two networks
provides a principled approach to ensure the quality of automatic video
annotation. Experiments on large scale tracking benchmarks demonstrate that our
method can deliver highly accurate bounding box annotations and significantly
reduce human labor by 94.0%, yielding an effective means to further boost
tracking performance with augmented training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_K/0/1/0/all/0/1"&gt;Kenan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jie Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianhua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xuesheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransForensics: Image Forgery Localization with Dense Self-Attention. (arXiv:2108.03871v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03871</id>
        <link href="http://arxiv.org/abs/2108.03871"/>
        <updated>2021-08-10T02:00:08.868Z</updated>
        <summary type="html"><![CDATA[Nowadays advanced image editing tools and technical skills produce tampered
images more realistically, which can easily evade image forensic systems and
make authenticity verification of images more difficult. To tackle this
challenging problem, we introduce TransForensics, a novel image forgery
localization method inspired by Transformers. The two major components in our
framework are dense self-attention encoders and dense correction modules. The
former is to model global context and all pairwise interactions between local
patches at different scales, while the latter is used for improving the
transparency of the hidden layers and correcting the outputs from different
branches. Compared to previous traditional and deep learning methods,
TransForensics not only can capture discriminative representations and obtain
high-quality mask predictions but is also not limited by tampering types and
patch sequence orders. By conducting experiments on main benchmarks, we show
that TransForensics outperforms the stateof-the-art methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jing Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shicai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1"&gt;Di Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble neuroevolution based approach for multivariate time series anomaly detection. (arXiv:2108.03585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03585</id>
        <link href="http://arxiv.org/abs/2108.03585"/>
        <updated>2021-08-10T02:00:08.861Z</updated>
        <summary type="html"><![CDATA[Multivariate time series anomaly detection is a very common problem in the
field of failure prevention. Fast prevention means lower repair costs and
losses. The amount of sensors in novel industry systems makes the anomaly
detection process quite difficult for humans. Algorithms which automates the
process of detecting anomalies are crucial in modern failure-prevention
systems. Therefore, many machine and deep learning models have been designed to
address this problem. Mostly, they are autoencoder-based architectures with
some generative adversarial elements. In this work, a framework is shown which
incorporates neuroevolution methods to boost the anomaly-detection scores of
new and already known models. The presented approach adapts evolution
strategies for evolving ensemble model, in which every single model works on a
subgroup of data sensors. The next goal of neuroevolution is to optimise
architecture and hyperparameters like window size, the number of layers, layer
depths, etc. The proposed framework shows that it is possible to boost most of
the anomaly detection deep learning models in a reasonable time and a fully
automated mode. The tests were run on SWAT and WADI datasets. To our knowledge,
this is the first approach in which an ensemble deep learning anomaly detection
model is built in a fully automatic way using a neuroevolution strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faber_K/0/1/0/all/0/1"&gt;Kamil Faber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zurek_D/0/1/0/all/0/1"&gt;Dominik &amp;#x17b;urek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1"&gt;Marcin Pietro&amp;#x144;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietak_K/0/1/0/all/0/1"&gt;Kamil Pi&amp;#x119;tak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Relational Algebra for Machine Learning System Design. (arXiv:2009.00524v3 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00524</id>
        <link href="http://arxiv.org/abs/2009.00524"/>
        <updated>2021-08-10T02:00:08.853Z</updated>
        <summary type="html"><![CDATA[We consider the question: what is the abstraction that should be implemented
by the computational engine of a machine learning system? Current machine
learning systems typically push whole tensors through a series of compute
kernels such as matrix multiplications or activation functions, where each
kernel runs on an AI accelerator (ASIC) such as a GPU. This implementation
abstraction provides little built-in support for ML systems to scale past a
single machine, or for handling large models with matrices or tensors that do
not easily fit into the RAM of an ASIC. In this paper, we present an
alternative implementation abstraction called the tensor relational algebra
(TRA). The TRA is a set-based algebra based on the relational algebra.
Expressions in the TRA operate over binary tensor relations, where keys are
multi-dimensional arrays and values are tensors. The TRA is easily executed
with high efficiency in a parallel or distributed environment, and amenable to
automatic optimization. Our empirical study shows that the optimized TRA-based
back-end can significantly outperform alternatives for running ML workflows in
distributed clusters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Binhang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jankov_D/0/1/0/all/0/1"&gt;Dimitrije Jankov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Jia Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yuxin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourgeois_D/0/1/0/all/0/1"&gt;Daniel Bourgeois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1"&gt;Chris Jermaine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DanceIt: Music-inspired Dancing Video Synthesis. (arXiv:2009.08027v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08027</id>
        <link href="http://arxiv.org/abs/2009.08027"/>
        <updated>2021-08-10T02:00:08.834Z</updated>
        <summary type="html"><![CDATA[Close your eyes and listen to music, one can easily imagine an actor dancing
rhythmically along with the music. These dance movements are usually made up of
dance movements you have seen before. In this paper, we propose to reproduce
such an inherent capability of the human-being within a computer vision system.
The proposed system consists of three modules. To explore the relationship
between music and dance movements, we propose a cross-modal alignment module
that focuses on dancing video clips, accompanied on pre-designed music, to
learn a system that can judge the consistency between the visual features of
pose sequences and the acoustic features of music. The learned model is then
used in the imagination module to select a pose sequence for the given music.
Such pose sequence selected from the music, however, is usually discontinuous.
To solve this problem, in the spatial-temporal alignment module we develop a
spatial alignment algorithm based on the tendency and periodicity of dance
movements to predict dance movements between discontinuous fragments. In
addition, the selected pose sequence is often misaligned with the music beat.
To solve this problem, we further develop a temporal alignment algorithm to
align the rhythm of music and dance. Finally, the processed pose sequence is
used to synthesize realistic dancing videos in the imagination module. The
generated dancing videos match the content and rhythm of the music.
Experimental results and subjective evaluations show that the proposed approach
can perform the function of generating promising dancing videos by inputting
music.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complementary Patch for Weakly Supervised Semantic Segmentation. (arXiv:2108.03852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03852</id>
        <link href="http://arxiv.org/abs/2108.03852"/>
        <updated>2021-08-10T02:00:08.824Z</updated>
        <summary type="html"><![CDATA[Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels
has been greatly advanced by exploiting the outputs of Class Activation Map
(CAM) to generate the pseudo labels for semantic segmentation. However, CAM
merely discovers seeds from a small number of regions, which may be
insufficient to serve as pseudo masks for semantic segmentation. In this paper,
we formulate the expansion of object regions in CAM as an increase in
information. From the perspective of information theory, we propose a novel
Complementary Patch (CP) Representation and prove that the information of the
sum of the CAMs by a pair of input images with complementary hidden (patched)
parts, namely CP Pair, is greater than or equal to the information of the
baseline CAM. Therefore, a CAM with more information related to object seeds
can be obtained by narrowing down the gap between the sum of CAMs generated by
the CP Pair and the original CAM. We propose a CP Network (CPN) implemented by
a triplet network and three regularization functions. To further improve the
quality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to
augment the contextual information by using object-region relations between the
feature maps and the CAMs. Experimental results on the PASCAL VOC 2012 datasets
show that our proposed method achieves a new state-of-the-art in WSSS,
validating the effectiveness of our CP Representation and CPN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1"&gt;Chaochen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chenyue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuchao Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Novel Target Discovery Through Open-Set Domain Adaptation. (arXiv:2105.02432v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02432</id>
        <link href="http://arxiv.org/abs/2105.02432"/>
        <updated>2021-08-10T02:00:08.815Z</updated>
        <summary type="html"><![CDATA[Open-set domain adaptation (OSDA) considers that the target domain contains
samples from novel categories unobserved in external source domain.
Unfortunately, existing OSDA methods always ignore the demand for the
information of unseen categories and simply recognize them as "unknown" set
without further explanation. This motivates us to understand the unknown
categories more specifically by exploring the underlying structures and
recovering their interpretable semantic attributes. In this paper, we propose a
novel framework to accurately identify the seen categories in target domain,
and effectively recover the semantic attributes for unseen categories.
Specifically, structure preserving partial alignment is developed to recognize
the seen categories through domain-invariant feature learning. Attribute
propagation over visual graph is designed to smoothly transit attributes from
seen to unseen categories via visual-semantic mapping. Moreover, two new
cross-main benchmarks are constructed to evaluate the proposed framework in the
novel and practical challenge. Experimental results on open-set recognition and
semantic recovery demonstrate the superiority of the proposed method over other
compared baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_T/0/1/0/all/0/1"&gt;Taotao Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhengming Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pedestrian Trajectory Prediction using Context-Augmented Transformer Networks. (arXiv:2012.01757v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01757</id>
        <link href="http://arxiv.org/abs/2012.01757"/>
        <updated>2021-08-10T02:00:08.807Z</updated>
        <summary type="html"><![CDATA[Forecasting the trajectory of pedestrians in shared urban traffic
environments is still considered one of the challenging problems facing the
development of autonomous vehicles (AVs). In the literature, this problem is
often tackled using recurrent neural networks (RNNs). Despite the powerful
capabilities of RNNs in capturing the temporal dependency in the pedestrians'
motion trajectories, they were argued to be challenged when dealing with longer
sequential data. Thus, in this work, we are introducing a framework based on
the transformer networks that were shown recently to be more efficient and
outperformed RNNs in many sequential-based tasks. We relied on a fusion of the
past positional information, agent interactions information and scene physical
semantics information as an input to our framework in order to provide a robust
trajectory prediction of pedestrians. We have evaluated our framework on two
real-life datasets of pedestrians in shared urban traffic environments and it
has outperformed the compared baseline approaches in both short-term and
long-term prediction horizons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saleh_K/0/1/0/all/0/1"&gt;Khaled Saleh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05248</id>
        <link href="http://arxiv.org/abs/2103.05248"/>
        <updated>2021-08-10T02:00:08.790Z</updated>
        <summary type="html"><![CDATA[Recent studies unveil the vulnerabilities of deep ranking models, where an
imperceptible perturbation can trigger dramatic changes in the ranking result.
While previous attempts focus on manipulating absolute ranks of certain
candidates, the possibility of adjusting their relative order remains
under-explored. In this paper, we formulate a new adversarial attack against
deep ranking systems, i.e., the Order Attack, which covertly alters the
relative order among a selected set of candidates according to an
attacker-specified permutation, with limited interference to other unrelated
candidates. Specifically, it is formulated as a triplet-style loss imposing an
inequality chain reflecting the specified permutation. However, direct
optimization of such white-box objective is infeasible in a real-world attack
scenario due to various black-box limitations. To cope with them, we propose a
Short-range Ranking Correlation metric as a surrogate objective for black-box
Order Attack to approximate the white-box method. The Order Attack is evaluated
on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box
and black-box threat models. The black-box attack is also successfully
implemented on a major e-commerce platform. Comprehensive experimental
evaluations demonstrate the effectiveness of the proposed methods, revealing a
new type of ranking model vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02346</id>
        <link href="http://arxiv.org/abs/2012.02346"/>
        <updated>2021-08-10T02:00:08.760Z</updated>
        <summary type="html"><![CDATA[A point cloud serves as a representation of the surface of a
three-dimensional (3D) shape. Deep generative models have been adapted to model
their variations typically using a map from a ball-like set of latent
variables. However, previous approaches did not pay much attention to the
topological structure of a point cloud, despite that a continuous map cannot
express the varying numbers of holes and intersections. Moreover, a point cloud
is often composed of multiple subparts, and it is also difficult to express. In
this study, we propose ChartPointFlow, a flow-based generative model with
multiple latent labels for 3D point clouds. Each label is assigned to points in
an unsupervised manner. Then, a map conditioned on a label is assigned to a
continuous subset of a point cloud, similar to a chart of a manifold. This
enables our proposed model to preserve the topological structure with clear
boundaries, whereas previous approaches tend to generate blurry point clouds
and fail to generate holes. The experimental results demonstrate that
ChartPointFlow achieves state-of-the-art performance in terms of generation and
reconstruction compared with other point cloud generators. Moreover,
ChartPointFlow divides an object into semantic subparts using charts, and it
demonstrates superior performance in case of unsupervised segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1"&gt;Takumi Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takashi Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1"&gt;Kuniaki Uehara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09134</id>
        <link href="http://arxiv.org/abs/2006.09134"/>
        <updated>2021-08-10T02:00:08.752Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) are formulated as minimax game
problems, whereby generators attempt to approach real data distributions by
virtue of adversarial learning against discriminators. The intrinsic problem
complexity poses the challenge to enhance the performance of generative
networks. In this work, we aim to boost model learning from the perspective of
network architectures, by incorporating recent progress on automated
architecture search into GANs. To this end, we propose a fully differentiable
search framework for generative adversarial networks, dubbed alphaGAN. The
searching process is formalized as solving a bi-level minimax optimization
problem, in which the outer-level objective aims for seeking a suitable network
architecture towards pure Nash Equilibrium conditioned on the generator and the
discriminator network parameters optimized with a traditional GAN loss in the
inner level. The entire optimization performs a first-order method by
alternately minimizing the two-level objective in a fully differentiable
manner, enabling architecture search to be completed in an enormous search
space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our
algorithm can obtain high-performing architectures only with 3-GPU hours on a
single GPU in the search space comprised of approximate 2 ? 1011 possible
configurations. We also provide a comprehensive analysis on the behavior of the
searching process and the properties of searched architectures, which would
benefit further research on architectures for generative models. Pretrained
models and codes are available at https://github.com/yuesongtian/AlphaGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuesong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1"&gt;Guinan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A distillation based approach for the diagnosis of diseases. (arXiv:2108.03470v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03470</id>
        <link href="http://arxiv.org/abs/2108.03470"/>
        <updated>2021-08-10T02:00:08.348Z</updated>
        <summary type="html"><![CDATA[Presently, Covid-19 is a serious threat to the world at large. Efforts are
being made to reduce disease screening times and in the development of a
vaccine to resist this disease, even as thousands succumb to it everyday. We
propose a novel method of automated screening of diseases like Covid-19 and
pneumonia from Chest X-Ray images with the help of Computer Vision. Unlike
computer vision classification algorithms which come with heavy computational
costs, we propose a knowledge distillation based approach which allows us to
bring down the model depth, while preserving the accuracy. We make use of an
augmentation of the standard distillation module with an auxiliary intermediate
assistant network that aids in the continuity of the flow of information.
Following this approach, we are able to build an extremely light student
network, consisting of just 3 convolutional blocks without any compromise on
accuracy. We thus propose a method of classification of diseases which can not
only lead to faster screening, but can also operate seamlessly on low-end
devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_H/0/1/0/all/0/1"&gt;Hmrishav Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dastidar_S/0/1/0/all/0/1"&gt;Shuvayan Ghosh Dastidar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_B/0/1/0/all/0/1"&gt;Bisakh Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Biplab Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1"&gt;Nibaran Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Segmentation and Object Detection Towards Instance Segmentation: Breast Tumor Identification. (arXiv:2108.03287v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03287</id>
        <link href="http://arxiv.org/abs/2108.03287"/>
        <updated>2021-08-10T02:00:08.341Z</updated>
        <summary type="html"><![CDATA[Breast cancer is one of the factors that cause the increase of mortality of
women. The most widely used method for diagnosing this geological disease i.e.
breast cancer is the ultrasound scan. Several key features such as the
smoothness and the texture of the tumor captured through ultrasound scans
encode the abnormality of the breast tumors (malignant from benign). However,
ultrasound scans are often noisy and include irrelevant parts of the breast
that may bias the segmentation of eventual tumors. In this paper, we are going
to extract the region of interest ( i.e, bounding boxes of the tumors) and
feed-forward them to one semantic segmentation encoder-decoder structure based
on its classification (i.e, malignant or benign). the whole process aims to
build an instance-based segmenter from a semantic segmenter and an object
detector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mejri_M/0/1/0/all/0/1"&gt;Mohamed Mejri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mejri_A/0/1/0/all/0/1"&gt;Aymen Mejri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mejri_O/0/1/0/all/0/1"&gt;Oumayma Mejri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fekih_C/0/1/0/all/0/1"&gt;Chiraz Fekih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Shot Object Affordance Detection in the Wild. (arXiv:2108.03658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03658</id>
        <link href="http://arxiv.org/abs/2108.03658"/>
        <updated>2021-08-10T02:00:08.330Z</updated>
        <summary type="html"><![CDATA[Affordance detection refers to identifying the potential action possibilities
of objects in an image, which is a crucial ability for robot perception and
manipulation. To empower robots with this ability in unseen scenarios, we first
study the challenging one-shot affordance detection problem in this paper,
i.e., given a support image that depicts the action purpose, all objects in a
scene with the common affordance should be detected. To this end, we devise a
One-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the
human action purpose and then transfers it to help detect the common affordance
from all candidate images. Through collaboration learning, OSAD-Net can capture
the common characteristics between objects having the same underlying
affordance and learn a good adaptation capability for perceiving unseen
affordances. Besides, we build a large-scale Purpose-driven Affordance Dataset
v2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103
object categories. With complex scenes and rich annotations, our PADv2 dataset
can be used as a test bed to benchmark affordance detection methods and may
also facilitate downstream vision tasks, such as scene understanding, action
recognition, and robot manipulation. Specifically, we conducted comprehensive
experiments on PADv2 dataset by including 11 advanced models from several
related research fields. Experimental results demonstrate the superiority of
our model over previous representative ones in terms of both objective metrics
and visual quality. The benchmark suite is available at
https://github.com/lhc1224/OSAD Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1"&gt;Wei Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hongchen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhanced Invertible Encoding for Learned Image Compression. (arXiv:2108.03690v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03690</id>
        <link href="http://arxiv.org/abs/2108.03690"/>
        <updated>2021-08-10T02:00:08.308Z</updated>
        <summary type="html"><![CDATA[Although deep learning based image compression methods have achieved
promising progress these days, the performance of these methods still cannot
match the latest compression standard Versatile Video Coding (VVC). Most of the
recent developments focus on designing a more accurate and flexible entropy
model that can better parameterize the distributions of the latent features.
However, few efforts are devoted to structuring a better transformation between
the image space and the latent feature space. In this paper, instead of
employing previous autoencoder style networks to build this transformation, we
propose an enhanced Invertible Encoding Network with invertible neural networks
(INNs) to largely mitigate the information loss problem for better compression.
Experimental results on the Kodak, CLIC, and Tecnick datasets show that our
method outperforms the existing learned image compression methods and
compression standards, including VVC (VTM 12.1), especially for high-resolution
images. Our source code is available at https://github.com/xyq7/InvCompress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yueqi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Ka Leong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder. (arXiv:2108.03648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03648</id>
        <link href="http://arxiv.org/abs/2108.03648"/>
        <updated>2021-08-10T02:00:08.281Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an Intersection-over-Union (IoU) guided two-stage
3D object detector with a voxel-to-point decoder. To preserve the necessary
information from all raw points and maintain the high box recall in voxel based
Region Proposal Network (RPN), we propose a residual voxel-to-point decoder to
extract the point features in addition to the map-view features from the voxel
based RPN. We use a 3D Region of Interest (RoI) alignment to crop and align the
features with the proposal boxes for accurately perceiving the object position.
The RoI-Aligned features are finally aggregated with the corner geometry
embeddings that can provide the potentially missing corner information in the
box refinement stage. We propose a simple and efficient method to align the
estimated IoUs to the refined proposal boxes as a more relevant localization
confidence. The comprehensive experiments on KITTI and Waymo Open Dataset
demonstrate that our method achieves significant improvements with novel
architectures against the existing methods. The code is available on Github
URL\footnote{\url{https://github.com/jialeli1/From-Voxel-to-Point}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiale Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yong Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach. (arXiv:2108.03596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03596</id>
        <link href="http://arxiv.org/abs/2108.03596"/>
        <updated>2021-08-10T02:00:08.251Z</updated>
        <summary type="html"><![CDATA[Chinese character style transfer is a very challenging problem because of the
complexity of the glyph shapes or underlying structures and large numbers of
existed characters, when comparing with English letters. Moreover, the
handwriting of calligraphy masters has a more irregular stroke and is difficult
to obtain in real-world scenarios. Recently, several GAN-based methods have
been proposed for font synthesis, but some of them require numerous reference
data and the other part of them have cumbersome preprocessing steps to divide
the character into different parts to be learned and transferred separately. In
this paper, we propose a simple but powerful end-to-end Chinese calligraphy
font generation framework ZiGAN, which does not require any manual operation or
redundant preprocessing to generate fine-grained target-style characters with
few-shot references. To be specific, a few paired samples from different
character styles are leveraged to attain a fine-grained correlation between
structures underlying different glyphs. To capture valuable style knowledge in
target and strengthen the coarse-grained understanding of character content, we
utilize multiple unpaired samples to align the feature distributions belonging
to different character styles. By doing so, only a few target Chinese
calligraphy characters are needed to generated expected style transferred
characters. Experiments demonstrate that our method has a state-of-the-art
generalization ability in few-shot Chinese character style transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1"&gt;Qi Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bingfeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yi Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Mixup for Domain Adaptive Semantic Segmentation. (arXiv:2108.03557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03557</id>
        <link href="http://arxiv.org/abs/2108.03557"/>
        <updated>2021-08-10T02:00:08.243Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to adapt a model of the labeled
source domain to an unlabeled target domain. Although the domain shifts may
exist in various dimensions such as appearance, textures, etc, the contextual
dependency, which is generally shared across different domains, is neglected by
recent methods. In this paper, we utilize this important clue as explicit prior
knowledge and propose end-to-end Context-Aware Mixup (CAMix) for domain
adaptive semantic segmentation. Firstly, we design a contextual mask generation
strategy by leveraging accumulated spatial distributions and contextual
relationships. The generated contextual mask is critical in this work and will
guide the domain mixup. In addition, we define the significance mask to
indicate where the pixels are credible. To alleviate the over-alignment (e.g.,
early performance degradation), the source and target significance masks are
mixed based on the contextual mask into the mixed significance mask, and we
introduce a significance-reweighted consistency loss on it. Experimental
results show that the proposed method outperforms the state-of-the-art methods
by a large margin on two widely-used domain adaptation benchmarks, i.e., GTAV
$\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhengyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Qiqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianping Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03702</id>
        <link href="http://arxiv.org/abs/2108.03702"/>
        <updated>2021-08-10T02:00:08.221Z</updated>
        <summary type="html"><![CDATA[The interest of the machine learning community in image synthesis has grown
significantly in recent years, with the introduction of a wide range of deep
generative models and means for training them. Such machines' ultimate goal is
to match the distributions of the given training images and the synthesized
ones. In this work, we propose a general model-agnostic technique for improving
the image quality and the distribution fidelity of generated images, obtained
by any generative model. Our method, termed BIGRoC (boosting image generation
via a robust classifier), is based on a post-processing procedure via the
guidance of a given robust classifier and without a need for additional
training of the generative model. Given a synthesized image, we propose to
update it through projected gradient steps over the robust classifier, in an
attempt to refine its recognition. We demonstrate this post-processing
algorithm on various image synthesis methods and show a significant improvement
of the generated images, both quantitatively and qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1"&gt;Roy Ganz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1"&gt;Michael Elad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud. (arXiv:2108.03634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03634</id>
        <link href="http://arxiv.org/abs/2108.03634"/>
        <updated>2021-08-10T02:00:08.212Z</updated>
        <summary type="html"><![CDATA[Most of the existing single-stage and two-stage 3D object detectors are
anchor-based methods, while the efficient but challenging anchor-free
single-stage 3D object detection is not well investigated. Recent studies on 2D
object detection show that the anchor-free methods also are of great potential.
However, the unordered and sparse properties of point clouds prevent us from
directly leveraging the advanced 2D methods on 3D point clouds. We overcome
this by converting the voxel-based sparse 3D feature volumes into the sparse 2D
feature maps. We propose an attentive module to fit the sparse feature maps to
dense mostly on the object regions through the deformable convolution tower and
the supervised mask-guided attention. By directly regressing the 3D bounding
box from the enhanced and dense feature maps, we construct a novel single-stage
3D detector for point clouds in an anchor-free manner. We propose an IoU-based
detection confidence re-calibration scheme to improve the correlation between
the detection confidence score and the accuracy of the bounding box regression.
Our code is publicly available at \url{https://github.com/jialeli1/MGAF-3DSSD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiale Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yong Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo DropBlock for Modelling Uncertainty in Object Detection. (arXiv:2108.03614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03614</id>
        <link href="http://arxiv.org/abs/2108.03614"/>
        <updated>2021-08-10T02:00:08.206Z</updated>
        <summary type="html"><![CDATA[With the advancements made in deep learning, computer vision problems like
object detection and segmentation have seen a great improvement in performance.
However, in many real-world applications such as autonomous driving vehicles,
the risk associated with incorrect predictions of objects is very high.
Standard deep learning models for object detection such as YOLO models are
often overconfident in their predictions and do not take into account the
uncertainty in predictions on out-of-distribution data. In this work, we
propose an efficient and effective approach to model uncertainty in object
detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock)
based inference. The proposed approach applies drop-block during training time
and test time on the convolutional layer of the deep learning models such as
YOLO. We show that this leads to a Bayesian convolutional neural network
capable of capturing the epistemic uncertainty in the model. Additionally, we
capture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate
the effectiveness of the proposed approach on modeling uncertainty in object
detection and segmentation tasks using out-of-distribution experiments.
Experimental results show that MC-DropBlock improves the generalization,
calibration, and uncertainty modeling capabilities of YOLO models in object
detection and segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1"&gt;Kumari Deepshikha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yelleni_S/0/1/0/all/0/1"&gt;Sai Harsha Yelleni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1"&gt;P.K. Srijith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1"&gt;C Krishna Mohan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03506</id>
        <link href="http://arxiv.org/abs/2108.03506"/>
        <updated>2021-08-10T02:00:08.199Z</updated>
        <summary type="html"><![CDATA[The vulnerability of the Lottery Ticket Hypothesis has not been studied from
the purview of Membership Inference Attacks. Through this work, we are the
first to empirically show that the lottery ticket networks are equally
vulnerable to membership inference attacks. A Membership Inference Attack (MIA)
is the process of determining whether a data sample belongs to a training set
of a trained model or not. Membership Inference Attacks could leak critical
information about the training data that can be used for targeted attacks.
Recent deep learning models often have very large memory footprints and a high
computational cost associated with training and drawing inferences. Lottery
Ticket Hypothesis is used to prune the networks to find smaller sub-networks
that at least match the performance of the original model in terms of test
accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and
ImageNet datasets to perform image classification tasks and observe that the
attack accuracies are similar. We also see that the attack accuracy varies
directly according to the number of classes in the dataset and the sparsity of
the network. We demonstrate that these attacks are transferable across models
with high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1"&gt;Aadesh Bagmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1"&gt;Shishira R Maiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1"&gt;Shruti Bidwalka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Amol Deshpande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triplet Contrastive Learning for Brain Tumor Classification. (arXiv:2108.03611v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03611</id>
        <link href="http://arxiv.org/abs/2108.03611"/>
        <updated>2021-08-10T02:00:08.192Z</updated>
        <summary type="html"><![CDATA[Brain tumor is a common and fatal form of cancer which affects both adults
and children. The classification of brain tumors into different types is hence
a crucial task, as it greatly influences the treatment that physicians will
prescribe. In light of this, medical imaging techniques, especially those
applying deep convolutional networks followed by a classification layer, have
been developed to make possible computer-aided classification of brain tumor
types. In this paper, we present a novel approach of directly learning deep
embeddings for brain tumor types, which can be used for downstream tasks such
as classification. Along with using triplet loss variants, our approach applies
contrastive learning to performing unsupervised pre-training, combined with a
rare-case data augmentation module to effectively ameliorate the lack of data
problem in the brain tumor imaging analysis domain. We evaluate our method on
an extensive brain tumor dataset which consists of 27 different tumor classes,
out of which 13 are defined as rare. With a common encoder during all the
experiments, we compare our approach with a baseline classification-layer based
model, and the results well prove the effectiveness of our approach across all
measured metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WideCaps: A Wide Attention based Capsule Network for Image Classification. (arXiv:2108.03627v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03627</id>
        <link href="http://arxiv.org/abs/2108.03627"/>
        <updated>2021-08-10T02:00:08.185Z</updated>
        <summary type="html"><![CDATA[The capsule network is a distinct and promising segment of the neural network
family that drew attention due to its unique ability to maintain the
equivariance property by preserving the spatial relationship amongst the
features. The capsule network has attained unprecedented success over image
classification tasks with datasets such as MNIST and affNIST by encoding the
characteristic features into the capsules and building the parse-tree
structure. However, on the datasets involving complex foreground and background
regions such as CIFAR-10, the performance of the capsule network is sub-optimal
due to its naive data routing policy and incompetence towards extracting
complex features. This paper proposes a new design strategy for capsule network
architecture for efficiently dealing with complex images. The proposed method
incorporates wide bottleneck residual modules and the Squeeze and Excitation
attention blocks upheld by the modified FM routing algorithm to address the
defined problem. A wide bottleneck residual module facilitates extracting
complex features followed by the squeeze and excitation attention block to
enable channel-wise attention by suppressing the trivial features. This setup
allows channel inter-dependencies at almost no computational cost, thereby
enhancing the representation ability of capsules on complex images. We
extensively evaluate the performance of the proposed model on three publicly
available datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the
top-5 performance on CIFAR-10 and Fashion MNIST with highly competitive
performance on the SVHN dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+J_P/0/1/0/all/0/1"&gt;Pawan S J&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1"&gt;Rishi Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_H/0/1/0/all/0/1"&gt;Hemanth Sai Ram Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vani_M/0/1/0/all/0/1"&gt;M Vani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_J/0/1/0/all/0/1"&gt;Jeny Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Inductive and Transductive Learning for Video Object Segmentation. (arXiv:2108.03679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03679</id>
        <link href="http://arxiv.org/abs/2108.03679"/>
        <updated>2021-08-10T02:00:08.164Z</updated>
        <summary type="html"><![CDATA[Semi-supervised video object segmentation is a task of segmenting the target
object in a video sequence given only a mask annotation in the first frame. The
limited information available makes it an extremely challenging task. Most
previous best-performing methods adopt matching-based transductive reasoning or
online inductive learning. Nevertheless, they are either less discriminative
for similar instances or insufficient in the utilization of spatio-temporal
information. In this work, we propose to integrate transductive and inductive
learning into a unified framework to exploit the complementarity between them
for accurate and robust video object segmentation. The proposed approach
consists of two functional branches. The transduction branch adopts a
lightweight transformer architecture to aggregate rich spatio-temporal cues
while the induction branch performs online inductive learning to obtain
discriminative target information. To bridge these two diverse branches, a
two-head label encoder is introduced to learn the suitable target prior for
each of them. The generated mask encodings are further forced to be
disentangled to better retain their complementarity. Extensive experiments on
several prevalent benchmarks show that, without the need of synthetic training
data, the proposed approach sets a series of new state-of-the-art records. Code
is available at https://github.com/maoyunyao/JOINT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yunyao Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments. (arXiv:2108.03332v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03332</id>
        <link href="http://arxiv.org/abs/2108.03332"/>
        <updated>2021-08-10T02:00:08.158Z</updated>
        <summary type="html"><![CDATA[We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1"&gt;Michael Lingelbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1"&gt;Zheng Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1"&gt;Cem Gokmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1"&gt;Hyowon Gweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03649</id>
        <link href="http://arxiv.org/abs/2108.03649"/>
        <updated>2021-08-10T02:00:08.148Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to joint depth and normal estimation for
time-of-flight (ToF) sensors. Our model learns to predict the high-quality
depth and normal maps jointly from ToF raw sensor data. To achieve this, we
meticulously constructed the first large-scale dataset (named ToF-100) with
paired raw ToF data and ground-truth high-resolution depth maps provided by an
industrial depth camera. In addition, we also design a simple but effective
framework for joint depth and normal estimation, applying a robust Chamfer loss
via jittering to improve the performance of our model. Our experiments
demonstrate that our proposed method can efficiently reconstruct
high-resolution depth and normal maps and significantly outperforms
state-of-the-art approaches. Our code and data will be available at
\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Rongrong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1"&gt;Na Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changlin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Light Field Reconstruction via Spatio-Angular Dense Network. (arXiv:2108.03635v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03635</id>
        <link href="http://arxiv.org/abs/2108.03635"/>
        <updated>2021-08-10T02:00:08.138Z</updated>
        <summary type="html"><![CDATA[As an image sensing instrument, light field images can supply extra angular
information compared with monocular images and have facilitated a wide range of
measurement applications. Light field image capturing devices usually suffer
from the inherent trade-off between the angular and spatial resolutions. To
tackle this problem, several methods, such as light field reconstruction and
light field super-resolution, have been proposed but leaving two problems
unaddressed, namely domain asymmetry and efficient information flow. In this
paper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for
light field reconstruction with two novel components, namely correlation blocks
and spatio-angular dense skip connections to address them. The former performs
effective modeling of the correlation information in a way that conforms with
the domain asymmetry. And the latter consists of three kinds of connections
enhancing the information flow within two domains. Extensive experiments on
both real-world and synthetic datasets have been conducted to demonstrate that
the proposed SADenseNet's state-of-the-art performance at significantly reduced
costs in memory and computation. The qualitative results show that the
reconstructed light field images are sharp with correct details and can serve
as pre-processing to improve the accuracy of related measurement applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zexi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_H/0/1/0/all/0/1"&gt;Henry Wing Fung Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1"&gt;Yuk Ying Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haisheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepFH Segmentations for Superpixel-based Object Proposal Refinement. (arXiv:2108.03503v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03503</id>
        <link href="http://arxiv.org/abs/2108.03503"/>
        <updated>2021-08-10T02:00:08.114Z</updated>
        <summary type="html"><![CDATA[Class-agnostic object proposal generation is an important first step in many
object detection pipelines. However, object proposals of modern systems are
rather inaccurate in terms of segmentation and only roughly adhere to object
boundaries. Since typical refinement steps are usually not applicable to
thousands of proposals, we propose a superpixel-based refinement system for
object proposal generation systems. Utilizing precise superpixels and
superpixel pooling on deep features, we refine initial coarse proposals in an
end-to-end learned system. Furthermore, we propose a novel DeepFH segmentation,
which enriches the classic Felzenszwalb and Huttenlocher (FH) segmentation with
deep features leading to improved segmentation results and better object
proposal refinements. On the COCO dataset with LVIS annotations, we show that
our refinement based on DeepFH superpixels outperforms state-of-the-art methods
and leads to more precise object proposals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1"&gt;Christian Wilms&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1"&gt;Simone Frintrop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Discriminative Representation Learning for Unsupervised Person Re-identification. (arXiv:2108.03439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03439</id>
        <link href="http://arxiv.org/abs/2108.03439"/>
        <updated>2021-08-10T02:00:08.091Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of unsupervised domain adaptation for
person re-ID where annotations are available for the source domain but not for
target. Previous methods typically follow a two-stage optimization pipeline,
where the network is first pre-trained on source and then fine-tuned on target
with pseudo labels created by feature clustering. Such methods sustain two main
limitations. (1) The label noise may hinder the learning of discriminative
features for recognizing target classes. (2) The domain gap may hinder
knowledge transferring from source to target. We propose three types of
technical schemes to alleviate these issues. First, we propose a cluster-wise
contrastive learning algorithm (CCL) by iterative optimization of feature
learning and cluster refinery to learn noise-tolerant representations in the
unsupervised manner. Second, we adopt a progressive domain adaptation (PDA)
strategy to gradually mitigate the domain gap between source and target data.
Third, we propose Fourier augmentation (FA) for further maximizing the class
separability of re-ID models by imposing extra constraints in the Fourier
space. We observe that these proposed schemes are capable of facilitating the
learning of discriminative feature representations. Experiments demonstrate
that our method consistently achieves notable improvements over the
state-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,
surpassing MMT largely by 8.1\%, 9.9\%, 11.4\% and 11.1\% mAP on the
Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1"&gt;Takashi Isobe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Lu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weihua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Yi Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengjin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visible Watermark Removal via Self-calibrated Localization and Background Refinement. (arXiv:2108.03581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03581</id>
        <link href="http://arxiv.org/abs/2108.03581"/>
        <updated>2021-08-10T02:00:08.077Z</updated>
        <summary type="html"><![CDATA[Superimposing visible watermarks on images provides a powerful weapon to cope
with the copyright issue. Watermark removal techniques, which can strengthen
the robustness of visible watermarks in an adversarial way, have attracted
increasing research interest. Modern watermark removal methods perform
watermark localization and background restoration simultaneously, which could
be viewed as a multi-task learning problem. However, existing approaches suffer
from incomplete detected watermark and degraded texture quality of restored
background. Therefore, we design a two-stage multi-task network to address the
above issues. The coarse stage consists of a watermark branch and a background
branch, in which the watermark branch self-calibrates the roughly estimated
mask and passes the calibrated mask to background branch to reconstruct the
watermarked area. In the refinement stage, we integrate multi-level features to
improve the texture quality of watermarked area. Extensive experiments on two
datasets demonstrate the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1"&gt;Li Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1"&gt;Fengjun Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1"&gt;Teng Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liqing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck Approach to Spatial Attention Learning. (arXiv:2108.03418v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03418</id>
        <link href="http://arxiv.org/abs/2108.03418"/>
        <updated>2021-08-10T02:00:08.065Z</updated>
        <summary type="html"><![CDATA[The selective visual attention mechanism in the human visual system (HVS)
restricts the amount of information to reach visual awareness for perceiving
natural scenes, allowing near real-time information processing with limited
computational capacity [Koch and Ullman, 1987]. This kind of selectivity acts
as an 'Information Bottleneck (IB)', which seeks a trade-off between
information compression and predictive accuracy. However, such information
constraints are rarely explored in the attention mechanism for deep neural
networks (DNNs). In this paper, we propose an IB-inspired spatial attention
module for DNN structures built for visual recognition. The module takes as
input an intermediate representation of the input image, and outputs a
variational 2D attention map that minimizes the mutual information (MI) between
the attention-modulated representation and the input, while maximizing the MI
between the attention-modulated representation and the task label. To further
restrict the information bypassed by the attention map, we quantize the
continuous attention scores to a set of learnable anchor values during
training. Extensive experiments show that the proposed IB-inspired spatial
attention mechanism can yield attention maps that neatly highlight the regions
of interest while suppressing backgrounds, and bootstrap standard DNN
structures for visual recognition tasks (e.g., image classification,
fine-grained recognition, cross-domain classification). The attention maps are
interpretable for the decision making of the DNNs as verified in the
experiments. Our code is available at https://github.com/ashleylqx/AIB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1"&gt;Qiuxia Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hanqiu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ContinuityLearner: Geometric Continuity Feature Learning for Lane Segmentation. (arXiv:2108.03507v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03507</id>
        <link href="http://arxiv.org/abs/2108.03507"/>
        <updated>2021-08-10T02:00:08.055Z</updated>
        <summary type="html"><![CDATA[Lane segmentation is a challenging issue in autonomous driving system
designing because lane marks show weak textural consistency due to occlusion or
extreme illumination but strong geometric continuity in traffic images, from
which general convolution neural networks (CNNs) are not capable of learning
semantic objects. To empower conventional CNNs in learning geometric clues of
lanes, we propose a deep network named ContinuityLearner to better learn
geometric prior within lane. Specifically, our proposed CNN-based paradigm
involves a novel Context-encoding image feature learning network to generate
class-dependent image feature maps and a new encoding layer to exploit the
geometric continuity feature representation by fusing both spatial and visual
information of lane together. The ContinuityLearner, performing on the
geometric continuity feature of lanes, is trained to directly predict the lane
in traffic scenarios with integrated and continuous instance semantic. The
experimental results on the CULane dataset and the Tusimple benchmark
demonstrate that our ContinuityLearner has superior performance over other
state-of-the-art techniques in lane segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Haoyu Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled High Quality Salient Object Detection. (arXiv:2108.03551v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03551</id>
        <link href="http://arxiv.org/abs/2108.03551"/>
        <updated>2021-08-10T02:00:08.046Z</updated>
        <summary type="html"><![CDATA[Aiming at discovering and locating most distinctive objects from visual
scenes, salient object detection (SOD) plays an essential role in various
computer vision systems. Coming to the era of high resolution, SOD methods are
facing new challenges. The major limitation of previous methods is that they
try to identify the salient regions and estimate the accurate objects
boundaries simultaneously with a single regression task at low-resolution. This
practice ignores the inherent difference between the two difficult problems,
resulting in poor detection quality. In this paper, we propose a novel deep
learning framework for high-resolution SOD task, which disentangles the task
into a low-resolution saliency classification network (LRSCN) and a
high-resolution refinement network (HRRN). As a pixel-wise classification task,
LRSCN is designed to capture sufficient semantics at low-resolution to identify
the definite salient, background and uncertain image regions. HRRN is a
regression task, which aims at accurately refining the saliency value of pixels
in the uncertain region to preserve a clear object boundary at high-resolution
with limited GPU memory. It is worth noting that by introducing uncertainty
into the training process, our HRRN can well address the high-resolution
refinement task without using any high-resolution training data. Extensive
experiments on high-resolution saliency datasets as well as some widely used
saliency benchmarks show that the proposed method achieves superior performance
compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1"&gt;Lv Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mofei Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03233</id>
        <link href="http://arxiv.org/abs/2108.03233"/>
        <updated>2021-08-10T02:00:08.019Z</updated>
        <summary type="html"><![CDATA[Incorporating boundaries of the imaging object as a priori information to
imaging algorithms can significantly improve the performance of electromagnetic
medical imaging systems. To avoid overly complicating the system by using
different sensors and the adverse effect of the subject's movement, a
learning-based method is proposed to estimate the boundary (external contour)
of the imaged object using the same electromagnetic imaging data. While imaging
techniques may discard the reflection coefficients for being dominant and
uninformative for imaging, these parameters are made use of for boundary
detection. The learned model is verified through independent clinical human
trials by using a head imaging system with a 16-element antenna array that
works across the band 0.7-1.6 GHz. The evaluation demonstrated that the model
achieves average dissimilarity of 0.012 in Hu-moment while detecting head
boundary. The model enables fast scan and image creation while eliminating the
need for additional devices for accurate boundary estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1"&gt;A. Al-Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1"&gt;A. Stancombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1"&gt;A. Zamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1"&gt;A. Abbosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformable Image Registration using Neural ODEs. (arXiv:2108.03443v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03443</id>
        <link href="http://arxiv.org/abs/2108.03443"/>
        <updated>2021-08-10T02:00:08.013Z</updated>
        <summary type="html"><![CDATA[Deformable image registration, aiming to find spatial correspondence between
a given image pair, is one of the most critical problems in the domain of
medical image analysis. In this paper, we present a generic, fast, and accurate
diffeomorphic image registration framework that leverages neural ordinary
differential equations (NODEs). We model each voxel as a moving particle and
consider the set of all voxels in a 3D image as a high-dimensional dynamical
system whose trajectory determines the targeted deformation field. Compared
with traditional optimization-based methods, our framework reduces the running
time from tens of minutes to tens of seconds. Compared with recent data-driven
deep learning methods, our framework is more accessible since it does not
require large amounts of training data. Our experiments show that the
registration results of our method outperform state-of-the-arts under various
metrics, indicating that our modeling approach is well fitted for the task of
deformable image registration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yifan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1"&gt;Tom Z.Jiahao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiancong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1"&gt;Paul A.Yushkevich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1"&gt;James C.Gee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1"&gt;M.Ani Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Portrait Shadow Removal via Generative Priors. (arXiv:2108.03466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03466</id>
        <link href="http://arxiv.org/abs/2108.03466"/>
        <updated>2021-08-10T02:00:07.999Z</updated>
        <summary type="html"><![CDATA[Portrait images often suffer from undesirable shadows cast by casual objects
or even the face itself. While existing methods for portrait shadow removal
require training on a large-scale synthetic dataset, we propose the first
unsupervised method for portrait shadow removal without any training data. Our
key idea is to leverage the generative facial priors embedded in the
off-the-shelf pretrained StyleGAN2. To achieve this, we formulate the shadow
removal task as a layer decomposition problem: a shadowed portrait image is
constructed by the blending of a shadow image and a shadow-free image. We
propose an effective progressive optimization algorithm to learn the
decomposition process. Our approach can also be extended to portrait tattoo
removal and watermark removal. Qualitative and quantitative experiments on a
real-world portrait shadow dataset demonstrate that our approach achieves
comparable performance with supervised shadow removal methods. Our source code
is available at
https://github.com/YingqingHe/Shadow-Removal-via-Generative-Priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yingqing He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1"&gt;Yazhou Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03378</id>
        <link href="http://arxiv.org/abs/2108.03378"/>
        <updated>2021-08-10T02:00:07.987Z</updated>
        <summary type="html"><![CDATA[Reconstructing a layout of indoor spaces has been a crucial part of growing
indoor location based services. One of the key challenges in the proliferation
of indoor location based services is the unavailability of indoor spatial maps
due to the complex nature of capturing an indoor space model (e.g., floor plan)
of an existing building. In this paper, we propose a system to automatically
generate floor plans that can recognize rooms from the point-clouds obtained
through smartphones like Google's Tango. In particular, we propose two
approaches - a Recurrent Neural Network based approach using Pointer Network
and a Convolutional Neural Network based approach using Mask-RCNN to identify
rooms (and thereby floor plans) from point-clouds. Experimental results on
different datasets demonstrate approximately 0.80-0.90 Intersection-over-Union
scores, which show that our models can effectively identify the rooms and
regenerate the shapes of the rooms in heterogeneous environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1"&gt;Md. Tareq Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mohammed Eunus Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03489</id>
        <link href="http://arxiv.org/abs/2108.03489"/>
        <updated>2021-08-10T02:00:07.884Z</updated>
        <summary type="html"><![CDATA[We investigate the impact of aliasing on generalization in Deep Convolutional
Networks and show that data augmentation schemes alone are unable to prevent it
due to structural limitations in widely used architectures. Drawing insights
from frequency analysis theory, we take a closer look at ResNet and
EfficientNet architectures and review the trade-off between aliasing and
information loss in each of their major components. We show how to mitigate
aliasing by inserting non-trainable low-pass filters at key locations,
particularly where networks lack the capacity to learn them. These simple
architectural changes lead to substantial improvements in generalization on
i.i.d. and even more on out-of-distribution conditions, such as image
classification under natural corruptions on ImageNet-C [11] and few-shot
learning on Meta-Dataset [26]. State-of-the art results are achieved on both
datasets without introducing additional trainable parameters and using the
default hyper-parameters of open source codebases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1"&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1"&gt;Rob Romijnders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1"&gt;Ross Goroshin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models. (arXiv:2108.03434v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03434</id>
        <link href="http://arxiv.org/abs/2108.03434"/>
        <updated>2021-08-10T02:00:07.850Z</updated>
        <summary type="html"><![CDATA[Fine-tuning from pre-trained ImageNet models has been a simple, effective,
and popular approach for various computer vision tasks. The common practice of
fine-tuning is to adopt a default hyperparameter setting with a fixed
pre-trained model, while both of them are not optimized for specific tasks and
time constraints. Moreover, in cloud computing or GPU clusters where the tasks
arrive sequentially in a stream, faster online fine-tuning is a more desired
and realistic strategy for saving money, energy consumption, and CO2 emission.
In this paper, we propose a joint Neural Architecture Search and Online
Adaption framework named NASOA towards a faster task-oriented fine-tuning upon
the request of users. Specifically, NASOA first adopts an offline NAS to
identify a group of training-efficient networks to form a pretrained model zoo.
We propose a novel joint block and macro-level search space to enable a
flexible and efficient search. Then, by estimating fine-tuning performance via
an adaptive model by accumulating experience from the past tasks, an online
schedule generator is proposed to pick up the most suitable model and generate
a personalized training regime with respect to each desired task in a one-shot
fashion. The resulting model zoo is more training efficient than SOTA models,
e.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3.
Experiments on multiple datasets also show that NASOA achieves much better
fine-tuning results, i.e. improving around 2.1% accuracy than the best
performance in RegNet series under various constraints and tasks; 40x faster
compared to the BOHB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1"&gt;Ning Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chuanlong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03457</id>
        <link href="http://arxiv.org/abs/2108.03457"/>
        <updated>2021-08-10T02:00:07.716Z</updated>
        <summary type="html"><![CDATA[Existing vision systems for autonomous driving or robots are sensitive to
waterdrops adhered to windows or camera lenses. Most recent waterdrop removal
approaches take a single image as input and often fail to recover the missing
content behind waterdrops faithfully. Thus, we propose a learning-based model
for waterdrop removal with stereo images. To better detect and remove
waterdrops from stereo images, we propose a novel row-wise dilated attention
module to enlarge attention's receptive field for effective information
propagation between the two stereo images. In addition, we propose an attention
consistency loss between the ground-truth disparity map and attention scores to
enhance the left-right consistency in stereo images. Because of related
datasets' unavailability, we collect a real-world dataset that contains stereo
images with and without waterdrops. Extensive experiments on our dataset
suggest that our model outperforms state-of-the-art methods both quantitatively
and qualitatively. Our source code and the stereo waterdrop dataset are
available at
\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zifan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1"&gt;Na Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1"&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeafMask: Towards Greater Accuracy on Leaf Segmentation. (arXiv:2108.03568v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03568</id>
        <link href="http://arxiv.org/abs/2108.03568"/>
        <updated>2021-08-10T02:00:07.688Z</updated>
        <summary type="html"><![CDATA[Leaf segmentation is the most direct and effective way for high-throughput
plant phenotype data analysis and quantitative researches of complex traits.
Currently, the primary goal of plant phenotyping is to raise the accuracy of
the autonomous phenotypic measurement. In this work, we present the LeafMask
neural network, a new end-to-end model to delineate each leaf region and count
the number of leaves, with two main components: 1) the mask assembly module
merging position-sensitive bases of each predicted box after non-maximum
suppression (NMS) and corresponding coefficients to generate original masks; 2)
the mask refining module elaborating leaf boundaries from the mask assembly
module by the point selection strategy and predictor. In addition, we also
design a novel and flexible multi-scale attention module for the dual
attention-guided mask (DAG-Mask) branch to effectively enhance information
expression and produce more accurate bases. Our main contribution is to
generate the final improved masks by combining the mask assembly module with
the mask refining module under the anchor-free instance segmentation paradigm.
We validate our LeafMask through extensive experiments on Leaf Segmentation
Challenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score
outperforming other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1"&gt;Ruohao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liao Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Dantong Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1"&gt;Jun Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Adversarial Disentangling for Specific Domain Adaptation. (arXiv:2108.03553v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03553</id>
        <link href="http://arxiv.org/abs/2108.03553"/>
        <updated>2021-08-10T02:00:07.681Z</updated>
        <summary type="html"><![CDATA[Domain adaptation aims to bridge the domain shifts between the source and
target domains. These shifts may span different dimensions such as fog,
rainfall, etc. However, recent methods typically do not consider explicit prior
knowledge on a specific dimension, thus leading to less desired adaptation
performance. In this paper, we study a practical setting called Specific Domain
Adaptation (SDA) that aligns the source and target domains in a
demanded-specific dimension. Within this setting, we observe the intra-domain
gap induced by different domainness (i.e., numerical magnitudes of this
dimension) is crucial when adapting to a specific domain. To address the
problem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In
particular, given a specific dimension, we first enrich the source domain by
introducing a domainness creator with providing additional supervisory signals.
Guided by the created domainness, we design a self-adversarial regularizer and
two loss functions to jointly disentangle the latent representations into
domainness-specific and domainness-invariant features, thus mitigating the
intra-domain gap. Our method can be easily taken as a plug-and-play framework
and does not introduce any extra costs in the inference time. We achieve
consistent improvements over state-of-the-art methods in both object detection
and semantic segmentation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qianyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Qiqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhengyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1"&gt;Guangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jianping Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Attention Mechanism and Knowledge Distillation for Lip Reading. (arXiv:2108.03543v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03543</id>
        <link href="http://arxiv.org/abs/2108.03543"/>
        <updated>2021-08-10T02:00:07.662Z</updated>
        <summary type="html"><![CDATA[Despite the advancement in the domain of audio and audio-visual speech
recognition, visual speech recognition systems are still quite under-explored
due to the visual ambiguity of some phonemes. In this work, we propose a new
lip-reading model that combines three contributions. First, the model front-end
adopts a spatio-temporal attention mechanism to help extract the informative
data from the input visual frames. Second, the model back-end utilizes a
sequence-level and frame-level Knowledge Distillation (KD) techniques that
allow leveraging audio data during the visual model training. Third, a data
preprocessing pipeline is adopted that includes facial landmarks
detection-based lip-alignment. On LRW lip-reading dataset benchmark, a
noticeable accuracy improvement is demonstrated; the spatio-temporal attention,
Knowledge Distillation, and lip-alignment contributions achieved 88.43%,
88.64%, and 88.37% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elashmawy_S/0/1/0/all/0/1"&gt;Shahd Elashmawy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramsis_M/0/1/0/all/0/1"&gt;Marian Ramsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldeshnawy_F/0/1/0/all/0/1"&gt;Farah Eldeshnawy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mabrouk_H/0/1/0/all/0/1"&gt;Hadeel Mabrouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abugabal_O/0/1/0/all/0/1"&gt;Omar Abugabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakr_N/0/1/0/all/0/1"&gt;Nourhan Sakr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Just) A Spoonful of Refinements Helps the Registration Error Go Down. (arXiv:2108.03257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03257</id>
        <link href="http://arxiv.org/abs/2108.03257"/>
        <updated>2021-08-10T02:00:07.656Z</updated>
        <summary type="html"><![CDATA[We tackle data-driven 3D point cloud registration. Given point
correspondences, the standard Kabsch algorithm provides an optimal rotation
estimate. This allows to train registration models in an end-to-end manner by
differentiating the SVD operation. However, given the initial rotation estimate
supplied by Kabsch, we show we can improve point correspondence learning during
model training by extending the original optimization problem. In particular,
we linearize the governing constraints of the rotation matrix and solve the
resulting linear system of equations. We then iteratively produce new solutions
by updating the initial estimate. Our experiments show that, by plugging our
differentiable layer to existing learning-based registration methods, we
improve the correspondence matching quality. This yields up to a 7% decrease in
rotation error for correspondence-based data-driven registration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agostinho_S/0/1/0/all/0/1"&gt;S&amp;#xe9;rgio Agostinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1"&gt;Aljo&amp;#x161;a O&amp;#x161;ep&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1"&gt;Alessio Del Bue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taix&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03541</id>
        <link href="http://arxiv.org/abs/2108.03541"/>
        <updated>2021-08-10T02:00:07.648Z</updated>
        <summary type="html"><![CDATA[Images tell powerful stories but cannot always be trusted. Matching images
back to trusted sources (attribution) enables users to make a more informed
judgment of the images they encounter online. We propose a robust image hashing
algorithm to perform such matching. Our hash is sensitive to manipulation of
subtle, salient visual details that can substantially change the story told by
an image. Yet the hash is invariant to benign transformations (changes in
quality, codecs, sizes, shapes, etc.) experienced by images during online
redistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph
Attention for Image Attribution Network); a robust image hashing model inspired
by recent successes of Transformers in the visual domain. OSCAR-Net constructs
a scene graph representation that attends to fine-grained changes of every
object's visual appearance and their spatial relationships. The network is
trained via contrastive learning on a dataset of original and manipulated
images yielding a state of the art image hash for content fingerprinting that
scales to millions of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1"&gt;Eric Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tu Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1"&gt;Vishy Swaminathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1"&gt;John Collomosse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Categorized Reflection Removal Dataset with Diverse Real-world Scenes. (arXiv:2108.03380v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03380</id>
        <link href="http://arxiv.org/abs/2108.03380"/>
        <updated>2021-08-10T02:00:07.641Z</updated>
        <summary type="html"><![CDATA[Due to the lack of a large-scale reflection removal dataset with diverse
real-world scenes, many existing reflection removal methods are trained on
synthetic data plus a small amount of real-world data, which makes it difficult
to evaluate the strengths or weaknesses of different reflection removal methods
thoroughly. Furthermore, existing real-world benchmarks and datasets do not
categorize image data based on the types and appearances of reflection (e.g.,
smoothness, intensity), making it hard to analyze reflection removal methods.
Hence, we construct a new reflection removal dataset that is categorized,
diverse, and real-world (CDR). A pipeline based on RAW data is used to capture
perfectly aligned input images and transmission images. The dataset is
constructed using diverse glass types under various environments to ensure
diversity. By analyzing several reflection removal methods and conducting
extensive experiments on our dataset, we show that state-of-the-art reflection
removal methods generally perform well on blurry reflection but fail in
obtaining satisfying performance on other types of real-world reflection. We
believe our dataset can help develop novel methods to remove real-world
reflection better. Our dataset is available at
https://alexzhao-hugga.github.io/Real-World-Reflection-Removal/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1"&gt;Chenyang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuhua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1"&gt;Chenyang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yankun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1"&gt;Qiong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Facial Representations from the Cycle-consistency of Face. (arXiv:2108.03427v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03427</id>
        <link href="http://arxiv.org/abs/2108.03427"/>
        <updated>2021-08-10T02:00:07.635Z</updated>
        <summary type="html"><![CDATA[Faces manifest large variations in many aspects, such as identity,
expression, pose, and face styling. Therefore, it is a great challenge to
disentangle and extract these characteristics from facial images, especially in
an unsupervised manner. In this work, we introduce cycle-consistency in facial
characteristics as free supervisory signal to learn facial representations from
unlabeled facial images. The learning is realized by superimposing the facial
motion cycle-consistency and identity cycle-consistency constraints. The main
idea of the facial motion cycle-consistency is that, given a face with
expression, we can perform de-expression to a neutral face via the removal of
facial motion and further perform re-expression to reconstruct back to the
original face. The main idea of the identity cycle-consistency is to exploit
both de-identity into mean face by depriving the given neutral face of its
identity via feature re-normalization and re-identity into neutral face by
adding the personal attributes to the mean face. At training time, our model
learns to disentangle two distinct facial representations to be useful for
performing cycle-consistent face reconstruction. At test time, we use the
linear protocol scheme for evaluating facial representations on various tasks,
including facial expression recognition and head pose regression. We also can
directly apply the learnt facial representations to person recognition,
frontalization and image-to-image translation. Our experiments show that the
results of our approach is competitive with those of existing methods,
demonstrating the rich and unique information embedded in the disentangled
representations. Code is available at https://github.com/JiaRenChang/FaceCycle .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jia-Ren Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yong-Sheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1"&gt;Wei-Chen Chiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03579</id>
        <link href="http://arxiv.org/abs/2108.03579"/>
        <updated>2021-08-10T02:00:07.627Z</updated>
        <summary type="html"><![CDATA[The goals of this paper are two-fold. The first goal is to serve as an
expository tutorial on the working of deep learning models which emphasizes
geometrical intuition about the reasons for success of deep learning. The
second goal is to complement the current results on the expressive power of
deep learning models and their loss surfaces with novel insights and results.
In particular, we describe how deep neural networks carve out manifolds
especially when the multiplication neurons are introduced. Multiplication is
used in dot products and the attention mechanism and it is employed in capsule
networks and self-attention based transformers. We also describe how random
polynomial, random matrix, spin glass and computational complexity perspectives
on the loss surfaces are interconnected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1"&gt;Simant Dube&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Right to Talk: An Audio-Visual Transformer Approach. (arXiv:2108.03256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03256</id>
        <link href="http://arxiv.org/abs/2108.03256"/>
        <updated>2021-08-10T02:00:07.606Z</updated>
        <summary type="html"><![CDATA[Turn-taking has played an essential role in structuring the regulation of a
conversation. The task of identifying the main speaker (who is properly taking
his/her turn of speaking) and the interrupters (who are interrupting or
reacting to the main speaker's utterances) remains a challenging task. Although
some prior methods have partially addressed this task, there still remain some
limitations. Firstly, a direct association of Audio and Visual features may
limit the correlations to be extracted due to different modalities. Secondly,
the relationship across temporal segments helping to maintain the consistency
of localization, separation, and conversation contexts is not effectively
exploited. Finally, the interactions between speakers that usually contain the
tracking and anticipatory decisions about the transition to a new speaker are
usually ignored. Therefore, this work introduces a new Audio-Visual Transformer
approach to the problem of localization and highlighting the main speaker in
both audio and visual channels of a multi-speaker conversation video in the
wild. The proposed method exploits different types of correlations presented in
both visual and audio signals. The temporal audio-visual relationships across
spatial-temporal space are anticipated and optimized via the self-attention
mechanism in a Transformerstructure. Moreover, a newly collected dataset is
introduced for the main speaker detection. To the best of our knowledge, it is
one of the first studies that is able to automatically localize and highlight
the main speaker in both visual and audio channels in multi-speaker
conversation videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Dat Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, The &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1"&gt;De Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hoang Anh Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-Supervised Action Modality Transfer. (arXiv:2108.03329v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03329</id>
        <link href="http://arxiv.org/abs/2108.03329"/>
        <updated>2021-08-10T02:00:07.599Z</updated>
        <summary type="html"><![CDATA[This paper strives for action recognition and detection in video modalities
like RGB, depth maps or 3D-skeleton sequences when only limited
modality-specific labeled examples are available. For the RGB, and derived
optical-flow, modality many large-scale labeled datasets have been made
available. They have become the de facto pre-training choice when recognizing
or detecting new actions from RGB datasets that have limited amounts of labeled
examples available. Unfortunately, large-scale labeled action datasets for
other modalities are unavailable for pre-training. In this paper, our goal is
to recognize actions from limited examples in non-RGB video modalities, by
learning from large-scale labeled RGB data. To this end, we propose a two-step
training process: (i) we extract action representation knowledge from an
RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we
then fine-tune the transfer model with available labeled examples of the target
modality. For the knowledge transfer we introduce feature-supervision
strategies, which rely on unlabeled pairs of two modalities (the RGB and the
target modality) to transfer feature level representations from the teacher to
the student network. Ablations and generalizations with two RGB source datasets
and two non-RGB target datasets demonstrate that an optical-flow teacher
provides better action transfer features than RGB for both depth maps and
3D-skeletons, even when evaluated on a different target domain, or for a
different task. Compared to alternative cross-modal action transfer methods we
show a good improvement in performance especially when labeled non-RGB examples
to learn from are scarce]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1"&gt;Fida Mohammad Thoker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03375</id>
        <link href="http://arxiv.org/abs/2108.03375"/>
        <updated>2021-08-10T02:00:07.591Z</updated>
        <summary type="html"><![CDATA[Temporal Action Localization (TAL) task in which the aim is to predict the
start and end of each action and its class label has many applications in the
real world. But due to its complexity, researchers have not reached great
results compared to the action recognition task. The complexity is related to
predicting precise start and end times for different actions in any video. In
this paper, we propose a new network based on Gated Recurrent Unit (GRU) and
two novel post-processing ideas for TAL task. Specifically, we propose a new
design for the output layer of the GRU resulting in the so-called GRU-Splitted
model. Moreover, linear interpolation is used to generate the action proposals
with precise start and end times. Finally, to rank the generated proposals
appropriately, we use a Learn to Rank (LTR) approach. We evaluated the
performance of the proposed method on Thumos14 dataset. Results show the
superiority of the performance of the proposed method compared to
state-of-the-art. Especially in the mean Average Precision (mAP) metric at
Intersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than
that of state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1"&gt;Hassan Keshvari Khojasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1"&gt;Hoda Mohammadzade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1"&gt;Hamid Behroozi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00294</id>
        <link href="http://arxiv.org/abs/2101.00294"/>
        <updated>2021-08-10T02:00:07.583Z</updated>
        <summary type="html"><![CDATA[Current open-domain question answering systems often follow a
Retriever-Reader architecture, where the retriever first retrieves relevant
passages and the reader then reads the retrieved passages to form an answer. In
this paper, we propose a simple and effective passage reranking method, named
Reader-guIDEd Reranker (RIDER), which does not involve training and reranks the
retrieved passages solely based on the top predictions of the reader before
reranking. We show that RIDER, despite its simplicity, achieves 10 to 20
absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains
without refining the retriever or reader. In addition, RIDER, without any
training, outperforms state-of-the-art transformer-based supervised rerankers.
Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM
on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are
used as the reader input after passage reranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03272</id>
        <link href="http://arxiv.org/abs/2108.03272"/>
        <updated>2021-08-10T02:00:07.575Z</updated>
        <summary type="html"><![CDATA[Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset will be publicly available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1"&gt;Michael Lingelbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1"&gt;Cem Gokmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1"&gt;Gokul Dharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1"&gt;Tanish Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1"&gt;Andrey Kurenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Karen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1"&gt;Hyowon Gweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03429</id>
        <link href="http://arxiv.org/abs/2108.03429"/>
        <updated>2021-08-10T02:00:07.555Z</updated>
        <summary type="html"><![CDATA[The success of neural networks on medical image segmentation tasks typically
relies on large labeled datasets for model training. However, acquiring and
manually labeling a large medical image set is resource-intensive, expensive,
and sometimes impractical due to data sharing and privacy issues. To address
this challenge, we propose an adversarial data augmentation approach to improve
the efficiency in utilizing training data and to enlarge the dataset via
simulated but realistic transformations. Specifically, we present a generic
task-driven learning framework, which jointly optimizes a data augmentation
model and a segmentation network during training, generating informative
examples to enhance network generalizability for the downstream task. The data
augmentation model utilizes a set of photometric and geometric image
transformations and chains them to simulate realistic complex imaging
variations that could exist in magnetic resonance (MR) imaging. The proposed
adversarial data augmentation does not rely on generative networks and can be
used as a plug-in module in general segmentation networks. It is
computationally efficient and applicable for both supervised and
semi-supervised learning. We analyze and evaluate the method on two MR image
segmentation tasks: cardiac segmentation and prostate segmentation. Results
show that the proposed approach can alleviate the need for labeled data while
improving model generalization ability, indicating its practical value in
medical imaging applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Chen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1"&gt;Cheng Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Huaqi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1"&gt;Giacomo Tarroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1"&gt;Wenjia Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15078</id>
        <link href="http://arxiv.org/abs/2106.15078"/>
        <updated>2021-08-10T02:00:07.546Z</updated>
        <summary type="html"><![CDATA[Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy loss, which encourages an exact
token-by-token match between a target sequence with a generated sequence. Such
training objective is sub-optimal when the target sequence not perfect, e.g.,
when the target sequence is corrupted with noises, or when only weak sequence
supervision is available. To address this challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL draws
inspirations from convolutional networks (ConvNets) which are shift-invariant
to images, hence is robust to the shift of n-grams to tolerate edits in the
target sequences. Moreover, the computation of EISL is essentially a
convolution operation with target n-grams as kernels, which is easy to
implement with existing libraries. To demonstrate the effectiveness of EISL, we
conduct experiments on three tasks: machine translation with noisy target
sequences, unsupervised text style transfer, and non-autoregressive machine
translation. Experimental results show our method significantly outperforms
cross entropy loss on these three tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zichao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1"&gt;Tianhua Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bowen Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhiting Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-08-10T02:00:07.537Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Annotating Load: Active Learning with Synthetic Images in Surgical Instrument Segmentation. (arXiv:2108.03534v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03534</id>
        <link href="http://arxiv.org/abs/2108.03534"/>
        <updated>2021-08-10T02:00:07.529Z</updated>
        <summary type="html"><![CDATA[Accurate instrument segmentation in endoscopic vision of robot-assisted
surgery is challenging due to reflection on the instruments and frequent
contacts with tissue. Deep neural networks (DNN) show competitive performance
and are in favor in recent years. However, the hunger of DNN for labeled data
poses a huge workload of annotation. Motivated by alleviating this workload, we
propose a general embeddable method to decrease the usage of labeled real
images, using active generated synthetic images. In each active learning
iteration, the most informative unlabeled images are first queried by active
learning and then labeled. Next, synthetic images are generated based on these
selected images. The instruments and backgrounds are cropped out and randomly
combined with each other with blending and fusion near the boundary. The
effectiveness of the proposed method is validated on 2 sinus surgery datasets
and 1 intraabdominal surgery dataset. The results indicate a considerable
improvement in performance, especially when the budget for annotation is small.
The effectiveness of different types of synthetic images, blending methods, and
external background are also studied. All the code is open-sourced at:
https://github.com/HaonanPeng/active_syn_generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Haonan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1"&gt;Daniel King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yun-Hsuan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1"&gt;Randall A. Bly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1"&gt;Kris S. Moe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1"&gt;Blake Hannaford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical image segmentation with imperfect 3D bounding boxes. (arXiv:2108.03300v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03300</id>
        <link href="http://arxiv.org/abs/2108.03300"/>
        <updated>2021-08-10T02:00:07.522Z</updated>
        <summary type="html"><![CDATA[The development of high quality medical image segmentation algorithms depends
on the availability of large datasets with pixel-level labels. The challenges
of collecting such datasets, especially in case of 3D volumes, motivate to
develop approaches that can learn from other types of labels that are cheap to
obtain, e.g. bounding boxes. We focus on 3D medical images with their
corresponding 3D bounding boxes which are considered as series of per-slice
non-tight 2D bounding boxes. While current weakly-supervised approaches that
use 2D bounding boxes as weak labels can be applied to medical image
segmentation, we show that their success is limited in cases when the
assumption about the tightness of the bounding boxes breaks. We propose a new
bounding box correction framework which is trained on a small set of
pixel-level annotations to improve the tightness of a larger set of non-tight
bounding box annotations. The effectiveness of our solution is demonstrated by
evaluating a known weakly-supervised segmentation approach with and without the
proposed bounding box correction algorithm. When the tightness is improved by
our solution, the results of the weakly-supervised segmentation become much
closer to those of the fully-supervised one.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Redekop_E/0/1/0/all/0/1"&gt;Ekaterina Redekop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1"&gt;Alexey Chernyavskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08553</id>
        <link href="http://arxiv.org/abs/2009.08553"/>
        <updated>2021-08-10T02:00:07.502Z</updated>
        <summary type="html"><![CDATA[We propose Generation-Augmented Retrieval (GAR) for answering open-domain
questions, which augments a query through text generation of heuristically
discovered relevant contexts without external resources as supervision. We
demonstrate that the generated contexts substantially enrich the semantics of
the queries and GAR with sparse representations (BM25) achieves comparable or
better performance than state-of-the-art dense retrieval methods such as DPR.
We show that generating diverse contexts for a query is beneficial as fusing
their results consistently yields better retrieval accuracy. Moreover, as
sparse and dense representations are often complementary, GAR can be easily
combined with DPR to achieve even better performance. GAR achieves
state-of-the-art performance on Natural Questions and TriviaQA datasets under
the extractive QA setup when equipped with an extractive reader, and
consistently outperforms other retrieval methods when the same generative
reader is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation. (arXiv:2108.03267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03267</id>
        <link href="http://arxiv.org/abs/2108.03267"/>
        <updated>2021-08-10T02:00:07.495Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation aims to predict pixel-level labels. It has become a
popular task in various computer vision applications. While fully supervised
segmentation methods have achieved high accuracy on large-scale vision
datasets, they are unable to generalize on a new test environment or a new
domain well. In this work, we first introduce a new Un-aligned Domain Score to
measure the efficiency of a learned model on a new target domain in
unsupervised manner. Then, we present the new Bijective Maximum
Likelihood(BiMaL) loss that is a generalized form of the Adversarial Entropy
Minimization without any assumption about pixel independence. We have evaluated
the proposed BiMaL on two domains. The proposed BiMaL approach consistently
outperforms the SOTA methods on empirical experiments on "SYNTHIA to
Cityscapes", "GTA5 to Cityscapes", and "SYNTHIA to Vistas".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1"&gt;Thanh-Dat Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_S/0/1/0/all/0/1"&gt;Son Lam Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainwater_C/0/1/0/all/0/1"&gt;Chase Rainwater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSViT: Better Vision Transformer via Token Pooling and Attention Sharing. (arXiv:2108.03428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03428</id>
        <link href="http://arxiv.org/abs/2108.03428"/>
        <updated>2021-08-10T02:00:07.487Z</updated>
        <summary type="html"><![CDATA[In this paper, we observe two levels of redundancies when applying vision
transformers (ViT) for image recognition. First, fixing the number of tokens
through the whole network produces redundant features at the spatial level.
Second, the attention maps among different transformer layers are redundant.
Based on the observations above, we propose a PSViT: a ViT with token Pooling
and attention Sharing to reduce the redundancy, effectively enhancing the
feature representation ability, and achieving a better speed-accuracy
trade-off. Specifically, in our PSViT, token pooling can be defined as the
operation that decreases the number of tokens at the spatial level. Besides,
attention sharing will be built between the neighboring transformer layers for
reusing the attention maps having a strong correlation among adjacent layers.
Then, a compact set of the possible combinations for different token pooling
and attention sharing mechanisms are constructed. Based on the proposed compact
set, the number of tokens in each layer and the choices of layers sharing
attention can be treated as hyper-parameters that are learned from data
automatically. Experimental results show that the proposed scheme can achieve
up to 6.6% accuracy improvement in ImageNet classification compared with the
DeiT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Boyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peixia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baopu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chuming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1"&gt;Lei Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Ming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision. (arXiv:2107.09852v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09852</id>
        <link href="http://arxiv.org/abs/2107.09852"/>
        <updated>2021-08-10T02:00:07.477Z</updated>
        <summary type="html"><![CDATA[Recent work has shown success in incorporating pre-trained models like BERT
to improve NLP systems. However, existing pre-trained models lack of causal
knowledge which prevents today's NLP systems from thinking like humans. In this
paper, we investigate the problem of injecting causal knowledge into
pre-trained models. There are two fundamental problems: 1) how to collect
various granularities of causal pairs from unstructured texts; 2) how to
effectively inject causal knowledge into pre-trained models. To address these
issues, we extend the idea of CausalBERT from previous studies, and conduct
experiments on various datasets to evaluate its effectiveness. In addition, we
adopt a regularization-based method to preserve the already learned knowledge
with an extra regularization term while injecting causal knowledge. Extensive
experiments on 7 datasets, including four causal pair classification tasks, two
causal QA tasks and a causal inference task, demonstrate that CausalBERT
captures rich causal knowledge and outperforms all pre-trained models-based
state-of-the-art methods, achieving a new causal inference benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1"&gt;Kuo Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06132</id>
        <link href="http://arxiv.org/abs/2106.06132"/>
        <updated>2021-08-10T02:00:07.470Z</updated>
        <summary type="html"><![CDATA[Answering questions about why characters perform certain actions is central
to understanding and reasoning about narratives. Despite recent progress in QA,
it is not clear if existing models have the ability to answer "why" questions
that may require commonsense knowledge external to the input narrative. In this
work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more
than 30k questions and free-form answers concerning why characters in short
narratives perform the actions described. For a third of this dataset, the
answers are not present within the narrative. Given the limitations of
automated evaluation for this task, we also present a systematized human
evaluation interface for this dataset. Our evaluation of state-of-the-art
models show that they are far below human performance on answering such
questions. They are especially worse on questions whose answers are external to
the narrative, thus providing a challenge for future QA and narrative
understanding research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1"&gt;Yash Kumar Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1"&gt;Nathanael Chambers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03372</id>
        <link href="http://arxiv.org/abs/2108.03372"/>
        <updated>2021-08-10T02:00:07.455Z</updated>
        <summary type="html"><![CDATA[In object re-identification (ReID), the development of deep learning
techniques often involves model update and deployment. It is unbearable to
re-extract image features of the large-scale gallery when deploying new models.
Therefore, backward-compatible representation is proposed to enable the "new"
features compatible with "old"' features, free from the re-extracting process.
The existing backward-compatible methods simply conduct constraints in the
embedding space or discriminative space and ignore the intra-class variance of
the old embeddings, resulting in a risk of damaging the discriminability of new
embeddings.

In this work, we propose a Neighborhood Consensus Contrastive Learning (NCCL)
method, which learns backward-compatible representation from a neighborhood
consensus perspective with both embedding structures and discriminative
knowledge. With NCCL, the new embeddings are aligned and improved with old
embeddings in a multi-cluster view. Besides, we also propose a scheme to filter
the old embeddings with low credibility, which can further improve the
compatibility robustness. Our method ensures backward compatibility without
impairing the accuracy of the new model. And it can even improve the new
model's accuracy in most scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shengsen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yihang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YanBai/0/1/0/all/0/1"&gt;YanBai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Lingyu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi. (arXiv:2104.01384v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01384</id>
        <link href="http://arxiv.org/abs/2104.01384"/>
        <updated>2021-08-10T02:00:07.428Z</updated>
        <summary type="html"><![CDATA[This paper describes the ExKaldi-RT online automatic speech recognition (ASR)
toolkit that is implemented based on the Kaldi ASR toolkit and Python language.
ExKaldi-RT provides tools for building online recognition pipelines. While
similar tools are available built on Kaldi, a key feature of ExKaldi-RT that it
works on Python, which has an easy-to-use interface that allows online ASR
system developers to develop original research, such as by applying neural
network-based signal processing and by decoding model trained with deep
learning frameworks. We performed benchmark experiments on the minimum
LibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR
performance in real-time recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leow_C/0/1/0/all/0/1"&gt;Chee Siang Leow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kobayashi_A/0/1/0/all/0/1"&gt;Akio Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Utsuro_T/0/1/0/all/0/1"&gt;Takehito Utsuro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nishizaki_H/0/1/0/all/0/1"&gt;Hiromitsu Nishizaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands. (arXiv:2103.02523v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02523</id>
        <link href="http://arxiv.org/abs/2103.02523"/>
        <updated>2021-08-10T02:00:07.421Z</updated>
        <summary type="html"><![CDATA[The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of
natural language processing to the command line. Participants were tasked with
building models that can transform descriptions of command line tasks in
English to their Bash syntax. This is a report on the competition with details
of the task, metrics, data, attempted solutions, and lessons learned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Mayank Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1"&gt;Tathagata Chakraborti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Quchen Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1"&gt;David Gros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xi Victoria Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maene_J/0/1/0/all/0/1"&gt;Jaron Maene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1"&gt;Kartik Talamadupula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1"&gt;Zhongwei Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jules White&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Geo-localization Using Satellite Imagery and Topography for Unmanned Aerial Vehicles. (arXiv:2108.03344v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03344</id>
        <link href="http://arxiv.org/abs/2108.03344"/>
        <updated>2021-08-10T02:00:07.413Z</updated>
        <summary type="html"><![CDATA[The capabilities of autonomous flight with unmanned aerial vehicles (UAVs)
have significantly increased in recent times. However, basic problems such as
fast and robust geo-localization in GPS-denied environments still remain
unsolved. Existing research has primarily concentrated on improving the
accuracy of localization at the cost of long and varying computation time in
various situations, which often necessitates the use of powerful ground station
machines. In order to make image-based geo-localization online and pragmatic
for lightweight embedded systems on UAVs, we propose a framework that is
reliable in changing scenes, flexible about computing resource allocation and
adaptable to common camera placements. The framework is comprised of two
stages: offline database preparation and online inference. At the first stage,
color images and depth maps are rendered as seen from potential vehicle poses
quantized over the satellite and topography maps of anticipated flying areas. A
database is then populated with the global and local descriptors of the
rendered images. At the second stage, for each captured real-world query image,
top global matches are retrieved from the database and the vehicle pose is
further refined via local descriptor matching. We present field experiments of
image-based localization on two different UAV platforms to validate our
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1"&gt;Mark W. Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreenath_K/0/1/0/all/0/1"&gt;Koushil Sreenath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Commonsense Knowledge on Classifying False News and Determining Checkworthiness of Claims. (arXiv:2108.03731v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03731</id>
        <link href="http://arxiv.org/abs/2108.03731"/>
        <updated>2021-08-10T02:00:07.402Z</updated>
        <summary type="html"><![CDATA[Widespread and rapid dissemination of false news has made fact-checking an
indispensable requirement. Given its time-consuming and labor-intensive nature,
the task calls for an automated support to meet the demand. In this paper, we
propose to leverage commonsense knowledge for the tasks of false news
classification and check-worthy claim detection. Arguing that commonsense
knowledge is a factor in human believability, we fine-tune the BERT language
model with a commonsense question answering task and the aforementioned tasks
in a multi-task learning environment. For predicting fine-grained false news
types, we compare the proposed fine-tuned model's performance with the false
news classification models on a public dataset as well as a newly collected
dataset. We compare the model's performance with the single-task BERT model and
a state-of-the-art check-worthy claim detection tool to evaluate the
check-worthy claim detection. Our experimental analysis demonstrates that
commonsense knowledge can improve performance in both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1"&gt;Ipek Baris Schlicht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezerer_E/0/1/0/all/0/1"&gt;Erhan Sezerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tekir_S/0/1/0/all/0/1"&gt;Selma Tekir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_O/0/1/0/all/0/1"&gt;Oul Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-08-10T02:00:07.395Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Compositional Wikidata Questions. (arXiv:2108.03509v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03509</id>
        <link href="http://arxiv.org/abs/2108.03509"/>
        <updated>2021-08-10T02:00:07.374Z</updated>
        <summary type="html"><![CDATA[Semantic parsing allows humans to leverage vast knowledge resources through
natural interaction. However, parsers are mostly designed for and evaluated on
English resources, such as CFQ (Keysers et al., 2020), the current standard
benchmark based on English data generated from grammar rules and oriented
towards Freebase, an outdated knowledge base. We propose a method for creating
a multilingual, parallel dataset of question-query pairs, grounded in Wikidata,
and introduce such a dataset called Compositional Wikidata Questions (CWQ). We
utilize this data to train and evaluate semantic parsers for Hebrew, Kannada,
Chinese and English, to better understand the current strengths and weaknesses
of multilingual semantic parsing. Experiments on zero-shot cross-lingual
transfer demonstrate that models fail to generate valid queries even with
pretrained multilingual encoders. Our methodology, dataset and results will
facilitate future research on semantic parsing in more realistic and diverse
settings than has been possible with existing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1"&gt;Rahul Aralikatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1"&gt;Heather Lent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes on Coalgebras in Stylometry. (arXiv:2010.02733v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02733</id>
        <link href="http://arxiv.org/abs/2010.02733"/>
        <updated>2021-08-10T02:00:07.366Z</updated>
        <summary type="html"><![CDATA[The syntactic behaviour of texts can highly vary depending on their contexts
(e.g. author, genre, etc.). From the standpoint of stylometry, it can be
helpful to objectively measure this behaviour. In this paper, we discuss how
coalgebras are used to formalise the notion of behaviour by embedding syntactic
features of a given text into probabilistic transition systems. By introducing
the behavioural distance, we are then able to quantitatively measure
differences between points in these systems and thus, comparing features of
different texts. Furthermore, the behavioural distance of points can be
approximated by a polynomial-time algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doat_J/0/1/0/all/0/1"&gt;Jo&amp;#xeb;l A. Doat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What do Bias Measures Measure?. (arXiv:2108.03362v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03362</id>
        <link href="http://arxiv.org/abs/2108.03362"/>
        <updated>2021-08-10T02:00:07.359Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) models propagate social biases about
protected attributes such as gender, race, and nationality. To create
interventions and mitigate these biases and associated harms, it is vital to be
able to detect and measure such biases. While many existing works propose bias
evaluation methodologies for different tasks, there remains a need to
cohesively understand what biases and normative harms each of these measures
captures and how different measures compare. To address this gap, this work
presents a comprehensive survey of existing bias measures in NLP as a function
of the associated NLP tasks, metrics, datasets, and social biases and
corresponding harms. This survey also organizes metrics into different
categories to present advantages and disadvantages. Finally, we propose a
documentation standard for bias measures to aid their development,
categorization, and appropriate usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1"&gt;Sunipa Dev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1"&gt;Emily Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanseverino_M/0/1/0/all/0/1"&gt;Mattie Sanseverino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jiin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-10T02:00:07.351Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12227</id>
        <link href="http://arxiv.org/abs/2104.12227"/>
        <updated>2021-08-10T02:00:07.342Z</updated>
        <summary type="html"><![CDATA[Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francielle Alves Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1"&gt;Isabelle Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1"&gt;Fabiana Rodrigues de G&amp;#xf3;es&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06977</id>
        <link href="http://arxiv.org/abs/2105.06977"/>
        <updated>2021-08-10T02:00:07.331Z</updated>
        <summary type="html"><![CDATA[Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1"&gt;Danish Pruthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1"&gt;Aditi Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.10684</id>
        <link href="http://arxiv.org/abs/2009.10684"/>
        <updated>2021-08-10T02:00:07.310Z</updated>
        <summary type="html"><![CDATA[Despite efforts to distinguish three different evaluation setups (Bekoulis et
al., 2018), numerous end-to-end Relation Extraction (RE) articles present
unreliable performance comparison to previous work. In this paper, we first
identify several patterns of invalid comparisons in published papers and
describe them to avoid their propagation. We then propose a small empirical
study to quantify the impact of the most common mistake and evaluate it leads
to overestimating the final RE performance by around 5% on ACE05. We also seize
this opportunity to study the unexplored ablations of two recent developments:
the use of language model pretraining (specifically BERT) and span-level NER.
This meta-analysis emphasizes the need for rigor in the report of both the
evaluation setting and the datasets statistics and we call for unifying the
evaluation setting in end-to-end RE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1"&gt;Bruno Taill&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1"&gt;Vincent Guigue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14353</id>
        <link href="http://arxiv.org/abs/2012.14353"/>
        <updated>2021-08-10T02:00:07.303Z</updated>
        <summary type="html"><![CDATA[The exponential growths of social media and micro-blogging sites not only
provide platforms for empowering freedom of expressions and individual voices,
but also enables people to express anti-social behaviour like online
harassment, cyberbullying, and hate speech. Numerous works have been proposed
to utilize textual data for social and anti-social behaviour analysis, by
predicting the contexts mostly for highly-resourced languages like English.
However, some languages are under-resourced, e.g., South Asian languages like
Bengali, that lack computational resources for accurate natural language
processing (NLP). In this paper, we propose an explainable approach for hate
speech detection from the under-resourced Bengali language, which we called
DeepHateExplainer. Bengali texts are first comprehensively preprocessed, before
classifying them into political, personal, geopolitical, and religious hates
using a neural ensemble method of transformer-based neural architectures (i.e.,
monolingual Bangla BERT-base, multilingual BERT-cased/uncased, and
XLM-RoBERTa). Important(most and least) terms are then identified using
sensitivity analysis and layer-wise relevance propagation(LRP), before
providing human-interpretable explanations. Finally, we compute
comprehensiveness and sufficiency scores to measure the quality of explanations
w.r.t faithfulness. Evaluations against machine learning~(linear and tree-based
models) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word
embeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,
personal, geopolitical, and religious hates, respectively, outperforming both
ML and DNN baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Md. Rezaul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1"&gt;Sumon Kanti Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1"&gt;Tanhim Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1"&gt;Sagor Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1"&gt;Mehadi Hasan Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1"&gt;Kabir Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1"&gt;Md. Azam Hossain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1"&gt;Stefan Decker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04049</id>
        <link href="http://arxiv.org/abs/2108.04049"/>
        <updated>2021-08-10T02:00:07.295Z</updated>
        <summary type="html"><![CDATA[Open-domain extractive question answering works well on textual data by first
retrieving candidate texts and then extracting the answer from those
candidates. However, some questions cannot be answered by text alone but
require information stored in tables. In this paper, we present an approach for
retrieving both texts and tables relevant to a question by jointly encoding
texts, tables and questions into a single vector space. To this end, we create
a new multi-modal dataset based on text and table datasets from related work
and compare the retrieval performance of different encoding schemata. We find
that dense vector embeddings of transformer models outperform sparse embeddings
on four out of six evaluation datasets. Comparing different dense embedding
models, tri-encoders, with one encoder for each question, text and table,
increase retrieval performance compared to bi-encoders with one encoder for the
question and one for both text and tables. We release the newly created
multi-modal dataset to the community so that it can be used for training and
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1"&gt;Bogdan Kosti&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1"&gt;Julian Risch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1"&gt;Timo M&amp;#xf6;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not quite there yet: Combining analogical patterns and encoder-decoder networks for cognitively plausible inflection. (arXiv:2108.03968v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03968</id>
        <link href="http://arxiv.org/abs/2108.03968"/>
        <updated>2021-08-10T02:00:07.281Z</updated>
        <summary type="html"><![CDATA[The paper presents four models submitted to Part 2 of the SIGMORPHON 2021
Shared Task 0, which aims at replicating human judgements on the inflection of
nonce lexemes. Our goal is to explore the usefulness of combining pre-compiled
analogical patterns with an encoder-decoder architecture. Two models are
designed using such patterns either in the input or the output of the network.
Two extra models controlled for the role of raw similarity of nonce inflected
forms to existing inflected forms in the same paradigm cell, and the role of
the type frequency of analogical patterns. Our strategy is entirely endogenous
in the sense that the models appealing solely to the data provided by the
SIGMORPHON organisers, without using external resources. Our model 2 ranks
second among all submitted systems, suggesting that the inclusion of analogical
patterns in the network architecture is useful in mimicking speakers'
predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Calderone_B/0/1/0/all/0/1"&gt;Basilio Calderone&lt;/a&gt; (CLLE), &lt;a href="http://arxiv.org/find/cs/1/au:+Hathout_N/0/1/0/all/0/1"&gt;Nabil Hathout&lt;/a&gt; (CLLE), &lt;a href="http://arxiv.org/find/cs/1/au:+Bonami_O/0/1/0/all/0/1"&gt;Olivier Bonami&lt;/a&gt; (LLF UMR7110)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03861</id>
        <link href="http://arxiv.org/abs/2108.03861"/>
        <updated>2021-08-10T02:00:07.264Z</updated>
        <summary type="html"><![CDATA[Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49\%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shangbin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zilong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Minnan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04024</id>
        <link href="http://arxiv.org/abs/2108.04024"/>
        <updated>2021-08-10T02:00:07.249Z</updated>
        <summary type="html"><![CDATA[We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1"&gt;Cristian Rodriguez-Opazo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1"&gt;Damien Teney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03945</id>
        <link href="http://arxiv.org/abs/2108.03945"/>
        <updated>2021-08-10T02:00:07.242Z</updated>
        <summary type="html"><![CDATA[Analogical proportions are statements of the form "A is to B as C is to D"
that are used for several reasoning and classification tasks in artificial
intelligence and natural language processing (NLP). For instance, there are
analogy based approaches to semantics as well as to morphology. In fact,
symbolic approaches were developed to solve or to detect analogies between
character strings, e.g., the axiomatic approach as well as that based on
Kolmogorov complexity. In this paper, we propose a deep learning approach to
detect morphological analogies, for instance, with reinflexion or conjugation.
We present empirical results that show that our framework is competitive with
the above-mentioned state of the art symbolic approaches. We also explore
empirically its transferability capacity across languages, which highlights
interesting similarities between them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1"&gt;Safa Alsaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1"&gt;Amandine Decker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1"&gt;Puthineath Lay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1"&gt;Esteban Marquer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1"&gt;Pierre-Alexandre Murena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1"&gt;Miguel Couceiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03881</id>
        <link href="http://arxiv.org/abs/2108.03881"/>
        <updated>2021-08-10T02:00:07.234Z</updated>
        <summary type="html"><![CDATA[Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities' stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities' stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shangbin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zilong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peisheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Minnan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Models for the First-stage Retrieval: A Comprehensive Review. (arXiv:2103.04831v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04831</id>
        <link href="http://arxiv.org/abs/2103.04831"/>
        <updated>2021-08-10T02:00:07.204Z</updated>
        <summary type="html"><![CDATA[Multi-stage ranking pipelines have been a practical solution in modern search
systems, where the first-stage retrieval is to return a subset of candidate
documents, and latter stages attempt to re-rank those candidates. Unlike
re-ranking stages going through quick technique shifts during past decades, the
first-stage retrieval has long been dominated by classical term-based models.
Unfortunately, these models suffer from the vocabulary mismatch problem, which
may block re-ranking stages from relevant documents at the very beginning.
Therefore, it has been a long-term desire to build semantic models for the
first-stage retrieval that can achieve high recall efficiently. Recently, we
have witnessed an explosive growth of research interests on the first-stage
semantic retrieval models. We believe it is the right time to survey current
status, learn from existing methods, and gain some insights for future
development. In this paper, we describe the current landscape of the
first-stage retrieval models under a unified framework to clarify the
connection between classical term-based retrieval methods, early semantic
retrieval methods and neural semantic retrieval methods. Moreover, we identify
some open challenges and envision some future directions, with the hope of
inspiring more researches on these important yet less investigated topics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yinqiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yixing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting and categorising the reactions to COVID-19 by the South African public -- A social media study. (arXiv:2006.06336v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06336</id>
        <link href="http://arxiv.org/abs/2006.06336"/>
        <updated>2021-08-10T02:00:07.197Z</updated>
        <summary type="html"><![CDATA[Social Media can be used to extract discussion topics during a disaster. With
the COVID-19 pandemic impact on South Africa, we need to understand how the law
and regulation promulgated by the government in response to the pandemic
contrasts with discussion topics social media users have been engaging in. In
this work, we expand on traditional media analysis by using Social Media
discussions driven by or directed to South African government officials. We
find topics that are similar as well as different in some cases. The findings
can inform further study into social media during disaster settings in South
Africa and beyond. This paper sets a framework for future analysis in
understanding the opinions of the public during a pandemic and how these
opinions can be distilled [in a semi-automated approach] to inform government
communication in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moodley_A/0/1/0/all/0/1"&gt;Avashlin Moodley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saba_A/0/1/0/all/0/1"&gt;Athandiwe Saba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-based distractor generation for Swedish reading comprehension questions using a small-scale dataset. (arXiv:2108.03973v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03973</id>
        <link href="http://arxiv.org/abs/2108.03973"/>
        <updated>2021-08-10T02:00:07.190Z</updated>
        <summary type="html"><![CDATA[An important part when constructing multiple-choice questions (MCQs) for
reading comprehension assessment are the distractors, the incorrect but
preferably plausible answer options. In this paper, we present a new BERT-based
method for automatically generating distractors using only a small-scale
dataset. We also release a new such dataset of Swedish MCQs (used for training
the model), and propose a methodology for assessing the generated distractors.
Evaluation shows that from a student's perspective, our method generated one or
more plausible distractors for more than 50% of the MCQs in our test set. From
a teacher's perspective, about 50% of the generated distractors were deemed
appropriate. We also do a thorough analysis of the results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1"&gt;Dmytro Kalpakchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1"&gt;Johan Boye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Images really do the Talking? Analysing the significance of Images in Tamil Troll meme classification. (arXiv:2108.03886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03886</id>
        <link href="http://arxiv.org/abs/2108.03886"/>
        <updated>2021-08-10T02:00:06.988Z</updated>
        <summary type="html"><![CDATA[A meme is an part of media created to share an opinion or emotion across the
internet. Due to its popularity, memes have become the new forms of
communication on social media. However, due to its nature, they are being used
in harmful ways such as trolling and cyberbullying progressively. Various data
modelling methods create different possibilities in feature extraction and
turning them into beneficial information. The variety of modalities included in
data plays a significant part in predicting the results. We try to explore the
significance of visual features of images in classifying memes. Memes are a
blend of both image and text, where the text is embedded into the image. We try
to incorporate the memes as troll and non-trolling memes based on the images
and the text on them. However, the images are to be analysed and combined with
the text to increase performance. Our work illustrates different textual
analysis methods and contrasting multimodal methods ranging from simple merging
to cross attention to utilising both worlds' - best visual and textual
features. The fine-tuned cross-lingual language model, XLM, performed the best
in textual analysis, and the multimodal transformer performs the best in
multimodal analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1"&gt;Siddhanth U Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1"&gt;Adeep Hande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1"&gt;Ruba Priyadharshini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1"&gt;Sajeetha Thavareesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakuntharaj_R/0/1/0/all/0/1"&gt;Ratnasingam Sakuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thangasamy_S/0/1/0/all/0/1"&gt;Sathiyaraj Thangasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharathi_B/0/1/0/all/0/1"&gt;B Bharathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1"&gt;Bharathi Raja Chakravarthi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facebook AI WMT21 News Translation Task Submission. (arXiv:2108.03265v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03265</id>
        <link href="http://arxiv.org/abs/2108.03265"/>
        <updated>2021-08-10T02:00:06.959Z</updated>
        <summary type="html"><![CDATA[We describe Facebook's multilingual model submission to the WMT2021 shared
task on news translation. We participate in 14 language directions: English to
and from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To
develop systems covering all these directions, we focus on multilingual models.
We utilize data from all available sources --- WMT, large-scale data mining,
and in-domain backtranslation --- to create high quality bilingual and
multilingual baselines. Subsequently, we investigate strategies for scaling
multilingual model size, such that one system has sufficient capacity for high
quality representations of all eight languages. Our final submission is an
ensemble of dense and sparse Mixture-of-Expert multilingual translation models,
followed by finetuning on in-domain news data and noisy channel reranking.
Compared to previous year's winning submissions, our multilingual system
improved the translation quality on all language directions, with an average
improvement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10
directions based on automatic evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Chau Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1"&gt;Shruti Bhosale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1"&gt;James Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1"&gt;Sergey Edunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1"&gt;Angela Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03578</id>
        <link href="http://arxiv.org/abs/2108.03578"/>
        <updated>2021-08-10T02:00:06.935Z</updated>
        <summary type="html"><![CDATA[Although current state-of-the-art language models have achieved impressive
results in numerous natural language processing tasks, still they could not
solve the problem of producing repetitive, dull and sometimes inconsistent text
in open-ended text generation. Studies often attribute this problem to the
maximum likelihood training objective, and propose alternative approaches by
using stochastic decoding methods or altering the training objective. However,
there is still a lack of consistent evaluation metrics to directly compare the
efficacy of these solutions. In this work, we study different evaluation
metrics that have been proposed to evaluate quality, diversity and consistency
of machine-generated text. From there, we propose a practical pipeline to
evaluate language models in open-ended generation task, and research on how to
improve the model's performance in all dimensions by leveraging different
auxiliary training objectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03857</id>
        <link href="http://arxiv.org/abs/2108.03857"/>
        <updated>2021-08-10T02:00:06.928Z</updated>
        <summary type="html"><![CDATA["Art is the lie that enables us to realize the truth." - Pablo Picasso. For
centuries, humans have dedicated themselves to producing arts to convey their
imagination. The advancement in technology and deep learning in particular, has
caught the attention of many researchers trying to investigate whether art
generation is possible by computers and algorithms. Using generative
adversarial networks (GANs), applications such as synthesizing photorealistic
human faces and creating captions automatically from images were realized. This
survey takes a comprehensive look at the recent works using GANs for generating
visual arts, music, and literary text. A performance comparison and description
of the various GAN architecture are also presented. Finally, some of the key
challenges in art generation using GANs are highlighted along with
recommendations for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1"&gt;Sakib Shahriar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[#StayHome or #Marathon? Social Media Enhanced Pandemic Surveillance on Spatial-temporal Dynamic Graphs. (arXiv:2108.03670v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.03670</id>
        <link href="http://arxiv.org/abs/2108.03670"/>
        <updated>2021-08-10T02:00:06.914Z</updated>
        <summary type="html"><![CDATA[COVID-19 has caused lasting damage to almost every domain in public health,
society, and economy. To monitor the pandemic trend, existing studies rely on
the aggregation of traditional statistical models and epidemic spread theory.
In other words, historical statistics of COVID-19, as well as the population
mobility data, become the essential knowledge for monitoring the pandemic
trend. However, these solutions can barely provide precise prediction and
satisfactory explanations on the long-term disease surveillance while the
ubiquitous social media resources can be the key enabler for solving this
problem. For example, serious discussions may occur on social media before and
after some breaking events take place. These events, such as marathon and
parade, may impact the spread of the virus. To take advantage of the social
media data, we propose a novel framework, Social Media enhAnced pandemic
suRveillance Technique (SMART), which is composed of two modules: (i)
information extraction module to construct heterogeneous knowledge graphs based
on the extracted events and relationships among them; (ii) time series
prediction module to provide both short-term and long-term forecasts of the
confirmed cases and fatality at the state-level in the United States and to
discover risk factors for COVID-19 interventions. Extensive experiments show
that our method largely outperforms the state-of-the-art baselines by 7.3% and
7.4% in confirmed case/fatality prediction, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jyun-yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiusi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-08-10T02:00:06.907Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tiny Neural Models for Seq2Seq. (arXiv:2108.03340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03340</id>
        <link href="http://arxiv.org/abs/2108.03340"/>
        <updated>2021-08-10T02:00:06.899Z</updated>
        <summary type="html"><![CDATA[Semantic parsing models with applications in task oriented dialog systems
require efficient sequence to sequence (seq2seq) architectures to be run
on-device. To this end, we propose a projection based encoder-decoder model
referred to as pQRNN-MAtt. Studies based on projection methods were restricted
to encoder-only models, and we believe this is the first study extending it to
seq2seq architectures. The resulting quantized models are less than 3.5MB in
size and are well suited for on-device latency critical applications. We show
that on MTOP, a challenging multilingual semantic parsing dataset, the average
model performance surpasses LSTM based seq2seq model that uses pre-trained
embeddings despite being 85x smaller. Furthermore, the model can be an
effective student for distilling large pre-trained models such as T5/BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kandoor_A/0/1/0/all/0/1"&gt;Arun Kandoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation. (arXiv:2108.03845v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03845</id>
        <link href="http://arxiv.org/abs/2108.03845"/>
        <updated>2021-08-10T02:00:06.879Z</updated>
        <summary type="html"><![CDATA[This paper describes our work in participation of the IWSLT-2021 offline
speech translation task. Our system was built in a cascade form, including a
speaker diarization module, an Automatic Speech Recognition (ASR) module and a
Machine Translation (MT) module. We directly use the LIUM SpkDiarization tool
as the diarization module. The ASR module is trained with three ASR datasets
from different sources, by multi-source training, using a modified Transformer
encoder. The MT module is pretrained on the large-scale WMT news translation
dataset and fine-tuned on the TED corpus. Our method achieves 24.6 BLEU score
on the 2021 test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Minghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiaxin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingtao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yujia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Shimin Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Ying Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Personalized Dialogue via Multi-Task Meta-Learning. (arXiv:2108.03377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03377</id>
        <link href="http://arxiv.org/abs/2108.03377"/>
        <updated>2021-08-10T02:00:06.872Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to personalized dialogue generation typically require
a large corpus, as well as predefined persona information. However, in a
real-world setting, neither a large corpus of training data nor persona
information are readily available. To address these practical limitations, we
propose a novel multi-task meta-learning approach which involves training a
model to adapt to new personas without relying on a large corpus, or on any
predefined persona information. Instead, the model is tasked with generating
personalized responses based on only the dialogue context. Unlike prior work,
our approach leverages on the provided persona information only during training
via the introduction of an auxiliary persona reconstruction task. In this
paper, we introduce 2 frameworks that adopt the proposed multi-task
meta-learning approach: the Multi-Task Meta-Learning (MTML) framework, and the
Alternating Multi-Task Meta-Learning (AMTML) framework. Experimental results
show that utilizing MTML and AMTML results in dialogue responses with greater
persona consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jing Yang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kong Aik Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1"&gt;Woon Seng Gan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Transferability of Neural Models of Morphological Analogies. (arXiv:2108.03938v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03938</id>
        <link href="http://arxiv.org/abs/2108.03938"/>
        <updated>2021-08-10T02:00:06.865Z</updated>
        <summary type="html"><![CDATA[Analogical proportions are statements expressed in the form "A is to B as C
is to D" and are used for several reasoning and classification tasks in
artificial intelligence and natural language processing (NLP). In this paper,
we focus on morphological tasks and we propose a deep learning approach to
detect morphological analogies. We present an empirical study to see how our
framework transfers across languages, and that highlights interesting
similarities and differences between these languages. In view of these results,
we also discuss the possibility of building a multilingual morphological model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1"&gt;Safa Alsaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1"&gt;Amandine Decker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1"&gt;Puthineath Lay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1"&gt;Esteban Marquer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1"&gt;Pierre-Alexandre Murena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1"&gt;Miguel Couceiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Summarization with Constrained Markov Decision Process. (arXiv:2108.03405v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03405</id>
        <link href="http://arxiv.org/abs/2108.03405"/>
        <updated>2021-08-10T02:00:06.858Z</updated>
        <summary type="html"><![CDATA[We study controllable text summarization which allows users to gain control
on a particular attribute (e.g., length limit) of the generated summaries. In
this work, we propose a novel training framework based on Constrained Markov
Decision Process (CMDP), which conveniently includes a reward function along
with a set of constraints, to facilitate better summarization control. The
reward function encourages the generation to resemble the human-written
reference, while the constraints are used to explicitly prevent the generated
summaries from violating user-imposed requirements. Our framework can be
applied to control important attributes of summarization, including length,
covered entities, and abstractiveness, as we devise specific constraints for
each of these aspects. Extensive experiments on popular benchmarks show that
our CMDP framework helps generate informative summaries while complying with a
given attribute's requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1"&gt;Hou Pong Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03478</id>
        <link href="http://arxiv.org/abs/2108.03478"/>
        <updated>2021-08-10T02:00:06.851Z</updated>
        <summary type="html"><![CDATA[Deep learning is providing very positive results in areas related to
conversational interfaces, such as speech recognition, but its potential
benefit for dialog management has still not been fully studied. In this paper,
we perform an assessment of different configurations for deep-learned dialog
management with three dialog corpora from different application domains and
varying in size, dimensionality and possible system responses. Our results have
allowed us to identify several aspects that can have an impact on accuracy,
including the approaches used for feature extraction, input representation,
context consideration and the hyper-parameters of the deep neural networks
employed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1"&gt;Luk&amp;#xe1;&amp;#x161; Mat&amp;#x11b;j&amp;#x16f;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1"&gt;David Griol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1"&gt;Zoraida Callejas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Manuel Molina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1"&gt;Araceli Sanchis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Translation of Low-Resource Indo-European Languages. (arXiv:2108.03739v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03739</id>
        <link href="http://arxiv.org/abs/2108.03739"/>
        <updated>2021-08-10T02:00:06.842Z</updated>
        <summary type="html"><![CDATA[Transfer learning has been an important technique for low-resource neural
machine translation. In this work, we build two systems to study how
relatedness can benefit the translation performance. The primary system adopts
machine translation model pre-trained on related language pair and the
contrastive system adopts that pre-trained on unrelated language pair. We show
that relatedness is not required for transfer learning to work but does benefit
the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Rui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03533</id>
        <link href="http://arxiv.org/abs/2108.03533"/>
        <updated>2021-08-10T02:00:06.822Z</updated>
        <summary type="html"><![CDATA[We investigate transfer learning based on pre-trained neural machine
translation models to translate between (low-resource) similar languages. This
work is part of our contribution to the WMT 2021 Similar Languages Translation
Shared Task where we submitted models for different language pairs, including
French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our
models for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)
rank top 1 in the official shared task evaluation, and we are the only team to
submit models for the French-Bambara pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1"&gt;Ife Adebara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03354</id>
        <link href="http://arxiv.org/abs/2108.03354"/>
        <updated>2021-08-10T02:00:06.814Z</updated>
        <summary type="html"><![CDATA[The research on human emotion under multimedia stimulation based on
physiological signals is an emerging field, and important progress has been
achieved for emotion recognition based on multi-modal signals. However, it is
challenging to make full use of the complementarity among
spatial-spectral-temporal domain features for emotion recognition, as well as
model the heterogeneity and correlation among multi-modal signals. In this
paper, we propose a novel two-stream heterogeneous graph recurrent neural
network, named HetEmotionNet, fusing multi-modal physiological signals for
emotion recognition. Specifically, HetEmotionNet consists of the
spatial-temporal stream and the spatial-spectral stream, which can fuse
spatial-spectral-temporal domain features in a unified framework. Each stream
is composed of the graph transformer network for modeling the heterogeneity,
the graph convolutional network for modeling the correlation, and the gated
recurrent unit for capturing the temporal domain or spectral domain dependency.
Extensive experiments on two real-world datasets demonstrate that our proposed
model achieves better performance than state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Ziyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youfang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiyang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiangheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Caijie Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05248</id>
        <link href="http://arxiv.org/abs/2103.05248"/>
        <updated>2021-08-10T02:00:06.806Z</updated>
        <summary type="html"><![CDATA[Recent studies unveil the vulnerabilities of deep ranking models, where an
imperceptible perturbation can trigger dramatic changes in the ranking result.
While previous attempts focus on manipulating absolute ranks of certain
candidates, the possibility of adjusting their relative order remains
under-explored. In this paper, we formulate a new adversarial attack against
deep ranking systems, i.e., the Order Attack, which covertly alters the
relative order among a selected set of candidates according to an
attacker-specified permutation, with limited interference to other unrelated
candidates. Specifically, it is formulated as a triplet-style loss imposing an
inequality chain reflecting the specified permutation. However, direct
optimization of such white-box objective is infeasible in a real-world attack
scenario due to various black-box limitations. To cope with them, we propose a
Short-range Ranking Correlation metric as a surrogate objective for black-box
Order Attack to approximate the white-box method. The Order Attack is evaluated
on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box
and black-box threat models. The black-box attack is also successfully
implemented on a major e-commerce platform. Comprehensive experimental
evaluations demonstrate the effectiveness of the proposed methods, revealing a
new type of ranking model vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for Scalable Image Retrieval. (arXiv:2108.03914v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.03914</id>
        <link href="http://arxiv.org/abs/2108.03914"/>
        <updated>2021-08-10T02:00:06.795Z</updated>
        <summary type="html"><![CDATA[Hashing learns compact binary codes to store and retrieve massive data
efficiently. Particularly, unsupervised deep hashing is supported by powerful
deep neural networks and has the desirable advantage of label independence. It
is a promising technique for scalable image retrieval. However, deep models
introduce a large number of parameters, which is hard to optimize due to the
lack of explicit semantic labels and brings considerable training cost. As a
result, the retrieval accuracy and training efficiency of existing unsupervised
deep hashing are still limited. To tackle the problems, in this paper, we
propose a simple and efficient \emph{Lightweight Augmented Graph Network
Hashing} (LAGNH) method with a two-pronged strategy. For one thing, we extract
the inner structure of the image as the auxiliary semantics to enhance the
semantic supervision of the unsupervised hash learning process. For another, we
design a lightweight network structure with the assistance of the auxiliary
semantics, which greatly reduces the number of network parameters that needs to
be optimized and thus greatly accelerates the training process. Specifically,
we design a cross-modal attention module based on the auxiliary semantic
information to adaptively mitigate the adverse effects in the deep image
features. Besides, the hash codes are learned by multi-layer message passing
within an adversarial regularized graph convolutional network. Simultaneously,
the semantic representation capability of hash codes is further enhanced by
reconstructing the similarity graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hui Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1"&gt;Zhiyong Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tag Embedding Based Personalized Point Of Interest Recommendation System. (arXiv:2004.06389v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.06389</id>
        <link href="http://arxiv.org/abs/2004.06389"/>
        <updated>2021-08-10T02:00:06.784Z</updated>
        <summary type="html"><![CDATA[Personalized Point of Interest recommendation is very helpful for satisfying
users' needs at new places. In this article, we propose a tag embedding based
method for Personalized Recommendation of Point Of Interest. We model the
relationship between tags corresponding to Point Of Interest. The model
provides representative embedding corresponds to a tag in a way that related
tags will be closer. We model Point of Interest-based on tag embedding and also
model the users (user profile) based on the Point Of Interest rated by them.
finally, we rank the user's candidate Point Of Interest based on cosine
similarity between user's embedding and Point of Interest's embedding. Further,
we find the parameters required to model user by discrete optimizing over
different measures (like ndcg@5, MRR, ...). We also analyze the result while
considering the same parameters for all users and individual parameters for
each user. Along with it we also analyze the effect on the result while
changing the dataset to model the relationship between tags. Our method also
minimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase
2 dataset and have significant improvement over all the measures on the state
of the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,
which shows the effectiveness of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Suraj Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1"&gt;Dwaipayan Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_M/0/1/0/all/0/1"&gt;Mandar Mitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.00414</id>
        <link href="http://arxiv.org/abs/1811.00414"/>
        <updated>2021-08-10T02:00:06.764Z</updated>
        <summary type="html"><![CDATA[A central roadblock to analyzing quantum algorithms on quantum states is the
lack of a comparable input model for classical algorithms. Inspired by recent
work of the author [E. Tang, STOC'19], we introduce such a model, where we
assume we can efficiently perform $\ell^2$-norm samples of input data, a
natural analogue to quantum algorithms that assume efficient state preparation
of classical data. Though this model produces less practical algorithms than
the (stronger) standard model of classical computation, it captures versions of
many of the features and nuances of quantum linear algebra algorithms. With
this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's
quantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]
and nearest-centroid clustering [arXiv:1307.0411]. Since they are only
polynomially slower, these algorithms suggest that the exponential speedups of
their quantum counterparts are simply an artifact of state preparation
assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1"&gt;Ewin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08553</id>
        <link href="http://arxiv.org/abs/2009.08553"/>
        <updated>2021-08-10T02:00:06.756Z</updated>
        <summary type="html"><![CDATA[We propose Generation-Augmented Retrieval (GAR) for answering open-domain
questions, which augments a query through text generation of heuristically
discovered relevant contexts without external resources as supervision. We
demonstrate that the generated contexts substantially enrich the semantics of
the queries and GAR with sparse representations (BM25) achieves comparable or
better performance than state-of-the-art dense retrieval methods such as DPR.
We show that generating diverse contexts for a query is beneficial as fusing
their results consistently yields better retrieval accuracy. Moreover, as
sparse and dense representations are often complementary, GAR can be easily
combined with DPR to achieve even better performance. GAR achieves
state-of-the-art performance on Natural Questions and TriviaQA datasets under
the extractive QA setup when equipped with an extractive reader, and
consistently outperforms other retrieval methods when the same generative
reader is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14463</id>
        <link href="http://arxiv.org/abs/2106.14463"/>
        <updated>2021-08-10T02:00:06.744Z</updated>
        <summary type="html"><![CDATA[Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ashwin Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1"&gt;Steven QH Truong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1"&gt;Du Nguyen Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Tan Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1"&gt;Pierre Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1"&gt;Curtis P. Langlotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-10T02:00:06.733Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Zero-shot Language Modeling. (arXiv:2108.03334v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03334</id>
        <link href="http://arxiv.org/abs/2108.03334"/>
        <updated>2021-08-10T02:00:06.725Z</updated>
        <summary type="html"><![CDATA[Can we construct a neural model that is inductively biased towards learning
human languages? Motivated by this question, we aim at constructing an
informative prior over neural weights, in order to adapt quickly to held-out
languages in the task of character-level language modeling. We infer this
distribution from a sample of typologically diverse training languages via
Laplace approximation. The use of such a prior outperforms baseline models with
an uninformative prior (so-called "fine-tuning") in both zero-shot and few-shot
settings. This shows that the prior is imbued with universal phonological
knowledge. Moreover, we harness additional language-specific side information
as distant supervision for held-out languages. Specifically, we condition
language models on features from typological databases, by concatenating them
to hidden states or generating weights with hyper-networks. These features
appear beneficial in the few-shot setting, but not in the zero-shot setting.
Since the paucity of digital texts affects the majority of the world's
languages, we hope that these findings will help broaden the scope of
applications for language technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo Maria Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1"&gt;Anna Korhonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tuning GPT-3 for Russian Text Summarization. (arXiv:2108.03502v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03502</id>
        <link href="http://arxiv.org/abs/2108.03502"/>
        <updated>2021-08-10T02:00:06.705Z</updated>
        <summary type="html"><![CDATA[Automatic summarization techniques aim to shorten and generalize information
given in the text while preserving its core message and the most relevant
ideas. This task can be approached and treated with a variety of methods,
however, not many attempts have been made to produce solutions specifically for
the Russian language despite existing localizations of the state-of-the-art
models. In this paper, we aim to showcase ruGPT3 ability to summarize texts,
fine-tuning it on the corpora of Russian news with their corresponding
human-generated summaries. Additionally, we employ hyperparameter tuning so
that the model's output becomes less random and more tied to the original text.
We evaluate the resulting texts with a set of metrics, showing that our
solution can surpass the state-of-the-art model's performance without
additional changes in architecture or loss function. Despite being able to
produce sensible summaries, our model still suffers from a number of flaws,
namely, it is prone to altering Named Entities present in the original text
(such as surnames, places, dates), deviating from facts stated in the given
document, and repeating the information in the summary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikolich_A/0/1/0/all/0/1"&gt;Alexandr Nikolich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puchkova_A/0/1/0/all/0/1"&gt;Arina Puchkova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00294</id>
        <link href="http://arxiv.org/abs/2101.00294"/>
        <updated>2021-08-10T02:00:06.692Z</updated>
        <summary type="html"><![CDATA[Current open-domain question answering systems often follow a
Retriever-Reader architecture, where the retriever first retrieves relevant
passages and the reader then reads the retrieved passages to form an answer. In
this paper, we propose a simple and effective passage reranking method, named
Reader-guIDEd Reranker (RIDER), which does not involve training and reranks the
retrieved passages solely based on the top predictions of the reader before
reranking. We show that RIDER, despite its simplicity, achieves 10 to 20
absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains
without refining the retriever or reader. In addition, RIDER, without any
training, outperforms state-of-the-art transformer-based supervised rerankers.
Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM
on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are
used as the reader input after passage reranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yuning Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08649</id>
        <link href="http://arxiv.org/abs/2105.08649"/>
        <updated>2021-08-10T02:00:06.683Z</updated>
        <summary type="html"><![CDATA[User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zekai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangtian Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1"&gt;Robert Pless&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuzhen Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03375</id>
        <link href="http://arxiv.org/abs/2108.03375"/>
        <updated>2021-08-10T02:00:06.666Z</updated>
        <summary type="html"><![CDATA[Temporal Action Localization (TAL) task in which the aim is to predict the
start and end of each action and its class label has many applications in the
real world. But due to its complexity, researchers have not reached great
results compared to the action recognition task. The complexity is related to
predicting precise start and end times for different actions in any video. In
this paper, we propose a new network based on Gated Recurrent Unit (GRU) and
two novel post-processing ideas for TAL task. Specifically, we propose a new
design for the output layer of the GRU resulting in the so-called GRU-Splitted
model. Moreover, linear interpolation is used to generate the action proposals
with precise start and end times. Finally, to rank the generated proposals
appropriately, we use a Learn to Rank (LTR) approach. We evaluated the
performance of the proposed method on Thumos14 dataset. Results show the
superiority of the performance of the proposed method compared to
state-of-the-art. Especially in the mean Average Precision (mAP) metric at
Intersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than
that of state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1"&gt;Hassan Keshvari Khojasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1"&gt;Hoda Mohammadzade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1"&gt;Hamid Behroozi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning. (arXiv:2108.03305v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03305</id>
        <link href="http://arxiv.org/abs/2108.03305"/>
        <updated>2021-08-10T02:00:06.656Z</updated>
        <summary type="html"><![CDATA[Toxic online speech has become a crucial problem nowadays due to an
exponential increase in the use of internet by people from different cultures
and educational backgrounds. Differentiating if a text message belongs to hate
speech and offensive language is a key challenge in automatic detection of
toxic text content. In this paper, we propose an approach to automatically
classify tweets into three classes: Hate, offensive and Neither. Using public
tweet data set, we first perform experiments to build BI-LSTM models from empty
embedding and then we also try the same neural network architecture with
pre-trained Glove embedding. Next, we introduce a transfer learning approach
for hate speech detection using an existing pre-trained language model BERT
(Bidirectional Encoder Representations from Transformers), DistilBert
(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform
hyper parameters tuning analysis of our best model (BI-LSTM) considering
different neural network architectures, learn-ratings and normalization methods
etc. After tuning the model and with the best combination of parameters, we
achieve over 92 percent accuracy upon evaluating it on test data. We also
create a class module which contains main functionality including text
classification, sentiment checking and text data augmentation. This model could
serve as an intermediate module between user and Twitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1"&gt;Bencheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jason Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ajay Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umair_H/0/1/0/all/0/1"&gt;Hafiza Umair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovor_A/0/1/0/all/0/1"&gt;Atsu Vovor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durzynski_N/0/1/0/all/0/1"&gt;Natalie Durzynski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.04049</id>
        <link href="http://arxiv.org/abs/2108.04049"/>
        <updated>2021-08-10T02:00:06.635Z</updated>
        <summary type="html"><![CDATA[Open-domain extractive question answering works well on textual data by first
retrieving candidate texts and then extracting the answer from those
candidates. However, some questions cannot be answered by text alone but
require information stored in tables. In this paper, we present an approach for
retrieving both texts and tables relevant to a question by jointly encoding
texts, tables and questions into a single vector space. To this end, we create
a new multi-modal dataset based on text and table datasets from related work
and compare the retrieval performance of different encoding schemata. We find
that dense vector embeddings of transformer models outperform sparse embeddings
on four out of six evaluation datasets. Comparing different dense embedding
models, tri-encoders, with one encoder for each question, text and table,
increase retrieval performance compared to bi-encoders with one encoder for the
question and one for both text and tables. We release the newly created
multi-modal dataset to the community so that it can be used for training and
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1"&gt;Bogdan Kosti&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1"&gt;Julian Risch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1"&gt;Timo M&amp;#xf6;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04031</id>
        <link href="http://arxiv.org/abs/2108.04031"/>
        <updated>2021-08-10T02:00:06.621Z</updated>
        <summary type="html"><![CDATA[In the current deep learning based recommendation system, the embedding
method is generally employed to complete the conversion from the
high-dimensional sparse feature vector to the low-dimensional dense feature
vector. However, as the dimension of the input vector of the embedding layer is
too large, the addition of the embedding layer significantly slows down the
convergence speed of the entire neural network, which is not acceptable in
real-world scenarios. In addition, as the interaction between users and items
increases and the relationship between items becomes more complicated, the
embedding method proposed for sequence data is no longer suitable for graphic
data in the current real environment. Therefore, in this paper, we propose the
Dual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes
two modes, static and dynamic. We first construct the item graph to extract the
graph structure and use random walk of unequal probability to capture the
high-order proximity between the items. Then we generate the graph embedding
vector through the Skip-Gram model, and finally feed the downstream deep neural
network for the recommendation task. The experimental results show that DGEM
can mine the high-order proximity between items and enhance the expression
ability of the recommendation model. Meanwhile it also improves the
recommendation performance by utilizing the time dependent relationship between
items.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Rongwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zhuyun Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cough Detection Using Selected Informative Features from Audio Signals. (arXiv:2108.03538v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.03538</id>
        <link href="http://arxiv.org/abs/2108.03538"/>
        <updated>2021-08-10T02:00:06.608Z</updated>
        <summary type="html"><![CDATA[Cough is a common symptom of respiratory and lung diseases. Cough detection
is important to prevent, assess and control epidemic, such as COVID-19. This
paper proposes a model to detect cough events from cough audio signals. The
models are trained by the dataset combined ESC-50 dataset with self-recorded
cough recordings. The test dataset contains inpatient cough recordings
collected from inpatients of the respiratory disease department in Ruijin
Hospital. We totally build 15 cough detection models based on different feature
numbers selected by Random Frog, Uninformative Variable Elimination (UVE), and
Variable influence on projection (VIP) algorithms respectively. The optimal
model is based on 20 features selected from Mel Frequency Cepstral Coefficients
(MFCC) features by UVE algorithm and classified with Support Vector Machine
(SVM) linear two-class classifier. The best cough detection model realizes the
accuracy, recall, precision and F1-score with 94.9%, 97.1%, 93.1% and 0.95
respectively. Its excellent performance with fewer dimensionality of the
feature vector shows the potential of being applied to mobile devices, such as
smartphones, thus making cough detection remote and non-contact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinru Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Menghan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.04024</id>
        <link href="http://arxiv.org/abs/2108.04024"/>
        <updated>2021-08-10T02:00:06.584Z</updated>
        <summary type="html"><![CDATA[We extend the task of composed image retrieval, where an input query consists
of an image and short textual description of how to modify the image. Existing
methods have only been applied to non-complex images within narrow domains,
such as fashion products, thereby limiting the scope of study on in-depth
visual reasoning in rich image and language contexts. To address this issue, we
collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which
consists of over 36,000 pairs of crowd-sourced, open-domain images with
human-generated modifying text. To extend current methods to the open-domain,
we propose CIRPLANT, a transformer based model that leverages rich pre-trained
vision-and-language (V&L) knowledge for modifying visual features conditioned
on natural language. Retrieval is then done by nearest neighbor lookup on the
modified features. We demonstrate that with a relatively simple architecture,
CIRPLANT outperforms existing methods on open-domain images, while matching
state-of-the-art accuracy on the existing narrow datasets, such as fashion.
Together with the release of CIRR, we believe this work will inspire further
research on composed image retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1"&gt;Cristian Rodriguez-Opazo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1"&gt;Damien Teney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IntenT5: Search Result Diversification using Causal Language Models. (arXiv:2108.04026v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.04026</id>
        <link href="http://arxiv.org/abs/2108.04026"/>
        <updated>2021-08-10T02:00:06.574Z</updated>
        <summary type="html"><![CDATA[Search result diversification is a beneficial approach to overcome
under-specified queries, such as those that are ambiguous or multi-faceted.
Existing approaches often rely on massive query logs and interaction data to
generate a variety of possible query intents, which then can be used to re-rank
documents. However, relying on user interaction data is problematic because one
first needs a massive user base to build a sufficient log; public query logs
are insufficient on their own. Given the recent success of causal language
models (such as the Text-To-Text Transformer (T5) model) at text generation
tasks, we explore the capacity of these models to generate potential query
intents. We find that to encourage diversity in the generated queries, it is
beneficial to adapt the model by including a new Distributional Causal Language
Modeling (DCLM) objective during fine-tuning and a representation replacement
during inference. Across six standard evaluation benchmarks, we find that our
method (which we call IntenT5) improves search result diversity and attains
(and sometimes exceeds) the diversity obtained when using query suggestions
based on a proprietary query log. Our analysis shows that our approach is most
effective for multi-faceted queries and is able to generalize effectively to
queries that were unseen in training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1"&gt;Sean MacAvaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1"&gt;Craig Macdonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1"&gt;Roderick Murray-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1"&gt;Iadh Ounis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03788</id>
        <link href="http://arxiv.org/abs/2108.03788"/>
        <updated>2021-08-10T02:00:06.565Z</updated>
        <summary type="html"><![CDATA[This paper presents a three-tier modality alignment approach to learning
text-image joint embedding, coined as JEMA, for cross-modal retrieval of
cooking recipes and food images. The first tier improves recipe text embedding
by optimizing the LSTM networks with term extraction and ranking enhanced
sequence patterns, and optimizes the image embedding by combining the
ResNeXt-101 image encoder with the category embedding using wideResNet-50 with
word2vec. The second tier modality alignment optimizes the textual-visual joint
embedding loss function using a double batch-hard triplet loss with soft-margin
optimization. The third modality alignment incorporates two types of
cross-modality alignments as the auxiliary loss regularizations to further
reduce the alignment errors in the joint learning of the two modality-specific
embedding functions. The category-based cross-modal alignment aims to align the
image category with the recipe category as a loss regularization to the joint
embedding. The cross-modal discriminator-based alignment aims to add the
visual-textual embedding distribution alignment to further regularize the joint
embedding loss. Extensive experiments with the one-million recipes benchmark
dataset Recipe1M demonstrate that the proposed JEMA approach outperforms the
state-of-the-art cross-modal embedding methods for both image-to-recipe and
recipe-to-image retrievals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based re-ranking for case law retrieval. (arXiv:2108.03937v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03937</id>
        <link href="http://arxiv.org/abs/2108.03937"/>
        <updated>2021-08-10T02:00:06.532Z</updated>
        <summary type="html"><![CDATA[In this paper, we present our approaches for the case law retrieval and the
legal case entailment task in the Competition on Legal Information
Extraction/Entailment (COLIEE) 2021. As first stage retrieval methods combined
with neural re-ranking methods using contextualized language models like BERT
achieved great performance improvements for information retrieval in the web
and news domain, we evaluate these methods for the legal domain. A distinct
characteristic of legal case retrieval is that the query case and case
description in the corpus tend to be long documents and therefore exceed the
input length of BERT. We address this challenge by combining lexical and dense
retrieval methods on the paragraph-level of the cases for the first stage
retrieval. Here we demonstrate that the retrieval on the paragraph-level
outperforms the retrieval on the document-level. Furthermore the experiments
suggest that dense retrieval methods outperform lexical retrieval. For
re-ranking we address the problem of long documents by summarizing the cases
and fine-tuning a BERT-based re-ranker with the summaries. Overall, our best
results were obtained with a combination of BM25 and dense passage retrieval
using domain-specific embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1"&gt;Sophia Althammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askari_A/0/1/0/all/0/1"&gt;Arian Askari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1"&gt;Suzan Verberne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1"&gt;Allan Hanbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning & Ranking Balance. (arXiv:2108.03586v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03586</id>
        <link href="http://arxiv.org/abs/2108.03586"/>
        <updated>2021-08-10T02:00:06.513Z</updated>
        <summary type="html"><![CDATA[Numerous neural retrieval models have been proposed in recent years. These
models learn to compute a ranking score between the given query and document.
The majority of existing models are trained in pairwise fashion using
human-judged labels directly without further calibration. The traditional
pairwise schemes can be time-consuming and require pre-defined
positive-negative document pairs for training, potentially leading to learning
bias due to document distribution mismatch between training and test
conditions. Some popular existing listwise schemes rely on the strong
pre-defined probabilistic assumptions and stark difference between relevant and
non-relevant documents for the given query, which may limit the model potential
due to the low-quality or ambiguous relevance labels. To address these
concerns, we turn to a physics-inspired ranking balance scheme and propose
PoolRank, a pooling-based listwise learning framework. The proposed scheme has
four major advantages: (1) PoolRank extracts training information from the best
candidates at the local level based on model performance and relative ranking
among abundant document candidates. (2) By combining four pooling-based loss
components in a multi-task learning fashion, PoolRank calibrates the ranking
balance for the partially relevant and the highly non-relevant documents
automatically without costly human inspection. (3) PoolRank can be easily
generalized to any neural retrieval model without requiring additional
learnable parameters or model structure modifications. (4) Compared to pairwise
learning and existing listwise learning schemes, PoolRank yields better ranking
performance for all studied retrieval models while retaining efficient
convergence rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03357</id>
        <link href="http://arxiv.org/abs/2108.03357"/>
        <updated>2021-08-10T02:00:06.499Z</updated>
        <summary type="html"><![CDATA[Traditional recommendation systems are faced with two long-standing
obstacles, namely, data sparsity and cold-start problems, which promote the
emergence and development of Cross-Domain Recommendation (CDR). The core idea
of CDR is to leverage information collected from other domains to alleviate the
two problems in one domain. Over the last decade, many efforts have been
engaged for cross-domain recommendation. Recently, with the development of deep
learning and neural networks, a large number of methods have emerged. However,
there is a limited number of systematic surveys on CDR, especially regarding
the latest proposed methods as well as the recommendation scenarios and
recommendation tasks they address. In this survey paper, we first proposed a
two-level taxonomy of cross-domain recommendation which classifies different
recommendation scenarios and recommendation tasks. We then introduce and
summarize existing cross-domain recommendation approaches under different
recommendation scenarios in a structured manner. We also organize datasets
commonly used. We conclude this survey by providing several potential research
directions about this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1"&gt;Tianzi Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanmin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haobing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiadi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to Rank Recommendation. (arXiv:2108.03440v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03440</id>
        <link href="http://arxiv.org/abs/2108.03440"/>
        <updated>2021-08-10T02:00:06.483Z</updated>
        <summary type="html"><![CDATA[Exposure bias is a well-known issue in recommender systems where items and
suppliers are not equally represented in the recommendation results. This is
especially problematic when bias is amplified over time as a few popular items
are repeatedly over-represented in recommendation lists. This phenomenon can be
viewed as a recommendation feedback loop: the system repeatedly recommends
certain items at different time points and interactions of users with those
items will amplify bias towards those items over time. This issue has been
extensively studied in the literature on model-based or neighborhood-based
recommendation algorithms, but less work has been done on online recommendation
models such as those based on multi-armed Bandit algorithms. In this paper, we
study exposure bias in a class of well-known bandit algorithms known as Linear
Cascade Bandits. We analyze these algorithms on their ability to handle
exposure bias and provide a fair representation for items and suppliers in the
recommendation results. Our analysis reveals that these algorithms fail to
treat items and suppliers fairly and do not sufficiently explore the item space
for each user. To mitigate this bias, we propose a discounting factor and
incorporate it into these algorithms that controls the exposure of items at
each time step. To show the effectiveness of the proposed discounting factor on
mitigating exposure bias, we perform experiments on two datasets using three
cascading bandit algorithms and our experimental results show that the proposed
method improves the exposure fairness for items and suppliers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1"&gt;Masoud Mansoury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1"&gt;Himan Abdollahpouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1"&gt;Bamshad Mobasher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1"&gt;Robin Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabouri_M/0/1/0/all/0/1"&gt;Milad Sabouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03322</id>
        <link href="http://arxiv.org/abs/2108.03322"/>
        <updated>2021-08-10T02:00:06.467Z</updated>
        <summary type="html"><![CDATA[Pre-trained transformers have recently clinched top spots in the gamut of
natural language tasks and pioneered solutions to software engineering tasks.
Even information retrieval has not been immune to the charm of the transformer,
though their large size and cost is generally a barrier to deployment. While
there has been much work in streamlining, caching, and modifying transformer
architectures for production, here we explore a new direction: distilling a
large pre-trained translation model into a lightweight bi-encoder which can be
efficiently cached and queried. We argue from a probabilistic perspective that
sequence-to-sequence models are a conceptually ideal---albeit highly
impractical---retriever. We derive a new distillation objective, implementing
it as a data augmentation scheme. Using natural language source code search as
a case study for cross-domain search, we demonstrate the validity of this idea
by significantly improving upon the current leader of the CodeSearchNet
challenge, a recent natural language code search benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1"&gt;Colin B. Clement&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1"&gt;Dawn Drain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1"&gt;Neel Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Represent Human Motives for Goal-directed Web Browsing. (arXiv:2108.03350v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.03350</id>
        <link href="http://arxiv.org/abs/2108.03350"/>
        <updated>2021-08-10T02:00:06.432Z</updated>
        <summary type="html"><![CDATA[Motives or goals are recognized in psychology literature as the most
fundamental drive that explains and predicts why people do what they do,
including when they browse the web. Although providing enormous value, these
higher-ordered goals are often unobserved, and little is known about how to
leverage such goals to assist people's browsing activities. This paper proposes
to take a new approach to address this problem, which is fulfilled through a
novel neural framework, Goal-directed Web Browsing (GoWeB). We adopt a
psychologically-sound taxonomy of higher-ordered goals and learn to build their
representations in a structure-preserving manner. Then we incorporate the
resulting representations for enhancing the experiences of common activities
people perform on the web. Experiments on large-scale data from Microsoft Edge
web browser show that GoWeB significantly outperforms competitive baselines for
in-session web page recommendation, re-visitation classification, and
goal-based web page grouping. A follow-up analysis further characterizes how
the variety of human motives can affect the difference observed in human
behavioral patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jyun-Yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chia-Jung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Longqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarrafzadeh_B/0/1/0/all/0/1"&gt;Bahareh Sarrafzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_B/0/1/0/all/0/1"&gt;Brent Hecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teevan_J/0/1/0/all/0/1"&gt;Jaime Teevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Profiling Web Archival Voids for Memento Routing. (arXiv:2108.03311v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2108.03311</id>
        <link href="http://arxiv.org/abs/2108.03311"/>
        <updated>2021-08-10T02:00:06.406Z</updated>
        <summary type="html"><![CDATA[Prior work on web archive profiling were focused on Archival Holdings to
describe what is present in an archive. This work defines and explores Archival
Voids to establish a means to represent portions of URI spaces that are not
present in a web archive. Archival Holdings and Archival Voids profiles can
work independently or as complements to each other to maximize the Accuracy of
Memento Aggregators. We discuss various sources of truth that can be used to
create Archival Voids profiles. We use access logs from Arquivo.pt to create
various Archival Voids profiles and analyze them against our MemGator access
logs for evaluation. We find that we could have avoided more than 8% of
additional False Positives on top of the 60% Accuracy we got from profiling
Archival Holdings in our prior work, if Arquivo.pt were to provide an Archival
Voids profile based on URIs that were requested hundreds of times and never
returned any success responses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sawood Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weigle_M/0/1/0/all/0/1"&gt;Michele C. Weigle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_M/0/1/0/all/0/1"&gt;Michael L. Nelson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What a million Indian farmers say?: A crowdsourcing-based method for pest surveillance. (arXiv:2108.03374v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03374</id>
        <link href="http://arxiv.org/abs/2108.03374"/>
        <updated>2021-08-10T02:00:06.386Z</updated>
        <summary type="html"><![CDATA[Many different technologies are used to detect pests in the crops, such as
manual sampling, sensors, and radar. However, these methods have scalability
issues as they fail to cover large areas, are uneconomical and complex. This
paper proposes a crowdsourced based method utilising the real-time farmer
queries gathered over telephones for pest surveillance. We developed
data-driven strategies by aggregating and analyzing historical data to find
patterns and get future insights into pest occurrence. We showed that it can be
an accurate and economical method for pest surveillance capable of enveloping a
large area with high spatio-temporal granularity. Forecasting the pest
population will help farmers in making informed decisions at the right time.
This will also help the government and policymakers to make the necessary
preparations as and when required and may also ensure food security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adhikari_P/0/1/0/all/0/1"&gt;Poonam Adhikari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ritesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyengar_S/0/1/0/all/0/1"&gt;S.R.S Iyengar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1"&gt;Rishemjit Kaur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.03576</id>
        <link href="http://arxiv.org/abs/2108.03576"/>
        <updated>2021-08-10T02:00:06.347Z</updated>
        <summary type="html"><![CDATA[The online estimation of rhythmic information, such as beat positions,
downbeat positions, and meter, is critical for many real-time music
applications. Musical rhythm comprises complex hierarchical relationships
across time, rendering its analysis intrinsically challenging and at times
subjective. Furthermore, systems which attempt to estimate rhythmic information
in real-time must be causal and must produce estimates quickly and efficiently.
In this work, we introduce an online system for joint beat, downbeat, and meter
tracking, which utilizes causal convolutional and recurrent layers, followed by
a pair of sequential Monte Carlo particle filters applied during inference. The
proposed system does not need to be primed with a time signature in order to
perform downbeat tracking, and is instead able to estimate meter and adjust the
predictions over time. Additionally, we propose an information gate strategy to
significantly decrease the computational cost of particle filtering during the
inference step, making the system much faster than previous sampling-based
methods. Experiments on the GTZAN dataset, which is unseen during training,
show that the system outperforms various online beat and downbeat tracking
systems and achieves comparable performance to a baseline offline joint method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1"&gt;Mojtaba Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1"&gt;Frank Cwitkowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1"&gt;Zhiyao Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08334</id>
        <link href="http://arxiv.org/abs/2106.08334"/>
        <updated>2021-08-09T00:49:28.895Z</updated>
        <summary type="html"><![CDATA[Tensor Networks are non-trivial representations of high-dimensional tensors,
originally designed to describe quantum many-body systems. We show that Tensor
Networks are ideal vehicles to connect quantum mechanical concepts to machine
learning techniques, thereby facilitating an improved interpretability of
neural networks. This study presents the discrimination of top quark signal
over QCD background processes using a Matrix Product State classifier. We show
that entanglement entropy can be used to interpret what a network learns, which
can be used to reduce the complexity of the network and feature space without
loss of generality or performance. For the optimisation of the network, we
compare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic
gradient descent (SGD) and propose a joined training algorithm to harness the
explainability of DMRG with the efficiency of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1"&gt;Jack Y. Araz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05825</id>
        <link href="http://arxiv.org/abs/2106.05825"/>
        <updated>2021-08-09T00:49:28.877Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are employed in an increasing number of
applications, some of which are safety critical. Unfortunately, DNNs are known
to be vulnerable to so-called adversarial attacks that manipulate inputs to
cause incorrect results that can be beneficial to an attacker or damaging to
the victim. Multiple defenses have been proposed to increase the robustness of
DNNs. In general, these defenses have high overhead, some require
attack-specific re-training of the model or careful tuning to adapt to
different attacks.

This paper presents HASI, a hardware-accelerated defense that uses a process
we call stochastic inference to detect adversarial inputs. We show that by
carefully injecting noise into the model at inference time, we can
differentiate adversarial inputs from benign ones. HASI uses the output
distribution characteristics of noisy inference compared to a non-noisy
reference to detect adversarial inputs. We show an adversarial detection rate
of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches, with a much lower
overhead. We demonstrate two software/hardware-accelerated co-designs, which
reduces the performance impact of stochastic inference to 1.58X-2X relative to
the unprotected baseline, compared to 15X-20X overhead for a software-only GPU
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning. (arXiv:2106.14144v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14144</id>
        <link href="http://arxiv.org/abs/2106.14144"/>
        <updated>2021-08-09T00:49:28.870Z</updated>
        <summary type="html"><![CDATA[As people spend up to 87% of their time indoors, intelligent Heating,
Ventilation, and Air Conditioning (HVAC) systems in buildings are essential for
maintaining occupant comfort and reducing energy consumption. These HVAC
systems in smart buildings rely on real-time sensor readings, which in practice
often suffer from various faults and could also be vulnerable to malicious
attacks. Such faulty sensor inputs may lead to the violation of indoor
environment requirements (e.g., temperature, humidity, etc.) and the increase
of energy consumption. While many model-based approaches have been proposed in
the literature for building HVAC control, it is costly to develop accurate
physical models for ensuring their performance and even more challenging to
address the impact of sensor faults. In this work, we present a novel
learning-based framework for sensor fault-tolerant HVAC control, which includes
three deep learning based components for 1) generating temperature proposals
with the consideration of possible sensor faults, 2) selecting one of the
proposals based on the assessment of their accuracy, and 3) applying
reinforcement learning with the selected temperature proposal. Moreover, to
address the challenge of training data insufficiency in building-related tasks,
we propose a model-assisted learning method leveraging an abstract model of
building physical dynamics. Through extensive experiments, we demonstrate that
the proposed fault-tolerant HVAC control framework can significantly reduce
building temperature violations under a variety of sensor fault patterns while
maintaining energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yangyang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1"&gt;Zheng O&amp;#x27;Neill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06073</id>
        <link href="http://arxiv.org/abs/2105.06073"/>
        <updated>2021-08-09T00:49:28.863Z</updated>
        <summary type="html"><![CDATA[A basic requirement for a mathematical model is often that its solution
(output) shouldn't change much if the model's parameters (input) are perturbed.
This is important because the exact values of parameters may not be known and
one would like to avoid being mislead by an output obtained using incorrect
values. Thus, it's rarely enough to address an application by formulating a
model, solving the resulting optimization problem and presenting the solution
as the answer. One would need to confirm that the model is suitable, i.e.,
"good," and this can, at least in part, be achieved by considering a family of
optimization problems constructed by perturbing parameters of concern. The
resulting sensitivity analysis uncovers troubling situations with unstable
solutions, which we referred to as "bad" models, and indicates better model
formulations. Embedding an actual problem of interest within a family of
problems is also a primary path to optimality conditions as well as
computationally attractive, alternative problems, which under ideal
circumstances, and when properly tuned, may even furnish the minimum value of
the actual problem. The tuning of these alternative problems turns out to be
intimately tied to finding multipliers in optimality conditions and thus
emerges as a main component of several optimization algorithms. In fact, the
tuning amounts to solving certain dual optimization problems. In this tutorial,
we'll discuss the opportunities and insights afforded by this broad
perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1"&gt;Johannes O. Royset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation. (arXiv:2108.03167v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03167</id>
        <link href="http://arxiv.org/abs/2108.03167"/>
        <updated>2021-08-09T00:49:28.856Z</updated>
        <summary type="html"><![CDATA[In CS literature, the efforts can be divided into two groups: finding a
measurement matrix that preserves the compressed information at the maximum
level, and finding a reconstruction algorithm for the compressed information.
In the traditional CS setup, the measurement matrices are selected as random
matrices, and optimization-based iterative solutions are used to recover the
signals. However, when we handle large signals, using random matrices become
cumbersome especially when it comes to iterative optimization-based solutions.
Even though recent deep learning-based solutions boost the reconstruction
accuracy performance while speeding up the recovery, still jointly learning the
whole measurement matrix is a difficult process. In this work, we introduce a
separable multi-linear learning of the CS matrix by representing it as the
summation of arbitrary number of tensors. For a special case where the CS
operation is set as a single tensor multiplication, the model is reduced to the
learning-based separable CS; while a dense CS matrix can be approximated and
learned as the summation of multiple tensors. Both cases can be used in CS of
two or multi-dimensional signals e.g., images, multi-spectral images, videos,
etc. Structural CS matrices can also be easily approximated and learned in our
multi-linear separable learning setup with structural tensor sum
representation. Hence, our learnable generalized tensor summation CS operation
encapsulates most CS setups including separable CS, non-separable CS
(traditional vector-matrix multiplication), structural CS, and CS of the
multi-dimensional signals. For both gray-scale and RGB images, the proposed
scheme surpasses most state-of-the-art solutions, especially in lower
measurement rates. Although the performance gain remains limited from tensor to
the sum of tensor representation for gray-scale images, it becomes significant
in the RGB case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamac_M/0/1/0/all/0/1"&gt;Mehmet Yamac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akpinar_U/0/1/0/all/0/1"&gt;Ugur Akpinar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahin_E/0/1/0/all/0/1"&gt;Erdem Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Round Active Learning. (arXiv:2107.06703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06703</id>
        <link href="http://arxiv.org/abs/2107.06703"/>
        <updated>2021-08-09T00:49:28.849Z</updated>
        <summary type="html"><![CDATA[Active learning (AL) aims at reducing labeling effort by identifying the most
valuable unlabeled data points from a large pool. Traditional AL frameworks
have two limitations: First, they perform data selection in a multi-round
manner, which is time-consuming and impractical. Second, they usually assume
that there are a small amount of labeled data points available in the same
domain as the data in the unlabeled pool. Recent work proposes a solution for
one-round active learning based on data utility learning and optimization,
which fixes the first issue but still requires the initially labeled data
points in the same domain. In this paper, we propose $\mathrm{D^2ULO}$ as a
solution that solves both issues. Specifically, $\mathrm{D^2ULO}$ leverages the
idea of domain adaptation (DA) to train a data utility model which can
effectively predict the utility for any given unlabeled data in the target
domain once labeled. The trained data utility model can then be used to select
high-utility data and at the same time, provide an estimate for the utility of
the selected data. Our algorithm does not rely on any feedback from annotators
in the target domain and hence, can be used to perform zero-round active
learning or warm-start existing multi-round active learning strategies. Our
experiments show that $\mathrm{D^2ULO}$ outperforms the existing
state-of-the-art AL strategies equipped with domain adaptation over various
domain shift settings (e.g., real-to-real data and synthetic-to-real data).
Particularly, $\mathrm{D^2ULO}$ is applicable to the scenario where source and
target labels have mismatches, which is not supported by the existing works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks for Anatomical Therapeutic Chemical (ATC) classification. (arXiv:2101.11713v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11713</id>
        <link href="http://arxiv.org/abs/2101.11713"/>
        <updated>2021-08-09T00:49:28.831Z</updated>
        <summary type="html"><![CDATA[Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is
a critical and highly competitive area of research in bioinformatics because of
its potential for expediting drug develop-ment and research. Predicting an
unknown compound's therapeutic and chemical characteristics ac-cording to how
these characteristics affect multiple organs/systems makes automatic ATC
classifica-tion a challenging multi-label problem. Results: In this work, we
propose combining multiple multi-label classifiers trained on distinct sets of
features, including sets extracted from a Bidirectional Long Short-Term Memory
Network (BiLSTM). Experiments demonstrate the power of this approach, which is
shown to outperform the best methods reported in the literature, including the
state-of-the-art developed by the fast.ai research group. Availability: All
source code developed for this study is available at
https://github.com/LorisNanni. Contact: loris.nanni@unipd.it]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1"&gt;Loris Nanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1"&gt;Alessandra Lumini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1"&gt;Sheryl Brahnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering and attention model based for intelligent trading. (arXiv:2107.06782v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06782</id>
        <link href="http://arxiv.org/abs/2107.06782"/>
        <updated>2021-08-09T00:49:28.824Z</updated>
        <summary type="html"><![CDATA[The foreign exchange market has taken an important role in the global
financial market. While foreign exchange trading brings high-yield
opportunities to investors, it also brings certain risks. Since the
establishment of the foreign exchange market in the 20th century, foreign
exchange rate forecasting has become a hot issue studied by scholars from all
over the world. Due to the complexity and number of factors affecting the
foreign exchange market, technical analysis cannot respond to administrative
intervention or unexpected events. Our team chose several pairs of foreign
currency historical data and derived technical indicators from 2005 to 2021 as
the dataset and established different machine learning models for event-driven
price prediction for oversold scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rana_M/0/1/0/all/0/1"&gt;Mimansa Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mao_N/0/1/0/all/0/1"&gt;Nanxiang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ao_M/0/1/0/all/0/1"&gt;Ming Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaohui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liang_P/0/1/0/all/0/1"&gt;Poning Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Khushi_M/0/1/0/all/0/1"&gt;Matloob Khushi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-08-09T00:49:28.812Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets. Code available at
https://github.com/qlilx/odgrlm.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch and MFCC Features. (arXiv:1806.00984v1 [cs.SD] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1806.00984</id>
        <link href="http://arxiv.org/abs/1806.00984"/>
        <updated>2021-08-09T00:49:28.805Z</updated>
        <summary type="html"><![CDATA[Speech is produced when time varying vocal tract system is excited with time
varying excitation source. Therefore, the information present in a speech such
as message, emotion, language, speaker is due to the combined effect of both
excitation source and vocal tract system. However, there is very less
utilization of excitation source features to recognize emotion. In our earlier
work, we have proposed a novel method to extract glottal closure instants
(GCIs) known as epochs. In this paper, we have explored epoch features namely
instantaneous pitch, phase and strength of epochs for discriminating emotions.
We have combined the excitation source features and the well known
Male-frequency cepstral coefficient (MFCC) features to develop an emotion
recognition system with improved performance. DNN-HMM speaker adaptive models
have been developed using MFCC, epoch and combined features. IEMOCAP emotional
database has been used to evaluate the models. The average accuracy for emotion
recognition system when using MFCC and epoch features separately is 59.25% and
54.52% respectively. The recognition performance improves to 64.2% when MFCC
and epoch features are combined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1"&gt;Md. Shah Fahad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_J/0/1/0/all/0/1"&gt;Jainath Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradhan_G/0/1/0/all/0/1"&gt;Gyadhar Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1"&gt;Akshay Deepak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12543</id>
        <link href="http://arxiv.org/abs/2106.12543"/>
        <updated>2021-08-09T00:49:28.797Z</updated>
        <summary type="html"><![CDATA[As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. This has spurred a flurry of research in model
explainability and has given rise to feature attribution methods such as LIME
and SHAP. Despite their widespread use, evaluating and comparing different
feature attribution methods remains challenging: evaluations ideally require
human studies, and empirical evaluation metrics are often data-intensive or
computationally prohibitive on real-world datasets. In this work, we address
this issue by releasing XAI-Bench: a suite of synthetic datasets along with a
library for benchmarking feature attribution algorithms. Unlike real-world
datasets, synthetic datasets allow the efficient computation of conditional
expected values that are needed to evaluate ground-truth Shapley values and
other metrics. The synthetic datasets we release offer a wide variety of
parameters that can be configured to simulate real-world data. We demonstrate
the power of our library by benchmarking popular explainability techniques
across several evaluation metrics and across a variety of settings. The
versatility and efficiency of our library will help researchers bring their
explainability methods from development to deployment. Our code is available at
https://github.com/abacusai/xai-bench.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1"&gt;Sujay Khandagale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning. (arXiv:2108.03169v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03169</id>
        <link href="http://arxiv.org/abs/2108.03169"/>
        <updated>2021-08-09T00:49:28.775Z</updated>
        <summary type="html"><![CDATA[This article elaborates on how machine learning (ML) can leverage the
solution of a contemporary problem related to the security of maritime domains.
The worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents
have led to serious environmental and economic consequences which involve
drastic changes in our ecosystems in addition to financial losses caused by the
depletion of natural resources. The Fisheries and Aquatic Department (FAD) of
the United Nation's Food and Agriculture Organization (FAO) issued a report
which indicated that the annual losses due to IUU fishing reached $25 Billion.
This imposes negative impacts on the future-biodiversity of the marine
ecosystem and domestic Gross National Product (GNP). Hence, robust interception
mechanisms are increasingly needed for detecting and pursuing the unrelenting
illegal fishing incidents in maritime territories. This article addresses the
problem of coordinating the motion of a fleet of marine vessels (pursuers) to
catch an IUU vessel while still in local waters. The problem is formulated as a
pursuer-evader problem that is tackled within an ML framework. One or more
pursuers, such as law enforcement vessels, intercept an evader (i.e., the
illegal fishing ship) using an online reinforcement learning mechanism that is
based on a value iteration process. It employs real-time navigation
measurements of the evader ship as well as those of the pursuing vessels and
returns back model-free interception strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1"&gt;Mohammed Abouheaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuzheng Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1"&gt;Wail Gueaieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abielmona_R/0/1/0/all/0/1"&gt;Rami Abielmona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harb_M/0/1/0/all/0/1"&gt;Moufid Harb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models. (arXiv:2102.03877v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03877</id>
        <link href="http://arxiv.org/abs/2102.03877"/>
        <updated>2021-08-09T00:49:28.767Z</updated>
        <summary type="html"><![CDATA[Like other experimental techniques, X-ray Photon Correlation Spectroscopy is
subject to various kinds of noise. Random and correlated fluctuations and
heterogeneities can be present in a two-time correlation function and obscure
the information about the intrinsic dynamics of a sample. Simultaneously
addressing the disparate origins of noise in the experimental data is
challenging. We propose a computational approach for improving the
signal-to-noise ratio in two-time correlation functions that is based on
Convolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models
extract features from an image via convolutional layers, project them to a low
dimensional space and then reconstruct a clean image from this reduced
representation via transposed convolutional layers. Not only are ED models a
general tool for random noise removal, but their application to low
signal-to-noise data can enhance the data quantitative usage since they are
able to learn the functional form of the signal. We demonstrate that the CNN-ED
models trained on real-world experimental data help to effectively extract
equilibrium dynamics parameters from two-time correlation functions, containing
statistical noise and dynamic heterogeneities. Strategies for optimizing the
models performance and their applicability limits are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Konstantinova_T/0/1/0/all/0/1"&gt;Tatiana Konstantinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wiegart_L/0/1/0/all/0/1"&gt;Lutz Wiegart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rakitin_M/0/1/0/all/0/1"&gt;Maksim Rakitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+DeGennaro_A/0/1/0/all/0/1"&gt;Anthony M. DeGennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Barbour_A/0/1/0/all/0/1"&gt;Andi M. Barbour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Federated Learning with Attack-Adaptive Aggregation. (arXiv:2102.05257v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05257</id>
        <link href="http://arxiv.org/abs/2102.05257"/>
        <updated>2021-08-09T00:49:28.759Z</updated>
        <summary type="html"><![CDATA[Federated learning is vulnerable to various attacks, such as model poisoning
and backdoor attacks, even if some existing defense strategies are used. To
address this challenge, we propose an attack-adaptive aggregation strategy to
defend against various attacks for robust federated learning. The proposed
approach is based on training a neural network with an attention mechanism that
learns the vulnerability of federated learning models from a set of plausible
attacks. To the best of our knowledge, our aggregation strategy is the first
one that can be adapted to defend against various attacks in a data-driven
fashion. Our approach has achieved competitive performance in defending model
poisoning and backdoor attacks in federated learning tasks on image and text
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Ching Pui Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00377</id>
        <link href="http://arxiv.org/abs/2012.00377"/>
        <updated>2021-08-09T00:49:28.752Z</updated>
        <summary type="html"><![CDATA[In many sequence learning tasks, such as program synthesis and document
summarization, a key problem is searching over a large space of possible output
sequences. We propose to learn representations of the outputs that are
specifically meant for search: rich enough to specify the desired output but
compact enough to make search more efficient. Discrete latent codes are
appealing for this purpose, as they naturally allow sophisticated combinatorial
search strategies. The latent codes are learned using a self-supervised
learning principle, in which first a discrete autoencoder is trained on the
output sequences, and then the resulting latent codes are used as intermediate
targets for the end-to-end sequence prediction task. Based on these insights,
we introduce the \emph{Latent Programmer}, a program synthesis method that
first predicts a discrete latent code from input/output examples, and then
generates the program in the target language. We evaluate the Latent Programmer
on two domains: synthesis of string transformation programs, and generation of
programs from natural language descriptions. We demonstrate that the discrete
latent representation significantly improves synthesis accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Joey Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1"&gt;David Dohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Gradient Coding with Optimal Decoding. (arXiv:2006.09638v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09638</id>
        <link href="http://arxiv.org/abs/2006.09638"/>
        <updated>2021-08-09T00:49:28.745Z</updated>
        <summary type="html"><![CDATA[In distributed optimization problems, a technique called gradient coding,
which involves replicating data points, has been used to mitigate the effect of
straggling machines. Recent work has studied approximate gradient coding, which
concerns coding schemes where the replication factor of the data is too low to
recover the full gradient exactly. Our work is motivated by the challenge of
creating approximate gradient coding schemes that simultaneously work well in
both the adversarial and stochastic models. To that end, we introduce novel
approximate gradient codes based on expander graphs, in which each machine
receives exactly two blocks of data points. We analyze the decoding error both
in the random and adversarial straggler setting, when optimal decoding
coefficients are used. We show that in the random setting, our schemes achieve
an error to the gradient that decays exponentially in the replication factor.
In the adversarial setting, the error is nearly a factor of two smaller than
any existing code with similar performance in the random setting. We show
convergence bounds both in the random and adversarial setting for gradient
descent under standard assumptions using our codes. In the random setting, our
convergence rate improves upon block-box bounds. In the adversarial setting, we
show that gradient descent can converge down to a noise floor that scales
linearly with the adversarial error to the gradient. We demonstrate empirically
that our schemes achieve near-optimal error in the random setting and converge
faster than algorithms which do not use the optimal decoding coefficients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glasgow_M/0/1/0/all/0/1"&gt;Margalit Glasgow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wootters_M/0/1/0/all/0/1"&gt;Mary Wootters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08862</id>
        <link href="http://arxiv.org/abs/2101.08862"/>
        <updated>2021-08-09T00:49:28.725Z</updated>
        <summary type="html"><![CDATA[The deadly triad refers to the instability of a reinforcement learning
algorithm when it employs off-policy learning, function approximation, and
bootstrapping simultaneously. In this paper, we investigate the target network
as a tool for breaking the deadly triad, providing theoretical support for the
conventional wisdom that a target network stabilizes training. We first propose
and analyze a novel target network update rule which augments the commonly used
Polyak-averaging style update with two projections. We then apply the target
network and ridge regularization in several divergent algorithms and show their
convergence to regularized TD fixed points. Those algorithms are off-policy
with linear function approximation and bootstrapping, spanning both policy
evaluation and control, as well as both discounted and average-reward settings.
In particular, we provide the first convergent linear $Q$-learning algorithms
under nonrestrictive and changing behavior policies without bi-level
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Hengshuai Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. (arXiv:2011.06175v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06175</id>
        <link href="http://arxiv.org/abs/2011.06175"/>
        <updated>2021-08-09T00:49:28.717Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to optimize fleet management by combining
multi-agent reinforcement learning with graph neural network. To provide
ride-hailing service, one needs to optimize dynamic resources and demands over
spatial domain. While the spatial structure was previously approximated with a
regular grid, our approach represents the road network with a graph, which
better reflects the underlying geometric structure. Dynamic resource allocation
is formulated as multi-agent reinforcement learning, whose action-value
function (Q function) is approximated with graph neural networks. We use
stochastic policy update rule over the graph with deep Q-networks (DQN), and
achieve superior results over the greedy policy update. We design a realistic
simulator that emulates the empirical taxi call data, and confirm the
effectiveness of the proposed model under various conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juhyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihyun Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10367</id>
        <link href="http://arxiv.org/abs/2011.10367"/>
        <updated>2021-08-09T00:49:28.710Z</updated>
        <summary type="html"><![CDATA[The aim of this project is to develop and test advanced analytical methods to
improve the prediction accuracy of Credit Risk Models, preserving at the same
time the model interpretability. In particular, the project focuses on applying
an explainable machine learning model to bank-related databases. The input data
were obtained from open data. Over the total proven models, CatBoost has shown
the highest performance. The algorithm implementation produces a GINI of 0.68
after tuning the hyper-parameters. SHAP package is used to provide a global and
local interpretation of the model predictions to formulate a
human-comprehensive approach to understanding the decision-maker algorithm. The
20 most important features are selected using the Shapley values to present a
full human-understandable model that reveals how the attributes of an
individual are related to its model prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torrent_N/0/1/0/all/0/1"&gt;Neus Llop Torrent&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1"&gt;Giorgio Visani&lt;/a&gt; (2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1"&gt;Enrico Bagli&lt;/a&gt; (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for surface prediction in ACTS. (arXiv:2108.03068v1 [physics.ins-det])]]></title>
        <id>http://arxiv.org/abs/2108.03068</id>
        <link href="http://arxiv.org/abs/2108.03068"/>
        <updated>2021-08-09T00:49:28.703Z</updated>
        <summary type="html"><![CDATA[We present an ongoing R&D activity for machine-learning-assisted navigation
through detectors to be used for track reconstruction. We investigate different
approaches of training neural networks for surface prediction and compare their
results. This work is carried out in the context of the ACTS tracking toolkit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Huth_B/0/1/0/all/0/1"&gt;Benjamin Huth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Salzburger_A/0/1/0/all/0/1"&gt;Andreas Salzburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wettig_T/0/1/0/all/0/1"&gt;Tilo Wettig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13370</id>
        <link href="http://arxiv.org/abs/2009.13370"/>
        <updated>2021-08-09T00:49:28.696Z</updated>
        <summary type="html"><![CDATA[This paper estimates free energy, average mutual information, and minimum
mean square error (MMSE) of a linear model under two assumptions: (1) the
source is generated by a Markov chain, (2) the source is generated via a hidden
Markov model. Our estimates are based on the replica method in statistical
physics. We show that under the posterior mean estimator, the linear model with
Markov sources or hidden Markov sources is decoupled into single-input AWGN
channels with state information available at both encoder and decoder where the
state distribution follows the left Perron-Frobenius eigenvector with unit
Manhattan norm of the stochastic matrix of Markov chains. Numerical results
show that the free energies and MSEs obtained via the replica method are
closely approximate to their counterparts achieved by the Metropolis-Hastings
algorithm or some well-known approximate message passing algorithms in the
research literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation. (arXiv:2108.03132v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03132</id>
        <link href="http://arxiv.org/abs/2108.03132"/>
        <updated>2021-08-09T00:49:28.677Z</updated>
        <summary type="html"><![CDATA[Random reconstruction of three-dimensional (3D) digital rocks from
two-dimensional (2D) slices is crucial for elucidating the microstructure of
rocks and its effects on pore-scale flow in terms of numerical modeling, since
massive samples are usually required to handle intrinsic uncertainties. Despite
remarkable advances achieved by traditional process-based methods, statistical
approaches and recently famous deep learning-based models, few works have
focused on producing several kinds of rocks with one trained model and allowing
the reconstructed samples to satisfy certain given properties, such as
porosity. To fill this gap, we propose a new framework, named RockGPT, which is
composed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a
single 2D slice from the perspective of video generation. The VQ-VAE is
utilized to compress high-dimensional input video, i.e., the sequence of
continuous rock slices, to discrete latent codes and reconstruct them. In order
to obtain diverse reconstructions, the discrete latent codes are modeled using
conditional GPT in an autoregressive manner, while incorporating conditional
information from a given slice, rock type, and porosity. We conduct two
experiments on five kinds of rocks, and the results demonstrate that RockGPT
can produce different kinds of rocks with the same model, and the reconstructed
samples can successfully meet certain specified porosities. In a broader sense,
through leveraging the proposed conditioning scheme, RockGPT constitutes an
effective way to build a general model to produce multiple kinds of rocks
simultaneously that also satisfy user-defined properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Physical Layer Design. (arXiv:2102.11777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11777</id>
        <link href="http://arxiv.org/abs/2102.11777"/>
        <updated>2021-08-09T00:49:28.669Z</updated>
        <summary type="html"><![CDATA[Model-free techniques, such as machine learning (ML), have recently attracted
much interest towards the physical layer design, e.g., symbol detection,
channel estimation, and beamforming. Most of these ML techniques employ
centralized learning (CL) schemes and assume the availability of datasets at a
parameter server (PS), demanding the transmission of data from edge devices,
such as mobile phones, to the PS. Exploiting the data generated at the edge,
federated learning (FL) has been proposed recently as a distributed learning
scheme, in which each device computes the model parameters and sends them to
the PS for model aggregation while the datasets are kept intact at the edge.
Thus, FL is more communication-efficient and privacy-preserving than CL and
applicable to the wireless communication scenarios, wherein the data are
generated at the edge devices. This article presents the recent advances in
FL-based training for physical layer design problems. Compared to CL, the
effectiveness of FL is presented in terms of communication overhead with a
slight performance loss in the learning accuracy. The design challenges, such
as model, data, and hardware complexity, are also discussed in detail along
with possible solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1"&gt;Ahmet M. Elbir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1"&gt;Anastasios K. Papazafeiropoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chatzinotas_S/0/1/0/all/0/1"&gt;Symeon Chatzinotas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Deep Model Reference Adaptive Control. (arXiv:2108.03120v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.03120</id>
        <link href="http://arxiv.org/abs/2108.03120"/>
        <updated>2021-08-09T00:49:28.663Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a Stochastic Deep Neural Network-based Model
Reference Adaptive Control. Building on our work "Deep Model Reference Adaptive
Control", we extend the controller capability by using Bayesian deep neural
networks (DNN) to represent uncertainties and model non-linearities. Stochastic
Deep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the
output-layer weights of the DNN model in real-time, while a data-driven
supervised learning algorithm is used to update the inner-layers parameters.
This asynchronous network update ensures boundedness and guaranteed tracking
performance with a learning-based real-time feedback controller. A Bayesian
approach to DNN learning helped avoid over-fitting the data and provide
confidence intervals over the predictions. The controller's stochastic nature
also ensured "Induced Persistency of excitation," leading to convergence of the
overall system signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Girish Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00478</id>
        <link href="http://arxiv.org/abs/2005.00478"/>
        <updated>2021-08-09T00:49:28.656Z</updated>
        <summary type="html"><![CDATA[In recent years, the concept of automated machine learning has become very
popular. Automated Machine Learning (AutoML) mainly refers to the automated
methods for model selection and hyper-parameter optimization of various
algorithms such as random forests, gradient boosting, neural networks, etc. In
this paper, we introduce a new package i.e. DriveML for automated machine
learning. DriveML helps in implementing some of the pillars of an automated
machine learning pipeline such as automated data preparation, feature
engineering, model building and model explanation by running the function
instead of writing lengthy R codes. The DriveML package is available in CRAN.
We compare the DriveML package with other relevant packages in CRAN/Github and
find that DriveML performs the best across different parameters. We also
provide an illustration by applying the DriveML package with default
configuration on a real world dataset. Overall, the main benefits of DriveML
are in development time savings, reduce developer's errors, optimal tuning of
machine learning models and reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1"&gt;Sayan Putatunda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1"&gt;Dayananda Ubrangala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1"&gt;Kiran Rama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1"&gt;Ravi Kondapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08428</id>
        <link href="http://arxiv.org/abs/2007.08428"/>
        <updated>2021-08-09T00:49:28.648Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep learning models has gained much traction in
the last few years. Various attacks and defenses are proposed to improve the
adversarial robustness of modern-day deep learning architectures. While all
these approaches help improve the robustness, one promising direction for
improving adversarial robustness is un-explored, i.e., the complex topology of
the neural network architecture. In this work, we answer the following
question: "Can the complex topology of a neural network give adversarial
robustness without any form of adversarial training?" empirically by
experimenting with different hand-crafted and NAS based architectures. Our
findings show that, for small-scale attacks, NAS-based architectures are more
robust for small-scale datasets and simple tasks than hand-crafted
architectures. However, as the dataset's size or the task's complexity
increase, hand-crafted architectures are more robust than NAS-based
architectures. We perform the first large scale study to understand adversarial
robustness purely from an architectural perspective. Our results show that
random sampling in the search space of DARTS (a popular NAS method) with simple
ensembling can improve the robustness to PGD attack by nearly ~12\%. We show
that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy
as a free add-on without any form of adversarial training. Our results show
that leveraging the power of neural network topology with methods like
ensembles can be an excellent way to achieve adversarial robustness without any
form of adversarial training. We also introduce a metric that can be used to
calculate the trade-off between clean accuracy and adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1"&gt;Devansh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1"&gt;Gaurav Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally Abstract Partial Models. (arXiv:2108.03213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03213</id>
        <link href="http://arxiv.org/abs/2108.03213"/>
        <updated>2021-08-09T00:49:28.628Z</updated>
        <summary type="html"><![CDATA[Humans and animals have the ability to reason and make predictions about
different courses of action at many time scales. In reinforcement learning,
option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the
framework for this kind of temporally abstract prediction and reasoning.
Natural intelligent agents are also able to focus their attention on courses of
action that are relevant or feasible in a given situation, sometimes termed
affordable actions. In this paper, we define a notion of affordances for
options, and develop temporally abstract partial option models, that take into
account the fact that an option might be affordable only in certain situations.
We analyze the trade-offs between estimation and approximation error in
planning and learning when using such models, and identify some interesting
special cases. Additionally, we demonstrate empirically the potential impact of
partial option models on the efficiency of planning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1"&gt;Khimya Khetarpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zafarali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1"&gt;Gheorghe Comanici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Label Correction for Noisy Label Learning. (arXiv:1911.03809v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03809</id>
        <link href="http://arxiv.org/abs/1911.03809"/>
        <updated>2021-08-09T00:49:28.622Z</updated>
        <summary type="html"><![CDATA[Leveraging weak or noisy supervision for building effective machine learning
models has long been an important research problem. Its importance has further
increased recently due to the growing need for large-scale datasets to train
deep learning models. Weak or noisy supervision could originate from multiple
sources including non-expert annotators or automatic labeling based on
heuristics or user interaction signals. There is an extensive amount of
previous work focusing on leveraging noisy labels. Most notably, recent work
has shown impressive gains by using a meta-learned instance re-weighting
approach where a meta-learning framework is used to assign instance weights to
noisy labels. In this paper, we extend this approach via posing the problem as
label correction problem within a meta-learning framework. We view the label
correction procedure as a meta-process and propose a new meta-learning based
framework termed MLC (Meta Label Correction) for learning with noisy labels.
Specifically, a label correction network is adopted as a meta-model to produce
corrected labels for noisy labels while the main model is trained to leverage
the corrected labeled. Both models are jointly trained by solving a bi-level
optimization problem. We run extensive experiments with different label noise
levels and types on both image recognition and text classification tasks. We
compare the reweighing and correction approaches showing that the correction
framing addresses some of the limitation of reweighting. We also show that the
proposed MLC approach achieves large improvements over previous methods in many
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumais_S/0/1/0/all/0/1"&gt;Susan Dumais&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01391</id>
        <link href="http://arxiv.org/abs/2103.01391"/>
        <updated>2021-08-09T00:49:28.615Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the dynamics of temporal difference learning with
neural network-based value function approximation over a general state space,
namely, \emph{Neural TD learning}. We consider two practically used algorithms,
projection-free and max-norm regularized Neural TD learning, and establish the
first convergence bounds for these algorithms. An interesting observation from
our results is that max-norm regularization can dramatically improve the
performance of TD learning algorithms, both in terms of sample complexity and
overparameterization. In particular, we prove that max-norm regularization
improves state-of-the-art sample complexity and overparameterization bounds.
The results in this work rely on a novel Lyapunov drift analysis of the network
parameters as a stopped and controlled random process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1"&gt;Siddhartha Satpathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A rigorous introduction for linear models. (arXiv:2105.04240v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04240</id>
        <link href="http://arxiv.org/abs/2105.04240"/>
        <updated>2021-08-09T00:49:28.608Z</updated>
        <summary type="html"><![CDATA[This survey is meant to provide an introduction to linear models and the
theories behind them. Our goal is to give a rigorous introduction to the
readers with prior exposure to ordinary least squares. In machine learning, the
output is usually a nonlinear function of the input. Deep learning even aims to
find a nonlinear dependence with many layers which require a large amount of
computation. However, most of these algorithms build upon simple linear models.
We then describe linear models from different views and find the properties and
theories behind the models. The linear model is the main technique in
regression problems and the primary tool for it is the least squares
approximation which minimizes a sum of squared errors. This is a natural choice
when we're interested in finding the regression function which minimizes the
corresponding expected squared error. This survey is primarily a summary of
purpose, significance of important theories behind linear models, e.g.,
distribution theory, minimum variance estimator. We first describe ordinary
least squares from three different points of view upon which we disturb the
model with random noise and Gaussian noise. By Gaussian noise, the model gives
rise to the likelihood so that we introduce a maximum likelihood estimator. It
also develops some distribution theories via this Gaussian disturbance. The
distribution theory of least squares will help us answer various questions and
introduce related applications. We then prove least squares is the best
unbiased linear model in the sense of mean squared error and most importantly,
it actually approaches the theoretical limit. We end up with linear models with
the Bayesian approach and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02870</id>
        <link href="http://arxiv.org/abs/2108.02870"/>
        <updated>2021-08-09T00:49:28.600Z</updated>
        <summary type="html"><![CDATA[Covid-19 detection at an early stage can aid in an effective treatment and
isolation plan to prevent its spread. Recently, transfer learning has been used
for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major
limitations inherent to these proposed methods is limited labeled dataset size
that affects the reliability of Covid-19 diagnosis and disease progression. In
this work, we demonstrate that how we can augment limited X-ray images data by
using Contrast limited adaptive histogram equalization (CLAHE) to train the
last layer of the pre-trained deep learning models to mitigate the bias of
transfer learning for Covid-19 detection. We transfer learned various
pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,
and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.
The experiment results reveal that the CLAHE-based augmentation to various
pre-trained deep learning models significantly improves the model efficiency.
The pre-trained VCG-16 model with CLAHEbased augmented images achieves a
sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when
trained on non-augmented data. Other models demonstrate a value of less than
60% when trained on non-augmented data. Our results reveal that the sample bias
can negatively impact the performance of transfer learning which is
significantly improved by using CLAHE-based augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1"&gt;Aparna Reji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-08-09T00:49:28.592Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05476</id>
        <link href="http://arxiv.org/abs/2106.05476"/>
        <updated>2021-08-09T00:49:28.574Z</updated>
        <summary type="html"><![CDATA[Node embedding learns a low-dimensional representation for each node in the
graph. Recent progress on node embedding shows that proximity matrix
factorization methods gain superb performance and scale to large graphs with
millions of nodes. Existing approaches first define a proximity matrix and then
learn the embeddings that fit the proximity by matrix factorization. Most
existing matrix factorization methods adopt the same proximity for different
tasks, while it is observed that different tasks and datasets may require
different proximity, limiting their representation power.

Motivated by this, we propose {\em Lemane}, a framework with trainable
proximity measures, which can be learned to best suit the datasets and tasks at
hand automatically. Our method is end-to-end, which incorporates differentiable
SVD in the pipeline so that the parameters can be trained via backpropagation.
However, this learning process is still expensive on large graphs. To improve
the scalability, we train proximity measures only on carefully subsampled
graphs, and then apply standard proximity matrix factorization on the original
graph using the learned proximity. Note that, computing the learned proximities
for each pair is still expensive for large graphs, and existing techniques for
computing proximities are not applicable to the learned proximities. Thus, we
present generalized push techniques to make our solution scalable to large
graphs with millions of nodes. Extensive experiments show that our proposed
solution outperforms existing solutions on both link prediction and node
classification tasks on almost all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1"&gt;Kun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sibo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02504</id>
        <link href="http://arxiv.org/abs/2102.02504"/>
        <updated>2021-08-09T00:49:28.566Z</updated>
        <summary type="html"><![CDATA[Online learning methods, like the online gradient algorithm (OGA) and
exponentially weighted aggregation (EWA), often depend on tuning parameters
that are difficult to set in practice. We consider an online meta-learning
scenario, and we propose a meta-strategy to learn these parameters from past
tasks. Our strategy is based on the minimization of a regret bound. It allows
to learn the initialization and the step size in OGA with guarantees. It also
allows to learn the prior or the learning rate in EWA. We provide a regret
analysis of the strategy. It allows to identify settings where meta-learning
indeed improves on learning each task in isolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1"&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1"&gt;Pierre Alquier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study on Dense and Sparse (Visual) Rewards in Robot Policy Learning. (arXiv:2108.03222v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03222</id>
        <link href="http://arxiv.org/abs/2108.03222"/>
        <updated>2021-08-09T00:49:28.560Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (DRL) is a promising approach for teaching robots
new behaviour. However, one of its main limitations is the need for carefully
hand-coded reward signals by an expert. We argue that it is crucial to automate
the reward learning process so that new skills can be taught to robots by their
users. To address such automation, we consider task success classifiers using
visual observations to estimate the rewards in terms of task success. In this
work, we study the performance of multiple state-of-the-art deep reinforcement
learning algorithms under different types of reward: Dense, Sparse, Visual
Dense, and Visual Sparse rewards. Our experiments in various simulation tasks
(Pendulum, Reacher, Pusher, and Fetch Reach) show that while DRL agents can
learn successful behaviours using visual rewards when the goal targets are
distinguishable, their performance may decrease if the task goal is not clearly
visible. Our results also show that visual dense rewards are more successful
than visual sparse rewards and that there is no single best algorithm for all
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1"&gt;Abdalkarim Mohtasib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1"&gt;Heriberto Cuayahuitl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03117</id>
        <link href="http://arxiv.org/abs/2108.03117"/>
        <updated>2021-08-09T00:49:28.553Z</updated>
        <summary type="html"><![CDATA[In recent years, deep learning based methods have shown success in essential
medical image analysis tasks such as segmentation. Post-processing and refining
the results of segmentation is a common practice to decrease the
misclassifications originating from the segmentation network. In addition to
widely used methods like Conditional Random Fields (CRFs) which focus on the
structure of the segmented volume/area, a graph-based recent approach makes use
of certain and uncertain points in a graph and refines the segmentation
according to a small graph convolutional network (GCN). However, there are two
drawbacks of the approach: most of the edges in the graph are assigned randomly
and the GCN is trained independently from the segmentation network. To address
these issues, we define a new neighbor-selection mechanism according to feature
distances and combine the two networks in the training procedure. According to
the experimental results on pancreas segmentation from Computed Tomography (CT)
images, we demonstrate improvement in the quantitative measures. Also,
examining the dynamic neighbors created by our method, edges between
semantically similar image parts are observed. The proposed method also shows
qualitative enhancements in the segmentation maps, as demonstrated in the
visual results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ufuk Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1"&gt;Atahan Ozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1"&gt;Yusuf H. Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02998</id>
        <link href="http://arxiv.org/abs/2108.02998"/>
        <updated>2021-08-09T00:49:28.536Z</updated>
        <summary type="html"><![CDATA[The aortic vessel tree is composed of the aorta and its branching arteries,
and plays a key role in supplying the whole body with blood. Aortic diseases,
like aneurysms or dissections, can lead to an aortic rupture, whose treatment
with open surgery is highly risky. Therefore, patients commonly undergo drug
treatment under constant monitoring, which requires regular inspections of the
vessels through imaging. The standard imaging modality for diagnosis and
monitoring is computed tomography (CT), which can provide a detailed picture of
the aorta and its branching vessels if combined with a contrast agent,
resulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree
geometry from consecutive CTAs, are overlaid and compared. This allows to not
only detect changes in the aorta, but also more peripheral vessel tree changes,
caused by the primary pathology or newly developed. When performed manually,
this reconstruction requires slice by slice contouring, which could easily take
a whole day for a single aortic vessel tree and, hence, is not feasible in
clinical practice. Automatic or semi-automatic vessel tree segmentation
algorithms, on the other hand, can complete this task in a fraction of the
manual execution time and run in parallel to the clinical routine of the
clinicians. In this paper, we systematically review computing techniques for
the automatic and semi-automatic segmentation of the aortic vessel tree. The
review concludes with an in-depth discussion on how close these
state-of-the-art approaches are to an application in clinical practice and how
active this research field is, taking into account the number of publications,
datasets and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1"&gt;Antonio Pepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1"&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fen-hua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1"&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1"&gt;Jan Egger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in Quantum Machine Learning: a Quantum Information Perspective. (arXiv:2102.08991v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08991</id>
        <link href="http://arxiv.org/abs/2102.08991"/>
        <updated>2021-08-09T00:49:28.528Z</updated>
        <summary type="html"><![CDATA[Quantum classification and hypothesis testing are two tightly related
subjects, the main difference being that the former is data driven: how to
assign to quantum states $\rho(x)$ the corresponding class $c$ (or hypothesis)
is learnt from examples during training, where $x$ can be either tunable
experimental parameters or classical data "embedded" into quantum states. Does
the model generalize? This is the main question in any data-driven strategy,
namely the ability to predict the correct class even of previously unseen
states. Here we establish a link between quantum machine learning
classification and quantum hypothesis testing (state and channel
discrimination) and then show that the accuracy and generalization capability
of quantum classifiers depend on the (R\'enyi) mutual informations $I(C{:}Q)$
and $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical
parameter space $X$ or class space $C$. Based on the above characterization, we
then show how different properties of $Q$ affect classification accuracy and
generalization, such as the dimension of the Hilbert space, the amount of
noise, and the amount of neglected information from $X$ via, e.g., pooling
layers. Moreover, we introduce a quantum version of the Information Bottleneck
principle that allows us to explore the various tradeoffs between accuracy and
generalization. Finally, in order to check our theoretical predictions, we
study the classification of the quantum phases of an Ising spin chain, and we
propose the Variational Quantum Information Bottleneck (VQIB) method to
optimize quantum embeddings of classical data to favor generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1"&gt;Leonardo Banchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1"&gt;Jason Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pirandola_S/0/1/0/all/0/1"&gt;Stefano Pirandola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08637</id>
        <link href="http://arxiv.org/abs/2007.08637"/>
        <updated>2021-08-09T00:49:28.521Z</updated>
        <summary type="html"><![CDATA[Coronaviruses constitute a family of viruses that gives rise to respiratory
diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is
crucial for an effective treatment strategy. However, the RT-PCR test which is
considered to be a gold standard in the diagnosis of COVID-19 suffers from a
high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a
feasible and effective diagnostic technique towards this objective. In this
work, we propose the COVID-19 classification problem as a three-class
classification problem to distinguish between COVID-19, normal, and pneumonia
classes. We propose a three-stage framework, named COV-ELM. Stage one deals
with preprocessing and transformation while stage two deals with feature
extraction. These extracted features are passed as an input to the ELM at the
third stage, resulting in the identification of COVID-19. The choice of ELM in
this work has been motivated by its faster convergence, better generalization
capability, and shorter training time in comparison to the conventional
gradient-based learning algorithms. As bigger and diverse datasets become
available, ELM can be quickly retrained as compared to its gradient-based
competitor models. The proposed model achieved a macro average F1-score of 0.95
and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval.
When compared to state-of-the-art machine learning algorithms, the COV-ELM is
found to outperform its competitors in this three-class classification
scenario. Further, LIME has been integrated with the proposed COV-ELM model to
generate annotated CXR images. The annotations are based on the superpixels
that have contributed to distinguish between the different classes. It was
observed that the superpixels correspond to the regions of the human lungs that
are clinically observed in COVID-19 and Pneumonia cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1"&gt;Sheetal Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Manoj Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1"&gt;Ankit Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1"&gt;Navin Lakhyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1"&gt;Arpita Saggar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Naveen Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04306</id>
        <link href="http://arxiv.org/abs/2106.04306"/>
        <updated>2021-08-09T00:49:28.514Z</updated>
        <summary type="html"><![CDATA[While classic control theory offers state of the art solutions in many
problem scenarios, it is often desired to improve beyond the structure of such
solutions and surpass their limitations. To this end, residual policy learning
(RPL) offers a formulation to improve existing controllers with reinforcement
learning (RL) by learning an additive "residual" to the output of a given
controller. However, the applicability of such an approach highly depends on
the structure of the controller. Often, internal feedback signals of the
controller limit an RL algorithm to adequately change the policy and, hence,
learn the task. We propose a new formulation that addresses these limitations
by also modifying the feedback signals to the controller with an RL policy and
show superior performance of our approach on a contact-rich peg-insertion task
under position and orientation uncertainty. In addition, we use a recent
Cartesian impedance control architecture as the control framework which can be
available to us as a black-box while assuming no knowledge about its
input/output structure, and show the difficulties of standard RPL. Furthermore,
we introduce an adaptive curriculum for the given task to gradually increase
the task difficulty in terms of position and orientation uncertainty. A video
showing the results can be found at https://youtu.be/SAZm_Krze7U .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1"&gt;Alireza Ranjbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1"&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. (arXiv:2108.02904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02904</id>
        <link href="http://arxiv.org/abs/2108.02904"/>
        <updated>2021-08-09T00:49:28.507Z</updated>
        <summary type="html"><![CDATA[Optimizing economic and public policy is critical to address socioeconomic
issues and trade-offs, e.g., improving equality, productivity, or wellness, and
poses a complex mechanism design problem. A policy designer needs to consider
multiple objectives, policy levers, and behavioral responses from strategic
actors who optimize for their individual objectives. Moreover, real-world
policies should be explainable and robust to simulation-to-reality gaps, e.g.,
due to calibration issues. Existing approaches are often limited to a narrow
set of policy levers or objectives that are hard to measure, do not yield
explicit optimal policies, or do not consider strategic behavior, for example.
Hence, it remains challenging to optimize policy in real-world scenarios. Here
we show that the AI Economist framework enables effective, flexible, and
interpretable policy design using two-level reinforcement learning (RL) and
data-driven simulations. We validate our framework on optimizing the stringency
of US state policies and Federal subsidies during a pandemic, e.g., COVID-19,
using a simulation fitted to real data. We find that log-linear policies
trained using RL significantly improve social welfare, based on both public
health and economic outcomes, compared to past outcomes. Their behavior can be
explained, e.g., well-performing policies respond strongly to changes in
recovery and vaccination rates. They are also robust to calibration errors,
e.g., infection rates that are over or underestimated. As of yet, real-world
policymaking has not seen adoption of machine learning methods at large,
including RL and AI-driven simulations. Our results show the potential of AI to
guide policy design and improve social welfare amidst the complexity of the
real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1"&gt;Alexander Trott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wal_D/0/1/0/all/0/1"&gt;Douwe van der Wal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haneuse_S/0/1/0/all/0/1"&gt;Sebastien Haneuse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Stephan Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work. (arXiv:2108.03087v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.03087</id>
        <link href="http://arxiv.org/abs/2108.03087"/>
        <updated>2021-08-09T00:49:28.489Z</updated>
        <summary type="html"><![CDATA[Requirements Engineering (RE) is the initial step towards building a software
system. The success or failure of a software project is firmly tied to this
phase, based on communication among stakeholders using natural language. The
problem with natural language is that it can easily lead to different
understandings if it is not expressed precisely by the stakeholders involved,
which results in building a product different from the expected one. Previous
work proposed to enhance the quality of the software requirements detecting
language errors based on ISO 29148 requirements language criteria. The existing
solutions apply classical Natural Language Processing (NLP) to detect them. NLP
has some limitations, such as domain dependability which results in poor
generalization capability. Therefore, this work aims to improve the previous
work by creating a manually labeled dataset and using ensemble learning, Deep
Learning (DL), and techniques such as word embeddings and transfer learning to
overcome the generalization problem that is tied with classical NLP and improve
precision and recall metrics using a manually labeled dataset. The current
findings show that the dataset is unbalanced and which class examples should be
added more. It is tempting to train algorithms even if the dataset is not
considerably representative. Whence, the results show that models are
overfitting; in Machine Learning this issue is solved by adding more instances
to the dataset, improving label quality, removing noise, and reducing the
learning algorithms complexity, which is planned for this research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1"&gt;Mohammad Kasra Habib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1"&gt;Stefan Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graziotin_D/0/1/0/all/0/1"&gt;Daniel Graziotin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06341</id>
        <link href="http://arxiv.org/abs/2012.06341"/>
        <updated>2021-08-09T00:49:28.462Z</updated>
        <summary type="html"><![CDATA[System identification aims to build models of dynamical systems from data.
Traditionally, choosing the model requires the designer to balance between two
goals of conflicting nature; the model must be rich enough to capture the
system dynamics, but not so flexible that it learns spurious random effects
from the dataset. It is typically observed that the model validation
performance follows a U-shaped curve as the model complexity increases. Recent
developments in machine learning and statistics, however, have observed
situations where a "double-descent" curve subsumes this U-shaped
model-performance curve. With a second decrease in performance occurring beyond
the point where the model has reached the capacity of interpolating - i.e.,
(near) perfectly fitting - the training data. To the best of our knowledge,
such phenomena have not been studied within the context of dynamic systems. The
present paper aims to answer the question: "Can such a phenomenon also be
observed when estimating parameters of dynamic systems?" We show that the
answer is yes, verifying such behavior experimentally both for artificially
generated and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ant&amp;#xf4;nio H. Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendriks_J/0/1/0/all/0/1"&gt;Johannes N. Hendriks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian G. Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03140</id>
        <link href="http://arxiv.org/abs/2108.03140"/>
        <updated>2021-08-09T00:49:28.455Z</updated>
        <summary type="html"><![CDATA[Extreme Learning Machine is a powerful classification method very competitive
existing classification methods. It is extremely fast at training.
Nevertheless, it cannot perform face verification tasks properly because face
verification tasks require comparison of facial images of two individuals at
the same time and decide whether the two faces identify the same person. The
structure of Extreme Leaning Machine was not designed to feed two input data
streams simultaneously, thus, in 2-input scenarios Extreme Learning Machine
methods are normally applied using concatenated inputs. However, this setup
consumes two times more computational resources and it is not optimized for
recognition tasks where learning a separable distance metric is critical. For
these reasons, we propose and develop a Siamese Extreme Learning Machine
(SELM). SELM was designed to be fed with two data streams in parallel
simultaneously. It utilizes a dual-stream Siamese condition in the extra
Siamese layer to transform the data before passing it along to the hidden
layer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature
exclusively trained on a variety of specific demographic groups. This feature
enables learning and extracting of useful facial features of each group.
Experiments were conducted to evaluate and compare the performances of SELM,
Extreme Learning Machine, and DCNN. The experimental results showed that the
proposed feature was able to perform correct classification at 97.87% accuracy
and 99.45% AUC. They also showed that using SELM in conjunction with the
proposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the
well-known DCNN and Extreme Leaning Machine methods by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1"&gt;Wasu Kudisthalert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1"&gt;Kitsuchart Pasupa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence. (arXiv:2108.02837v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2108.02837</id>
        <link href="http://arxiv.org/abs/2108.02837"/>
        <updated>2021-08-09T00:49:28.448Z</updated>
        <summary type="html"><![CDATA[The elastic properties of materials derive from their electronic and atomic
nature. However, simulating bulk materials fully at these scales is not
feasible, so that typically homogenized continuum descriptions are used
instead. A seamless and lossless transition of the constitutive description of
the elastic response of materials between these two scales has been so far
elusive. Here we show how this problem can be overcome by using Artificial
Intelligence (AI). A Convolutional Neural Network (CNN) model is trained, by
taking the structure image of a nanoporous material as input and the
corresponding elasticity tensor, calculated from Molecular Statics (MS), as
output. Trained with the atomistic data, the CNN model captures the size- and
pore-dependency of the material's elastic properties which, on the physics
side, can stem from surfaces and non-local effects. Such effects are often
ignored in upscaling from atomistic to classical continuum theory. To
demonstrate the accuracy and the efficiency of the trained CNN model, a Finite
Element Method (FEM) based result of an elastically deformed nanoporous beam
equipped with the CNN as constitutive law is compared with that by a full
atomistic simulation. The good agreement between the atomistic simulations and
the FEM-AI combination for a system with size and surface effects establishes a
new lossless scale bridging approach to such problems. The trained CNN model
deviates from the atomistic result by 9.6\% for porosity scenarios of up to
90\% but it is about 230 times faster than the MS calculation and does not
require to change simulation methods between different scales. The efficiency
of the CNN evaluation together with the preservation of important atomistic
effects makes the trained model an effective atomistically-informed
constitutive model for macroscopic simulations of nanoporous materials and
solving of inverse problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mianroodi_J/0/1/0/all/0/1"&gt;Jaber Rezaei Mianroodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Shahed Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Siboni_N/0/1/0/all/0/1"&gt;Nima H. Siboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bai-Xiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Raabe_D/0/1/0/all/0/1"&gt;Dierk Raabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07942</id>
        <link href="http://arxiv.org/abs/1912.07942"/>
        <updated>2021-08-09T00:49:28.441Z</updated>
        <summary type="html"><![CDATA[To continuously improve quality and reflect changes in data, machine learning
applications have to regularly retrain and update their core models. We show
that a differential analysis of language model snapshots before and after an
update can reveal a surprising amount of detailed information about changes in
the training data. We propose two new metrics---\emph{differential score} and
\emph{differential rank}---for analyzing the leakage due to updates of natural
language models. We perform leakage analysis using these metrics across models
trained on several different datasets using different methods and
configurations. We discuss the privacy implications of our findings, propose
mitigation strategies and evaluate their effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1"&gt;Santiago Zanella-B&amp;#xe9;guelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1"&gt;Victor R&amp;#xfc;hle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1"&gt;Andrew Paverd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1"&gt;Boris K&amp;#xf6;pf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects. (arXiv:2108.03039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03039</id>
        <link href="http://arxiv.org/abs/2108.03039"/>
        <updated>2021-08-09T00:49:28.423Z</updated>
        <summary type="html"><![CDATA[Conditional average treatment effects (CATEs) allow us to understand the
effect heterogeneity across a large population of individuals. However, typical
CATE learners assume all confounding variables are measured in order for the
CATE to be identifiable. Often, this requirement is satisfied by simply
collecting many variables, at the expense of increased sample complexity for
estimating CATEs. To combat this, we propose an energy-based model (EBM) that
learns a low-dimensional representation of the variables by employing a noise
contrastive loss function. With our EBM we introduce a preprocessing step that
alleviates the dimensionality curse for any existing model and learner
developed for estimating CATE. We prove that our EBM keeps the representations
partially identifiable up to some universal constant, as well as having
universal approximation capability to avoid excessive information loss from
model misspecification; these properties combined with our loss function,
enable the representations to converge and keep the CATE estimation consistent.
Experiments demonstrate the convergence of the representations, as well as show
that estimating CATEs on our representations performs better than on the
variables or the representations obtained via various benchmark dimensionality
reduction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1"&gt;Jeroen Berrevoets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.03002</id>
        <link href="http://arxiv.org/abs/2108.03002"/>
        <updated>2021-08-09T00:49:28.416Z</updated>
        <summary type="html"><![CDATA[More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition
(CSVD-QR) method for matrix complete problem is presented, whose computational
complexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than
$\min\{m,n\}$, where $r$ represents the largest number of singular values of
matrix $X$. What is particularly interesting is that after replacing the
nuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as
the upper bound of the nuclear norm, when the intermediate matrix $D$ in its
decomposition is close to the diagonal matrix, it will converge to the nuclear
norm, and is exactly equal, when the $D$ matrix is equal to the diagonal
matrix, to the nuclear norm, which ingeniously avoids the calculation of the
singular value of the matrix. To the best of our knowledge, there is no
literature to generalize and apply it to solve tensor complete problems.
Inspired by this, in this paper we propose a class of tensor minimization model
based on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,
which is convex and therefore has a global minimum solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1"&gt;HongBing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;XinYi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1"&gt;HongTao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;YaJing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinlin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03131</id>
        <link href="http://arxiv.org/abs/2108.03131"/>
        <updated>2021-08-09T00:49:28.409Z</updated>
        <summary type="html"><![CDATA[The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of
life globally, and a critical factor in mitigating its effects is screening
individuals for infections, thereby allowing for both proper treatment for
those individuals as well as action to be taken to prevent further spread of
the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a
screening tool as it is a much cheaper and easier to apply imaging modality
than others that are traditionally used for pulmonary examinations, namely
chest x-ray and computed tomography. Given the scarcity of expert radiologists
for interpreting POCUS examinations in many highly affected regions around the
world, low-cost deep learning-driven clinical decision support solutions can
have a large impact during the on-going pandemic. Motivated by this, we
introduce COVID-Net US, a highly efficient, self-attention deep convolutional
neural network design tailored for COVID-19 screening from lung POCUS images.
Experimental results show that the proposed COVID-Net US can achieve an AUC of
over 0.98 while achieving 353X lower architectural complexity, 62X lower
computational complexity, and 14.3X faster inference times on a Raspberry Pi.
Clinical validation was also conducted, where select cases were reviewed and
reported on by a practicing clinician (20 years of clinical practice)
specializing in intensive care (ICU) and 15 years of expertise in POCUS
interpretation. To advocate affordable healthcare and artificial intelligence
for resource-constrained environments, we have made COVID-Net US open source
and publicly available as part of the COVID-Net open source initiative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1"&gt;Alexander MacLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1"&gt;Andy Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1"&gt;Maya Pavlova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1"&gt;Sonny Kohli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques. (arXiv:2108.02838v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02838</id>
        <link href="http://arxiv.org/abs/2108.02838"/>
        <updated>2021-08-09T00:49:28.402Z</updated>
        <summary type="html"><![CDATA[Market indicators such as CPI and GDP have been widely used over decades to
identify the stage of business cycles and also investment attractiveness of
sectors given market conditions. In this paper, we propose a two-stage
methodology that consists of predicting ETF prices for each sector using market
indicators and ranking sectors based on their predicted rate of returns. We
initially start with choosing sector specific macroeconomic indicators and
implement Recursive Feature Elimination algorithm to select the most important
features for each sector. Using our prediction tool, we implement different
Recurrent Neural Networks models to predict the future ETF prices for each
sector. We then rank the sectors based on their predicted rate of returns. We
select the best performing model by evaluating the annualized return,
annualized Sharpe ratio, and Calmar ratio of the portfolios that includes the
top four ranked sectors chosen by the model. We also test the robustness of the
model performance with respect to lookback windows and look ahead windows. Our
empirical results show that our methodology beats the equally weighted
portfolio performance even in the long run. We also find that Echo State
Networks exhibits an outstanding performance compared to other models yet it is
faster to implement compared to other RNN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1"&gt;Tugce Karatas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1"&gt;Ali Hirsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Driving Scenario Trajectories with Active Learning. (arXiv:2108.03217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03217</id>
        <link href="http://arxiv.org/abs/2108.03217"/>
        <updated>2021-08-09T00:49:28.385Z</updated>
        <summary type="html"><![CDATA[Annotating the driving scenario trajectories based only on explicit rules
(i.e., knowledge-based methods) can be subject to errors, such as false
positive/negative classification of scenarios that lie on the border of two
scenario classes, missing unknown scenario classes, and also anomalies. On the
other side, verifying the labels by the annotators is not cost-efficient. For
this purpose, active learning (AL) could potentially improve the annotation
procedure by inclusion of an annotator/expert in an efficient way. In this
study, we develop an active learning framework to annotate driving trajectory
time-series data. At the first step, we compute an embedding of the time-series
trajectories into a latent space in order to extract the temporal nature. For
this purpose, we study three different latent space representations:
multivariate Time Series t-Distributed Stochastic Neighbor Embedding (mTSNE),
Recurrent Auto-Encoder (RAE) and Variational Recurrent Auto-Encoder (VRAE). We
then apply different active learning paradigms with different classification
models to the embedded data. In particular, we study the two classifiers Neural
Network (NN) and Support Vector Machines (SVM), with three active learning
query strategies (i.e., entropy, margin and random). In the following, we
explore the possibilities of the framework to discover unknown classes and
demonstrate how it can be used to identify the out-of-class trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarl_S/0/1/0/all/0/1"&gt;Sanna Jarl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahrovani_S/0/1/0/all/0/1"&gt;Sadegh Rahrovani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1"&gt;Morteza Haghir Chehreghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03177</id>
        <link href="http://arxiv.org/abs/2108.03177"/>
        <updated>2021-08-09T00:49:28.378Z</updated>
        <summary type="html"><![CDATA[Seizure detection algorithms must discriminate abnormal neuronal activity
associated with a seizure from normal neural activity in a variety of
conditions. Our approach is to seek spatiotemporal waveforms with distinct
morphology in electrocorticographic (ECoG) recordings of epileptic patients
that are indicative of a subsequent seizure (preictal) versus non-seizure
segments (interictal). To find these waveforms we apply a shift-invariant
k-means algorithm to segments of spatially filtered signals to learn codebooks
of prototypical waveforms. The frequency of the cluster labels from the
codebooks is then used to train a binary classifier that predicts the class
(preictal or interictal) of a test ECoG segment. We use the Matthews
correlation coefficient to evaluate the performance of the classifier and the
quality of the codebooks. We found that our method finds recurrent
non-sinusoidal waveforms that could be used to build interpretable features for
seizure prediction and that are also physiologically meaningful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1"&gt;Carlos H. Mendoza-Cardenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1"&gt;Austin J. Brockmeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rectified Euler k-means and Beyond. (arXiv:2108.03081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03081</id>
        <link href="http://arxiv.org/abs/2108.03081"/>
        <updated>2021-08-09T00:49:28.371Z</updated>
        <summary type="html"><![CDATA[Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of
equi-dimensional space via a complex mapping which induces the robust Euler
kernel and next employs the popular $k$-means. Consequently, besides enjoying
the virtues of k-means such as simplicity and scalability to large data sets,
EulerK is also robust to noises and outliers. Although so, the centroids
captured by EulerK deviate from the unit hyper-sphere surface and thus in
strict distributional sense, actually are outliers. This weird phenomenon also
occurs in some generic kernel clustering methods. Intuitively, using such
outlier-like centroids should not be quite reasonable but it is still seldom
attended. To eliminate the deviation, we propose two Rectified Euler k-means
methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquire
real centroids residing on the mapped space to better characterize the data
structures. Specifically, REK1 rectifies EulerK by imposing the constraint on
the centroids while REK2 views each centroid as the mapped image from a
pre-image in the original space and optimizes these pre-images in Euler kernel
induced space. Undoubtedly, our proposed REKs can methodologically be extended
to solve problems of such a category. Finally, the experiments validate the
effectiveness of REK1 and REK2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yunxia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+chen_S/0/1/0/all/0/1"&gt;Songcan chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inspecting the Process of Bank Credit Rating via Visual Analytics. (arXiv:2108.03011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03011</id>
        <link href="http://arxiv.org/abs/2108.03011"/>
        <updated>2021-08-09T00:49:28.361Z</updated>
        <summary type="html"><![CDATA[Bank credit rating classifies banks into different levels based on publicly
disclosed and internal information, serving as an important input in financial
risk management. However, domain experts have a vague idea of exploring and
comparing different bank credit rating schemes. A loose connection between
subjective and quantitative analysis and difficulties in determining
appropriate indicator weights obscure understanding of bank credit ratings.
Furthermore, existing models fail to consider bank types by just applying a
unified indicator weight set to all banks. We propose RatingVis to assist
experts in exploring and comparing different bank credit rating schemes. It
supports interactively inferring indicator weights for banks by involving
domain knowledge and considers bank types in the analysis loop. We conduct a
case study with real-world bank data to verify the efficacy of RatingVis.
Expert feedback suggests that our approach helps them better understand
different rating schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiangqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Quan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Tangzhi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaojuan Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features. (arXiv:2108.02850v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02850</id>
        <link href="http://arxiv.org/abs/2108.02850"/>
        <updated>2021-08-09T00:49:28.354Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition is a difficult problem in pattern recognition
because several sources of variability exist in the speech input like the
channel variations, the input might be clean or noisy, the speakers may have
different accent and variations in the gender, etc. As a result, domain
adaptation is important in speech recognition where we train the model for a
particular source domain and test it on a different target domain. In this
paper, we propose a technique to perform unsupervised gender-based domain
adaptation in speech recognition using phonetic features. The experiments are
performed on the TIMIT dataset and there is a considerable decrease in the
phoneme error rate using the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ojha_R/0/1/0/all/0/1"&gt;Rupam Ojha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sekhar_C/0/1/0/all/0/1"&gt;C Chandra Sekhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning. (arXiv:2108.02889v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02889</id>
        <link href="http://arxiv.org/abs/2108.02889"/>
        <updated>2021-08-09T00:49:28.345Z</updated>
        <summary type="html"><![CDATA[Many of the devices used in Internet-of-Things (IoT) applications are
energy-limited, and thus supplying energy while maintaining seamless
connectivity for IoT devices is of considerable importance. In this context, we
propose a simultaneous wireless power transfer and information transmission
scheme for IoT devices with support from reconfigurable intelligent surface
(RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a
first phase, IoT devices harvest energy from the UAV through wireless power
transfer; and then in a second phase, the UAV collects data from the IoT
devices through information transmission. To characterise the agility of the
UAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at
maximizing the total network sum-rate, we jointly optimize the trajectory of
the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift
matrix of the RIS. We formulate a Markov decision process and propose two deep
reinforcement learning algorithms to solve the optimization problem of
maximizing the total network sum-rate. Numerical results illustrate the
effectiveness of the UAV's flying path optimization and the network's
throughput of our proposed techniques compared with other benchmark schemes.
Given the strict requirements of the RIS and UAV, the significant improvement
in processing time and throughput performance demonstrates that our proposed
scheme is well applicable for practical IoT applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Khac Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1"&gt;Antonino Masaracchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Do_Duy_T/0/1/0/all/0/1"&gt;Tan Do-Duy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1"&gt;Trung Q. Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private n-gram Extraction. (arXiv:2108.02831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02831</id>
        <link href="http://arxiv.org/abs/2108.02831"/>
        <updated>2021-08-09T00:49:28.337Z</updated>
        <summary type="html"><![CDATA[We revisit the problem of $n$-gram extraction in the differential privacy
setting. In this problem, given a corpus of private text data, the goal is to
release as many $n$-grams as possible while preserving user level privacy.
Extracting $n$-grams is a fundamental subroutine in many NLP applications such
as sentence completion, response generation for emails etc. The problem also
arises in other applications such as sequence mining, and is a generalization
of recently studied differentially private set union (DPSU). In this paper, we
develop a new differentially private algorithm for this problem which, in our
experiments, significantly outperforms the state-of-the-art. Our improvements
stem from combining recent advances in DPSU, privacy accounting, and new
heuristics for pruning in the tree-based approach initiated by Chen et al.
(2012).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kunho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1"&gt;Sergey Yekhanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03084</id>
        <link href="http://arxiv.org/abs/2108.03084"/>
        <updated>2021-08-09T00:49:28.330Z</updated>
        <summary type="html"><![CDATA[Recently published graph neural networks (GNNs) show promising performance at
social event detection tasks. However, most studies are oriented toward
monolingual data in languages with abundant training samples. This has left the
more common multilingual settings and lesser-spoken languages relatively
unexplored. Thus, we present a GNN that incorporates cross-lingual word
embeddings for detecting events in multilingual data streams. The first exploit
is to make the GNN work with multilingual data. For this, we outline a
construction strategy that aligns messages in different languages at both the
node and semantic levels. Relationships between messages are established by
merging entities that are the same but are referred to in different languages.
Non-English message representations are converted into English semantic space
via the cross-lingual word embeddings. The resulting message graph is then
uniformly encoded by a GNN model. In special cases where a lesser-spoken
language needs to be detected, a novel cross-lingual knowledge distillation
framework, called CLKD, exploits prior knowledge learned from similar threads
in English to make up for the paucity of annotated data. Experiments on both
synthetic and real-world datasets show the framework to be highly effective at
detection in both multilingual data and in languages where training samples are
scarce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jiaqian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Elementary Proof that Q-learning Converges Almost Surely. (arXiv:2108.02827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02827</id>
        <link href="http://arxiv.org/abs/2108.02827"/>
        <updated>2021-08-09T00:49:28.310Z</updated>
        <summary type="html"><![CDATA[Watkins' and Dayan's Q-learning is a model-free reinforcement learning
algorithm that iteratively refines an estimate for the optimal action-value
function of an MDP by stochastically "visiting" many state-ation pairs [Watkins
and Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent
state-of-the-art achievements in reinforcement learning, including the
superhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this
paper is to reproduce a precise and (nearly) self-contained proof that
Q-learning converges. Much of the available literature leverages powerful
theory to obtain highly generalizable results in this vein. However, this
approach requires the reader to be familiar with and make many deep connections
to different research areas. A student seeking to deepen their understand of
Q-learning risks becoming caught in a vicious cycle of "RL-learning Hell". For
this reason, we give a complete proof from start to finish using only one
external result from the field of stochastic approximation, despite the fact
that this minimal dependence on other results comes at the expense of some
"shininess".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regehr_M/0/1/0/all/0/1"&gt;Matthew T. Regehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1"&gt;Alex Ayoub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories. (arXiv:2108.02834v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02834</id>
        <link href="http://arxiv.org/abs/2108.02834"/>
        <updated>2021-08-09T00:49:28.302Z</updated>
        <summary type="html"><![CDATA[Anomalous diffusion occurs at very different scales in nature, from atomic
systems to motions in cell organelles, biological tissues or ecology, and also
in artificial materials, such as cement. Being able to accurately measure the
anomalous exponent associated with a given particle trajectory, thus
determining whether the particle subdiffuses, superdiffuses or performs normal
diffusion is of key importance to understand the diffusion process. Also, it is
often important to trustingly identify the model behind the trajectory, as this
gives a large amount of information on the system dynamics. Both aspects are
particularly difficult when the input data are short and noisy trajectories. It
is even more difficult if one cannot guarantee that the trajectories output in
experiments is homogeneous, hindering the statistical methods based on
ensembles of trajectories. We present a data-driven method able to infer the
anomalous exponent and to identify the type of anomalous diffusion process
behind single, noisy and short trajectories, with good accuracy. This model was
used in our participation in the Anomalous Diffusion (AnDi) Challenge. A
combination of convolutional and recurrent neural networks were used to achieve
state-of-the-art results when compared to methods participating in the AnDi
Challenge, ranking top 4 in both classification and diffusion exponent
regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orts_O/0/1/0/all/0/1"&gt;&amp;#xd2;scar Garibo i Orts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_March_M/0/1/0/all/0/1"&gt;Miguel A. Garcia-March&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conejero_J/0/1/0/all/0/1"&gt;J. Alberto Conejero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.03090</id>
        <link href="http://arxiv.org/abs/2108.03090"/>
        <updated>2021-08-09T00:49:28.288Z</updated>
        <summary type="html"><![CDATA[We investigate the functioning of a classifying biological neural network
from the perspective of statistical learning theory, modelled, in a simplified
setting, as a continuous-time stochastic recurrent neural network (RNN) with
identity activation function. In the purely stochastic (robust) regime, we give
a generalisation error bound that holds with high probability, thus showing
that the empirical risk minimiser is the best-in-class hypothesis. We show that
RNNs retain a partial signature of the paths they are fed as the unique
information exploited for training and classification tasks. We argue that
these RNNs are easy to train and robust and back these observations with
numerical experiments on both synthetic and real data. We also exhibit a
trade-off phenomenon between accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1"&gt;Wiebke Bartolomaeus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1"&gt;Youness Boutaib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1"&gt;Sandra Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1"&gt;Holger Rauhut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.02846</id>
        <link href="http://arxiv.org/abs/2108.02846"/>
        <updated>2021-08-09T00:49:28.272Z</updated>
        <summary type="html"><![CDATA[Human-robot collaboration is an essential research topic in artificial
intelligence (AI), enabling researchers to devise cognitive AI systems and
affords an intuitive means for users to interact with the robot. Of note,
communication plays a central role. To date, prior studies in embodied agent
navigation have only demonstrated that human languages facilitate communication
by instructions in natural languages. Nevertheless, a plethora of other forms
of communication is left unexplored. In fact, human communication originated in
gestures and oftentimes is delivered through multimodal cues, e.g. "go there"
with a pointing gesture. To bridge the gap and fill in the missing dimension of
communication in embodied agent navigation, we propose investigating the
effects of using gestures as the communicative interface instead of verbal
cues. Specifically, we develop a VR-based 3D simulation environment, named
Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human
player is placed in the same virtual scene and shepherds the artificial agent
using only gestures. The agent is tasked to solve the navigation problem guided
by natural gestures with unknown semantics; we do not use any predefined
gestures due to the diversity and versatile nature of human gestures. We argue
that learning the semantics of natural gestures is mutually beneficial to
learning the navigation task--learn to communicate and communicate to learn. In
a series of experiments, we demonstrate that human gesture cues, even without
predefined semantics, improve the object-goal navigation for an embodied agent,
outperforming various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng-Ju Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yixin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attainment Regions in Feature-Parameter Space for High-Level Debugging in Autonomous Robots. (arXiv:2108.03150v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03150</id>
        <link href="http://arxiv.org/abs/2108.03150"/>
        <updated>2021-08-09T00:49:28.258Z</updated>
        <summary type="html"><![CDATA[Understanding a controller's performance in different scenarios is crucial
for robots that are going to be deployed in safety-critical tasks. If we do not
have a model of the dynamics of the world, which is often the case in complex
domains, we may need to approximate a performance function of the robot based
on its interaction with the environment. Such a performance function gives us
insights into the behaviour of the robot, allowing us to fine-tune the
controller with manual interventions. In high-dimensionality systems, where the
actionstate space is large, fine-tuning a controller is non-trivial. To
overcome this problem, we propose a performance function whose domain is
defined by external features and parameters of the controller. Attainment
regions are defined over such a domain defined by feature-parameter pairs, and
serve the purpose of enabling prediction of successful execution of the task.
The use of the feature-parameter space -in contrast to the action-state space-
allows us to adapt, explain and finetune the controller over a simpler (i.e.,
lower dimensional space). When the robot successfully executes the task, we use
the attainment regions to gain insights into the limits of the controller, and
its robustness. When the robot fails to execute the task, we use the regions to
debug the controller and find adaptive and counterfactual changes to the
solutions. Another advantage of this approach is that we can generalise through
the use of Gaussian processes regression of the performance function in the
high-dimensional space. To test our approach, we demonstrate learning an
approximation to the performance function in simulation, with a mobile robot
traversing different terrain conditions. Then, with a sample-efficient method,
we propagate the attainment regions to a physical robot in a similar
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1"&gt;Sim&amp;#xf3;n C. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1"&gt;Subramanian Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03214</id>
        <link href="http://arxiv.org/abs/2108.03214"/>
        <updated>2021-08-09T00:49:28.237Z</updated>
        <summary type="html"><![CDATA[There is growing interest in neural network architectures for tabular data.
Many general-purpose tabular deep learning models have been introduced
recently, with performance sometimes rivaling gradient boosted decision trees
(GBDTs). These recent models draw inspiration from various sources, including
GBDTs, factorization machines, and neural networks from other application
domains. Previous tabular neural networks are also drawn upon, but are possibly
under-considered, especially models associated with specific tabular problems.
This paper focuses on several such models, and proposes modifications for
improving their performance. When modified, these models are shown to be
competitive with leading general-purpose tabular models, including GBDTs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1"&gt;James Fiedler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Augmented Hybrid CNN for Stress Recognition Using Wrist-based Photoplethysmography Sensor. (arXiv:2108.03166v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03166</id>
        <link href="http://arxiv.org/abs/2108.03166"/>
        <updated>2021-08-09T00:49:28.230Z</updated>
        <summary type="html"><![CDATA[Stress is a physiological state that hampers mental health and has serious
consequences to physical health. Moreover, the COVID-19 pandemic has increased
stress levels among people across the globe. Therefore, continuous monitoring
and detection of stress are necessary. The recent advances in wearable devices
have allowed the monitoring of several physiological signals related to stress.
Among them, wrist-worn wearable devices like smartwatches are most popular due
to their convenient usage. And the photoplethysmography (PPG) sensor is the
most prevalent sensor in almost all consumer-grade wrist-worn smartwatches.
Therefore, this paper focuses on using a wrist-based PPG sensor that collects
Blood Volume Pulse (BVP) signals to detect stress which may be applicable for
consumer-grade wristwatches. Moreover, state-of-the-art works have used either
classical machine learning algorithms to detect stress using hand-crafted
features or have used deep learning algorithms like Convolutional Neural
Network (CNN) which automatically extracts features. This paper proposes a
novel hybrid CNN (H-CNN) classifier that uses both the hand-crafted features
and the automatically extracted features by CNN to detect stress using the BVP
signal. Evaluation on the benchmark WESAD dataset shows that, for 3-class
classification (Baseline vs. Stress vs. Amusement), our proposed H-CNN
outperforms traditional classifiers and normal CNN by 5% and 7% accuracy, and
10% and 7% macro F1 score, respectively. Also for 2-class classification
(Stress vs. Non-stress), our proposed H-CNN outperforms traditional classifiers
and normal CNN by 3% and ~5% accuracy, and ~3% and ~7% macro F1 score,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_N/0/1/0/all/0/1"&gt;Nafiul Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Luke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dautta_M/0/1/0/all/0/1"&gt;Manik Dautta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_A/0/1/0/all/0/1"&gt;Abel Jimenez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tseng_P/0/1/0/all/0/1"&gt;Peter Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpolation can hurt robust generalization even when there is no noise. (arXiv:2108.02883v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02883</id>
        <link href="http://arxiv.org/abs/2108.02883"/>
        <updated>2021-08-09T00:49:28.209Z</updated>
        <summary type="html"><![CDATA[Numerous recent works show that overparameterization implicitly reduces
variance for min-norm interpolators and max-margin classifiers. These findings
suggest that ridge regularization has vanishing benefits in high dimensions. We
challenge this narrative by showing that, even in the absence of noise,
avoiding interpolation through ridge regularization can significantly improve
generalization. We prove this phenomenon for the robust risk of both linear
regression and classification and hence provide the first theoretical result on
robust overfitting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Donhauser_K/0/1/0/all/0/1"&gt;Konstantin Donhauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tifrea_A/0/1/0/all/0/1"&gt;Alexandru &amp;#x162;ifrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aerni_M/0/1/0/all/0/1"&gt;Michael Aerni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1"&gt;Reinhard Heckel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fanny Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03067</id>
        <link href="http://arxiv.org/abs/2108.03067"/>
        <updated>2021-08-09T00:49:28.201Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates a two-stage method for deriving insights from social
media data relating to disinformation by applying a combination of geospatial
classification and embedding-based language modelling across multiple
languages. In particular, the analysis in centered on Twitter and
disinformation for three European languages: English, French and Spanish.
Firstly, Twitter data is classified into European and non-European sets using
BERT. Secondly, Word2vec is applied to the classified texts resulting in
Eurocentric, non-Eurocentric and global representations of the data for the
three target languages. This comparative analysis demonstrates not only the
efficacy of the classification method but also highlights geographic, temporal
and linguistic differences in the disinformation-related media. Thus, the
contributions of the work are threefold: (i) a novel language-independent
transformer-based geolocation method; (ii) an analytical approach that exploits
lexical specificity and word embeddings to interrogate user-generated content;
and (iii) a dataset of 36 million disinformation related tweets in English,
French and Spanish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1"&gt;David Tuxworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1"&gt;Dimosthenis Antypas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Scheduling for Federated Learning Through Over-the-Air Computation. (arXiv:2108.02891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02891</id>
        <link href="http://arxiv.org/abs/2108.02891"/>
        <updated>2021-08-09T00:49:28.181Z</updated>
        <summary type="html"><![CDATA[A new machine learning (ML) technique termed as federated learning (FL) aims
to preserve data at the edge devices and to only exchange ML model parameters
in the learning process. FL not only reduces the communication needs but also
helps to protect the local privacy. Although FL has these advantages, it can
still experience large communication latency when there are massive edge
devices connected to the central parameter server (PS) and/or millions of model
parameters involved in the learning process. Over-the-air computation (AirComp)
is capable of computing while transmitting data by allowing multiple devices to
send data simultaneously by using analog modulation. To achieve good
performance in FL through AirComp, user scheduling plays a critical role. In
this paper, we investigate and compare different user scheduling policies,
which are based on various criteria such as wireless channel conditions and the
significance of model updates. Receiver beamforming is applied to minimize the
mean-square-error (MSE) of the distortion of function aggregation result via
AirComp. Simulation results show that scheduling based on the significance of
model updates has smaller fluctuations in the training process while scheduling
based on channel condition has the advantage on energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haijian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rose Qingyang Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary Class Based Multiple Choice Learning. (arXiv:2108.02949v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02949</id>
        <link href="http://arxiv.org/abs/2108.02949"/>
        <updated>2021-08-09T00:49:28.174Z</updated>
        <summary type="html"><![CDATA[The merit of ensemble learning lies in having different outputs from many
individual models on a single input, i.e., the diversity of the base models.
The high quality of diversity can be achieved when each model is specialized to
different subsets of the whole dataset. Moreover, when each model explicitly
knows to which subsets it is specialized, more opportunities arise to improve
diversity. In this paper, we propose an advanced ensemble method, called
Auxiliary class based Multiple Choice Learning (AMCL), to ultimately specialize
each model under the framework of multiple choice learning (MCL). The
advancement of AMCL is originated from three novel techniques which control the
framework from different directions: 1) the concept of auxiliary class to
provide more distinct information through the labels, 2) the strategy, named
memory-based assignment, to determine the association between the inputs and
the models, and 3) the feature fusion module to achieve generalized features.
To demonstrate the performance of our method compared to all variants of MCL
methods, we conduct extensive experiments on the image classification and
segmentation tasks. Overall, the performance of AMCL exceeds all others in most
of the public datasets trained with various networks as members of the
ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sihwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Dae Yon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1"&gt;Taejang Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enterprise Analytics using Graph Database and Graph-based Deep Learning. (arXiv:2108.02867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02867</id>
        <link href="http://arxiv.org/abs/2108.02867"/>
        <updated>2021-08-09T00:49:28.168Z</updated>
        <summary type="html"><![CDATA[In a business-to-business (B2B) customer relationship management (CRM) use
case, each client is a potential business organization/company with a solid
business strategy and focused and rational decisions. This paper introduces a
graph-based analytics approach to improve CRM within a B2B environment. In our
approach, in the first instance, we have designed a graph database using the
Neo4j platform. Secondly, the graph database has been investigated by using
data mining and exploratory analysis coupled with cypher graph query language.
Specifically, we have applied the graph convolution network (GCN) to enable CRM
analytics to forecast sales. This is the first step towards a GCN-based binary
classification based on graph databases in the domain of B2B CRM. We evaluate
the performance of the proposed GCN model on graph databases and compare it
with Random Forest (RF), Convolutional Neural Network (CNN), and Artificial
Neural Network (ANN). The proposed GCN approach is further augmented with the
shortest path and eigenvector centrality attribute to significantly improve the
accuracy of sales prediction. Experimental results reveal that the proposed
graph-based deep learning approach outperforms the Random Forests (RsF) and two
deep learning models, i.e., CNN and ANN under different combinations of graph
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalliadan_S/0/1/0/all/0/1"&gt;Shyam Krishnan Kalliadan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03064</id>
        <link href="http://arxiv.org/abs/2108.03064"/>
        <updated>2021-08-09T00:49:28.161Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised contrastive learning approach for facial
expression recognition (FER) in videos. We propose a novel temporal
sampling-based augmentation scheme to be utilized in addition to standard
spatial augmentations used for contrastive learning. Our proposed temporal
augmentation scheme randomly picks from one of three temporal sampling
techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential
sampling. This is followed by a combination of up to three standard spatial
augmentations. We then use a deep R(2+1)D network for FER, which we train in a
self-supervised fashion based on the augmentations and subsequently fine-tune.
Experiments are performed on the Oulu-CASIA dataset and the performance is
compared to other works in FER. The results indicate that our method achieves
an accuracy of 89.4%, setting a new state-of-the-art by outperforming other
works. Additional experiments and analysis confirm the considerable
contribution of the proposed temporal augmentation versus the existing spatial
ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02943</id>
        <link href="http://arxiv.org/abs/2108.02943"/>
        <updated>2021-08-09T00:49:28.153Z</updated>
        <summary type="html"><![CDATA[Dataset bias is a critical challenge in machine learning, and its negative
impact is aggravated when models capture unintended decision rules with
spurious correlations. Although existing works often handle this issue using
human supervision, the availability of the proper annotations is impractical
and even unrealistic. To better tackle this challenge, we propose a simple but
effective debiasing technique in an unsupervised manner. Specifically, we
perform clustering on the feature embedding space and identify pseudoattributes
by taking advantage of the clustering results even without an explicit
attribute supervision. Then, we employ a novel cluster-based reweighting scheme
for learning debiased representation; this prevents minority groups from being
discounted for minimizing the overall loss, which is desirable for worst-case
generalization. The extensive experiments demonstrate the outstanding
performance of our approach on multiple standard benchmarks, which is even as
competitive as the supervised counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Seonguk Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joon-Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating dataset harms requires stewardship: Lessons from 1000 papers. (arXiv:2108.02922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02922</id>
        <link href="http://arxiv.org/abs/2108.02922"/>
        <updated>2021-08-09T00:49:28.130Z</updated>
        <summary type="html"><![CDATA[Concerns about privacy, bias, and harmful applications have shone a light on
the ethics of machine learning datasets, even leading to the retraction of
prominent datasets including DukeMTMC, MS-Celeb-1M, TinyImages, and VGGFace2.
In response, the machine learning community has called for higher ethical
standards, transparency efforts, and technical fixes in the dataset creation
process. The premise of our work is that these efforts can be more effective if
informed by an understanding of how datasets are used in practice in the
research community. We study three influential face and person recognition
datasets - DukeMTMC, MS-Celeb-1M, and Labeled Faces in the Wild (LFW) - by
analyzing nearly 1000 papers that cite them. We found that the creation of
derivative datasets and models, broader technological and social change, the
lack of clarity of licenses, and dataset management practices can introduce a
wide range of ethical concerns. We conclude by suggesting a distributed
approach that can mitigate these harms, making recommendations to dataset
creators, conference program committees, dataset users, and the broader
research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kenny Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1"&gt;Arunesh Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arvind Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Human Innate Immune System Dependencies using Graph Neural Networks. (arXiv:2108.02872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02872</id>
        <link href="http://arxiv.org/abs/2108.02872"/>
        <updated>2021-08-09T00:49:28.108Z</updated>
        <summary type="html"><![CDATA[Since the rapid outbreak of Covid-19 and with no approved vaccines to date,
profound research interest has emerged to understand the innate immune response
to viruses. This understanding can help to inhibit virus replication, prolong
adaptive immune response, accelerated virus clearance, and tissue recovery, a
key milestone to propose a vaccine to combat coronaviruses (CoVs), e.g.,
Covid-19. Although an innate immune system triggers inflammatory responses
against CoVs upon recognition of viruses, however, a vaccine is the ultimate
protection against CoV spread. The development of this vaccine is
time-consuming and requires a deep understanding of the innate immune response
system. In this work, we propose a graph neural network-based model that
exploits the interactions between pattern recognition receptors (PRRs), i.e.,
the human immune response system. These interactions can help to recognize
pathogen-associated molecular patterns (PAMPs) to predict the activation
requirements of each PRR. The immune response information of each PRR is
derived from combining its historical PAMPs activation coupled with the modeled
effect on the same from PRRs in its neighborhood. On one hand, this work can
help to understand how long Covid-19 can confer immunity where a strong immune
response means people already been infected can safely return to work. On the
other hand, this GNN-based understanding can also abode well for vaccine
development efforts. Our proposal has been evaluated using CoVs immune response
dataset, with results showing an average IFNs activation prediction accuracy of
90%, compared to 85% using feed-forward neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09018</id>
        <link href="http://arxiv.org/abs/2106.09018"/>
        <updated>2021-08-09T00:49:28.100Z</updated>
        <summary type="html"><![CDATA[This paper presents an end-to-end semi-supervised object detection approach,
in contrast to previous more complex multi-stage methods. The end-to-end
training gradually improves pseudo label qualities during the curriculum, and
the more and more accurate pseudo labels in turn benefit object detection
training. We also propose two simple yet effective techniques within this
framework: a soft teacher mechanism where the classification loss of each
unlabeled bounding box is weighed by the classification score produced by the
teacher network; a box jittering approach to select reliable pseudo boxes for
the learning of box regression. On the COCO benchmark, the proposed approach
outperforms previous methods by a large margin under various labeling ratios,
i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when
the amount of labeled data is relatively large. For example, it can improve a
40.9 mAP baseline detector trained using the full COCO training set by +3.6
mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the
state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),
it can still significantly improve the detection accuracy by +1.5 mAP, reaching
60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching
52.4 mAP. Further incorporating with the Object365 pre-trained model, the
detection accuracy reaches 61.3 mAP and the instance segmentation accuracy
reaches 53.0 mAP, pushing the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengde Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02941</id>
        <link href="http://arxiv.org/abs/2108.02941"/>
        <updated>2021-08-09T00:49:28.092Z</updated>
        <summary type="html"><![CDATA[Disinformation through fake news is an ongoing problem in our society and has
become easily spread through social media. The most cost and time effective way
to filter these large amounts of data is to use a combination of human and
technical interventions to identify it. From a technical perspective, Natural
Language Processing (NLP) is widely used in detecting fake news. Social media
companies use NLP techniques to identify the fake news and warn their users,
but fake news may still slip through undetected. It is especially a problem in
more localised contexts (outside the United States of America). How do we
adjust fake news detection systems to work better for local contexts such as in
South Africa. In this work we investigate fake news detection on South African
websites. We curate a dataset of South African fake news and then train
detection models. We contrast this with using widely available fake news
datasets (from mostly USA website). We also explore making the datasets more
diverse by combining them and observe the differences in behaviour in writing
between nations' fake news using interpretable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1"&gt;Harm de Wet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Meta-Learning for Time Series Regression. (arXiv:2108.02842v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02842</id>
        <link href="http://arxiv.org/abs/2108.02842"/>
        <updated>2021-08-09T00:49:28.085Z</updated>
        <summary type="html"><![CDATA[Recent work has shown the efficiency of deep learning models such as Fully
Convolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with
Time Series Regression (TSR) problems. These models sometimes need a lot of
data to be able to generalize, yet the time series are sometimes not long
enough to be able to learn patterns. Therefore, it is important to make use of
information across time series to improve learning. In this paper, we will
explore the idea of using meta-learning for quickly adapting model parameters
to new short-history time series by modifying the original idea of Model
Agnostic Meta-Learning (MAML) \cite{finn2017model}. Moreover, based on prior
work on multimodal MAML \cite{vuorio2019multimodal}, we propose a method for
conditioning parameters of the model through an auxiliary network that encodes
global information of the time series to extract meta-features. Finally, we
apply the data to time series of different domains, such as pollution
measurements, heart-rate sensors, and electrical battery data. We show
empirically that our proposed meta-learning method learns TSR with few data
fast and outperforms the baselines in 9 of 12 experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1"&gt;Sebastian Pineda Arango&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinrich_F/0/1/0/all/0/1"&gt;Felix Heinrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1"&gt;Kiran Madhusudhanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery. (arXiv:2108.03013v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03013</id>
        <link href="http://arxiv.org/abs/2108.03013"/>
        <updated>2021-08-09T00:49:28.078Z</updated>
        <summary type="html"><![CDATA[The need of predictive maintenance comes with an increasing number of
incidents reported by monitoring systems and equipment/software users. In the
front line, on-call engineers (OCEs) have to quickly assess the degree of
severity of an incident and decide which service to contact for corrective
actions. To automate these decisions, several predictive models have been
proposed, but the most efficient models are opaque (say, black box), strongly
limiting their adoption. In this paper, we propose an efficient black box model
based on 170K incidents reported to our company over the last 7 years and
emphasize on the need of automating triage when incidents are massively
reported on thousands of servers running our product, an ERP. Recent
developments in eXplainable Artificial Intelligence (XAI) help in providing
global explanations to the model, but also, and most importantly, with local
explanations for each model prediction/outcome. Sadly, providing a human with
an explanation for each outcome is not conceivable when dealing with an
important number of daily predictions. To address this problem, we propose an
original data-mining method rooted in Subgroup Discovery, a pattern mining
technique with the natural ability to group objects that share similar
explanations of their black box predictions and provide a description for each
group. We evaluate this approach and present our preliminary results which give
us good hope towards an effective OCE's adoption. We believe that this approach
provides a new way to address the problem of model agnostic outcome
explanation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Remil_Y/0/1/0/all/0/1"&gt;Youcef Remil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendimerad_A/0/1/0/all/0/1"&gt;Anes Bendimerad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plantevit_M/0/1/0/all/0/1"&gt;Marc Plantevit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robardet_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Robardet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaytoue_M/0/1/0/all/0/1"&gt;Mehdi Kaytoue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications. (arXiv:2108.02892v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02892</id>
        <link href="http://arxiv.org/abs/2108.02892"/>
        <updated>2021-08-09T00:49:28.057Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep reinforcement learning (DRL) approach for
solving the optimisation problem of the network's sum-rate in device-to-device
(D2D) communications supported by an intelligent reflecting surface (IRS). The
IRS is deployed to mitigate the interference and enhance the signal between the
D2D transmitter and the associated D2D receiver. Our objective is to jointly
optimise the transmit power at the D2D transmitter and the phase shift matrix
at the IRS to maximise the network sum-rate. We formulate a Markov decision
process and then propose the proximal policy optimisation for solving the
maximisation game. Simulation results show impressive performance in terms of
the achievable rate and processing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Khac Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1"&gt;Antonino Masaracchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yin_C/0/1/0/all/0/1"&gt;Cheng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Long D. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dobre_O/0/1/0/all/0/1"&gt;Octavia A. Dobre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1"&gt;Trung Q. Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Feature Learning For Infinite Data. (arXiv:2108.02932v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02932</id>
        <link href="http://arxiv.org/abs/2108.02932"/>
        <updated>2021-08-09T00:49:28.037Z</updated>
        <summary type="html"><![CDATA[This study addresses the actual behavior of the credit-card fraud detection
environment where financial transactions containing sensitive data must not be
amassed in an enormous amount to conduct learning. We introduce a new adaptive
learning approach that adjusts frequently and efficiently to new transaction
chunks; each chunk is discarded after each incremental training step. Our
approach combines transfer learning and incremental feature learning. The
former improves the feature relevancy for subsequent chunks, and the latter, a
new paradigm, increases accuracy during training by determining the optimal
network architecture dynamically for each new chunk. The architectures of past
incremental approaches are fixed; thus, the accuracy may not improve with new
chunks. We show the effectiveness and superiority of our approach
experimentally on an actual fraud dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadreddin_A/0/1/0/all/0/1"&gt;Armin Sadreddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1"&gt;Samira Sadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13415</id>
        <link href="http://arxiv.org/abs/2104.13415"/>
        <updated>2021-08-09T00:49:28.028Z</updated>
        <summary type="html"><![CDATA[This work presents a novel approach for semi-supervised semantic
segmentation. The key element of this approach is our contrastive learning
module that enforces the segmentation network to yield similar pixel-level
feature representations for same-class samples across the whole dataset. To
achieve this, we maintain a memory bank continuously updated with relevant and
high-quality feature vectors from labeled data. In an end-to-end training, the
features from both labeled and unlabeled data are optimized to be similar to
same-class samples from the memory bank. Our approach outperforms the current
state-of-the-art for semi-supervised semantic segmentation and semi-supervised
domain adaptation on well-known public benchmarks, with larger improvements on
the most challenging scenarios, i.e., less available labeled data.
https://github.com/Shathe/SemiSeg-Contrastive]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1"&gt;Inigo Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1"&gt;Alberto Sabater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1"&gt;David Ferstl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1"&gt;Luis Montesano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1"&gt;Ana C. Murillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting. (arXiv:2108.02853v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02853</id>
        <link href="http://arxiv.org/abs/2108.02853"/>
        <updated>2021-08-09T00:49:28.021Z</updated>
        <summary type="html"><![CDATA[Institutional investors have been increasing the allocation of the illiquid
alternative assets such as private equity funds in their portfolios, yet there
exists a very limited literature on cash flow forecasting of illiquid
alternative assets. The net cash flow of private equity funds typically follow
a J-curve pattern, however the timing and the size of the contributions and
distributions depend on the investment opportunities. In this paper, we develop
a benchmark model and present two novel approaches (direct vs. indirect) to
predict the cash flows of private equity funds. We introduce a sliding window
approach to apply on our cash flow data because different vintage year funds
contain different lengths of cash flow information. We then pass the data to an
LSTM/ GRU model to predict the future cash flows either directly or indirectly
(based on the benchmark model). We further integrate macroeconomic indicators
into our data, which allows us to consider the impact of market environment on
cash flows and to apply stress testing. Our results indicate that the direct
model is easier to implement compared to the benchmark model and the indirect
model, but still the predicted cash flows align better with the actual cash
flows. We also show that macroeconomic variables improve the performance of the
direct model whereas the impact is not obvious on the indirect model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1"&gt;Tugce Karatas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Klinkert_F/0/1/0/all/0/1"&gt;Federico Klinkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1"&gt;Ali Hirsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02899</id>
        <link href="http://arxiv.org/abs/2108.02899"/>
        <updated>2021-08-09T00:49:28.013Z</updated>
        <summary type="html"><![CDATA[Document digitization is essential for the digital transformation of our
societies, yet a crucial step in the process, Optical Character Recognition
(OCR), is still not perfect. Even commercial OCR systems can produce
questionable output depending on the fidelity of the scanned documents. In this
paper, we demonstrate an effective framework for mitigating OCR errors for any
downstream NLP task, using Named Entity Recognition (NER) as an example. We
first address the data scarcity problem for model training by constructing a
document synthesis pipeline, generating realistic but degraded data with NER
labels. We measure the NER accuracy drop at various degradation levels and show
that a text restoration model, trained on the degraded data, significantly
closes the NER accuracy gaps caused by OCR errors, including on an
out-of-domain dataset. For the benefit of the community, we have made the
document synthesis pipeline available as an open-source project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1"&gt;Amit Gupte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1"&gt;Alexey Romanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1"&gt;Sahitya Mantravadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1"&gt;Dalitso Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1"&gt;Raza Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1"&gt;Lakshmanan Ramu Meenal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Benjamin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Soundar Srinivasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy. (arXiv:2108.02817v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.02817</id>
        <link href="http://arxiv.org/abs/2108.02817"/>
        <updated>2021-08-09T00:49:27.993Z</updated>
        <summary type="html"><![CDATA[Although cancer patients survive years after oncologic therapy, they are
plagued with long-lasting or permanent residual symptoms, whose severity, rate
of development, and resolution after treatment vary largely between survivors.
The analysis and interpretation of symptoms is complicated by their partial
co-occurrence, variability across populations and across time, and, in the case
of cancers that use radiotherapy, by further symptom dependency on the tumor
location and prescribed treatment. We describe THALIS, an environment for
visual analysis and knowledge discovery from cancer therapy symptom data,
developed in close collaboration with oncology experts. Our approach leverages
unsupervised machine learning methodology over cohorts of patients, and, in
conjunction with custom visual encodings and interactions, provides context for
new patients based on patients with similar diagnostic features and symptom
evolution. We evaluate this approach on data collected from a cohort of head
and neck cancer patients. Feedback from our clinician collaborators indicates
that THALIS supports knowledge discovery beyond the limits of machines or
humans alone, and that it serves as a valuable tool in both the clinic and
symptom research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Floricel_C/0/1/0/all/0/1"&gt;Carla Floricel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nipu_N/0/1/0/all/0/1"&gt;Nafiul Nipu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggs_M/0/1/0/all/0/1"&gt;Mikayla Biggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wentzel_A/0/1/0/all/0/1"&gt;Andrew Wentzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canahuate_G/0/1/0/all/0/1"&gt;Guadalupe Canahuate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_L/0/1/0/all/0/1"&gt;Lisanne Van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdallah Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuller_C/0/1/0/all/0/1"&gt;C. David Fuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marai_G/0/1/0/all/0/1"&gt;G. Elisabeta Marai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FrequentNet : A New Interpretable Deep Learning Baseline for Image Classification. (arXiv:2001.01034v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01034</id>
        <link href="http://arxiv.org/abs/2001.01034"/>
        <updated>2021-08-09T00:49:27.986Z</updated>
        <summary type="html"><![CDATA[This paper has proposed a new baseline deep learning model of more benefits
for image classification. Different from the convolutional neural network(CNN)
practice where filters are trained by back propagation to represent different
patterns of an image, we are inspired by a method called "PCANet" in "PCANet: A
Simple Deep Learning Baseline for Image Classification?" to choose filter
vectors from basis vectors in frequency domain like Fourier coefficients or
wavelets without back propagation. Researchers have demonstrated that those
basis in frequency domain can usually provide physical insights, which adds to
the interpretability of the model by analyzing the frequencies selected.
Besides, the training process will also be more time efficient, mathematically
clear and interpretable compared with the "black-box" training process of CNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kuangyan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08637</id>
        <link href="http://arxiv.org/abs/2007.08637"/>
        <updated>2021-08-09T00:49:27.980Z</updated>
        <summary type="html"><![CDATA[Coronaviruses constitute a family of viruses that gives rise to respiratory
diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is
crucial for an effective treatment strategy. However, the RT-PCR test which is
considered to be a gold standard in the diagnosis of COVID-19 suffers from a
high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a
feasible and effective diagnostic technique towards this objective. In this
work, we propose the COVID-19 classification problem as a three-class
classification problem to distinguish between COVID-19, normal, and pneumonia
classes. We propose a three-stage framework, named COV-ELM. Stage one deals
with preprocessing and transformation while stage two deals with feature
extraction. These extracted features are passed as an input to the ELM at the
third stage, resulting in the identification of COVID-19. The choice of ELM in
this work has been motivated by its faster convergence, better generalization
capability, and shorter training time in comparison to the conventional
gradient-based learning algorithms. As bigger and diverse datasets become
available, ELM can be quickly retrained as compared to its gradient-based
competitor models. The proposed model achieved a macro average F1-score of 0.95
and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval.
When compared to state-of-the-art machine learning algorithms, the COV-ELM is
found to outperform its competitors in this three-class classification
scenario. Further, LIME has been integrated with the proposed COV-ELM model to
generate annotated CXR images. The annotations are based on the superpixels
that have contributed to distinguish between the different classes. It was
observed that the superpixels correspond to the regions of the human lungs that
are clinically observed in COVID-19 and Pneumonia cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1"&gt;Sheetal Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Manoj Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1"&gt;Ankit Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1"&gt;Navin Lakhyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1"&gt;Arpita Saggar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Naveen Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02830</id>
        <link href="http://arxiv.org/abs/2108.02830"/>
        <updated>2021-08-09T00:49:27.972Z</updated>
        <summary type="html"><![CDATA[Hate speech is a specific type of controversial content that is widely
legislated as a crime that must be identified and blocked. However, due to the
sheer volume and velocity of the Twitter data stream, hate speech detection
cannot be performed manually. To address this issue, several studies have been
conducted for hate speech detection in European languages, whereas little
attention has been paid to low-resource South Asian languages, making the
social media vulnerable for millions of users. In particular, to the best of
our knowledge, no study has been conducted for hate speech detection in Roman
Urdu text, which is widely used in the sub-continent. In this study, we have
scrapped more than 90,000 tweets and manually parsed them to identify 5,000
Roman Urdu tweets. Subsequently, we have employed an iterative approach to
develop guidelines and used them for generating the Hate Speech Roman Urdu 2020
corpus. The tweets in the this corpus are classified at three levels:
Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another
contribution, we have used five supervised learning techniques, including a
deep learning technique, to evaluate and compare their effectiveness for hate
speech detection. The results show that Logistic Regression outperformed all
other techniques, including deep learning techniques for the two levels of
classification, by achieved an F1 score of 0.906 for distinguishing between
Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate
speech tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Moin Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1"&gt;Khurram Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kamran Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08428</id>
        <link href="http://arxiv.org/abs/2007.08428"/>
        <updated>2021-08-09T00:49:27.964Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep learning models has gained much traction in
the last few years. Various attacks and defenses are proposed to improve the
adversarial robustness of modern-day deep learning architectures. While all
these approaches help improve the robustness, one promising direction for
improving adversarial robustness is un-explored, i.e., the complex topology of
the neural network architecture. In this work, we answer the following
question: "Can the complex topology of a neural network give adversarial
robustness without any form of adversarial training?" empirically by
experimenting with different hand-crafted and NAS based architectures. Our
findings show that, for small-scale attacks, NAS-based architectures are more
robust for small-scale datasets and simple tasks than hand-crafted
architectures. However, as the dataset's size or the task's complexity
increase, hand-crafted architectures are more robust than NAS-based
architectures. We perform the first large scale study to understand adversarial
robustness purely from an architectural perspective. Our results show that
random sampling in the search space of DARTS (a popular NAS method) with simple
ensembling can improve the robustness to PGD attack by nearly ~12\%. We show
that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy
as a free add-on without any form of adversarial training. Our results show
that leveraging the power of neural network topology with methods like
ensembles can be an excellent way to achieve adversarial robustness without any
form of adversarial training. We also introduce a metric that can be used to
calculate the trade-off between clean accuracy and adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1"&gt;Devansh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1"&gt;Gaurav Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-08-09T00:49:27.890Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets. Code available at
https://github.com/qlilx/odgrlm.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03225</id>
        <link href="http://arxiv.org/abs/2108.03225"/>
        <updated>2021-08-09T00:49:27.882Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of training generative models on a very sparse
collection of 3D models. We use geometrically motivated energies to augment and
thus boost a sparse collection of example (training) models. We analyze the
Hessian of the as-rigid-as-possible (ARAP) energy to sample from and project to
the underlying (local) shape space, and use the augmented dataset to train a
variational autoencoder (VAE). We iterate the process of building latent spaces
of VAE and augmenting the associated dataset, to progressively reveal a richer
and more expressive generative space for creating geometrically and
semantically valid samples. Our framework allows us to train generative 3D
models even with a small set of good quality 3D models, which are typically
hard to curate. We extensively evaluate our method against a set of strong
baselines, provide ablation studies and demonstrate application towards
establishing shape correspondences. We present multiple examples of interesting
and meaningful shape variations even when starting from as few as 3-10 training
shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1"&gt;Sanjeev Muralikrishnan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt; (2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1"&gt;Noam Aigerman&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vladimir Kim&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1"&gt;Matthew Fisher&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt; (1 and 2) ((1) University College London, (2) Adobe Research, (3) IIT Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TightCap: 3D Human Shape Capture with Clothing Tightness Field. (arXiv:1904.02601v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.02601</id>
        <link href="http://arxiv.org/abs/1904.02601"/>
        <updated>2021-08-09T00:49:27.874Z</updated>
        <summary type="html"><![CDATA[In this paper, we present TightCap, a data-driven scheme to capture both the
human shape and dressed garments accurately with only a single 3D human scan,
which enables numerous applications such as virtual try-on, biometrics and body
evaluation. To break the severe variations of the human poses and garments, we
propose to model the clothing tightness - the displacements from the garments
to the human shape implicitly in the global UV texturing domain. To this end,
we utilize an enhanced statistical human template and an effective multi-stage
alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on
this 2D representation, we propose a novel framework to predicted clothing
tightness via a novel tightness formulation, as well as an effective
optimization scheme to further reconstruct multi-layer human shape and garments
under various clothing categories and human postures. We further propose a new
clothing tightness dataset (CTD) of human scans with a large variety of
clothing styles, poses and corresponding ground-truth human shapes to stimulate
further research. Extensive experiments demonstrate the effectiveness of our
TightCap to achieve high-quality human shape and dressed garments
reconstruction, as well as the further applications for clothing segmentation,
retargeting and animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xui_L/0/1/0/all/0/1"&gt;Lan Xui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning to Predict Game Outcomes Based on Player-Champion Experience in League of Legends. (arXiv:2108.02799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02799</id>
        <link href="http://arxiv.org/abs/2108.02799"/>
        <updated>2021-08-09T00:49:27.867Z</updated>
        <summary type="html"><![CDATA[League of Legends (LoL) is the most widely played multiplayer online battle
arena (MOBA) game in the world. An important aspect of LoL is competitive
ranked play, which utilizes a skill-based matchmaking system to form fair
teams. However, players' skill levels vary widely depending on which champion,
or hero, that they choose to play as. In this paper, we propose a method for
predicting game outcomes in ranked LoL games based on players' experience with
their selected champion. Using a deep neural network, we found that game
outcomes can be predicted with 75.1% accuracy after all players have selected
champions, which occurs before gameplay begins. Our results have important
implications for playing LoL and matchmaking. Firstly, individual champion
skill plays a significant role in the outcome of a match, regardless of team
composition. Secondly, even after the skill-based matchmaking, there is still a
wide variance in team skill before gameplay begins. Finally, players should
only play champions that they have mastered, if they want to win games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Tiffany D. Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Seong Ioi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dylan S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMillian_M/0/1/0/all/0/1"&gt;Matthew G. McMillian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1"&gt;Ryan P. McMahan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06801</id>
        <link href="http://arxiv.org/abs/2106.06801"/>
        <updated>2021-08-09T00:49:27.859Z</updated>
        <summary type="html"><![CDATA[Contrastive Learning (CL) is a recent representation learning approach, which
encourages inter-class separability and intra-class compactness in learned
image representations. Since medical images often contain multiple semantic
classes in an image, using CL to learn representations of local features (as
opposed to global) is important. In this work, we present a novel
semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization (CR) scheme, which
works in synergy with CL. It addresses the problem of confirmation bias, and
encourages better clustering in the feature space. We evaluate our method on
four public medical segmentation datasets and a novel histopathology dataset
that we introduce. Our method obtains consistent improvements over
state-of-the-art semi-supervised segmentation approaches for all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1"&gt;Ajey Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nisarg Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Prasenjit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1"&gt;Govind Makharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Continual Learning Overcoming Catastrophic Forgetting. (arXiv:2108.02786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02786</id>
        <link href="http://arxiv.org/abs/2108.02786"/>
        <updated>2021-08-09T00:49:27.841Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting describes the fact that machine learning models will
likely forget the knowledge of previously learned tasks after the learning
process of a new one. It is a vital problem in the continual learning scenario
and recently has attracted tremendous concern across different communities. In
this paper, we explore the catastrophic forgetting phenomena in the context of
quantum machine learning. We find that, similar to those classical learning
models based on neural networks, quantum learning systems likewise suffer from
such forgetting problem in classification tasks emerging from various
application scenes. We show that based on the local geometrical information in
the loss function landscape of the trained model, a uniform strategy can be
adapted to overcome the forgetting problem in the incremental learning setting.
Our results uncover the catastrophic forgetting phenomena in quantum machine
learning and offer a practical method to overcome this problem, which opens a
new avenue for exploring potential quantum advantages towards continual
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenjie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhide Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1"&gt;Dong-Ling Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08430</id>
        <link href="http://arxiv.org/abs/2107.08430"/>
        <updated>2021-08-09T00:49:27.834Z</updated>
        <summary type="html"><![CDATA[In this report, we present some experienced improvements to YOLO series,
forming a new high-performance detector -- YOLOX. We switch the YOLO detector
to an anchor-free manner and conduct other advanced detection techniques, i.e.,
a decoupled head and the leading label assignment strategy SimOTA to achieve
state-of-the-art results across a large scale range of models: For YOLO-Nano
with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing
NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in
industry, we boost it to 47.3% AP on COCO, outperforming the current best
practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as
YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on
Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on
Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)
using a single YOLOX-L model. We hope this report can provide useful experience
for developers and researchers in practical scenes, and we also provide deploy
versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at
https://github.com/Megvii-BaseDetection/YOLOX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zheng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songtao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Topological Data Analysis with Linear Depth and Exponential Speedup. (arXiv:2108.02811v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.02811</id>
        <link href="http://arxiv.org/abs/2108.02811"/>
        <updated>2021-08-09T00:49:27.826Z</updated>
        <summary type="html"><![CDATA[Quantum computing offers the potential of exponential speedups for certain
classical computations. Over the last decade, many quantum machine learning
(QML) algorithms have been proposed as candidates for such exponential
improvements. However, two issues unravel the hope of exponential speedup for
some of these QML algorithms: the data-loading problem and, more recently, the
stunning dequantization results of Tang et al. A third issue, namely the
fault-tolerance requirements of most QML algorithms, has further hindered their
practical realization. The quantum topological data analysis (QTDA) algorithm
of Lloyd, Garnerone and Zanardi was one of the first QML algorithms that
convincingly offered an expected exponential speedup. From the outset, it did
not suffer from the data-loading problem. A recent result has also shown that
the generalized problem solved by this algorithm is likely classically
intractable, and would therefore be immune to any dequantization efforts.
However, the QTDA algorithm of Lloyd et~al. has a time complexity of
$O(n^4/(\epsilon^2 \delta))$ (where $n$ is the number of data points,
$\epsilon$ is the error tolerance, and $\delta$ is the smallest nonzero
eigenvalue of the restricted Laplacian) and requires fault-tolerant quantum
computing, which has not yet been achieved. In this paper, we completely
overhaul the QTDA algorithm to achieve an improved exponential speedup and
depth complexity of $O(n\log(1/(\delta\epsilon)))$. Our approach includes three
key innovations: (a) an efficient realization of the combinatorial Laplacian as
a sum of Pauli operators; (b) a quantum rejection sampling approach to restrict
the superposition to the simplices in the complex; and (c) a stochastic rank
estimation method to estimate the Betti numbers. We present a theoretical error
analysis, and the circuit and computational time and depth complexities for
Betti number estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ubaru_S/0/1/0/all/0/1"&gt;Shashanka Ubaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Akhalwaya_I/0/1/0/all/0/1"&gt;Ismail Yunus Akhalwaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Squillante_M/0/1/0/all/0/1"&gt;Mark S. Squillante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Clarkson_K/0/1/0/all/0/1"&gt;Kenneth L. Clarkson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Horesh_L/0/1/0/all/0/1"&gt;Lior Horesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Fusion Transformer. (arXiv:2107.09011v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09011</id>
        <link href="http://arxiv.org/abs/2107.09011"/>
        <updated>2021-08-09T00:49:27.815Z</updated>
        <summary type="html"><![CDATA[In image fusion, images obtained from different sensors are fused to generate
a single image with enhanced information. In recent years, state-of-the-art
methods have adopted Convolution Neural Networks (CNNs) to encode meaningful
features for image fusion. Specifically, CNN-based methods perform image fusion
by fusing local features. However, they do not consider long-range dependencies
that are present in the image. Transformer-based models are designed to
overcome this by modeling the long-range dependencies with the help of
self-attention mechanism. This motivates us to propose a novel Image Fusion
Transformer (IFT) where we develop a transformer-based multi-scale fusion
strategy that attends to both local and long-range information (or global
context). The proposed method follows a two-stage training approach. In the
first stage, we train an auto-encoder to extract deep features at multiple
scales. In the second stage, multi-scale features are fused using a
Spatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of
a CNN and a transformer branch which capture local and long-range features,
respectively. Extensive experiments on multiple benchmark datasets show that
the proposed method performs better than many competitive fusion algorithms.
Furthermore, we show the effectiveness of the proposed ST fusion strategy with
an ablation analysis. The source code is available at:
https://github.com/Vibashan/Image-Fusion-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1"&gt;Vibashan VS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1"&gt;Poojan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08077</id>
        <link href="http://arxiv.org/abs/2106.08077"/>
        <updated>2021-08-09T00:49:27.797Z</updated>
        <summary type="html"><![CDATA[Plant species identification is time consuming, costly, and requires lots of
efforts, and expertise knowledge. In recent, many researchers use deep learning
methods to classify plants directly using plant images. While deep learning
models have achieved a great success, the lack of interpretability limit their
widespread application. To overcome this, we explore the use of interpretable,
measurable and computer-aided features extracted from plant leaf images. Image
processing is one of the most challenging, and crucial steps in
feature-extraction. The purpose of image processing is to improve the leaf
image by removing undesired distortion. The main image processing steps of our
algorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,
ii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove
stalk, vi) Closing holes, and vii) Resize image. The next step after image
processing is to extract features from plant leaf images. We introduced 52
computationally efficient features to classify plant species. These features
are mainly classified into four groups as: i) shape-based features, ii)
color-based features, iii) texture-based features, and iv) scagnostic features.
Length, width, area, texture correlation, monotonicity and scagnostics are to
name few of them. We explore the ability of features to discriminate the
classes of interest under supervised learning and unsupervised learning
settings. For that, supervised dimensionality reduction technique, Linear
Discriminant Analysis (LDA), and unsupervised dimensionality reduction
technique, Principal Component Analysis (PCA) are used to convert and visualize
the images from digital-image space to feature space. The results show that the
features are sufficient to discriminate the classes of interest under both
supervised and unsupervised learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1"&gt;Jayani P. G. Lakshika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1"&gt;Thiyanga S. Talagala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering. (arXiv:2108.02814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02814</id>
        <link href="http://arxiv.org/abs/2108.02814"/>
        <updated>2021-08-09T00:49:27.788Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence and machine learning are poised to disrupt PET
imaging from bench to clinic. In this perspective we offer insights into how
the technology could be applied to improve the design and synthesis of new
radiopharmaceuticals for PET imaging, including identification of an optimal
labeling approach as well as strategies for radiolabeling reaction
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Webb_E/0/1/0/all/0/1"&gt;E. William Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_P/0/1/0/all/0/1"&gt;Peter J.H. Scott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00334</id>
        <link href="http://arxiv.org/abs/2103.00334"/>
        <updated>2021-08-09T00:49:27.768Z</updated>
        <summary type="html"><![CDATA[Salient object detection (SOD) is viewed as a pixel-wise saliency modeling
task by traditional deep learning-based methods. A limitation of current SOD
models is insufficient utilization of inter-pixel information, which usually
results in imperfect segmentation near edge regions and low spatial coherence.
As we demonstrate, using a saliency mask as the only label is suboptimal. To
address this limitation, we propose a connectivity-based approach called
bilateral connectivity network (BiconNet), which uses connectivity masks
together with saliency masks as labels for effective modeling of inter-pixel
relationships and object saliency. Moreover, we propose a bilateral voting
module to enhance the output connectivity map, and a novel edge feature
enhancement method that efficiently utilizes edge-specific features. Through
comprehensive experiments on five benchmark datasets, we demonstrate that our
proposed method can be plugged into any existing state-of-the-art
saliency-based SOD framework to improve its performance with negligible
parameter increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1"&gt;Somayyeh Soltanian-Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1"&gt;Sina Farsiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02798</id>
        <link href="http://arxiv.org/abs/2108.02798"/>
        <updated>2021-08-09T00:49:27.760Z</updated>
        <summary type="html"><![CDATA[Fundus photography is the primary method for retinal imaging and essential
for diabetic retinopathy prevention. Automated segmentation of fundus
photographs would improve the quality, capacity, and cost-effectiveness of eye
care screening programs. However, current segmentation methods are not robust
towards the diversity in imaging conditions and pathologies typical for
real-world clinical applications. To overcome these limitations, we utilized
contrastive self-supervised learning to exploit the large variety of unlabeled
fundus images in the publicly available EyePACS dataset. We pre-trained an
encoder of a U-Net, which we later fine-tuned on several retinal vessel and
lesion segmentation datasets. We demonstrate for the first time that by using
contrastive self-supervised learning, the pre-trained network can recognize
blood vessels, optic disc, fovea, and various lesions without being provided
any labels. Furthermore, when fine-tuned on a downstream blood vessel
segmentation task, such pre-trained networks achieve state-of-the-art
performance on images from different datasets. Additionally, the pre-training
also leads to shorter training times and an improved few-shot performance on
both blood vessel and lesion segmentation tasks. Altogether, our results
showcase the benefits of contrastive self-supervised pre-training which can
play a crucial role in real-world clinical applications requiring robust models
able to adapt to new devices with only a few annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1"&gt;Jan Kuka&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1"&gt;Anja Zenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1"&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1"&gt;Dominik J&amp;#xfc;stel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1"&gt;Vasilis Ntziachristos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03227</id>
        <link href="http://arxiv.org/abs/2108.03227"/>
        <updated>2021-08-09T00:49:27.753Z</updated>
        <summary type="html"><![CDATA[Bird's-Eye-View (BEV) maps have emerged as one of the most powerful
representations for scene understanding due to their ability to provide rich
spatial context while being easy to interpret and process. However, generating
BEV maps requires complex multi-stage paradigms that encapsulate a series of
distinct tasks such as depth estimation, ground plane estimation, and semantic
segmentation. These sub-tasks are often learned in a disjoint manner which
prevents the model from holistic reasoning and results in erroneous BEV maps.
Moreover, existing algorithms only predict the semantics in the BEV space,
which limits their use in applications where the notion of object instances is
critical. In this work, we present the first end-to-end learning approach for
directly predicting dense panoptic segmentation maps in the BEV, given a single
monocular image in the frontal view (FV). Our architecture follows the top-down
paradigm and incorporates a novel dense transformer module consisting of two
distinct transformers that learn to independently map vertical and flat regions
in the input image from the FV to the BEV. Additionally, we derive a
mathematical formulation for the sensitivity of the FV-BEV transformation which
allows us to intelligently weight pixels in the BEV space to account for the
varying descriptiveness across the FV image. Extensive evaluations on the
KITTI-360 and nuScenes datasets demonstrate that our approach exceeds the
state-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1"&gt;Nikhil Gosala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02701</id>
        <link href="http://arxiv.org/abs/2106.02701"/>
        <updated>2021-08-09T00:49:27.746Z</updated>
        <summary type="html"><![CDATA[Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. In this paper we present a
probabilistic method which combines a hidden Markov state process that encodes
neuron geometric properties with a random field appearance model of the
flourescence process. Our method utilizes dynamic programming to efficiently
compute the global maximizers of what we call the "most probable" neuron path.
We applied our algorithm to the output of image segmentation models where false
negatives severed neuronal processes, and showed that it can follow axons in
the presence of noise or nearby neurons. Our method has the potential to be
integrated into a semi or fully automated reconstruction pipeline.
Additionally, it creates a framework for conditioning the probability to fixed
start and endpoints through which users can intervene with hard constraints to,
for example, rule out certain reconstructions, or assign axons to particular
cell bodies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1"&gt;Thomas L. Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1"&gt;Daniel J. Tward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1"&gt;Ulrich Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1"&gt;Michael I. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations. (arXiv:2104.10824v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10824</id>
        <link href="http://arxiv.org/abs/2104.10824"/>
        <updated>2021-08-09T00:49:27.739Z</updated>
        <summary type="html"><![CDATA[Colorectal cancer (CRC) is one of the most common types of cancer with a high
mortality rate. Colonoscopy is the preferred procedure for CRC screening and
has proven to be effective in reducing CRC mortality. Thus, a reliable
computer-aided polyp detection and classification system can significantly
increase the effectiveness of colonoscopy. In this paper, we create an
endoscopic dataset collected from various sources and annotate the ground truth
of polyp location and classification results with the help of experienced
gastroenterologists. The dataset can serve as a benchmark platform to train and
evaluate the machine learning models for polyp classification. We have also
compared the performance of eight state-of-the-art deep learning-based object
detection models. The results demonstrate that deep CNN models are promising in
CRC screening. This work can serve as a baseline for future research in polyp
detection and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kaidong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathan_M/0/1/0/all/0/1"&gt;Mohammad I. Fathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1"&gt;Krushi Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Cuncong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Ajay Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Amit Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jean S. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10984</id>
        <link href="http://arxiv.org/abs/2107.10984"/>
        <updated>2021-08-09T00:49:27.720Z</updated>
        <summary type="html"><![CDATA[Deep generative models have made great progress in synthesizing images with
arbitrary human poses and transferring poses of one person to others. However,
most existing approaches explicitly leverage the pose information extracted
from the source images as a conditional input for the generative networks.
Meanwhile, they usually focus on the visual fidelity of the synthesized images
but neglect the inherent consistency, which further confines their performance
of pose transfer. To alleviate the current limitations and improve the quality
of the synthesized images, we propose a pose transfer network with Disentangled
Feature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair
of images containing the source and target person, DFC-Net extracts pose and
static information from the source and target respectively, then synthesizes an
image of the target person with the desired pose from the source. Moreover,
DFC-Net leverages disentangled feature consistency losses in the adversarial
training to strengthen the transfer coherence and integrates the keypoint
amplifier to enhance the pose feature extraction. Additionally, an unpaired
support dataset Mixamo-Sup providing more extra pose information has been
further utilized during the training to improve the generality and robustness
of DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have
demonstrated DFC-Net achieves state-of-the-art performance on pose transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zheng Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Gangyi Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14618</id>
        <link href="http://arxiv.org/abs/2012.14618"/>
        <updated>2021-08-09T00:49:27.712Z</updated>
        <summary type="html"><![CDATA[Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. Compared with the rapid development of deep
learning for two-dimensional (2D) image tasks, deep learning-based instance
segmentation of 3D point cloud still has a lot of room for development. In
particular, distinguishing a large number of occluded objects of the same class
is a highly challenging problem, which is seen in a robotic bin-picking. In a
usual bin-picking scene, many indentical objects are stacked together and the
model of the objects is known. Thus, the semantic information can be ignored;
instead, the focus in the bin-picking is put on the segmentation of instances.
Based on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)
for instance segmentation of bin-picking scene. FPCC includes a network named
FPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for
inferring the geometric centers for clustering and the other for describing
features of each point. FPCC-Net extracts features of each point and infers
geometric center points of each instance simultaneously. After that, the
proposed clustering algorithm clusters the remaining points to the closest
geometric center in feature embedding space. Experiments show that FPCC also
surpasses the existing works in bin-picking scenes and is more computationally
efficient. Our code and data are available at https://github.com/xyjbaal/FPCC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yajun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1"&gt;Shogo Arai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Diyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fangzhou Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1"&gt;Kazuhiro Kosuge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06652</id>
        <link href="http://arxiv.org/abs/2107.06652"/>
        <updated>2021-08-09T00:49:27.704Z</updated>
        <summary type="html"><![CDATA[This paper explores the use of self-supervised deep learning in medical
imaging in cases where two scan modalities are available for the same subject.
Specifically, we use a large publicly-available dataset of over 20,000 subjects
from the UK Biobank with both whole body Dixon technique magnetic resonance
(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three
contributions: (i) We introduce a multi-modal image-matching contrastive
framework, that is able to learn to match different-modality scans of the same
subject with high accuracy. (ii) Without any adaption, we show that the
correspondences learnt during this contrastive training step can be used to
perform automatic cross-modal scan registration in a completely unsupervised
manner. (iii) Finally, we use these registrations to transfer segmentation maps
from the DXA scans to the MR scans where they are used to train a network to
segment anatomical regions without requiring ground-truth MR examples. To aid
further research, our code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1"&gt;Rhydian Windsor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05260</id>
        <link href="http://arxiv.org/abs/2101.05260"/>
        <updated>2021-08-09T00:49:27.687Z</updated>
        <summary type="html"><![CDATA[In cases of serious crime, including sexual abuse, often the only available
information with demonstrated potential for identification is images of the
hands. Since this evidence is captured in uncontrolled situations, it is
difficult to analyse. As global approaches to feature comparison are limited in
this case, it is important to extend to consider local information. In this
work, we propose hand-based person identification by learning both global and
local deep feature representation. Our proposed method, Global and Part-Aware
Network (GPA-Net), creates global and local branches on the conv-layer for
learning robust discriminative global and part-level features. For learning the
local (part-level) features, we perform uniform partitioning on the conv-layer
in both horizontal and vertical directions. We retrieve the parts by conducting
a soft partition without explicitly partitioning the images or requiring
external cues such as pose estimation. We make extensive evaluations on two
large multi-ethnic and publicly available hand datasets, demonstrating that our
proposed method significantly outperforms competing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1"&gt;Ritesh Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Bryan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1"&gt;Plamen Angelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1"&gt;Sue Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain. (arXiv:2011.13080v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13080</id>
        <link href="http://arxiv.org/abs/2011.13080"/>
        <updated>2021-08-09T00:49:27.679Z</updated>
        <summary type="html"><![CDATA[Curvelet frame is of special significance for photoacoustic tomography (PAT)
due to its sparsifying and microlocalisation properties. We derive a one-to-one
map between wavefront directions in image and data spaces in PAT which suggests
near equivalence between the recovery of the initial pressure and PAT data from
compressed/subsampled measurements when assuming sparsity in Curvelet frame. As
the latter is computationally more tractable, investigation to which extent
this equivalence holds conducted in this paper is of immediate practical
significance. To this end we formulate and compare DR, a two step approach
based on the recovery of the complete volume of the photoacoustic data from the
subsampled data followed by the acoustic inversion, and p0R, a one step
approach where the photoacoustic image (the initial pressure, p0) is directly
recovered from the subsampled data. Effective representation of the
photoacoustic data requires basis defined on the range of the photoacoustic
forward operator. To this end we propose a novel wedge-restriction of Curvelet
transform which enables us to construct such basis. Both recovery problems are
formulated in a variational framework. As the Curvelet frame is heavily
overdetermined, we use reweighted l1 norm penalties to enhance the sparsity of
the solution. The data reconstruction problem DR is a standard compressed
sensing recovery problem, which we solve using an ADMMtype algorithm, SALSA.
Subsequently, the initial pressure is recovered using time reversal as
implemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to
recover the photoacoustic image directly via FISTA, or ADMM when in addition
including a non-negativity constraint. We compare and discuss the relative
merits of the two approaches and illustrate them on 2D simulated and 3D real
data in a fair and rigorous manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bolin Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1"&gt;Simon R. Arridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1"&gt;Felix Lucka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1"&gt;Ben T. Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nam Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1"&gt;Paul C. Beard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1"&gt;Edward Z. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1"&gt;Marta M. Betcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. (arXiv:2101.10030v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10030</id>
        <link href="http://arxiv.org/abs/2101.10030"/>
        <updated>2021-08-09T00:49:27.654Z</updated>
        <summary type="html"><![CDATA[Anomaly detection with weakly supervised video-level labels is typically
formulated as a multiple instance learning (MIL) problem, in which we aim to
identify snippets containing abnormal events, with each video represented as a
bag of video snippets. Although current methods show effective detection
performance, their recognition of the positive instances, i.e., rare abnormal
snippets in the abnormal videos, is largely biased by the dominant negative
instances, especially when the abnormal events are subtle anomalies that
exhibit only small differences compared with normal events. This issue is
exacerbated in many methods that ignore important video temporal dependencies.
To address this issue, we introduce a novel and theoretically sound method,
named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature
magnitude learning function to effectively recognise the positive instances,
substantially improving the robustness of the MIL approach to the negative
instances from abnormal videos. RTFM also adapts dilated convolutions and
self-attention mechanisms to capture long- and short-range temporal
dependencies to learn the feature magnitude more faithfully. Extensive
experiments show that the RTFM-enabled MIL model (i) outperforms several
state-of-the-art methods by a large margin on four benchmark data sets
(ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves
significantly improved subtle anomaly discriminability and sample efficiency.
Code is available at https://github.com/tianyu0207/RTFM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajvinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1"&gt;Johan W. Verjans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1"&gt;Gustavo Carneiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SofGAN: A Portrait Image Generator with Dynamic Styling. (arXiv:2007.03780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03780</id>
        <link href="http://arxiv.org/abs/2007.03780"/>
        <updated>2021-08-09T00:49:27.635Z</updated>
        <summary type="html"><![CDATA[Recently, Generative Adversarial Networks (GANs)} have been widely used for
portrait image generation. However, in the latent space learned by GANs,
different attributes, such as pose, shape, and texture style, are generally
entangled, making the explicit control of specific attributes difficult. To
address this issue, we propose a SofGAN image generator to decouple the latent
space of portraits into two subspaces: a geometry space and a texture space.
The latent codes sampled from the two subspaces are fed to two network branches
separately, one to generate the 3D geometry of portraits with canonical pose,
and the other to generate textures. The aligned 3D geometries also come with
semantic part segmentation, encoded as a semantic occupancy field (SOF). The
SOF allows the rendering of consistent 2D semantic segmentation maps at
arbitrary views, which are then fused with the generated texture maps and
stylized to a portrait photo using our semantic instance-wise (SIW) module.
Through extensive experiments, we show that our system can generate high
quality portrait images with independently controllable geometry and texture
attributes. The method also generalizes well in various applications such as
appearance-consistent facial animation and dynamic styling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anpei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Ling Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13005</id>
        <link href="http://arxiv.org/abs/2011.13005"/>
        <updated>2021-08-09T00:49:27.628Z</updated>
        <summary type="html"><![CDATA[We introduce PREDATOR, a model for pairwise point-cloud registration with
deep attention to the overlap region. Different from previous work, our model
is specifically designed to handle (also) point-cloud pairs with low overlap.
Its key novelty is an overlap-attention block for early information exchange
between the latent encodings of the two point clouds. In this way the
subsequent decoding of the latent representations into per-point features is
conditioned on the respective other point cloud, and thus can predict which
points are not only salient, but also lie in the overlap region between the two
point clouds. The ability to focus on points that are relevant for matching
greatly improves performance: PREDATOR raises the rate of successful
registrations by more than 20% in the low-overlap scenario, and also sets a new
state of the art for the 3DMatch benchmark with 89% registration recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1"&gt;Zan Gojcic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1"&gt;Mikhail Usvyatsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wieser_A/0/1/0/all/0/1"&gt;Andreas Wieser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction. (arXiv:1909.09872v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.09872</id>
        <link href="http://arxiv.org/abs/1909.09872"/>
        <updated>2021-08-09T00:49:27.585Z</updated>
        <summary type="html"><![CDATA[We show dense voxel embeddings learned via deep metric learning can be
employed to produce a highly accurate segmentation of neurons from 3D electron
microscopy images. A "metric graph" on a set of edges between voxels is
constructed from the dense voxel embeddings generated by a convolutional
network. Partitioning the metric graph with long-range edges as repulsive
constraints yields an initial segmentation with high precision, with
substantial accuracy gain for very thin objects. The convolutional embedding
net is reused without any modification to agglomerate the systematic splits
caused by complex "self-contact" motifs. Our proposed method achieves
state-of-the-art accuracy on the challenging problem of 3D neuron
reconstruction from the brain images acquired by serial section electron
microscopy. Our alternative, object-centered representation could be more
generally useful for other computational tasks in automated neural circuit
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kisuk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Ran Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1"&gt;Kyle Luther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1"&gt;H. Sebastian Seung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of 3D Porous Media From 2D Slices. (arXiv:1901.10233v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.10233</id>
        <link href="http://arxiv.org/abs/1901.10233"/>
        <updated>2021-08-09T00:49:27.551Z</updated>
        <summary type="html"><![CDATA[In many branches of earth sciences, the problem of rock study on the
micro-level arises. However, a significant number of representative samples is
not always feasible. Thus the problem of the generation of samples with similar
properties becomes actual. In this paper, we propose a novel deep learning
architecture for three-dimensional porous media reconstruction from
two-dimensional slices. We fit a distribution on all possible three-dimensional
structures of a specific type based on the given dataset of samples. Then,
given partial information (central slices), we recover the three-dimensional
structure around such slices as the most probable one according to that
constructed distribution. Technically, we implement this in the form of a deep
neural network with encoder, generator and discriminator modules. Numerical
experiments show that this method provides a good reconstruction in terms of
Minkowski functionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volkhonskiy_D/0/1/0/all/0/1"&gt;Denis Volkhonskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1"&gt;Ekaterina Muravleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudakov_O/0/1/0/all/0/1"&gt;Oleg Sudakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1"&gt;Denis Orlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belozerov_B/0/1/0/all/0/1"&gt;Boris Belozerov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1"&gt;Dmitry Koroteev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03168</id>
        <link href="http://arxiv.org/abs/2108.03168"/>
        <updated>2021-08-09T00:49:27.474Z</updated>
        <summary type="html"><![CDATA[Spectrograms visualize the frequency components of a given signal which may
be an audio signal or even a time-series signal. Audio signals have higher
sampling rate and high variability of frequency with time. Spectrograms can
capture such variations well. But, vital signs which are time-series signals
have less sampling frequency and low-frequency variability due to which,
spectrograms fail to express variations and patterns. In this paper, we propose
a novel solution to introduce frequency variability using frequency modulation
on vital signs. Then we apply spectrograms on frequency modulated signals to
capture the patterns. The proposed approach has been evaluated on 4 different
medical datasets across both prediction and classification tasks. Significant
results are found showing the efficacy of the approach for vital sign signals.
The results from the proposed approach are promising with an accuracy of 91.55%
and 91.67% in prediction and classification tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1"&gt;Sidharth Srivatsav Sribhashyam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1"&gt;Md Sirajus Salekin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1"&gt;Dmitry Goldgof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference. (arXiv:2108.03180v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03180</id>
        <link href="http://arxiv.org/abs/2108.03180"/>
        <updated>2021-08-09T00:49:27.297Z</updated>
        <summary type="html"><![CDATA[This paper reports on a dynamic semantic mapping framework that incorporates
3D scene flow measurements into a closed-form Bayesian inference model.
Existence of dynamic objects in the environment cause artifacts and traces in
current mapping algorithms, leading to an inconsistent map posterior. We
leverage state-of-the-art semantic segmentation and 3D flow estimation using
deep learning to provide measurements for map inference. We develop a
continuous (i.e., can be queried at arbitrary resolution) Bayesian model that
propagates the scene with flow and infers a 3D semantic occupancy map with
better performance than its static counterpart. Experimental results using
publicly available data sets show that the proposed framework generalizes its
predecessors and improves over direct measurements from deep neural networks
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_A/0/1/0/all/0/1"&gt;Aishwarya Unnikrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1"&gt;Joseph Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Lu Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1"&gt;Andrew Capodieci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1"&gt;Paramsothy Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1"&gt;Kira Barton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1"&gt;Maani Ghaffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03140</id>
        <link href="http://arxiv.org/abs/2108.03140"/>
        <updated>2021-08-09T00:49:27.273Z</updated>
        <summary type="html"><![CDATA[Extreme Learning Machine is a powerful classification method very competitive
existing classification methods. It is extremely fast at training.
Nevertheless, it cannot perform face verification tasks properly because face
verification tasks require comparison of facial images of two individuals at
the same time and decide whether the two faces identify the same person. The
structure of Extreme Leaning Machine was not designed to feed two input data
streams simultaneously, thus, in 2-input scenarios Extreme Learning Machine
methods are normally applied using concatenated inputs. However, this setup
consumes two times more computational resources and it is not optimized for
recognition tasks where learning a separable distance metric is critical. For
these reasons, we propose and develop a Siamese Extreme Learning Machine
(SELM). SELM was designed to be fed with two data streams in parallel
simultaneously. It utilizes a dual-stream Siamese condition in the extra
Siamese layer to transform the data before passing it along to the hidden
layer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature
exclusively trained on a variety of specific demographic groups. This feature
enables learning and extracting of useful facial features of each group.
Experiments were conducted to evaluate and compare the performances of SELM,
Extreme Learning Machine, and DCNN. The experimental results showed that the
proposed feature was able to perform correct classification at 97.87% accuracy
and 99.45% AUC. They also showed that using SELM in conjunction with the
proposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the
well-known DCNN and Extreme Leaning Machine methods by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1"&gt;Wasu Kudisthalert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1"&gt;Kitsuchart Pasupa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for View Classification of Echocardiograms. (arXiv:2108.03124v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03124</id>
        <link href="http://arxiv.org/abs/2108.03124"/>
        <updated>2021-08-09T00:49:27.253Z</updated>
        <summary type="html"><![CDATA[Analysis of cardiac ultrasound images is commonly performed in routine
clinical practice for quantification of cardiac function. Its increasing
automation frequently employs deep learning networks that are trained to
predict disease or detect image features. However, such models are extremely
data-hungry and training requires labelling of many thousands of images by
experienced clinicians. Here we propose the use of contrastive learning to
mitigate the labelling bottleneck. We train view classification models for
imbalanced cardiac ultrasound datasets and show improved performance for
views/classes for which minimal labelled data is available. Compared to a naive
baseline model, we achieve an improvement in F1 score of up to 26% in those
views while maintaining state-of-the-art performance for the views with
sufficiently many labelled training observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mumith_A/0/1/0/all/0/1"&gt;Angela Mumith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1"&gt;Jorge Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1"&gt;Kanwal Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1"&gt;Arian Beqiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03152</id>
        <link href="http://arxiv.org/abs/2108.03152"/>
        <updated>2021-08-09T00:49:27.245Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) has drawn high interest for its capacity to adapt a
model trained on labeled source data to perform well on unlabeled or weakly
labeled target data from a different domain. Most common DA techniques require
concurrent access to the input images of both the source and target domains.
However, in practice, privacy concerns often impede the availability of source
images in the adaptation phase. This is a very frequent DA scenario in medical
imaging, where, for instance, the source and target images could come from
different clinical sites. We introduce a source-free domain adaptation for
image segmentation. Our formulation is based on minimizing a label-free entropy
loss defined over target-domain data, which we further guide with a
domain-invariant prior on the segmentation regions. Many priors can be derived
from anatomical information. Here, a class ratio prior is estimated from
anatomical knowledge and integrated in the form of a Kullback Leibler (KL)
divergence in our overall loss function. Furthermore, we motivate our overall
loss with an interesting link to maximizing the mutual information between the
target images and their label predictions. We show the effectiveness of our
prior aware entropy minimization in a variety of domain-adaptation scenarios,
with different modalities and applications, including spine, prostate, and
cardiac segmentation. Our method yields comparable results to several state of
the art adaptation techniques, despite having access to much less information,
as the source images are entirely absent in our adaptation phase. Our
straightforward adaptation strategy uses only one network, contrary to popular
adversarial techniques, which are not applicable to a source-free DA setting.
Our framework can be readily used in a breadth of segmentation problems, and
our code is publicly available: https://github.com/mathilde-b/SFDA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1"&gt;Mathilde Bateson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1"&gt;Hoel Kervadec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Lombaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02938</id>
        <link href="http://arxiv.org/abs/2108.02938"/>
        <updated>2021-08-09T00:49:27.238Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPM) have shown remarkable
performance in unconditional image generation. However, due to the
stochasticity of the generative process in DDPM, it is challenging to generate
images with the desired semantics. In this work, we propose Iterative Latent
Variable Refinement (ILVR), a method to guide the generative process in DDPM to
generate high-quality images based on a given reference image. Here, the
refinement of the generative process in DDPM enables a single DDPM to sample
images from various sets directed by the reference image. The proposed ILVR
method generates high-quality images while controlling the generation. The
controllability of our method allows adaptation of a single DDPM without any
additional learning in various image generation tasks, such as generation from
various downsampling factors, multi-domain image translation, paint-to-image,
and editing with scribbles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jooyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungwon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yonghyun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1"&gt;Youngjune Gwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03144</id>
        <link href="http://arxiv.org/abs/2108.03144"/>
        <updated>2021-08-09T00:49:27.231Z</updated>
        <summary type="html"><![CDATA[Detecting local features, such as corners, segments or blobs, is the first
step in the pipeline of many Computer Vision applications. Its speed is crucial
for real time applications. In this paper we present ELSED, the fastest line
segment detector in the literature. The key for its efficiency is a local
segment growing algorithm that connects gradient aligned pixels in presence of
small discontinuities. The proposed algorithm not only runs in devices with
very low end hardware, but may also be parametrized to foster the detection of
short or longer segments, depending on the task at hand. We also introduce new
metrics to evaluate the accuracy and repeatability of segment detectors. In our
experiments with different public benchmarks we prove that our method is the
most efficient in the literature and quantify the accuracy traded for such
gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1"&gt;Iago Su&amp;#xe1;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Buenaposada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1"&gt;Luis Baumela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03131</id>
        <link href="http://arxiv.org/abs/2108.03131"/>
        <updated>2021-08-09T00:49:27.223Z</updated>
        <summary type="html"><![CDATA[The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of
life globally, and a critical factor in mitigating its effects is screening
individuals for infections, thereby allowing for both proper treatment for
those individuals as well as action to be taken to prevent further spread of
the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a
screening tool as it is a much cheaper and easier to apply imaging modality
than others that are traditionally used for pulmonary examinations, namely
chest x-ray and computed tomography. Given the scarcity of expert radiologists
for interpreting POCUS examinations in many highly affected regions around the
world, low-cost deep learning-driven clinical decision support solutions can
have a large impact during the on-going pandemic. Motivated by this, we
introduce COVID-Net US, a highly efficient, self-attention deep convolutional
neural network design tailored for COVID-19 screening from lung POCUS images.
Experimental results show that the proposed COVID-Net US can achieve an AUC of
over 0.98 while achieving 353X lower architectural complexity, 62X lower
computational complexity, and 14.3X faster inference times on a Raspberry Pi.
Clinical validation was also conducted, where select cases were reviewed and
reported on by a practicing clinician (20 years of clinical practice)
specializing in intensive care (ICU) and 15 years of expertise in POCUS
interpretation. To advocate affordable healthcare and artificial intelligence
for resource-constrained environments, we have made COVID-Net US open source
and publicly available as part of the COVID-Net open source initiative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1"&gt;Alexander MacLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1"&gt;Andy Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1"&gt;Maya Pavlova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1"&gt;Sonny Kohli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03151</id>
        <link href="http://arxiv.org/abs/2108.03151"/>
        <updated>2021-08-09T00:49:27.203Z</updated>
        <summary type="html"><![CDATA[Appearance and motion are two important sources of information in video
object segmentation (VOS). Previous methods mainly focus on using simplex
solutions, lowering the upper bound of feature collaboration among and across
these two cues. In this paper, we study a novel framework, termed the FSNet
(Full-duplex Strategy Network), which designs a relational cross-attention
module (RCAM) to achieve the bidirectional message propagation across embedding
subspaces. Furthermore, the bidirectional purification module (BPM) is
introduced to update the inconsistent features between the spatial-temporal
embeddings, effectively improving the model robustness. By considering the
mutual restraint within the full-duplex strategy, our FSNet performs the
cross-modal feature-passing (i.e., transmission and receiving) simultaneously
before the fusion and decoding stage, making it robust to various challenging
scenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five
popular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and
DAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both
the VOS and video salient object detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Keren Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhe Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianbing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MmWave Radar and Vision Fusion based Object Detection for Autonomous Driving: A Survey. (arXiv:2108.03004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03004</id>
        <link href="http://arxiv.org/abs/2108.03004"/>
        <updated>2021-08-09T00:49:27.196Z</updated>
        <summary type="html"><![CDATA[With autonomous driving developing in a booming stage, accurate object
detection in complex scenarios attract wide attention to ensure the safety of
autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a
mainstream solution for accurate obstacle detection. This article presents a
detailed survey on mmWave radar and vision fusion based obstacle detection
methods. Firstly, we introduce the tasks, evaluation criteria and datasets of
object detection for autonomous driving. Then, the process of mmWave radar and
vision fusion is divided into three parts: sensor deployment, sensor
calibration and sensor fusion, which are reviewed comprehensively. Especially,
we classify the fusion methods into data level, decision level and feature
level fusion methods. Besides, we introduce the fusion of lidar and vision in
autonomous driving in the aspects of obstacle detection, object classification
and road segmentation, which is promising in the future. Finally, we summarize
this article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhiqing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fengkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huici Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiyong Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Contrastive Learning by Visualizing Feature Transformation. (arXiv:2108.02982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02982</id>
        <link href="http://arxiv.org/abs/2108.02982"/>
        <updated>2021-08-09T00:49:27.189Z</updated>
        <summary type="html"><![CDATA[Contrastive learning, which aims at minimizing the distance between positive
pairs while maximizing that of negative ones, has been widely and successfully
applied in unsupervised feature learning, where the design of positive and
negative (pos/neg) pairs is one of its keys. In this paper, we attempt to
devise a feature-level data manipulation, differing from data augmentation, to
enhance the generic contrastive self-supervised learning. To this end, we first
design a visualization scheme for pos/neg score (Pos/neg score indicates cosine
similarity of pos/neg pair.) distribution, which enables us to analyze,
interpret and understand the learning process. To our knowledge, this is the
first attempt of its kind. More importantly, leveraging this tool, we gain some
significant observations, which inspire our novel Feature Transformation
proposals including the extrapolation of positives. This operation creates
harder positives to boost the learning because hard positives enable the model
to be more view-invariant. Besides, we propose the interpolation among
negatives, which provides diversified negatives and makes the model more
discriminative. It is the first attempt to deal with both challenges
simultaneously. Experiment results show that our proposed Feature
Transformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo
baseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline.
Transferring to the downstream tasks successfully demonstrate our model is less
task-bias. Visualization tools and codes
https://github.com/DTennant/CL-Visualizing-Feature-Transformation .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenglong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chang Wen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02998</id>
        <link href="http://arxiv.org/abs/2108.02998"/>
        <updated>2021-08-09T00:49:27.181Z</updated>
        <summary type="html"><![CDATA[The aortic vessel tree is composed of the aorta and its branching arteries,
and plays a key role in supplying the whole body with blood. Aortic diseases,
like aneurysms or dissections, can lead to an aortic rupture, whose treatment
with open surgery is highly risky. Therefore, patients commonly undergo drug
treatment under constant monitoring, which requires regular inspections of the
vessels through imaging. The standard imaging modality for diagnosis and
monitoring is computed tomography (CT), which can provide a detailed picture of
the aorta and its branching vessels if combined with a contrast agent,
resulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree
geometry from consecutive CTAs, are overlaid and compared. This allows to not
only detect changes in the aorta, but also more peripheral vessel tree changes,
caused by the primary pathology or newly developed. When performed manually,
this reconstruction requires slice by slice contouring, which could easily take
a whole day for a single aortic vessel tree and, hence, is not feasible in
clinical practice. Automatic or semi-automatic vessel tree segmentation
algorithms, on the other hand, can complete this task in a fraction of the
manual execution time and run in parallel to the clinical routine of the
clinicians. In this paper, we systematically review computing techniques for
the automatic and semi-automatic segmentation of the aortic vessel tree. The
review concludes with an in-depth discussion on how close these
state-of-the-art approaches are to an application in clinical practice and how
active this research field is, taking into account the number of publications,
datasets and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1"&gt;Antonio Pepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1"&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fen-hua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1"&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1"&gt;Jan Egger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS4Net: Two-Stage Sample Selective Strategy for Rotating Object Detection. (arXiv:2108.03116v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03116</id>
        <link href="http://arxiv.org/abs/2108.03116"/>
        <updated>2021-08-09T00:49:27.173Z</updated>
        <summary type="html"><![CDATA[Rotating object detection has wide applications in aerial photographs, remote
sensing images, UAVs, etc. At present, most of the rotating object detection
datasets focus on the field of remote sensing, and these images are usually
shot in high-altitude scenes. However, image datasets captured at low-altitude
areas also should be concerned, such as drone-based datasets. So we present a
low-altitude dronebased dataset, named UAV-ROD, aiming to promote the research
and development in rotating object detection and UAV applications. The UAV-ROD
consists of 1577 images and 30,090 instances of car category annotated by
oriented bounding boxes. In particular, The UAV-ROD can be utilized for the
rotating object detection, vehicle orientation recognition and object counting
tasks. Compared with horizontal object detection, the regression stage of the
rotation detection is a tricky problem. In this paper, we propose a rotating
object detector TS4Net, which contains anchor refinement module (ARM) and
two-stage sample selective strategy (TS4). The ARM can convert preseted
horizontal anchors into high-quality rotated anchors through twostage anchor
refinement. The TS4 module utilizes different constrained sample selective
strategies to allocate positive and negative samples, which is adaptive to the
regression task in different stages. Benefiting from the ARM and TS4, the
TS4Net can achieve superior performance for rotating object detection solely
with one preseted horizontal anchor. Extensive experimental results on UAV-ROD
dataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD
demonstrate that our method achieves competitive performance against most
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1"&gt;Kai Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weixing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Dongdong Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03117</id>
        <link href="http://arxiv.org/abs/2108.03117"/>
        <updated>2021-08-09T00:49:27.153Z</updated>
        <summary type="html"><![CDATA[In recent years, deep learning based methods have shown success in essential
medical image analysis tasks such as segmentation. Post-processing and refining
the results of segmentation is a common practice to decrease the
misclassifications originating from the segmentation network. In addition to
widely used methods like Conditional Random Fields (CRFs) which focus on the
structure of the segmented volume/area, a graph-based recent approach makes use
of certain and uncertain points in a graph and refines the segmentation
according to a small graph convolutional network (GCN). However, there are two
drawbacks of the approach: most of the edges in the graph are assigned randomly
and the GCN is trained independently from the segmentation network. To address
these issues, we define a new neighbor-selection mechanism according to feature
distances and combine the two networks in the training procedure. According to
the experimental results on pancreas segmentation from Computed Tomography (CT)
images, we demonstrate improvement in the quantitative measures. Also,
examining the dynamic neighbors created by our method, edges between
semantically similar image parts are observed. The proposed method also shows
qualitative enhancements in the segmentation maps, as demonstrated in the
visual results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ufuk Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1"&gt;Atahan Ozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1"&gt;Yusuf H. Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2108.03114</id>
        <link href="http://arxiv.org/abs/2108.03114"/>
        <updated>2021-08-09T00:49:27.144Z</updated>
        <summary type="html"><![CDATA[Two objects may be close in the Hausdor? metric, yet have very different
geometric and topological properties. We examine other methods of comparing
digital images such that objects close in each of these measures have some
similar geometric or topological property. Such measures may be combined with
the Hausdorff metric to yield a metric in which close images are similar with
respect to multiple properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1"&gt;Laurence Boxer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.03002</id>
        <link href="http://arxiv.org/abs/2108.03002"/>
        <updated>2021-08-09T00:49:27.136Z</updated>
        <summary type="html"><![CDATA[More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition
(CSVD-QR) method for matrix complete problem is presented, whose computational
complexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than
$\min\{m,n\}$, where $r$ represents the largest number of singular values of
matrix $X$. What is particularly interesting is that after replacing the
nuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as
the upper bound of the nuclear norm, when the intermediate matrix $D$ in its
decomposition is close to the diagonal matrix, it will converge to the nuclear
norm, and is exactly equal, when the $D$ matrix is equal to the diagonal
matrix, to the nuclear norm, which ingeniously avoids the calculation of the
singular value of the matrix. To the best of our knowledge, there is no
literature to generalize and apply it to solve tensor complete problems.
Inspired by this, in this paper we propose a class of tensor minimization model
based on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,
which is convex and therefore has a global minimum solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1"&gt;HongBing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;XinYi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1"&gt;HongTao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;YaJing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinlin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03064</id>
        <link href="http://arxiv.org/abs/2108.03064"/>
        <updated>2021-08-09T00:49:27.128Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised contrastive learning approach for facial
expression recognition (FER) in videos. We propose a novel temporal
sampling-based augmentation scheme to be utilized in addition to standard
spatial augmentations used for contrastive learning. Our proposed temporal
augmentation scheme randomly picks from one of three temporal sampling
techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential
sampling. This is followed by a combination of up to three standard spatial
augmentations. We then use a deep R(2+1)D network for FER, which we train in a
self-supervised fashion based on the augmentations and subsequently fine-tune.
Experiments are performed on the Oulu-CASIA dataset and the performance is
compared to other works in FER. The results indicate that our method achieves
an accuracy of 89.4%, setting a new state-of-the-art by outperforming other
works. Additional experiments and analysis confirm the considerable
contribution of the proposed temporal augmentation versus the existing spatial
ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Ultrasound Segmentation and Adaptation between COVID-19 and Community-Acquired Pneumonia. (arXiv:2108.03138v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03138</id>
        <link href="http://arxiv.org/abs/2108.03138"/>
        <updated>2021-08-09T00:49:27.121Z</updated>
        <summary type="html"><![CDATA[Lung ultrasound imaging has been shown effective in detecting typical
patterns for interstitial pneumonia, as a point-of-care tool for both patients
with COVID-19 and other community-acquired pneumonia (CAP). In this work, we
focus on the hyperechoic B-line segmentation task. Using deep neural networks,
we automatically outline the regions that are indicative of pathology-sensitive
artifacts and their associated sonographic patterns. With a real-world
data-scarce scenario, we investigate approaches to utilize both COVID-19 and
CAP lung ultrasound data to train the networks; comparing fine-tuning and
unsupervised domain adaptation. Segmenting either type of lung condition at
inference may support a range of clinical applications during evolving epidemic
stages, but also demonstrates value in resource-constrained clinical scenarios.
Adapting real clinical data acquired from COVID-19 patients to those from CAP
patients significantly improved Dice scores from 0.60 to 0.87 (p < 0.001) and
from 0.43 to 0.71 (p < 0.001), on independent COVID-19 and CAP test cases,
respectively. It is of practical value that the improvement was demonstrated
with only a small amount of data in both training and adaptation data sets, a
common constraint for deploying machine learning models in clinical practice.
Interestingly, we also report that the inverse adaptation, from labelled CAP
data to unlabeled COVID-19 data, did not demonstrate an improvement when tested
on either condition. Furthermore, we offer a possible explanation that
correlates the segmentation performance to label consistency and data domain
diversity in this point-of-care lung ultrasound application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1"&gt;Harry Mason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristoni_L/0/1/0/all/0/1"&gt;Lorenzo Cristoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walden_A/0/1/0/all/0/1"&gt;Andrew Walden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazzari_R/0/1/0/all/0/1"&gt;Roberto Lazzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulimood_T/0/1/0/all/0/1"&gt;Thomas Pulimood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandjean_L/0/1/0/all/0/1"&gt;Louis Grandjean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1"&gt;Claudia AM Gandini Wheeler-Kingshott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1"&gt;Zachary MC Baum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02958</id>
        <link href="http://arxiv.org/abs/2108.02958"/>
        <updated>2021-08-09T00:49:27.112Z</updated>
        <summary type="html"><![CDATA[Currently, the state-of-the-art methods treat few-shot semantic segmentation
task as a conditional foreground-background segmentation problem, assuming each
class is independent. In this paper, we introduce the concept of meta-class,
which is the meta information (e.g. certain middle-level features) shareable
among all classes. To explicitly learn meta-class representations in few-shot
segmentation task, we propose a novel Meta-class Memory based few-shot
segmentation method (MM-Net), where we introduce a set of learnable memory
embeddings to memorize the meta-class information during the base class
training and transfer to novel classes during the inference stage. Moreover,
for the $k$-shot scenario, we propose a novel image quality measurement module
to select images from the set of support images. A high-quality class prototype
could be obtained with the weighted sum of support image features based on the
quality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that
our proposed method is able to achieve state-of-the-art results in both 1-shot
and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\% mIoU on
the COCO dataset in 1-shot setting, which is 5.1\% higher than the previous
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiangxi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1"&gt;Guosheng lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. (arXiv:2108.02934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02934</id>
        <link href="http://arxiv.org/abs/2108.02934"/>
        <updated>2021-08-09T00:49:27.086Z</updated>
        <summary type="html"><![CDATA[Single image dehazing is a challenging task, for which the domain shift
between synthetic training data and real-world testing images usually leads to
degradation of existing methods. To address this issue, we propose a novel
image dehazing framework collaborating with unlabeled real data. First, we
develop a disentangled image dehazing network (DID-Net), which disentangles the
feature representations into three component maps, i.e. the latent haze-free
image, the transmission map, and the global atmospheric light estimate,
respecting the physical model of a haze process. Our DID-Net predicts the three
component maps by progressively integrating features across scales, and refines
each map by passing an independent refinement network. Then a
disentangled-consistency mean-teacher network (DMT-Net) is employed to
collaborate unlabeled real data for boosting single image dehazing.
Specifically, we encourage the coarse predictions and refinements of each
disentangled component to be consistent between the student and teacher
networks by using a consistency loss on unlabeled real data. We make comparison
with 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K)
and two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on
real-world hazy images. Experimental results demonstrate that our method has
obvious quantitative and qualitative improvements over the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1"&gt;Shunda Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1"&gt;Liang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02924</id>
        <link href="http://arxiv.org/abs/2108.02924"/>
        <updated>2021-08-09T00:49:27.078Z</updated>
        <summary type="html"><![CDATA[While image understanding on recognition-level has achieved remarkable
advancements, reliable visual scene understanding requires comprehensive image
understanding on recognition-level but also cognition-level, which calls for
exploiting the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge. In this paper, we propose a
novel Cognitive Attention Network (CAN) for visual commonsense reasoning to
achieve interpretable visual understanding. Specifically, we first introduce an
image-text fusion module to fuse information from images and text collectively.
Second, a novel inference module is designed to encode commonsense among image,
query and response. Extensive experiments on large-scale Visual Commonsense
Reasoning (VCR) benchmark dataset demonstrate the effectiveness of our
approach. The implementation is publicly available at
https://github.com/tanjatang/CAN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1"&gt;Kea Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1"&gt;Tyler Derr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1"&gt;Eirini Ntoutsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting. (arXiv:2108.02970v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02970</id>
        <link href="http://arxiv.org/abs/2108.02970"/>
        <updated>2021-08-09T00:49:27.071Z</updated>
        <summary type="html"><![CDATA[Labeling is onerous for crowd counting as it should annotate each individual
in crowd images. Recently, several methods have been proposed for
semi-supervised crowd counting to reduce the labeling efforts. Given a limited
labeling budget, they typically select a few crowd images and densely label all
individuals in each of them. Despite the promising results, we argue the
None-or-All labeling strategy is suboptimal as the densely labeled individuals
in each crowd image usually appear similar while the massive unlabeled crowd
images may contain entirely diverse individuals. To this end, we propose to
break the labeling chain of previous methods and make the first attempt to
reduce spatial labeling redundancy for semi-supervised crowd counting. First,
instead of annotating all the regions in each crowd image, we propose to
annotate the representative ones only. We analyze the region representativeness
from both vertical and horizontal directions, and formulate them as cluster
centers of Gaussian Mixture Models. Additionally, to leverage the rich
unlabeled regions, we exploit the similarities among individuals in each crowd
image to directly supervise the unlabeled regions via feature propagation
instead of the error-prone label propagation employed in the previous methods.
In this way, we can transfer the original spatial labeling redundancy caused by
individual similarities to effective supervision signals on the unlabeled
regions. Extensive experiments on the widely-used benchmarks demonstrate that
our method can outperform previous best approaches by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Liangyu Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanjie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing. (arXiv:2108.03072v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03072</id>
        <link href="http://arxiv.org/abs/2108.03072"/>
        <updated>2021-08-09T00:49:27.063Z</updated>
        <summary type="html"><![CDATA[Geometry-aware modules are widely applied in recent deep learning
architectures for scene representation and rendering. However, these modules
require intrinsic camera information that might not be obtained accurately. In
this paper, we propose a Spatial Transformation Routing (STR) mechanism to
model the spatial properties without applying any geometric prior. The STR
mechanism treats the spatial transformation as the message passing process, and
the relation between the view poses and the routing weights is modeled by an
end-to-end trainable neural network. Besides, an Occupancy Concept Mapping
(OCM) framework is proposed to provide explainable rationals for scene-fusion
processes. We conducted experiments on several datasets and show that the
proposed STR mechanism improves the performance of the Generative Query Network
(GQN). The visualization results reveal that the routing process can pass the
observed information from one location of some view to the associated location
in the other view, which demonstrates the advantage of the proposed model in
terms of spatial cognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen-Cheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Min-Chun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chu-Song Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03032</id>
        <link href="http://arxiv.org/abs/2108.03032"/>
        <updated>2021-08-09T00:49:27.056Z</updated>
        <summary type="html"><![CDATA[A few-shot semantic segmentation model is typically composed of a CNN
encoder, a CNN decoder and a simple classifier (separating foreground and
background pixels). Most existing methods meta-learn all three model components
for fast adaptation to a new class. However, given that as few as a single
support set image is available, effective model adaption of all three
components to the new class is extremely challenging. In this work we propose
to simplify the meta-learning task by focusing solely on the simplest
component, the classifier, whilst leaving the encoder and decoder to
pre-training. We hypothesize that if we pre-train an off-the-shelf segmentation
model over a set of diverse training classes with sufficient annotations, the
encoder and decoder can capture rich discriminative features applicable for any
unseen classes, rendering the subsequent meta-learning stage unnecessary. For
the classifier meta-learning, we introduce a Classifier Weight Transformer
(CWT) designed to dynamically adapt the supportset trained classifier's weights
to each query image in an inductive way. Extensive experiments on two standard
benchmarks show that despite its simplicity, our method outperforms the
state-of-the-art alternatives, often by a large margin.Code is available on
https://github.com/zhiheLu/CWTfor-FSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1"&gt;Zhihe lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Sen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AceNAS: Learning to Rank Ace Neural Architectures with Weak Supervision of Weight Sharing. (arXiv:2108.03001v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03001</id>
        <link href="http://arxiv.org/abs/2108.03001"/>
        <updated>2021-08-09T00:49:27.038Z</updated>
        <summary type="html"><![CDATA[Architecture performance predictors have been widely used in neural
architecture search (NAS). Although they are shown to be simple and effective,
the optimization objectives in previous arts (e.g., precise accuracy estimation
or perfect ranking of all architectures in the space) did not capture the
ranking nature of NAS. In addition, a large number of ground-truth
architecture-accuracy pairs are usually required to build a reliable predictor,
making the process too computationally expensive. To overcome these, in this
paper, we look at NAS from a novel point of view and introduce Learning to Rank
(LTR) methods to select the best (ace) architectures from a space.
Specifically, we propose to use Normalized Discounted Cumulative Gain (NDCG) as
the target metric and LambdaRank as the training algorithm. We also propose to
leverage weak supervision from weight sharing by pretraining architecture
representation on weak labels obtained from the super-net and then finetuning
the ranking model using a small number of architectures trained from scratch.
Extensive experiments on NAS benchmarks and large-scale search spaces
demonstrate that our approach outperforms SOTA with a significantly reduced
search cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenqian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanlu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Lyna Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaotian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuqing Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03021</id>
        <link href="http://arxiv.org/abs/2108.03021"/>
        <updated>2021-08-09T00:49:27.029Z</updated>
        <summary type="html"><![CDATA[Deep learning models achieve outstanding accuracy in semantic segmentation,
however they require a huge amount of labeled data for their optimization.
Hence, domain adaptation approaches have come into play to transfer knowledge
acquired on a label-abundant source domain to a related label-scarce target
domain. However, such models do not generalize well to data with statistical
properties not perfectly matching the ones of the training samples. In this
work, we design and carefully analyze multiple latent space-shaping
regularization strategies that work in conjunction to reduce the domain
discrepancy in semantic segmentation. In particular, we devise a feature
clustering strategy to increase domain alignment, a feature perpendicularity
constraint to space apart feature belonging to different semantic classes,
including those not present in the current batch, and a feature norm alignment
strategy to separate active and inactive channels. Additionally, we propose a
novel performance metric to capture the relative efficacy of an adaptation
strategy compared to supervised training. We verify the effectiveness of our
framework in synthetic-to-real and real-to-real adaptation scenarios,
outperforming previous state-of-the-art methods on multiple road scenes
benchmarks and using different backbones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1"&gt;Francesco Barbato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1"&gt;Umberto Michieli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1"&gt;Marco Toldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1"&gt;Pietro Zanuttigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Based Food Analysis for Automatic Dietary Assessment. (arXiv:2108.02947v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02947</id>
        <link href="http://arxiv.org/abs/2108.02947"/>
        <updated>2021-08-09T00:49:27.021Z</updated>
        <summary type="html"><![CDATA[Background: Maintaining a healthy diet is vital to avoid health-related
issues, e.g., undernutrition, obesity and many non-communicable diseases. An
indispensable part of the health diet is dietary assessment. Traditional manual
recording methods are burdensome and contain substantial biases and errors.
Recent advances in Artificial Intelligence, especially computer vision
technologies, have made it possible to develop automatic dietary assessment
solutions, which are more convenient, less time-consuming and even more
accurate to monitor daily food intake.

Scope and approach: This review presents one unified Vision-Based Dietary
Assessment (VBDA) framework, which generally consists of three stages: food
image analysis, volume estimation and nutrient derivation. Vision-based food
analysis methods, including food recognition, detection and segmentation, are
systematically summarized, and methods of volume estimation and nutrient
derivation are also given. The prosperity of deep learning makes VBDA gradually
move to an end-to-end implementation, which applies food images to a single
network to directly estimate the nutrition. The recently proposed end-to-end
methods are also discussed. We further analyze existing dietary assessment
datasets, indicating that one large-scale benchmark is urgently needed, and
finally highlight key challenges and future trends for VBDA.

Key findings and conclusions: After thorough exploration, we find that
multi-task end-to-end deep learning approaches are one important trend of VBDA.
Despite considerable research progress, many challenges remain for VBDA due to
the meal complexity. We also provide the latest ideas for future development of
VBDA, e.g., fine-grained food analysis and accurate volume estimation. This
survey aims to encourage researchers to propose more practical solutions for
VBDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1"&gt;Weiqing Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haisheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuqiang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Biological Anatomical Landmark Detection in Colonoscopy Videos. (arXiv:2108.02948v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02948</id>
        <link href="http://arxiv.org/abs/2108.02948"/>
        <updated>2021-08-09T00:49:27.013Z</updated>
        <summary type="html"><![CDATA[Colonoscopy is a standard imaging tool for visualizing the entire
gastrointestinal (GI) tract of patients to capture lesion areas. However, it
takes the clinicians excessive time to review a large number of images
extracted from colonoscopy videos. Thus, automatic detection of biological
anatomical landmarks within the colon is highly demanded, which can help reduce
the burden of clinicians by providing guidance information for the locations of
lesion areas. In this article, we propose a novel deep learning-based approach
to detect biological anatomical landmarks in colonoscopy videos. First, raw
colonoscopy video sequences are pre-processed to reject interference frames.
Second, a ResNet-101 based network is used to detect three biological
anatomical landmarks separately to obtain the intermediate detection results.
Third, to achieve more reliable localization of the landmark periods within the
whole video period, we propose to post-process the intermediate detection
results by identifying the incorrectly predicted frames based on their temporal
distribution and reassigning them back to the correct class. Finally, the
average detection accuracy reaches 99.75\%. Meanwhile, the average IoU of 0.91
shows a high degree of similarity between our predicted landmark periods and
ground truth. The experimental results demonstrate that our proposed model is
capable of accurately detecting and localizing biological anatomical landmarks
from colonoscopy videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1"&gt;Kaiwei Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chengwei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yibing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Nachuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiankun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1"&gt;Max Q.-H. Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Detection for Hand Hygiene Stages. (arXiv:2108.03015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03015</id>
        <link href="http://arxiv.org/abs/2108.03015"/>
        <updated>2021-08-09T00:49:27.004Z</updated>
        <summary type="html"><![CDATA[The process of hand washing involves complex hand movements. There are six
principal sequential steps for washing hands as per the World Health
Organisation (WHO) guidelines. In this work, a detailed description of an
aluminium rig construction for creating a robust hand-washing dataset is
discussed. The preliminary results with the help of image processing and
computer vision algorithms for hand pose extraction and feature detection such
as Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene
pose- Rub hands palm to palm was captured as an input image for running all the
experiments. The future work will focus upon processing the video recordings of
hand movements captured and applying deep-learning solutions for the
classification of hand-hygiene stages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1"&gt;Rashmi Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1"&gt;Jane Courtney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1"&gt;Damon Berry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1"&gt;Graham Gavin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-frequency shape recovery from shading by CNN and domain adaptation. (arXiv:2108.02937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02937</id>
        <link href="http://arxiv.org/abs/2108.02937"/>
        <updated>2021-08-09T00:49:26.997Z</updated>
        <summary type="html"><![CDATA[Importance of structured-light based one-shot scanning technique is
increasing because of its simple system configuration and ability of capturing
moving objects. One severe limitation of the technique is that it can capture
only sparse shape, but not high frequency shapes, because certain area of
projection pattern is required to encode spatial information. In this paper, we
propose a technique to recover high-frequency shapes by using shading
information, which is captured by one-shot RGB-D sensor based on structured
light with single camera. Since color image comprises shading information of
object surface, high-frequency shapes can be recovered by shape from shading
techniques. Although multiple images with different lighting positions are
required for shape from shading techniques, we propose a learning based
approach to recover shape from a single image. In addition, to overcome the
problem of preparing sufficient amount of data for training, we propose a new
data augmentation method for high-frequency shapes using synthetic data and
domain adaptation. Experimental results are shown to confirm the effectiveness
of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tokieda_K/0/1/0/all/0/1"&gt;Kodai Tokieda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1"&gt;Takafumi Iwaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1"&gt;Hiroshi Kawasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Mesh Estimation from Depth Data using Non-Smooth Convex Optimization. (arXiv:2108.02957v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02957</id>
        <link href="http://arxiv.org/abs/2108.02957"/>
        <updated>2021-08-09T00:49:26.978Z</updated>
        <summary type="html"><![CDATA[Meshes are commonly used as 3D maps since they encode the topology of the
scene while being lightweight.

Unfortunately, 3D meshes are mathematically difficult to handle directly
because of their combinatorial and discrete nature.

Therefore, most approaches generate 3D meshes of a scene after fusing depth
data using volumetric or other representations.

Nevertheless, volumetric fusion remains computationally expensive both in
terms of speed and memory.

In this paper, we leapfrog these intermediate representations and build a 3D
mesh directly from a depth map and the sparse landmarks triangulated with
visual odometry.

To this end, we formulate a non-smooth convex optimization problem that we
solve using a primal-dual method.

Our approach generates a smooth and accurate 3D mesh that substantially
improves the state-of-the-art on direct mesh reconstruction while running in
real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1"&gt;Antoni Rosinol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1"&gt;Luca Carlone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detailed Avatar Recovery from Single Image. (arXiv:2108.02931v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02931</id>
        <link href="http://arxiv.org/abs/2108.02931"/>
        <updated>2021-08-09T00:49:26.970Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel framework to recover \emph{detailed} avatar from
a single image. It is a challenging task due to factors such as variations in
human shapes, body poses, texture, and viewpoints. Prior methods typically
attempt to recover the human body shape using a parametric-based template that
lacks the surface details. As such resulting body shape appears to be without
clothing. In this paper, we propose a novel learning-based framework that
combines the robustness of the parametric model with the flexibility of
free-form 3D deformation. We use the deep neural networks to refine the 3D
shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the
constraints from body joints, silhouettes, and per-pixel shading information.
Our method can restore detailed human body shapes with complete textures beyond
skinned models. Experiments demonstrate that our method has outperformed
previous state-of-the-art approaches, achieving better accuracy in terms of
both 2D IoU number and 3D metric distance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haotian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (arXiv:2108.02858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02858</id>
        <link href="http://arxiv.org/abs/2108.02858"/>
        <updated>2021-08-09T00:49:26.962Z</updated>
        <summary type="html"><![CDATA[mmWave radar has been shown as an effective sensing technique in low
visibility, smoke, dusty, and dense fog environment. However tapping the
potential of radar sensing to reconstruct 3D object shapes remains a great
challenge, due to the characteristics of radar data such as sparsity, low
resolution, specularity, high noise, and multi-path induced shadow reflections
and artifacts. In this paper we propose 3D Reconstruction and Imaging via
mmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D
shape of an object in dense detailed point cloud format, based on sparse raw
mmWave radar intensity data. The architecture consists of two back-to-back
conditional GAN deep neural networks: the first generator network generates 2D
depth images based on raw radar intensity data, and the second generator
network outputs 3D point clouds based on the results of the first generator.
The architecture exploits both convolutional neural network's convolutional
operation (that extracts local structure neighborhood information) and the
efficiency and detailed geometry capture capability of point clouds (other than
costly voxelization of 3D space or distance fields). Our experiments have
demonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its
performance improvement over standard techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yue Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhuoming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honggang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Deqiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. (arXiv:2108.02840v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02840</id>
        <link href="http://arxiv.org/abs/2108.02840"/>
        <updated>2021-08-09T00:49:26.954Z</updated>
        <summary type="html"><![CDATA[This paper introduces a method for image semantic segmentation grounded on a
novel fusion scheme, which takes place inside a deep convolutional neural
network. The main goal of our proposal is to explore object boundary
information to improve the overall segmentation performance. Unlike previous
works that combine boundary and segmentation features, or those that use
boundary information to regularize semantic segmentation, we instead propose a
novel approach that embodies boundary information onto segmentation. For that,
our semantic segmentation method uses two streams, which are combined through
an attention gate, forming an end-to-end Y-model. To the best of our knowledge,
ours is the first work to show that boundary detection can improve semantic
segmentation when fused through a semantic fusion gate (attention model). We
performed an extensive evaluation of our method over public data sets. We found
competitive results on all data sets after comparing our proposed model with
other twelve state-of-the-art segmenters, considering the same training
conditions. Our proposed model achieved the best mIoU on the CityScapes,
CamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontinele_J/0/1/0/all/0/1"&gt;Jefferson Fontinele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lefundes_G/0/1/0/all/0/1"&gt;Gabriel Lefundes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1"&gt;Luciano Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning. (arXiv:2108.02959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02959</id>
        <link href="http://arxiv.org/abs/2108.02959"/>
        <updated>2021-08-09T00:49:26.935Z</updated>
        <summary type="html"><![CDATA[Visual retrieval system faces frequent model update and deployment. It is a
heavy workload to re-extract features of the whole database every time.Feature
compatibility enables the learned new visual features to be directly compared
with the old features stored in the database. In this way, when updating the
deployed model, we can bypass the inflexible and time-consuming feature
re-extraction process. However, the old feature space that needs to be
compatible is not ideal and faces the distribution discrepancy problem with the
new space caused by different supervision losses. In this work, we propose a
global optimization Dual-Tuning method to obtain feature compatibility against
different networks and losses. A feature-level prototype loss is proposed to
explicitly align two types of embedding features, by transferring global
prototype information. Furthermore, we design a component-level mutual
structural regularization to implicitly optimize the feature intrinsic
structure. Experimental results on million-scale datasets demonstrate that our
Dual-Tuning is able to obtain feature compatibility without sacrificing
performance. (Our code will be avaliable at
https://github.com/yanbai1993/Dual-Tuning)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yan Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jile Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shengsen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yihang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xuetao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Ling-Yu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02953</id>
        <link href="http://arxiv.org/abs/2108.02953"/>
        <updated>2021-08-09T00:49:26.927Z</updated>
        <summary type="html"><![CDATA[This paper investigates a valuable setting called few-shot unsupervised
domain adaptation (FS-UDA), which has not been sufficiently studied in the
literature. In this setting, the source domain data are labelled, but with
few-shot per category, while the target domain data are unlabelled. To address
the FS-UDA setting, we develop a general UDA model to solve the following two
key issues: the few-shot labeled data per category and the domain adaptation
between support and query sets. Our model is general in that once trained it
will be able to be applied to various FS-UDA tasks from the same source and
target domains. Inspired by the recent local descriptor based few-shot learning
(FSL), our general UDA model is fully built upon local descriptors (LDs) for
image classification and domain adaptation. By proposing a novel concept called
similarity patterns (SPs), our model not only effectively considers the spatial
relationship of LDs that was ignored in previous FSL methods, but also makes
the learned image similarity better serve the required domain alignment.
Specifically, we propose a novel IMage-to-class sparse Similarity Encoding
(IMSE) method. It learns SPs to extract the local discriminative information
for classification and meanwhile aligns the covariance matrix of the SPs for
domain adaptation. Also, domain adversarial training and multi-scale local
feature matching are performed upon LDs. Extensive experiments conducted on a
multi-domain benchmark dataset DomainNet demonstrates the state-of-the-art
performance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,
our IMSE can also show better performance than most of recent FSL methods on
miniImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wanqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation. (arXiv:2108.02980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02980</id>
        <link href="http://arxiv.org/abs/2108.02980"/>
        <updated>2021-08-09T00:49:26.920Z</updated>
        <summary type="html"><![CDATA[Existing domain adaptation methods for crowd counting view each crowd image
as a whole and reduce domain discrepancies on crowds and backgrounds
simultaneously. However, we argue that these methods are suboptimal, as crowds
and backgrounds have quite different characteristics and backgrounds may vary
dramatically in different crowd scenes (see Fig.~\ref{teaser}). This makes
crowds not well aligned across domains together with backgrounds in a holistic
manner. To this end, we propose to untangle crowds and backgrounds from crowd
images and design fine-grained domain adaption methods for crowd counting.
Different from other tasks which possess region-based fine-grained annotations
(e.g., segments or bounding boxes), crowd counting only annotates one point on
each human head, which impedes the implementation of fine-grained adaptation
methods. To tackle this issue, we propose a novel and effective schema to learn
crowd segmentation from point-level crowd counting annotations in the context
of Multiple Instance Learning. We further leverage the derived segments to
propose a crowd-aware fine-grained domain adaptation framework for crowd
counting, which consists of two novel adaptation modules, i.e., Crowd Region
Transfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module
is designed to guide crowd features transfer across domains beyond background
distractions, and the CDA module dedicates to constraining the target-domain
crowd density distributions. Extensive experiments on multiple cross-domain
settings (i.e., Synthetic $\rightarrow$ Real, Fixed $\rightarrow$ Fickle,
Normal $\rightarrow$ BadWeather) demonstrate the superiority of the proposed
method compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanjie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongmin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images. (arXiv:2108.02996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02996</id>
        <link href="http://arxiv.org/abs/2108.02996"/>
        <updated>2021-08-09T00:49:26.910Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation of medical images is an essential first step in
computer-aided diagnosis systems for many applications. However, given many
disparate imaging modalities and inherent variations in the patient data, it is
difficult to consistently achieve high accuracy using modern deep neural
networks (DNNs). This has led researchers to propose interactive image
segmentation techniques where a medical expert can interactively correct the
output of a DNN to the desired accuracy. However, these techniques often need
separate training data with the associated human interactions, and do not
generalize to various diseases, and types of medical images. In this paper, we
suggest a novel conditional inference technique for DNNs which takes the
intervention by a medical expert as test time constraints and performs
inference conditioned upon these constraints. Our technique is generic can be
used for medical images from any modality. Unlike other methods, our approach
can correct multiple structures simultaneously and add structures missed at
initial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and
12.4 times in user annotation time than full human annotation for the nucleus,
multiple cells, liver and tumor, organ, and brain segmentation respectively. We
report a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other
interactive segmentation techniques. Our method can be useful to clinicians for
diagnosis and post-surgical follow-up with minimal intervention from the
medical expert. The source-code and the detailed results are available here
[1].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sambaturu_B/0/1/0/all/0/1"&gt;Bhavani Sambaturu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ashutosh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V. Jawahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1"&gt;Chetan Arora&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. (arXiv:2108.02818v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02818</id>
        <link href="http://arxiv.org/abs/2108.02818"/>
        <updated>2021-08-09T00:49:26.901Z</updated>
        <summary type="html"><![CDATA[Recently, there have been breakthroughs in computer vision ("CV") models that
are more generalizable with the advent of models such as CLIP and ALIGN. In
this paper, we analyze CLIP and highlight some of the challenges such models
pose. CLIP reduces the need for task specific training data, potentially
opening up many niche tasks to automation. CLIP also allows its users to
flexibly specify image classification classes in natural language, which we
find can shift how biases manifest. Additionally, through some preliminary
probes we find that CLIP can inherit biases found in prior computer vision
systems. Given the wide and unpredictable domain of uses for such models, this
raises questions regarding what sufficiently safe behaviour for such systems
may look like. These results add evidence to the growing body of work calling
for a change in the notion of a 'better' model--to move beyond simply looking
at higher accuracy at task-oriented capability evaluations, and towards a
broader 'better' that takes into account deployment-critical features such as
different use contexts, and people who interact with the model when thinking
about model deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sandhini Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1"&gt;Gretchen Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1"&gt;Jack Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1"&gt;Alec Radford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jong Wook Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1"&gt;Miles Brundage&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinaFood21: A Novel Dataset for Evaluating Vietnamese Food Recognition. (arXiv:2108.02929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02929</id>
        <link href="http://arxiv.org/abs/2108.02929"/>
        <updated>2021-08-09T00:49:26.874Z</updated>
        <summary type="html"><![CDATA[Vietnam is such an attractive tourist destination with its stunning and
pristine landscapes and its top-rated unique food and drink. Among thousands of
Vietnamese dishes, foreigners and native people are interested in easy-to-eat
tastes and easy-to-do recipes, along with reasonable prices, mouthwatering
flavors, and popularity. Due to the diversity and almost all the dishes have
significant similarities and the lack of quality Vietnamese food datasets, it
is hard to implement an auto system to classify Vietnamese food, therefore,
make people easier to discover Vietnamese food. This paper introduces a new
Vietnamese food dataset named VinaFood21, which consists of 13,950 images
corresponding to 21 dishes. We use 10,044 images for model training and 6,682
test images to classify each food in the VinaFood21 dataset and achieved an
average accuracy of 74.81% when fine-tuning CNN EfficientNet-B0.
(https://github.com/nguyenvd-uit/uit-together-dataset)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuan Trong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuan Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1"&gt;Dung Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Ngoc Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_N/0/1/0/all/0/1"&gt;Nguyen D. Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khang Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04130</id>
        <link href="http://arxiv.org/abs/2102.04130"/>
        <updated>2021-08-09T00:49:26.852Z</updated>
        <summary type="html"><![CDATA[The capabilities of natural language models trained on large-scale data have
increased immensely over the past few years. Open source libraries such as
HuggingFace have made these models easily available and accessible. While prior
research has identified biases in large language models, this paper considers
biases contained in the most popular versions of these models when applied
`out-of-the-box' for downstream tasks. We focus on generative language models
as they are well-suited for extracting biases inherited from training data.
Specifically, we conduct an in-depth analysis of GPT-2, which is the most
downloaded text generation model on HuggingFace, with over half a million
downloads in the past month alone. We assess biases related to occupational
associations for different protected categories by intersecting gender with
religion, sexuality, ethnicity, political affiliation, and continental name
origin. Using a template-based data collection pipeline, we collect 396K
sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are
less diverse and more stereotypical for women than for men, especially for
intersections; (ii) Intersectional interactions are highly relevant for
occupational associations, which we quantify by fitting 262 logistic models;
(iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity
distribution found in US Labour Bureau data, and even pulls the
societally-skewed distribution towards gender parity in cases where its
predictions deviate from real labor market observations. This raises the
normative question of what language models _should_ learn - whether they should
reflect or correct for existing inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1"&gt;Hannah Kirk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1"&gt;Yennie Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haider Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1"&gt;Elias Benussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1"&gt;Filippo Volpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1"&gt;Frederic A. Dreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1"&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1"&gt;Yuki M. Asano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02923</id>
        <link href="http://arxiv.org/abs/2108.02923"/>
        <updated>2021-08-09T00:49:26.844Z</updated>
        <summary type="html"><![CDATA[Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuxi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yuchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiameng Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02833</id>
        <link href="http://arxiv.org/abs/2108.02833"/>
        <updated>2021-08-09T00:49:26.836Z</updated>
        <summary type="html"><![CDATA[The growing number of action classes has posed a new challenge for video
understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.
The ZSAR task aims to recognize target (unseen) actions without training
examples by leveraging semantic representations to bridge seen and unseen
actions. However, due to the complexity and diversity of actions, it remains
challenging to semantically represent action classes and transfer knowledge
from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by
an effective human memory technique Elaborative Rehearsal (ER), which involves
elaborating a new concept and relating it to known concepts. Specifically, we
expand each action class as an Elaborative Description (ED) sentence, which is
more discriminative than a class name and less costly than manual-defined
attributes. Besides directly aligning class semantics with videos, we
incorporate objects from the video as Elaborative Concepts (EC) to improve
video semantics and generalization from seen actions to unseen actions. Our
ER-enhanced ZSAR model achieves state-of-the-art results on three existing
benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics
dataset to overcome limitations of current benchmarks and demonstrate the first
case where ZSAR performance is comparable to few-shot learning baselines on
this more realistic setting. We will release our codes and collected EDs at
https://github.com/DeLightCMU/ElaborativeRehearsal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Dong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation. (arXiv:2103.06523v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06523</id>
        <link href="http://arxiv.org/abs/2103.06523"/>
        <updated>2021-08-09T00:49:26.829Z</updated>
        <summary type="html"><![CDATA[BERT-based Neural Ranking Models (NRMs) can be classified according to how
the query and document are encoded through BERT's self-attention layers -
bi-encoder versus cross-encoder. Bi-encoder models are highly efficient because
all the documents can be pre-processed before the query time, but their
performance is inferior compared to cross-encoder models. Both models utilize a
ranker that receives BERT representations as the input and generates a
relevance score as the output. In this work, we propose a method where
multi-teacher distillation is applied to a cross-encoder NRM and a bi-encoder
NRM to produce a bi-encoder NRM with two rankers. The resulting student
bi-encoder achieves an improved performance by simultaneously learning from a
cross-encoder teacher and a bi-encoder teacher and also by combining relevance
scores from the two rankers. We call this method TRMD (Two Rankers and
Multi-teacher Distillation). In the experiments, TwinBERT and ColBERT are
considered as baseline bi-encoders. When monoBERT is used as the cross-encoder
teacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,
TRMD produces a student bi-encoder that performs better than the corresponding
baseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the
average improvement was 6.8%. As an additional experiment, we considered
producing cross-encoder students with TRMD, and found that it could also
improve the cross-encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaekeol Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1"&gt;Euna Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1"&gt;Jangwon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02940</id>
        <link href="http://arxiv.org/abs/2108.02940"/>
        <updated>2021-08-09T00:49:26.809Z</updated>
        <summary type="html"><![CDATA[In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jindi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kejie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaohua Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-domain Generalization from a Single Source: A Uncertainty Quantification Approach. (arXiv:2108.02888v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02888</id>
        <link href="http://arxiv.org/abs/2108.02888"/>
        <updated>2021-08-09T00:49:26.797Z</updated>
        <summary type="html"><![CDATA[We study a worst-case scenario in generalization: Out-of-domain
generalization from a single source. The goal is to learn a robust model from a
single source and expect it to generalize over many unknown distributions. This
challenging problem has been seldom investigated while existing solutions
suffer from various limitations such as the ignorance of uncertainty assessment
and label augmentation. In this paper, we propose uncertainty-guided domain
generalization to tackle the aforementioned limitations. The key idea is to
augment the source capacity in both feature and label spaces, while the
augmentation is guided by uncertainty assessment. To the best of our knowledge,
this is the first work to (1) quantify the generalization uncertainty from a
single source and (2) leverage it to guide both feature and label augmentation
for robust generalization. The model training and deployment are effectively
organized in a Bayesian meta-learning framework. We conduct extensive
comparisons and ablation study to validate our approach. The results prove our
superior performance in a wide scope of tasks including image classification,
semantic segmentation, text classification, and speech recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1"&gt;Fengchun Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basis Scaling and Double Pruning for Efficient Transfer Learning. (arXiv:2108.02893v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02893</id>
        <link href="http://arxiv.org/abs/2108.02893"/>
        <updated>2021-08-09T00:49:26.790Z</updated>
        <summary type="html"><![CDATA[Transfer learning allows the reuse of deep learning features on new datasets
with limited data. However, the resulting models could be unnecessarily large
and thus inefficient. Although network pruning can be applied to improve
inference efficiency, existing algorithms usually require fine-tuning and may
not be suitable for small datasets. In this paper, we propose an algorithm that
transforms the convolutional weights into the subspaces of orthonormal bases
where a model is pruned. Using singular value decomposition, we decompose a
convolutional layer into two layers: a convolutional layer with the orthonormal
basis vectors as the filters, and a layer that we name "BasisScalingConv",
which is responsible for rescaling the features and transforming them back to
the original space. As the filters in each transformed layer are linearly
independent with known relative importance, pruning can be more effective and
stable, and fine tuning individual weights is unnecessary. Furthermore, as the
numbers of input and output channels of the original convolutional layer remain
unchanged, basis pruning is applicable to virtually all network architectures.
Basis pruning can also be combined with existing pruning algorithms for double
pruning to further increase the pruning capability. With less than 1% reduction
in the classification accuracy, we can achieve pruning ratios up to 98.9% in
parameters and 98.6% in FLOPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Ken C. L. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features. (arXiv:2108.02927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02927</id>
        <link href="http://arxiv.org/abs/2108.02927"/>
        <updated>2021-08-09T00:49:26.782Z</updated>
        <summary type="html"><![CDATA[Image Retrieval is a fundamental task of obtaining images similar to the
query one from a database. A common image retrieval practice is to firstly
retrieve candidate images via similarity search using global image features and
then re-rank the candidates by leveraging their local features. Previous
learning-based studies mainly focus on either global or local image
representation learning to tackle the retrieval task. In this paper, we abandon
the two-stage paradigm and seek to design an effective single-stage solution by
integrating local and global information inside images into compact image
representations. Specifically, we propose a Deep Orthogonal Local and Global
(DOLG) information fusion framework for end-to-end image retrieval. It
attentively extracts representative local information with multi-atrous
convolutions and self-attention at first. Components orthogonal to the global
image representation are then extracted from the local information. At last,
the orthogonal components are concatenated with the global representation as a
complementary, and then aggregation is performed to generate the final
representation. The whole framework is end-to-end differentiable and can be
trained with image-level labels. Extensive experimental results validate the
effectiveness of our solution and show that our model achieves state-of-the-art
image retrieval performances on Revisited Oxford and Paris datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Miao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Baorong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xuetong Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jizhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning. (arXiv:2108.02832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02832</id>
        <link href="http://arxiv.org/abs/2108.02832"/>
        <updated>2021-08-09T00:49:26.762Z</updated>
        <summary type="html"><![CDATA[Most of the existing works in supervised spatio-temporal video
super-resolution (STVSR) heavily rely on a large-scale external dataset
consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution
high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these
methods make a prior assumption that the low-resolution video is obtained by
down-scaling the high-resolution video using a known degradation kernel, which
does not hold in practical settings. Another problem with these methods is that
they cannot exploit instance-specific internal information of video at testing
time. Recently, deep internal learning approaches have gained attention due to
their ability to utilize the instance-specific statistics of a video. However,
these methods have a large inference time as they require thousands of gradient
updates to learn the intrinsic structure of the data. In this work, we
presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as
well as internal, information through meta-transfer learning and internal
learning, respectively. Specifically, meta-learning is employed to obtain
adaptive parameters, using a large-scale external dataset, that can adapt
quickly to the novel condition (degradation model) of the given test video
during the internal learning task, thereby exploiting external and internal
information of a video for super-resolution. The model trained using our
approach can quickly adapt to a specific video condition with only a few
gradient updates, which reduces the inference time significantly. Extensive
experiments on standard datasets demonstrate that our method performs favorably
against various state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Akash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1"&gt;Padmaja Jonnalagedda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02874</id>
        <link href="http://arxiv.org/abs/2108.02874"/>
        <updated>2021-08-09T00:49:26.755Z</updated>
        <summary type="html"><![CDATA[A lifespan face synthesis (LFS) model aims to generate a set of
photo-realistic face images of a person's whole life, given only one snapshot
as reference. The generated face image given a target age code is expected to
be age-sensitive reflected by bio-plausible transformations of shape and
texture, while being identity preserving. This is extremely challenging because
the shape and texture characteristics of a face undergo separate and highly
nonlinear transformations w.r.t. age. Most recent LFS models are based on
generative adversarial networks (GANs) whereby age code conditional
transformations are applied to a latent face representation. They benefit
greatly from the recent advancements of GANs. However, without explicitly
disentangling their latent representations into the texture, shape and identity
factors, they are fundamentally limited in modeling the nonlinear age-related
transformation on texture and shape whilst preserving identity. In this work, a
novel LFS model is proposed to disentangle the key face characteristics
including shape, texture and identity so that the unique shape and texture age
transformations can be modeled effectively. This is achieved by extracting
shape, texture and identity features separately from an encoder. Critically,
two transformation modules, one conditional convolution based and the other
channel attention based, are designed for modeling the nonlinear shape and
texture feature transformations respectively. This is to accommodate their
rather distinct aging processes and ensure that our synthesized images are both
age-sensitive and identity preserving. Extensive experiments show that our LFS
model is clearly superior to the state-of-the-art alternatives. Codes and demo
are available on our project website:
\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Sen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wentong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Michael Ying Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12700</id>
        <link href="http://arxiv.org/abs/2106.12700"/>
        <updated>2021-08-09T00:49:26.734Z</updated>
        <summary type="html"><![CDATA[With the increasing scale of search engine marketing, designing an efficient
bidding system is becoming paramount for the success of e-commerce companies.
The critical challenges faced by a modern industrial-level bidding system
include: 1. the catalog is enormous, and the relevant bidding features are of
high sparsity; 2. the large volume of bidding requests induces significant
computation burden to both the offline and online serving. Leveraging
extraneous user-item information proves essential to mitigate the sparsity
issue, for which we exploit the natural language signals from the users' query
and the contextual knowledge from the products. In particular, we extract the
vector representations of ads via the Transformer model and leverage their
geometric relation to building collaborative bidding predictions via
clustering. The two-step procedure also significantly reduces the computation
stress of bid evaluation and optimization. In this paper, we introduce the
end-to-end structure of the bidding system for search engine marketing for
Walmart e-commerce, which successfully handles tens of millions of bids each
day. We analyze the online and offline performances of our approach and discuss
how we find it as a production-efficient solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1"&gt;Cheng Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Visual Analysis of High-Volume Social Media Posts. (arXiv:2108.03052v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.03052</id>
        <link href="http://arxiv.org/abs/2108.03052"/>
        <updated>2021-08-09T00:49:26.725Z</updated>
        <summary type="html"><![CDATA[Breaking news and first-hand reports often trend on social media platforms
before traditional news outlets cover them. The real-time analysis of posts on
such platforms can reveal valuable and timely insights for journalists,
politicians, business analysts, and first responders, but the high number and
diversity of new posts pose a challenge. In this work, we present an
interactive system that enables the visual analysis of streaming social media
data on a large scale in real-time. We propose an efficient and explainable
dynamic clustering algorithm that powers a continuously updated visualization
of the current thematic landscape as well as detailed visual summaries of
specific topics of interest. Our parallel clustering strategy provides an
adaptive stream with a digestible but diverse selection of recent posts related
to relevant topics. We also integrate familiar visual metaphors that are highly
interlinked for enabling both explorative and more focused monitoring tasks.
Analysts can gradually increase the resolution to dive deeper into particular
topics. In contrast to previous work, our system also works with non-geolocated
posts and avoids extensive preprocessing such as detecting events. We evaluated
our dynamic clustering algorithm and discuss several use cases that show the
utility of our system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1"&gt;Johannes Knittel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1"&gt;Steffen Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yingcai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shixia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ertl_T/0/1/0/all/0/1"&gt;Thomas Ertl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Bandwidth Auto-encoder for Hybrid Recommender Systems. (arXiv:2105.07597v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07597</id>
        <link href="http://arxiv.org/abs/2105.07597"/>
        <updated>2021-08-09T00:49:26.713Z</updated>
        <summary type="html"><![CDATA[Hybrid recommendations have recently attracted a lot of attention where user
features are utilized as auxiliary information to address the sparsity problem
caused by insufficient user-item interactions. However, extracted user features
generally contain rich multimodal information, and most of them are irrelevant
to the recommendation purpose. Therefore, excessive reliance on these features
will make the model overfit on noise and difficult to generalize. In this
article, we propose a variational bandwidth auto-encoder (VBAE) for
recommendations, aiming to address the sparsity and noise problems
simultaneously. VBAE first encodes user collaborative and feature information
into Gaussian latent variables via deep neural networks to capture non-linear
user similarities. Moreover, by considering the fusion of collaborative and
feature variables as a virtual communication channel from an
information-theoretic perspective, we introduce a user-dependent channel to
dynamically control the information allowed to be accessed from the feature
embeddings. A quantum-inspired uncertainty measurement of the hidden rating
embeddings is proposed accordingly to infer the channel bandwidth by
disentangling the uncertainty information in the ratings from the semantic
information. Through this mechanism, VBAE incorporates adequate auxiliary
information from user features if collaborative information is insufficient,
while avoiding excessive reliance on noisy user features to improve its
generalization ability to new users. Extensive experiments conducted on three
real-world datasets demonstrate the effectiveness of the proposed method. Codes
and datasets are released at https://github.com/yaochenzhu/vbae.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02870</id>
        <link href="http://arxiv.org/abs/2108.02870"/>
        <updated>2021-08-09T00:49:26.704Z</updated>
        <summary type="html"><![CDATA[Covid-19 detection at an early stage can aid in an effective treatment and
isolation plan to prevent its spread. Recently, transfer learning has been used
for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major
limitations inherent to these proposed methods is limited labeled dataset size
that affects the reliability of Covid-19 diagnosis and disease progression. In
this work, we demonstrate that how we can augment limited X-ray images data by
using Contrast limited adaptive histogram equalization (CLAHE) to train the
last layer of the pre-trained deep learning models to mitigate the bias of
transfer learning for Covid-19 detection. We transfer learned various
pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,
and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.
The experiment results reveal that the CLAHE-based augmentation to various
pre-trained deep learning models significantly improves the model efficiency.
The pre-trained VCG-16 model with CLAHEbased augmented images achieves a
sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when
trained on non-augmented data. Other models demonstrate a value of less than
60% when trained on non-augmented data. Our results reveal that the sample bias
can negatively impact the performance of transfer learning which is
significantly improved by using CLAHE-based augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1"&gt;Aparna Reji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse. (arXiv:2108.02800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02800</id>
        <link href="http://arxiv.org/abs/2108.02800"/>
        <updated>2021-08-09T00:49:26.684Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a case study that performs an unmanned aerial
vehicle (UAV) based fine-scale 3D change detection and monitoring of
progressive collapse performance of a building during a demolition event.
Multi-temporal oblique photogrammetry images are collected with 3D point clouds
generated at different stages of the demolition. The geometric accuracy of the
generated point clouds has been evaluated against both airborne and terrestrial
LiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof
and facade respectively. We propose a hierarchical volumetric change detection
framework that unifies multi-temporal UAV images for pose estimation (free of
ground control points), reconstruction, and a coarse-to-fine 3D density change
analysis. This work has provided a solution capable of addressing change
detection on full 3D time-series datasets where dramatic scene content changes
are presented progressively. Our change detection results on the building
demolition event have been evaluated against the manually marked ground-truth
changes and have achieved an F-1 score varying from 0.78 to 0.92, with
consistently high precision (0.92 - 0.99). Volumetric changes through the
demolition progress are derived from change detection and have shown to
favorably reflect the qualitative and quantitative building demolition
progression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ningli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Debao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiao Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strasbaugh_C/0/1/0/all/0/1"&gt;Chris Strasbaugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Alper Yilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1"&gt;Halil Sezen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07942</id>
        <link href="http://arxiv.org/abs/1912.07942"/>
        <updated>2021-08-09T00:49:26.673Z</updated>
        <summary type="html"><![CDATA[To continuously improve quality and reflect changes in data, machine learning
applications have to regularly retrain and update their core models. We show
that a differential analysis of language model snapshots before and after an
update can reveal a surprising amount of detailed information about changes in
the training data. We propose two new metrics---\emph{differential score} and
\emph{differential rank}---for analyzing the leakage due to updates of natural
language models. We perform leakage analysis using these metrics across models
trained on several different datasets using different methods and
configurations. We discuss the privacy implications of our findings, propose
mitigation strategies and evaluate their effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1"&gt;Santiago Zanella-B&amp;#xe9;guelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1"&gt;Victor R&amp;#xfc;hle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1"&gt;Andrew Paverd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1"&gt;Boris K&amp;#xf6;pf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02798</id>
        <link href="http://arxiv.org/abs/2108.02798"/>
        <updated>2021-08-09T00:49:26.664Z</updated>
        <summary type="html"><![CDATA[Fundus photography is the primary method for retinal imaging and essential
for diabetic retinopathy prevention. Automated segmentation of fundus
photographs would improve the quality, capacity, and cost-effectiveness of eye
care screening programs. However, current segmentation methods are not robust
towards the diversity in imaging conditions and pathologies typical for
real-world clinical applications. To overcome these limitations, we utilized
contrastive self-supervised learning to exploit the large variety of unlabeled
fundus images in the publicly available EyePACS dataset. We pre-trained an
encoder of a U-Net, which we later fine-tuned on several retinal vessel and
lesion segmentation datasets. We demonstrate for the first time that by using
contrastive self-supervised learning, the pre-trained network can recognize
blood vessels, optic disc, fovea, and various lesions without being provided
any labels. Furthermore, when fine-tuned on a downstream blood vessel
segmentation task, such pre-trained networks achieve state-of-the-art
performance on images from different datasets. Additionally, the pre-training
also leads to shorter training times and an improved few-shot performance on
both blood vessel and lesion segmentation tasks. Altogether, our results
showcase the benefits of contrastive self-supervised pre-training which can
play a crucial role in real-world clinical applications requiring robust models
able to adapt to new devices with only a few annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1"&gt;Jan Kuka&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1"&gt;Anja Zenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1"&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1"&gt;Dominik J&amp;#xfc;stel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1"&gt;Vasilis Ntziachristos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection. (arXiv:2108.03070v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03070</id>
        <link href="http://arxiv.org/abs/2108.03070"/>
        <updated>2021-08-09T00:49:26.638Z</updated>
        <summary type="html"><![CDATA[Online sexism has become an increasing concern in social media platforms as
it has affected the healthy development of the Internet and can have negative
effects in society. While research in the sexism detection domain is growing,
most of this research focuses on English as the language and on Twitter as the
platform. Our objective here is to broaden the scope of this research by
considering the Chinese language on Sina Weibo. We propose the first Chinese
sexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a
large Chinese lexicon SexHateLex made of abusive and gender-related terms. We
introduce our data collection and annotation process, and provide an
exploratory analysis of the dataset characteristics to validate its quality and
to show how sexism is manifested in Chinese. The SWSR dataset provides labels
at different levels of granularity including (i) sexism or non-sexism, (ii)
sexism category and (iii) target type, which can be exploited, among others,
for building computational methods to identify and investigate finer-grained
gender-related abusive language. We conduct experiments for the three sexism
classification tasks making use of state-of-the-art machine learning models.
Our results show competitive performance, providing a benchmark for sexism
detection in the Chinese language, as well as an error analysis highlighting
open challenges needing more research in Chinese NLP. The SWSR dataset and
SexHateLex lexicon are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1"&gt;Aiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaohan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1"&gt;Arkaitz Zubiaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual Capsule Network for Hate Speech Detection in Social Media. (arXiv:2108.03089v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03089</id>
        <link href="http://arxiv.org/abs/2108.03089"/>
        <updated>2021-08-09T00:49:26.629Z</updated>
        <summary type="html"><![CDATA[Most hate speech detection research focuses on a single language, generally
English, which limits their generalisability to other languages. In this paper
we investigate the cross-lingual hate speech detection task, tackling the
problem by adapting the hate speech resources from one language to another. We
propose a cross-lingual capsule network learning model coupled with extra
domain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves
state-of-the-art performance on benchmark datasets from AMI@Evalita2018 and
AMI@Ibereval2018 involving three languages: English, Spanish and Italian,
outperforming state-of-the-art baselines on all six language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1"&gt;Aiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1"&gt;Arkaitz Zubiaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.02846</id>
        <link href="http://arxiv.org/abs/2108.02846"/>
        <updated>2021-08-09T00:49:26.621Z</updated>
        <summary type="html"><![CDATA[Human-robot collaboration is an essential research topic in artificial
intelligence (AI), enabling researchers to devise cognitive AI systems and
affords an intuitive means for users to interact with the robot. Of note,
communication plays a central role. To date, prior studies in embodied agent
navigation have only demonstrated that human languages facilitate communication
by instructions in natural languages. Nevertheless, a plethora of other forms
of communication is left unexplored. In fact, human communication originated in
gestures and oftentimes is delivered through multimodal cues, e.g. "go there"
with a pointing gesture. To bridge the gap and fill in the missing dimension of
communication in embodied agent navigation, we propose investigating the
effects of using gestures as the communicative interface instead of verbal
cues. Specifically, we develop a VR-based 3D simulation environment, named
Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human
player is placed in the same virtual scene and shepherds the artificial agent
using only gestures. The agent is tasked to solve the navigation problem guided
by natural gestures with unknown semantics; we do not use any predefined
gestures due to the diversity and versatile nature of human gestures. We argue
that learning the semantics of natural gestures is mutually beneficial to
learning the navigation task--learn to communicate and communicate to learn. In
a series of experiments, we demonstrate that human gesture cues, even without
predefined semantics, improve the object-goal navigation for an embodied agent,
outperforming various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng-Ju Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yixin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Twins Talk & Alternative Calculations. (arXiv:2108.02807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02807</id>
        <link href="http://arxiv.org/abs/2108.02807"/>
        <updated>2021-08-09T00:49:26.609Z</updated>
        <summary type="html"><![CDATA[Inspired by how the human brain employs a higher number of neural pathways
when describing a highly focused subject, we show that deep attentive models
used for the main vision-language task of image captioning, could be extended
to achieve better performance. Image captioning bridges a gap between computer
vision and natural language processing. Automated image captioning is used as a
tool to eliminate the need for human agent for creating descriptive captions
for unseen images.Automated image captioning is challenging and yet
interesting. One reason is that AI based systems capable of generating
sentences that describe an input image could be used in a wide variety of tasks
beyond generating captions for unseen images found on web or uploaded to social
media. For example, in biology and medical sciences, these systems could
provide researchers and physicians with a brief linguistic description of
relevant images, potentially expediting their work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1"&gt;Zanyar Zohourianshahzadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal K. Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence Semantic Matching. (arXiv:2108.02915v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02915</id>
        <link href="http://arxiv.org/abs/2108.02915"/>
        <updated>2021-08-09T00:49:26.493Z</updated>
        <summary type="html"><![CDATA[Sentence semantic matching requires an agent to determine the semantic
relation between two sentences, which is widely used in various natural
language tasks, such as Natural Language Inference (NLI), Paraphrase
Identification (PI), and so on. Much recent progress has been made in this
area, especially attention-based methods and pre-trained language model based
methods. However, most of these methods focus on all the important parts in
sentences in a static way and only emphasize how important the words are to the
query, inhibiting the ability of attention mechanism. In order to overcome this
problem and boost the performance of attention mechanism, we propose a novel
dynamic re-read attention, which can pay close attention to one small region of
sentences at each step and re-read the important parts for better sentence
representations. Based on this attention variation, we develop a novel Dynamic
Re-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting
one small region in dynamic re-read attention seems insufficient for sentence
semantics, and employing pre-trained language models as input encoders will
introduce incomplete and fragile representation problems. To this end, we
extend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in
which local structure of sentences is employed to alleviate the shortcoming of
Byte-Pair Encoding (BPE) in pre-trained language models and boost the
performance of dynamic reread attention. Extensive experiments on two popular
sentence semantic matching tasks demonstrate that DRr-Net can significantly
improve the performance of sentence semantic matching. Meanwhile, LadRa-Net is
able to achieve better performance by considering the local structures of
sentences. In addition, it is exceedingly interesting that some discoveries in
our experiments are consistent with some findings of psychological research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1"&gt;Guangyi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Le Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolution of emotion semantics. (arXiv:2108.02887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02887</id>
        <link href="http://arxiv.org/abs/2108.02887"/>
        <updated>2021-08-09T00:49:26.483Z</updated>
        <summary type="html"><![CDATA[Humans possess the unique ability to communicate emotions through language.
Although concepts like anger or awe are abstract, there is a shared consensus
about what these English emotion words mean. This consensus may give the
impression that their meaning is static, but we propose this is not the case.
We cannot travel back to earlier periods to study emotion concepts directly,
but we can examine text corpora, which have partially preserved the meaning of
emotion words. Using natural language processing of historical text, we found
evidence for semantic change in emotion words over the past century and that
varying rates of change were predicted in part by an emotion concept's
prototypicality - how representative it is of the broader category of
"emotion". Prototypicality negatively correlated with historical rates of
emotion semantic change obtained from text-based word embeddings, beyond more
established variables including usage frequency in English and a second
comparison language, French. This effect for prototypicality did not
consistently extend to the semantic category of birds, suggesting its relevance
for predicting semantic change may be category-dependent. Our results suggest
emotion semantics are evolving over time, with prototypical emotion words
remaining semantically stable, while other emotion words evolve more freely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1"&gt;Aotao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stellar_J/0/1/0/all/0/1"&gt;Jennifer E. Stellar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. (arXiv:2108.02859v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02859</id>
        <link href="http://arxiv.org/abs/2108.02859"/>
        <updated>2021-08-09T00:49:26.458Z</updated>
        <summary type="html"><![CDATA[We analyze the tradeoff between factuality and abstractiveness of summaries.
We introduce abstractiveness constraints to control the degree of
abstractiveness at decoding time, and we apply this technique to characterize
the abstractiveness-factuality tradeoff across multiple widely-studied
datasets, using extensive human evaluations. We train a neural summarization
model on each dataset and visualize the rates of change in factuality as we
gradually increase abstractiveness using our abstractiveness constraints. We
observe that, while factuality generally drops with increased abstractiveness,
different datasets lead to different rates of factuality decay. We propose new
measures to quantify the tradeoff between factuality and abstractiveness, incl.
muQAGS, which balances factuality with abstractiveness. We also quantify this
tradeoff in previous works, aiming to establish baselines for the
abstractiveness-factuality tradeoff that future publications can compare
against.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1"&gt;Markus Dreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1"&gt;Feng Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1"&gt;Sandeep Atluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sujith Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering. (arXiv:2108.02866v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02866</id>
        <link href="http://arxiv.org/abs/2108.02866"/>
        <updated>2021-08-09T00:49:26.438Z</updated>
        <summary type="html"><![CDATA[The current state-of-the-art generative models for open-domain question
answering (ODQA) have focused on generating direct answers from unstructured
textual information. However, a large amount of world's knowledge is stored in
structured databases, and need to be accessed using query languages such as
SQL. Furthermore, query languages can answer questions that require complex
reasoning, as well as offering full explainability. In this paper, we propose a
hybrid framework that takes both textual and tabular evidence as input and
generates either direct answers or SQL queries depending on which form could
better answer the question. The generated SQL queries can then be executed on
the associated databases to obtain the final answers. To the best of our
knowledge, this is the first paper that applies Text2SQL to ODQA tasks.
Empirically, we demonstrate that on several ODQA datasets, the hybrid methods
consistently outperforms the baseline models that only take homogeneous input
by a large margin. Specifically we achieve state-of-the-art performance on
OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate
that the being able to generate structural SQL queries can always bring gains,
especially for those questions that requires complex reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Alexander Hanbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Patrick Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Henghui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiguo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1"&gt;Bing Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03067</id>
        <link href="http://arxiv.org/abs/2108.03067"/>
        <updated>2021-08-09T00:49:26.427Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates a two-stage method for deriving insights from social
media data relating to disinformation by applying a combination of geospatial
classification and embedding-based language modelling across multiple
languages. In particular, the analysis in centered on Twitter and
disinformation for three European languages: English, French and Spanish.
Firstly, Twitter data is classified into European and non-European sets using
BERT. Secondly, Word2vec is applied to the classified texts resulting in
Eurocentric, non-Eurocentric and global representations of the data for the
three target languages. This comparative analysis demonstrates not only the
efficacy of the classification method but also highlights geographic, temporal
and linguistic differences in the disinformation-related media. Thus, the
contributions of the work are threefold: (i) a novel language-independent
transformer-based geolocation method; (ii) an analytical approach that exploits
lexical specificity and word embeddings to interrogate user-generated content;
and (iii) a dataset of 36 million disinformation related tweets in English,
French and Spanish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1"&gt;David Tuxworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1"&gt;Dimosthenis Antypas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tell me a story about yourself: The words of shopping experience and self-satisfaction. (arXiv:2108.03016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03016</id>
        <link href="http://arxiv.org/abs/2108.03016"/>
        <updated>2021-08-09T00:49:26.413Z</updated>
        <summary type="html"><![CDATA[In this paper we investigate the verbal expression of shopping experience
obtained by a sample of customers asked to freely verbalize how they felt when
entering a store. Using novel tools of Text Mining and Social Network Analysis,
we analyzed the interviews to understand the connection between the emotions
aroused during the shopping experience, satisfaction and the way participants
link these concepts to self-satisfaction and self-identity. The results show a
prominent role of emotions in the discourse about the shopping experience
before purchasing and an inward-looking connection to the self. Our results
also suggest that modern retail environment should enhance the hedonic shopping
experience in terms of fun, fantasy, moods, and emotions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petruzzellis_L/0/1/0/all/0/1"&gt;L Petruzzellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Visentin_M/0/1/0/all/0/1"&gt;M Visentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chebat_J/0/1/0/all/0/1"&gt;J.-C. Chebat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02899</id>
        <link href="http://arxiv.org/abs/2108.02899"/>
        <updated>2021-08-09T00:49:26.398Z</updated>
        <summary type="html"><![CDATA[Document digitization is essential for the digital transformation of our
societies, yet a crucial step in the process, Optical Character Recognition
(OCR), is still not perfect. Even commercial OCR systems can produce
questionable output depending on the fidelity of the scanned documents. In this
paper, we demonstrate an effective framework for mitigating OCR errors for any
downstream NLP task, using Named Entity Recognition (NER) as an example. We
first address the data scarcity problem for model training by constructing a
document synthesis pipeline, generating realistic but degraded data with NER
labels. We measure the NER accuracy drop at various degradation levels and show
that a text restoration model, trained on the degraded data, significantly
closes the NER accuracy gaps caused by OCR errors, including on an
out-of-domain dataset. For the benefit of the community, we have made the
document synthesis pipeline available as an open-source project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1"&gt;Amit Gupte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1"&gt;Alexey Romanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1"&gt;Sahitya Mantravadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1"&gt;Dalitso Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1"&gt;Raza Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1"&gt;Lakshmanan Ramu Meenal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Benjamin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Soundar Srinivasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02923</id>
        <link href="http://arxiv.org/abs/2108.02923"/>
        <updated>2021-08-09T00:49:26.382Z</updated>
        <summary type="html"><![CDATA[Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuxi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yuchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiameng Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02941</id>
        <link href="http://arxiv.org/abs/2108.02941"/>
        <updated>2021-08-09T00:49:26.371Z</updated>
        <summary type="html"><![CDATA[Disinformation through fake news is an ongoing problem in our society and has
become easily spread through social media. The most cost and time effective way
to filter these large amounts of data is to use a combination of human and
technical interventions to identify it. From a technical perspective, Natural
Language Processing (NLP) is widely used in detecting fake news. Social media
companies use NLP techniques to identify the fake news and warn their users,
but fake news may still slip through undetected. It is especially a problem in
more localised contexts (outside the United States of America). How do we
adjust fake news detection systems to work better for local contexts such as in
South Africa. In this work we investigate fake news detection on South African
websites. We curate a dataset of South African fake news and then train
detection models. We contrast this with using widely available fake news
datasets (from mostly USA website). We also explore making the datasets more
diverse by combining them and observe the differences in behaviour in writing
between nations' fake news using interpretable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1"&gt;Harm de Wet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence Semantic Regression for Text Generation. (arXiv:2108.02984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02984</id>
        <link href="http://arxiv.org/abs/2108.02984"/>
        <updated>2021-08-09T00:49:26.323Z</updated>
        <summary type="html"><![CDATA[Recall the classical text generation works, the generation framework can be
briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface
realization}. The target of idea reasoning is to figure out the main idea which
will be presented in the following talking/writing periods. Surface realization
aims to arrange the most appropriate sentence to depict and convey the
information distilled from the main idea. However, the current popular
token-by-token text generation methods ignore this crucial process and suffer
from many serious issues, such as idea/topic drift. To tackle the problems and
realize this two-phase paradigm, we propose a new framework named Sentence
Semantic Regression (\textbf{SSR}) based on sentence-level language modeling.
For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR}
are designed to conduct sentence semantic regression autoregressively (like
GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a
mixed-granularity sentence decoder is designed to generate text with better
consistency by jointly incorporating the predicted sentence-level main idea as
well as the preceding contextual token-level information. We conduct
experiments on four tasks of story ending prediction, story ending generation,
dialogue generation, and sentence infilling. The results show that SSR can
obtain better performance in terms of automatic metrics and human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hai-Tao Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02854</id>
        <link href="http://arxiv.org/abs/2108.02854"/>
        <updated>2021-08-09T00:49:26.127Z</updated>
        <summary type="html"><![CDATA[Languages differ in terms of the absence or presence of gender features, the
number of gender classes and whether and where gender features are explicitly
marked. These cross-linguistic differences can lead to ambiguities that are
difficult to resolve, especially for sentence-level MT systems. The
identification of ambiguity and its subsequent resolution is a challenging task
for which currently there aren't any specific resources or challenge sets
available. In this paper, we introduce gENder-IT, an English--Italian challenge
set focusing on the resolution of natural gender phenomena by providing
word-level gender tags on the English source side and multiple gender
alternative translations, where needed, on the Italian target side.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1"&gt;Eva Vanmassenhove&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1"&gt;Johanna Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02830</id>
        <link href="http://arxiv.org/abs/2108.02830"/>
        <updated>2021-08-09T00:49:26.085Z</updated>
        <summary type="html"><![CDATA[Hate speech is a specific type of controversial content that is widely
legislated as a crime that must be identified and blocked. However, due to the
sheer volume and velocity of the Twitter data stream, hate speech detection
cannot be performed manually. To address this issue, several studies have been
conducted for hate speech detection in European languages, whereas little
attention has been paid to low-resource South Asian languages, making the
social media vulnerable for millions of users. In particular, to the best of
our knowledge, no study has been conducted for hate speech detection in Roman
Urdu text, which is widely used in the sub-continent. In this study, we have
scrapped more than 90,000 tweets and manually parsed them to identify 5,000
Roman Urdu tweets. Subsequently, we have employed an iterative approach to
develop guidelines and used them for generating the Hate Speech Roman Urdu 2020
corpus. The tweets in the this corpus are classified at three levels:
Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another
contribution, we have used five supervised learning techniques, including a
deep learning technique, to evaluate and compare their effectiveness for hate
speech detection. The results show that Logistic Regression outperformed all
other techniques, including deep learning techniques for the two levels of
classification, by achieved an F1 score of 0.906 for distinguishing between
Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate
speech tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Moin Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1"&gt;Khurram Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kamran Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02953</id>
        <link href="http://arxiv.org/abs/2108.02953"/>
        <updated>2021-08-09T00:49:26.032Z</updated>
        <summary type="html"><![CDATA[This paper investigates a valuable setting called few-shot unsupervised
domain adaptation (FS-UDA), which has not been sufficiently studied in the
literature. In this setting, the source domain data are labelled, but with
few-shot per category, while the target domain data are unlabelled. To address
the FS-UDA setting, we develop a general UDA model to solve the following two
key issues: the few-shot labeled data per category and the domain adaptation
between support and query sets. Our model is general in that once trained it
will be able to be applied to various FS-UDA tasks from the same source and
target domains. Inspired by the recent local descriptor based few-shot learning
(FSL), our general UDA model is fully built upon local descriptors (LDs) for
image classification and domain adaptation. By proposing a novel concept called
similarity patterns (SPs), our model not only effectively considers the spatial
relationship of LDs that was ignored in previous FSL methods, but also makes
the learned image similarity better serve the required domain alignment.
Specifically, we propose a novel IMage-to-class sparse Similarity Encoding
(IMSE) method. It learns SPs to extract the local discriminative information
for classification and meanwhile aligns the covariance matrix of the SPs for
domain adaptation. Also, domain adversarial training and multi-scale local
feature matching are performed upon LDs. Extensive experiments conducted on a
multi-domain benchmark dataset DomainNet demonstrates the state-of-the-art
performance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,
our IMSE can also show better performance than most of recent FSL methods on
miniImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wanqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14230</id>
        <link href="http://arxiv.org/abs/2107.14230"/>
        <updated>2021-08-06T01:58:38.210Z</updated>
        <summary type="html"><![CDATA[Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the validation set of a popular but noisy
real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment
and future research. Our code and data are available at
\url{https://shuquanye.com/PNAL_website/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shuquan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Songfang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08365</id>
        <link href="http://arxiv.org/abs/2106.08365"/>
        <updated>2021-08-06T00:51:48.178Z</updated>
        <summary type="html"><![CDATA[Piecewise linear neural networks can be split into subfunctions, each with
its own activation pattern, domain, and empirical error. Empirical error for
the full network can be written as an expectation over empirical error of
subfunctions. Constructing a generalization bound on subfunction empirical
error indicates that the more densely a subfunction is surrounded by training
samples in representation space, the more reliable its predictions are.
Further, it suggests that models with fewer activation regions generalize
better, and models that abstract knowledge to a greater degree generalize
better, all else equal. We propose not only a theoretical framework to reason
about subfunction error bounds but also a pragmatic way of approximately
evaluating it, which we apply to predicting which samples the network will not
successfully generalize to. We test our method on detection of
misclassification and out-of-distribution samples, finding that it performs
competitively in both cases. In short, some network activation patterns are
associated with higher reliability than others, and these can be identified
using subfunction error bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xu Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1"&gt;Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy Logic based Logical Query Answering on Knowledge Graph. (arXiv:2108.02390v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02390</id>
        <link href="http://arxiv.org/abs/2108.02390"/>
        <updated>2021-08-06T00:51:48.138Z</updated>
        <summary type="html"><![CDATA[Answering complex First-Order Logical (FOL) queries on large-scale incomplete
knowledge graphs (KGs) is an important yet challenging task. Recent advances
embed logical queries and KG entities in the vector space and conduct query
answering via dense similarity search. However, most of the designed logical
operators in existing works do not satisfy the axiomatic system of classical
logic. Moreover, these logical operators are parameterized so that they require
a large number of complex FOL queries as training data, which are often arduous
or even inaccessible to collect in most real-world KGs. In this paper, we
present FuzzQE, a fuzzy logic based query embedding framework for answering FOL
queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a
principled and learning free manner. Extensive experiments on two benchmark
datasets demonstrate that FuzzQE achieves significantly better performance in
answering FOL queries compared to the state-of-the-art methods. In addition,
FuzzQE trained with only KG link prediction without any complex queries can
achieve comparable performance with the systems trained with all FOL queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuelu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Ziniu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02233</id>
        <link href="http://arxiv.org/abs/2108.02233"/>
        <updated>2021-08-06T00:51:47.947Z</updated>
        <summary type="html"><![CDATA[Lung nodules are commonly missed in chest radiographs. We propose and
evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules
in radiographs. P-AnoGAN modifies the fast anomaly detection generative
adversarial network (f-AnoGAN) by utilizing a progressive GAN and a
convolutional encoder-decoder-encoder pipeline. Model training uses only
unlabelled healthy lung patches extracted from the Indiana University Chest
X-Ray Collection. External validation and testing are performed using healthy
and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for
Radiological Technology datasets, respectively. Our model robustly identifies
patches containing lung nodules in external validation and test data with
ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised
methods may be useful in challenging tasks such as lung nodule detection in
radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nitish Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1"&gt;David Ramon Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1"&gt;Nedim Hodzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1"&gt;Christos Karanassios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02743</id>
        <link href="http://arxiv.org/abs/2108.02743"/>
        <updated>2021-08-06T00:51:47.898Z</updated>
        <summary type="html"><![CDATA[Recent developments in fluorescence microscopy allow capturing
high-resolution 3D images over time for living model organisms. To be able to
image even large specimens, techniques like multi-view light-sheet imaging
record different orientations at each time point that can then be fused into a
single high-quality volume. Based on measured point spread functions (PSF),
deconvolution and content fusion are able to largely revert the inevitable
degradation occurring during the imaging process. Classical multi-view
deconvolution and fusion methods mainly use iterative procedures and
content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been
deployed to approach 3D single-view deconvolution microscopy, but the
multi-view case waits to be studied. We investigated the efficacy of CNN-based
multi-view deconvolution and fusion with two synthetic data sets that mimic
developing embryos and involve either two or four complementary 3D views.
Compared with classical state-of-the-art methods, the proposed semi- and
self-supervised models achieve competitive and superior deconvolution and
fusion quality in the two-view and quad-view cases, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Canyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1"&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1"&gt;Johannes Stegmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Self-Supervised Learning: A Survey. (arXiv:2103.00111v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00111</id>
        <link href="http://arxiv.org/abs/2103.00111"/>
        <updated>2021-08-06T00:51:47.887Z</updated>
        <summary type="html"><![CDATA[Deep learning on graphs has attracted significant interests recently.
However, most of the works have focused on (semi-) supervised learning,
resulting in shortcomings including heavy label reliance, poor generalization,
and weak robustness. To address these issues, self-supervised learning (SSL),
which extracts informative knowledge through well-designed pretext tasks
without relying on manual labels, has become a promising and trending learning
paradigm for graph data. Different from SSL on other domains like computer
vision and natural language processing, SSL on graphs has an exclusive
background, design ideas, and taxonomies. Under the umbrella of graph
self-supervised learning, we present a timely and comprehensive review of the
existing approaches which employ SSL techniques for graph data. We construct a
unified framework that mathematically formalizes the paradigm of graph SSL.
According to the objectives of pretext tasks, we divide these approaches into
four categories: generation-based, auxiliary property-based, contrast-based,
and hybrid approaches. We further conclude the applications of graph SSL across
various research fields and summarize the commonly used datasets, evaluation
benchmark, performance comparison and open-source codes of graph SSL. Finally,
we discuss the remaining challenges and potential future directions in this
research field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Ming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02774</id>
        <link href="http://arxiv.org/abs/2108.02774"/>
        <updated>2021-08-06T00:51:47.880Z</updated>
        <summary type="html"><![CDATA[Can a user create a deep generative model by sketching a single example?
Traditionally, creating a GAN model has required the collection of a
large-scale dataset of exemplars and specialized knowledge in deep learning. In
contrast, sketching is possibly the most universally accessible way to convey a
visual concept. In this work, we present a method, GAN Sketching, for rewriting
GANs with one or more sketches, to make GANs training easier for novice users.
In particular, we change the weights of an original GAN model according to user
sketches. We encourage the model's output to match the user sketches through a
cross-domain adversarial loss. Furthermore, we explore different regularization
methods to preserve the original model's diversity and image quality.
Experiments have shown that our method can mold GANs to match shapes and poses
specified by sketches while maintaining realism and diversity. Finally, we
demonstrate a few applications of the resulting GAN, including latent space
interpolation and image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng-Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1"&gt;David Bau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02576</id>
        <link href="http://arxiv.org/abs/2108.02576"/>
        <updated>2021-08-06T00:51:47.874Z</updated>
        <summary type="html"><![CDATA[Music Performers have their own idiosyncratic way of interpreting a musical
piece. A group of skilled performers playing the same piece of music would
likely to inject their unique artistic styles in their performances. The
variations of the tempo, timing, dynamics, articulation etc. from the actual
notated music are what make the performers unique in their performances. This
study presents a dataset consisting of four movements of Schubert's ``Sonata in
B-flat major, D.960" performed by nine virtuoso pianists individually. We
proposed and extracted a set of expressive features that are able to capture
the characteristics of an individual performer's style. We then present a
performer identification method based on the similarity of feature
distribution, given a set of piano performances. The identification is done
considering each feature individually as well as a fusion of the features.
Results show that the proposed method achieved a precision of 0.903 using
fusion features. Moreover, the onset time deviation feature shows promising
result when considered individually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1"&gt;Syed Rifat Mahmud Rafee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gyorgy Fazekas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1"&gt;Geraint A.~Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MER-SDN: Machine Learning Framework for Traffic Aware Energy Efficient Routing in SDN. (arXiv:1909.08074v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08074</id>
        <link href="http://arxiv.org/abs/1909.08074"/>
        <updated>2021-08-06T00:51:47.852Z</updated>
        <summary type="html"><![CDATA[Software Defined Networking (SDN) achieves programmability of a network
through separation of the control and data planes. It enables flexibility in
network management and control. Energy efficiency is one of the challenging
global problems which has both economic and environmental impact. A massive
amount of information is generated in the controller of an SDN based network.
Machine learning gives the ability to computers to progressively learn from
data without having to write specific instructions. In this work, we propose
MER-SDN: a machine learning framework for traffic-aware energy efficient
routing in SDN. Feature extraction, training, and testing are the three main
stages of the learning machine. Experiments are conducted on Mininet and POX
controller using real-world network topology and dynamic traffic traces from
SNDlib. Results show that our approach achieves more than 65\% feature size
reduction, more than 70% accuracy in parameter prediction of an energy
efficient heuristics algorithm, also our prediction refine heuristics converges
the predicted value to the optimal parameters values with up to 25X speedup as
compared to the brute force method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assefa_B/0/1/0/all/0/1"&gt;Beakal Gizachew Assefa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozkasap_O/0/1/0/all/0/1"&gt;Oznur Ozkasap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introduction to Normalizing Flows for Lattice Field Theory. (arXiv:2101.08176v2 [hep-lat] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08176</id>
        <link href="http://arxiv.org/abs/2101.08176"/>
        <updated>2021-08-06T00:51:47.845Z</updated>
        <summary type="html"><![CDATA[This notebook tutorial demonstrates a method for sampling Boltzmann
distributions of lattice field theories using a class of machine learning
models known as normalizing flows. The ideas and approaches proposed in
arXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a
concrete implementation of the framework is presented. We apply this framework
to a lattice scalar field theory and to U(1) gauge theory, explicitly encoding
gauge symmetries in the flow-based approach to the latter. This presentation is
intended to be interactive and working with the attached Jupyter notebook is
recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1"&gt;Michael S. Albergo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1"&gt;Denis Boyda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1"&gt;Daniel C. Hackett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1"&gt;Gurtej Kanwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1"&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Racani&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1"&gt;Danilo Jimenez Rezende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1"&gt;Phiala E. Shanahan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning. (arXiv:2108.02755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02755</id>
        <link href="http://arxiv.org/abs/2108.02755"/>
        <updated>2021-08-06T00:51:47.833Z</updated>
        <summary type="html"><![CDATA[AI and reinforcement learning (RL) have improved many areas, but are not yet
widely adopted in economic policy design, mechanism design, or economics at
large. At the same time, current economic methodology is limited by a lack of
counterfactual data, simplistic behavioral models, and limited opportunities to
experiment with policies and evaluate behavioral responses. Here we show that
machine-learning-based economic simulation is a powerful policy and mechanism
design framework to overcome these limitations. The AI Economist is a
two-level, deep RL framework that trains both agents and a social planner who
co-adapt, providing a tractable solution to the highly unstable and novel
two-level RL challenge. From a simple specification of an economy, we learn
rational agent behaviors that adapt to learned planner policies and vice versa.
We demonstrate the efficacy of the AI Economist on the problem of optimal
taxation. In simple one-step economies, the AI Economist recovers the optimal
tax policy of economic theory. In complex, dynamic economies, the AI Economist
substantially improves both utilitarian social welfare and the trade-off
between equality and productivity over baselines. It does so despite emergent
tax-gaming strategies, while accounting for agent interactions and behavioral
change more accurately than economic theory. These results demonstrate for the
first time that two-level, deep RL can be used for understanding and as a
complement to theory for economic design, unlocking a new computational
learning-based approach to understanding economic policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Stephan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1"&gt;Alexander Trott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1"&gt;David C. Parkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1"&gt;Richard Socher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00116</id>
        <link href="http://arxiv.org/abs/2107.00116"/>
        <updated>2021-08-06T00:51:47.826Z</updated>
        <summary type="html"><![CDATA[We explore methodologies to improve the robustness of generative adversarial
imitation learning (GAIL) algorithms to observation noise. Towards this
objective, we study the effect of local Lipschitzness of the discriminator and
the generator on the robustness of policies learned by GAIL. In many robotics
applications, the learned policies by GAIL typically suffer from a degraded
performance at test time since the observations from the environment might be
corrupted by noise. Hence, robustifying the learned policies against the
observation noise is of critical importance. To this end, we propose a
regularization method to induce local Lipschitzness in the generator and the
discriminator of adversarial imitation learning methods. We show that the
modified objective leads to learning significantly more robust policies.
Moreover, we demonstrate --- both theoretically and experimentally --- that
training a locally Lipschitz discriminator leads to a locally Lipschitz
generator, thereby improving the robustness of the resultant policy. We perform
extensive experiments on simulated robot locomotion environments from the
MuJoCo suite that demonstrate the proposed method learns policies that
significantly outperform the state-of-the-art generative adversarial imitation
learning algorithm when applied to test scenarios with noise-corrupted
observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1"&gt;Farzan Memarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection. (arXiv:2108.02501v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02501</id>
        <link href="http://arxiv.org/abs/2108.02501"/>
        <updated>2021-08-06T00:51:47.820Z</updated>
        <summary type="html"><![CDATA[For the highly imbalanced credit card fraud detection problem, most existing
methods either use data augmentation methods or conventional machine learning
models, while neural network-based anomaly detection approaches are lacking.
Furthermore, few studies have employed AI interpretability tools to investigate
the feature importance of transaction data, which is crucial for the black-box
fraud detection module. Considering these two points together, we propose a
novel anomaly detection framework for credit card fraud detection as well as a
model-explaining module responsible for prediction explanations. The fraud
detection model is composed of two deep neural networks, which are trained in
an unsupervised and adversarial manner. Precisely, the generator is an
AutoEncoder aiming to reconstruct genuine transaction data, while the
discriminator is a fully-connected network for fraud detection. The explanation
module has three white-box explainers in charge of interpretations of the
AutoEncoder, discriminator, and the whole detection model, respectively.
Experimental results show the state-of-the-art performances of our fraud
detection model on the benchmark dataset compared with baselines. In addition,
prediction analyses by three explainers are presented, offering a clear
perspective on how each feature of an instance of interest contributes to the
final model output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tungyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Youting Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare. (arXiv:2108.02428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02428</id>
        <link href="http://arxiv.org/abs/2108.02428"/>
        <updated>2021-08-06T00:51:47.801Z</updated>
        <summary type="html"><![CDATA[The study presents a new method for analyzing medical data based on the
LogNNet neural network, which uses chaotic mappings to transform input
information. The technique calculates risk factors for the presence of a
disease in a patient according to a set of medical health indicators. The
LogNNet architecture allows the implementation of artificial intelligence on
medical pe-ripherals of the Internet of Things with low RAM resources, and the
development of edge computing in healthcare. The efficiency of LogNNet in
assessing perinatal risk is illustrated on cardiotocogram data of 2126 pregnant
women, obtained from the UC Irvine machine learning repository. The
classification accuracy reaches ~ 91%, with the ~ 3-10 kB of RAM used on the
Arduino microcontroller. In addition, examples for diagnosing COVID-19 are
provided, using LogNNet trained on a publicly available database from the
Israeli Ministry of Health. The service concept has been developed, which uses
the data of the express test for COVID-19 and reaches the classification
accuracy of ~ 95% with the ~ 0.6 kB of RAM used on Arduino microcontrollers. In
all examples, the model is tested using standard classification quality
metrics: Precision, Recall, and F1-measure. The study results can be used in
clinical decision support systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1"&gt;Andrei Velichko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Communication via Mixed Distributions. (arXiv:2108.02658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02658</id>
        <link href="http://arxiv.org/abs/2108.02658"/>
        <updated>2021-08-06T00:51:47.788Z</updated>
        <summary type="html"><![CDATA[Neural networks and other machine learning models compute continuous
representations, while humans communicate mostly through discrete symbols.
Reconciling these two forms of communication is desirable for generating
human-readable interpretations or learning discrete latent variable models,
while maintaining end-to-end differentiability. Some existing approaches (such
as the Gumbel-Softmax transformation) build continuous relaxations that are
discrete approximations in the zero-temperature limit, while others (such as
sparsemax transformations and the Hard Concrete distribution) produce
discrete/continuous hybrids. In this paper, we build rigorous theoretical
foundations for these hybrids, which we call "mixed random variables." Our
starting point is a new "direct sum" base measure defined on the face lattice
of the probability simplex. From this measure, we introduce new entropy and
Kullback-Leibler divergence functions that subsume the discrete and
differential cases and have interpretations in terms of code optimality. Our
framework suggests two strategies for representing and sampling mixed random
variables, an extrinsic ("sample-and-project") and an intrinsic one (based on
face stratification). We experiment with both approaches on an emergent
communication benchmark and on modeling MNIST and Fashion-MNIST data with
variational auto-encoders with mixed latent variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Farinhas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1"&gt;Wilker Aziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spotify Danceability and Popularity Analysis using SAP. (arXiv:2108.02370v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.02370</id>
        <link href="http://arxiv.org/abs/2108.02370"/>
        <updated>2021-08-06T00:51:47.749Z</updated>
        <summary type="html"><![CDATA[Our analysis reviews and visualizes the audio features and popularity of
songs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally
sourced from Spotify API, consists of multiple Excel files containing
information relevant to our visualization and regression analysis. The exercise
seeks to determine the connection between the popularity of the songs and the
danceability. Insights to be included and factored as part of our analysis
include song energy, valence, BPM, release date, and year.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochi_V/0/1/0/all/0/1"&gt;Virginia Ochi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1"&gt;Ricardo Estrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaji_T/0/1/0/all/0/1"&gt;Teezal Gaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadea_W/0/1/0/all/0/1"&gt;Wendy Gadea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_E/0/1/0/all/0/1"&gt;Emily Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-08-06T00:51:47.743Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication. (arXiv:2102.01583v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01583</id>
        <link href="http://arxiv.org/abs/2102.01583"/>
        <updated>2021-08-06T00:51:47.735Z</updated>
        <summary type="html"><![CDATA[We resolve the min-max complexity of distributed stochastic convex
optimization (up to a log factor) in the intermittent communication setting,
where $M$ machines work in parallel over the course of $R$ rounds of
communication to optimize the objective, and during each round of
communication, each machine may sequentially compute $K$ stochastic gradient
estimates. We present a novel lower bound with a matching upper bound that
establishes an optimal algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bullins_B/0/1/0/all/0/1"&gt;Brian Bullins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06610</id>
        <link href="http://arxiv.org/abs/2010.06610"/>
        <updated>2021-08-06T00:51:47.717Z</updated>
        <summary type="html"><![CDATA[Recent approaches to efficiently ensemble neural networks have shown that
strong robustness and uncertainty performance can be achieved with a negligible
gain in parameters over the original network. However, these methods still
require multiple forward passes for prediction, leading to a significant
computational cost. In this work, we show a surprising result: the benefits of
using multiple predictions can be achieved `for free' under a single model's
forward pass. In particular, we show that, using a multi-input multi-output
(MIMO) configuration, one can utilize a single model's capacity to train
multiple subnetworks that independently learn the task at hand. By ensembling
the predictions made by the subnetworks, we improve model robustness without
increasing compute. We observe a significant improvement in negative
log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,
and their out-of-distribution variants compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human strategic decision making in parametrized games. (arXiv:2104.14744v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14744</id>
        <link href="http://arxiv.org/abs/2104.14744"/>
        <updated>2021-08-06T00:51:47.711Z</updated>
        <summary type="html"><![CDATA[Many real-world games contain parameters which can affect payoffs, action
spaces, and information states. For fixed values of the parameters, the game
can be solved using standard algorithms. However, in many settings agents must
act without knowing the values of the parameters that will be encountered in
advance. Often the decisions must be made by a human under time and resource
constraints, and it is unrealistic to assume that a human can solve the game in
real time. We present a new framework that enables human decision makers to
make fast decisions without the aid of real-time solvers. We demonstrate
applicability to a variety of situations including settings with multiple
players and imperfect information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1"&gt;Sam Ganzfried&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02574</id>
        <link href="http://arxiv.org/abs/2108.02574"/>
        <updated>2021-08-06T00:51:47.704Z</updated>
        <summary type="html"><![CDATA[Recently, much progress has been made in unsupervised restoration learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised restoration learning without
any prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
restoration learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we show that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zeyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rendong Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peilin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02235</id>
        <link href="http://arxiv.org/abs/2108.02235"/>
        <updated>2021-08-06T00:51:47.697Z</updated>
        <summary type="html"><![CDATA[Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods based on meta-learning have achieved promising performance, such as
Meta R-CNN series. However, only a single category of support data is used as
the attention to guide the detecting of query images each time. Their relevance
to each other remains unexploited. Moreover, a lot of recent works treat the
support data and query images as independent branch without considering the
relationship between them. To address this issue, we propose a dynamic
relevance learning model, which utilizes the relationship between all support
images and Region of Interest (RoI) on the query images to construct a dynamic
graph convolutional network (GCN). By adjusting the prediction distribution of
the base detector using the output of this GCN, the proposed model can guide
the detector to improve the class representation implicitly. Comprehensive
experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed
model achieves the best overall performance, which shows its effectiveness of
learning more generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1"&gt;Chong Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haohe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shenghao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Song Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xulun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiafei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02644</id>
        <link href="http://arxiv.org/abs/2108.02644"/>
        <updated>2021-08-06T00:51:47.691Z</updated>
        <summary type="html"><![CDATA[Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arijit Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VC-dimensions of nondeterministic finite automata for words of equal length. (arXiv:2001.02309v2 [cs.FL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02309</id>
        <link href="http://arxiv.org/abs/2001.02309"/>
        <updated>2021-08-06T00:51:47.679Z</updated>
        <summary type="html"><![CDATA[Let $NFA_b(q)$ denote the set of languages accepted by nondeterministic
finite automata with $q$ states over an alphabet with $b$ letters. Let $B_n$
denote the set of words of length $n$. We give a quadratic lower bound on the
VC dimension of \[

NFA_2(q)\cap B_n = \{L\cap B_n \mid L \in NFA_2(q)\} \] as a function of $q$.

Next, the work of Gruber and Holzer (2007) gives an upper bound for the
nondeterministic state complexity of finite languages contained in $B_n$, which
we strengthen using our methods.

Finally, we give some theoretical and experimental results on the dependence
on $n$ of the VC dimension and testing dimension of $NFA_2(q)\cap B_n$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kjos_Hanssen_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rn Kjos-Hanssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felix_C/0/1/0/all/0/1"&gt;Clyde James Felix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sun Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_E/0/1/0/all/0/1"&gt;Ethan Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_D/0/1/0/all/0/1"&gt;Davin Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02602</id>
        <link href="http://arxiv.org/abs/2108.02602"/>
        <updated>2021-08-06T00:51:47.662Z</updated>
        <summary type="html"><![CDATA[It is common to have to process signals or images whose values are cyclic and
can be represented as points on the complex circle, like wrapped phases,
angles, orientations, or color hues. We consider a Tikhonov-type regularization
model to smoothen or interpolate circle-valued signals defined on arbitrary
graphs. We propose a convex relaxation of this nonconvex problem as a
semidefinite program, and an efficient algorithm to solve it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1"&gt;Laurent Condat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:47.654Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02704</id>
        <link href="http://arxiv.org/abs/2108.02704"/>
        <updated>2021-08-06T00:51:47.648Z</updated>
        <summary type="html"><![CDATA[Regularization in convolutional neural networks (CNNs) is usually addressed
with dropout layers. However, dropout is sometimes detrimental in the
convolutional part of a CNN as it simply sets to zero a percentage of pixels in
the feature maps, adding unrepresentative examples during training. Here, we
propose a CNN layer that performs regularization by applying random rotations
of reflections to a small percentage of feature maps after every convolutional
layer. We prove how this concept is beneficial for images with orientational
symmetries, such as in medical images, as it provides a certain degree of
rotational invariance. We tested this method in two datasets, a patch-based set
of histopathology images (PatchCamelyon) to perform classification using a
generic DenseNet, and a set of specular microscopy images of the corneal
endothelium to perform segmentation using a tailored U-net, improving the
performance in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13611</id>
        <link href="http://arxiv.org/abs/2011.13611"/>
        <updated>2021-08-06T00:51:47.641Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation has been revolutionized with GAN-based methods.
However, existing methods lack the ability to preserve the identity of the
source domain. As a result, synthesized images can often over-adapt to the
reference domain, losing important structural characteristics and suffering
from suboptimal visual quality. To solve these challenges, we propose a novel
frequency domain image translation (FDIT) framework, exploiting frequency
information for enhancing the image generation process. Our key idea is to
decompose the image into low-frequency and high-frequency components, where the
high-frequency feature captures object structure akin to the identity. Our
training objective facilitates the preservation of frequency information in
both pixel space and Fourier spectral space. We broadly evaluate FDIT across
five large-scale datasets and multiple tasks including image translation and
GAN inversion. Extensive experiments and ablations show that FDIT effectively
preserves the identity of the source image, and produces photo-realistic
images. FDIT establishes state-of-the-art performance, reducing the average FID
score by 5.6% compared to the previous best method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1"&gt;Mu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huijuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_Q/0/1/0/all/0/1"&gt;Qichuan Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02671</id>
        <link href="http://arxiv.org/abs/2108.02671"/>
        <updated>2021-08-06T00:51:47.635Z</updated>
        <summary type="html"><![CDATA[Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1"&gt;Julia Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1"&gt;Lazaros Nalpantidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking machine learning models on multi-centre eICU critical care dataset. (arXiv:1910.00964v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00964</id>
        <link href="http://arxiv.org/abs/1910.00964"/>
        <updated>2021-08-06T00:51:47.616Z</updated>
        <summary type="html"><![CDATA[Progress of machine learning in critical care has been difficult to track, in
part due to absence of public benchmarks. Other fields of research (such as
computer vision and natural language processing) have established various
competitions and public benchmarks. Recent availability of large clinical
datasets has enabled the possibility of establishing public benchmarks. Taking
advantage of this opportunity, we propose a public benchmark suite to address
four areas of critical care, namely mortality prediction, estimation of length
of stay, patient phenotyping and risk of decompensation. We define each task
and compare the performance of both clinical models as well as baseline and
deep learning models using eICU critical care dataset of around 73,000
patients. This is the first public benchmark on a multi-centre critical care
dataset, comparing the performance of clinical gold standard with our
predictive model. We also investigate the impact of numerical variables as well
as handling of categorical variables on each of the defined tasks. The source
code, detailing our methods and experiments is publicly available such that
anyone can replicate our results and build upon our work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheikhalishahi_S/0/1/0/all/0/1"&gt;Seyedmostafa Sheikhalishahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaraman_V/0/1/0/all/0/1"&gt;Vevake Balaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osmani_V/0/1/0/all/0/1"&gt;Venet Osmani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Unintended Bias of ML Models on Tabular and Textual Data. (arXiv:2108.02662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02662</id>
        <link href="http://arxiv.org/abs/2108.02662"/>
        <updated>2021-08-06T00:51:47.605Z</updated>
        <summary type="html"><![CDATA[Unintended biases in machine learning (ML) models are among the major
concerns that must be addressed to maintain public trust in ML. In this paper,
we address process fairness of ML models that consists in reducing the
dependence of models on sensitive features, without compromising their
performance. We revisit the framework FixOut that is inspired in the approach
"fairness through unawareness" to build fairer models. We introduce several
improvements such as automating the choice of FixOut's parameters. Also, FixOut
was originally proposed to improve fairness of ML models on tabular data. We
also demonstrate the feasibility of FixOut's workflow for models on textual
data. We present several experimental results that illustrate the fact that
FixOut improves process fairness on different classification settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alves_G/0/1/0/all/0/1"&gt;Guilherme Alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1"&gt;Maxime Amblard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernier_F/0/1/0/all/0/1"&gt;Fabien Bernier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1"&gt;Miguel Couceiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napoli_A/0/1/0/all/0/1"&gt;Amedeo Napoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition. (arXiv:2108.02241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02241</id>
        <link href="http://arxiv.org/abs/2108.02241"/>
        <updated>2021-08-06T00:51:47.598Z</updated>
        <summary type="html"><![CDATA[Classification of human emotions can play an essential role in the design and
improvement of human-machine systems. While individual biological signals such
as Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely
used for emotion recognition with machine learning methods, multimodal
approaches generally fuse extracted features or final classification/regression
results to boost performance. To enhance multimodal learning, we present a
novel attentive cross-modal connection to share information between
convolutional neural networks responsible for learning individual modalities.
Specifically, these connections improve emotion classification by sharing
intermediate representations among EDA and ECG and apply attention weights to
the shared information, thus learning more effective multimodal embeddings. We
perform experiments on the WESAD dataset to identify the best configuration of
the proposed method for emotion classification. Our experiments show that the
proposed approach is capable of learning strong multimodal representations and
outperforms a number of baselines methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1"&gt;Anubhav Bhatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behinaein_B/0/1/0/all/0/1"&gt;Behnam Behinaein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodenburg_D/0/1/0/all/0/1"&gt;Dirk Rodenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hungler_P/0/1/0/all/0/1"&gt;Paul Hungler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Addressing Heterogeneity in Federated Learning for Autonomous Vehicles Connected to a Drone Orchestrator. (arXiv:2108.02712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02712</id>
        <link href="http://arxiv.org/abs/2108.02712"/>
        <updated>2021-08-06T00:51:47.589Z</updated>
        <summary type="html"><![CDATA[In this paper we envision a federated learning (FL) scenario in service of
amending the performance of autonomous road vehicles, through a drone traffic
monitor (DTM), that also acts as an orchestrator. Expecting non-IID data
distribution, we focus on the issue of accelerating the learning of a
particular class of critical object (CO), that may harm the nominal operation
of an autonomous vehicle. This can be done through proper allocation of the
wireless resources for addressing learner and data heterogeneity. Thus, we
propose a reactive method for the allocation of wireless resources, that
happens dynamically each FL round, and is based on each learner's contribution
to the general model. In addition to this, we explore the use of static methods
that remain constant across all rounds. Since we expect partial work from each
learner, we use the FedProx FL algorithm, in the task of computer vision. For
testing, we construct a non-IID data distribution of the MNIST and FMNIST
datasets among four types of learners, in scenarios that represent the quickly
changing environment. The results show that proactive measures are effective
and versatile at improving system accuracy, and quickly learning the CO class
when underrepresented in the network. Furthermore, the experiments show a
tradeoff between FedProx intensity and resource allocation efforts.
Nonetheless, a well adjusted FedProx local optimizer allows for an even better
overall accuracy, particularly when using deeper neural network (NN)
implementations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donevski_I/0/1/0/all/0/1"&gt;Igor Donevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_J/0/1/0/all/0/1"&gt;Jimmy Jessen Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1"&gt;Petar Popovski&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. (arXiv:2108.02717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02717</id>
        <link href="http://arxiv.org/abs/2108.02717"/>
        <updated>2021-08-06T00:51:47.581Z</updated>
        <summary type="html"><![CDATA[The theory of reinforcement learning has focused on two fundamental problems:
achieving low regret, and identifying $\epsilon$-optimal policies. While a
simple reduction allows one to apply a low-regret algorithm to obtain an
$\epsilon$-optimal policy and achieve the worst-case optimal rate, it is
unknown whether low-regret algorithms can obtain the instance-optimal rate for
policy identification. We show that this is not possible -- there exists a
fundamental tradeoff between achieving low regret and identifying an
$\epsilon$-optimal policy at the instance-optimal rate.

Motivated by our negative finding, we propose a new measure of
instance-dependent sample complexity for PAC tabular reinforcement learning
which explicitly accounts for the attainable state visitation distributions in
the underlying MDP. We then propose and analyze a novel, planning-based
algorithm which attains this sample complexity -- yielding a complexity which
scales with the suboptimality gaps and the ``reachability'' of a state. We show
that our algorithm is nearly minimax optimal, and on several examples that our
instance-dependent sample complexity offers significant improvements over
worst-case bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Post-Concussion Syndrome Outcomes with Machine Learning. (arXiv:2108.02570v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2108.02570</id>
        <link href="http://arxiv.org/abs/2108.02570"/>
        <updated>2021-08-06T00:51:47.564Z</updated>
        <summary type="html"><![CDATA[In this paper, machine learning models are used to predict outcomes for
patients with persistent post-concussion syndrome (PCS). Patients had sustained
a concussion at an average of two to three months before the study. By
utilizing assessed data, the machine learning models aimed to predict whether
or not a patient would continue to have PCS after four to five months. The
random forest classifier achieved the highest performance with an 85% accuracy
and an area under the receiver operating characteristic curve (AUC) of 0.94.
Factors found to be predictive of PCS outcome were Post-Traumatic Stress
Disorder (PTSD), perceived injustice, self-rated prognosis, and symptom
severity post-injury. The results of this study demonstrate that machine
learning models can predict PCS outcomes with high accuracy. With further
research, machine learning models may be implemented in healthcare settings to
help patients with persistent PCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minhong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLL: Automatic Linear Layout of Graphs based on Deep Neural Network. (arXiv:2108.02431v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02431</id>
        <link href="http://arxiv.org/abs/2108.02431"/>
        <updated>2021-08-06T00:51:47.558Z</updated>
        <summary type="html"><![CDATA[Linear layouts are a graph visualization method that can be used to capture
an entry pattern in an adjacency matrix of a given graph. By reordering the
node indices of the original adjacency matrix, linear layouts provide knowledge
of latent graph structures. Conventional linear layout methods commonly aim to
find an optimal reordering solution based on predefined features of a given
matrix and loss function. However, prior knowledge of the appropriate features
to use or structural patterns in a given adjacency matrix is not always
available. In such a case, performing the reordering based on data-driven
feature extraction without assuming a specific structure in an adjacency matrix
is preferable. Recently, a neural-network-based matrix reordering method called
DeepTMR has been proposed to perform this function. However, it is limited to a
two-mode reordering (i.e., the rows and columns are reordered separately) and
it cannot be applied in the one-mode setting (i.e., the same node order is used
for reordering both rows and columns), owing to the characteristics of its
model architecture. In this study, we extend DeepTMR and propose a new one-mode
linear layout method referred to as AutoLL. We developed two types of neural
network models, AutoLL-D and AutoLL-U, for reordering directed and undirected
networks, respectively. To perform one-mode reordering, these AutoLL models
have specific encoder architectures, which extract node features from an
observed adjacency matrix. We conducted both qualitative and quantitative
evaluations of the proposed approach, and the experimental results demonstrate
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-08-06T00:51:47.552Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Modeling with Spline Partitions. (arXiv:2108.02507v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02507</id>
        <link href="http://arxiv.org/abs/2108.02507"/>
        <updated>2021-08-06T00:51:47.545Z</updated>
        <summary type="html"><![CDATA[Shape modelling (with methods that output shapes) is a new and important task
in Bayesian nonparametrics and bioinformatics. In this work, we focus on
Bayesian nonparametric methods for capturing shapes by partitioning a space
using curves. In related work, the classical Mondrian process is used to
partition spaces recursively with axis-aligned cuts, and is widely applied in
multi-dimensional and relational data. The Mondrian process outputs
hyper-rectangles. Recently, the random tessellation process was introduced as a
generalization of the Mondrian process, partitioning a domain with non-axis
aligned cuts in an arbitrary dimensional space, and outputting polytopes.
Motivated by these processes, in this work, we propose a novel parallelized
Bayesian nonparametric approach to partition a domain with curves, enabling
complex data-shapes to be acquired. We apply our method to HIV-1-infected human
macrophage image dataset, and also simulated datasets sets to illustrate our
approach. We compare to support vector machines, random forests and
state-of-the-art computer vision methods such as simple linear iterative
clustering super pixel image segmentation. We develop an R package that is
available at
\url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shufei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1"&gt;Lloyd Elliott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12498</id>
        <link href="http://arxiv.org/abs/2011.12498"/>
        <updated>2021-08-06T00:51:47.538Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning aims to boost the accuracy of a model by exploring
unlabeled images. The state-of-the-art methods are consistency-based which
learn about unlabeled images by encouraging the model to give consistent
predictions for images under different augmentations. However, when applied to
pose estimation, the methods degenerate and predict every pixel in unlabeled
images as background. This is because contradictory predictions are gradually
pushed to the background class due to highly imbalanced class distribution. But
this is not an issue in supervised learning because it has accurate labels.
This inspires us to stabilize the training by obtaining reliable pseudo labels.
Specifically, we learn two networks to mutually teach each other. In
particular, for each image, we compose an easy-hard pair by applying different
augmentations and feed them to both networks. The more reliable predictions on
easy images in each network are used to teach the other network to learn about
the corresponding hard images. The approach successfully avoids degeneration
and achieves promising results on public datasets. The source code will be
released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Rongchang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis. (arXiv:2108.02572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02572</id>
        <link href="http://arxiv.org/abs/2108.02572"/>
        <updated>2021-08-06T00:51:47.491Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved great success in a wide spectrum of multimedia
applications such as image classification, natural language processing and
multimodal data analysis. Recent years have seen the development of many deep
learning frameworks that provide a high-level programming interface for users
to design models, conduct training and deploy inference. However, it remains
challenging to build an efficient end-to-end multimedia application with most
existing frameworks. Specifically, in terms of usability, it is demanding for
non-experts to implement deep learning models, obtain the right settings for
the entire machine learning pipeline, manage models and datasets, and exploit
external data sources all together. Further, in terms of adaptability, elastic
computation solutions are much needed as the actual serving workload fluctuates
constantly, and scaling the hardware resources to handle the fluctuating
workload is typically infeasible. To address these challenges, we introduce
SINGA-Easy, a new deep learning framework that provides distributed
hyper-parameter tuning at the training stage, dynamic computational cost
control at the inference stage, and intuitive user interactions with multimedia
contents facilitated by model explanation. Our experiments on the training and
deployment of multi-modality data analysis applications show that the framework
is both usable and adaptable to dynamic inference loads. We implement
SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire
machine learning life cycle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_N/0/1/0/all/0/1"&gt;Naili Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1"&gt;Sai Ho Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Chenghao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1"&gt;Teck Khim Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1"&gt;Nan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meihui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1"&gt;Beng Chin Ooi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised optimization of random material microstructures in the small-data regime. (arXiv:2108.02606v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02606</id>
        <link href="http://arxiv.org/abs/2108.02606"/>
        <updated>2021-08-06T00:51:47.485Z</updated>
        <summary type="html"><![CDATA[While the forward and backward modeling of the process-structure-property
chain has received a lot of attention from the materials community, fewer
efforts have taken into consideration uncertainties. Those arise from a
multitude of sources and their quantification and integration in the inversion
process are essential in meeting the materials design objectives. The first
contribution of this paper is a flexible, fully probabilistic formulation of
such optimization problems that accounts for the uncertainty in the
process-structure and structure-property linkages and enables the
identification of optimal, high-dimensional, process parameters. We employ a
probabilistic, data-driven surrogate for the structure-property link which
expedites computations and enables handling of non-differential objectives. We
couple this with a novel active learning strategy, i.e. a self-supervised
collection of data, which significantly improves accuracy while requiring small
amounts of training data. We demonstrate its efficacy in optimizing the
mechanical and thermal properties of two-phase, random media but envision its
applicability encompasses a wide variety of microstructure-sensitive design
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rixner_M/0/1/0/all/0/1"&gt;Maximilian Rixner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1"&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02497</id>
        <link href="http://arxiv.org/abs/2108.02497"/>
        <updated>2021-08-06T00:51:47.458Z</updated>
        <summary type="html"><![CDATA[This document gives a concise outline of some of the common mistakes that
occur when using machine learning techniques, and what can be done to avoid
them. It is intended primarily as a guide for research students, and focuses on
issues that are of particular concern within academic research, such as the
need to do rigorous comparisons and reach valid conclusions. It covers five
stages of the machine learning process: what to do before model building, how
to reliably build models, how to robustly evaluate models, how to compare
models fairly, and how to report results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1"&gt;Michael A. Lones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep multi-task mining Calabi-Yau four-folds. (arXiv:2108.02221v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2108.02221</id>
        <link href="http://arxiv.org/abs/2108.02221"/>
        <updated>2021-08-06T00:51:47.441Z</updated>
        <summary type="html"><![CDATA[We continue earlier efforts in computing the dimensions of tangent space
cohomologies of Calabi-Yau manifolds using deep learning. In this paper, we
consider the dataset of all Calabi-Yau four-folds constructed as complete
intersections in products of projective spaces. Employing neural networks
inspired by state-of-the-art computer vision architectures, we improve earlier
benchmarks and demonstrate that all four non-trivial Hodge numbers can be
learned at the same time using a multi-task architecture. With 30% (80%)
training ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for
$h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for
$h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute,
and taking into account the linear constraint arising from index computations,
we get 100% total accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1"&gt;Harold Erbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1"&gt;Riccardo Finotello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Schneider_R/0/1/0/all/0/1"&gt;Robin Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Tamaazousti_M/0/1/0/all/0/1"&gt;Mohamed Tamaazousti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperJump: Accelerating HyperBand via Risk Modelling. (arXiv:2108.02479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02479</id>
        <link href="http://arxiv.org/abs/2108.02479"/>
        <updated>2021-08-06T00:51:47.433Z</updated>
        <summary type="html"><![CDATA[In the literature on hyper-parameter tuning, a number of recent solutions
rely on low-fidelity observations (e.g., training with sub-sampled datasets or
for short periods of time) to extrapolate good configurations to use when
performing full training. Among these, HyperBand is arguably one of the most
popular solutions, due to its efficiency and theoretically provable robustness.
In this work, we introduce HyperJump, a new approach that builds on HyperBand's
robust search strategy and complements it with novel model-based risk analysis
techniques that accelerate the search by jumping the evaluation of low risk
configurations, i.e., configurations that are likely to be discarded by
HyperBand. We evaluate HyperJump on a suite of hyper-parameter optimization
problems and show that it provides over one-order of magnitude speed-ups on a
variety of deep-learning and kernel-based learning problems when compared to
HyperBand as well as to a number of state of the art optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1"&gt;Pedro Mendes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casimiro_M/0/1/0/all/0/1"&gt;Maria Casimiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1"&gt;Paolo Romano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRPS Learning. (arXiv:2102.00968v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00968</id>
        <link href="http://arxiv.org/abs/2102.00968"/>
        <updated>2021-08-06T00:51:47.426Z</updated>
        <summary type="html"><![CDATA[Combination and aggregation techniques can significantly improve forecast
accuracy. This also holds for probabilistic forecasting methods where
predictive distributions are combined. There are several time-varying and
adaptive weighting schemes such as Bayesian model averaging (BMA). However, the
quality of different forecasts may vary not only over time but also within the
distribution. For example, some distribution forecasts may be more accurate in
the center of the distributions, while others are better at predicting the
tails. Therefore, we introduce a new weighting method that considers the
differences in performance over time and within the distribution. We discuss
pointwise combination algorithms based on aggregation across quantiles that
optimize with respect to the continuous ranked probability score (CRPS). After
analyzing the theoretical properties of pointwise CRPS learning, we discuss B-
and P-Spline-based estimation techniques for batch and online learning, based
on quantile regression and prediction with expert advice. We prove that the
proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise
CRPS online learning has optimal convergence properties. They are confirmed in
simulations and a probabilistic forecasting study for European emission
allowance (EUA) prices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Berrisch_J/0/1/0/all/0/1"&gt;Jonathan Berrisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1"&gt;Florian Ziel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04767</id>
        <link href="http://arxiv.org/abs/2010.04767"/>
        <updated>2021-08-06T00:51:47.406Z</updated>
        <summary type="html"><![CDATA[In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1"&gt;Tanmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1"&gt;Chinmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1"&gt;Sivanathan Kandhasamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing an architecture for a neural network. (arXiv:2108.02231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02231</id>
        <link href="http://arxiv.org/abs/2108.02231"/>
        <updated>2021-08-06T00:51:47.399Z</updated>
        <summary type="html"><![CDATA[We propose a new kind of automatic architecture search algorithm. The
algorithm alternates pruning connections and adding neurons, and it is not
restricted to layered architectures only. Here architecture is an arbitrary
oriented graph with some weights (along with some biases and an activation
function), so there may be no layered structure in such a network. The
algorithm minimizes the complexity of staying within a given error. We
demonstrate our algorithm on the brightness prediction problem of the next
point through the previous points on an image. Our second test problem is the
approximation of the bivariate function defining the brightness of a black and
white image. Our optimized networks significantly outperform the standard
solution for neural network architectures in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khashin_S/0/1/0/all/0/1"&gt;Sergey Khashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shemyakova_E/0/1/0/all/0/1"&gt;Ekaterina Shemyakova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02676</id>
        <link href="http://arxiv.org/abs/2108.02676"/>
        <updated>2021-08-06T00:51:47.393Z</updated>
        <summary type="html"><![CDATA[The automated segmentation of cancer tissue in histopathology images can help
clinicians to detect, diagnose, and analyze such disease. Different from other
natural images used in many convolutional networks for benchmark,
histopathology images can be extremely large, and the cancerous patterns can
reach beyond 1000 pixels. Therefore, the well-known networks in the literature
were never conceived to handle these peculiarities. In this work, we propose a
Fully Convolutional DenseUNet that is particularly designed to solve
histopathology problems. We evaluated our network in two public pathology
datasets published as challenges in the recent MICCAI 2019: binary segmentation
in colon cancer images (DigestPath2019), and multi-class segmentation in
prostate cancer images (Gleason2019), achieving similar and better results than
the winners of the challenges, respectively. Furthermore, we discussed some
good practices in the training setup to yield the best performance and the main
challenges in these histopathology datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds. (arXiv:2108.02482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02482</id>
        <link href="http://arxiv.org/abs/2108.02482"/>
        <updated>2021-08-06T00:51:47.386Z</updated>
        <summary type="html"><![CDATA[Cerebral microbleeds are small, dark, round lesions that can be visualised on
T2*-weighted MRI or other sequences sensitive to susceptibility effects. In
this work, we propose a multi-stage approach to both microbleed detection and
segmentation. First, possible microbleed locations are detected with a Mask
R-CNN technique. Second, at each possible microbleed location, a simple U-Net
performs the final segmentation. This work used the 72 subjects as training
data provided by the "Where is VALDO?" challenge of MICCAI 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1"&gt;Marta Girones Sanguesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1"&gt;Denis Kutnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1"&gt;Bas H.M. van der Velden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting to Function Difficulty and Growth Conditions in Private Optimization. (arXiv:2108.02391v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02391</id>
        <link href="http://arxiv.org/abs/2108.02391"/>
        <updated>2021-08-06T00:51:47.369Z</updated>
        <summary type="html"><![CDATA[We develop algorithms for private stochastic convex optimization that adapt
to the hardness of the specific function we wish to optimize. While previous
work provide worst-case bounds for arbitrary convex functions, it is often the
case that the function at hand belongs to a smaller class that enjoys faster
rates. Concretely, we show that for functions exhibiting $\kappa$-growth around
the optimum, i.e., $f(x) \ge f(x^*) + \lambda \kappa^{-1} \|x-x^*\|_2^\kappa$
for $\kappa > 1$, our algorithms improve upon the standard
${\sqrt{d}}/{n\varepsilon}$ privacy rate to the faster
$({\sqrt{d}}/{n\varepsilon})^{\tfrac{\kappa}{\kappa - 1}}$. Crucially, they
achieve these rates without knowledge of the growth constant $\kappa$ of the
function. Our algorithms build upon the inverse sensitivity mechanism, which
adapts to instance difficulty (Asi & Duchi, 2020), and recent localization
techniques in private optimization (Feldman et al., 2020). We complement our
algorithms with matching lower bounds for these function classes and
demonstrate that our adaptive algorithm is \emph{simultaneously} (minimax)
optimal over all $\kappa \ge 1+c$ whenever $c = \Theta(1)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1"&gt;Hilal Asi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1"&gt;Daniel Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02696</id>
        <link href="http://arxiv.org/abs/2108.02696"/>
        <updated>2021-08-06T00:51:47.359Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning is just at a tipping point where it could really take
off. Among these approaches, contrastive learning has seen tremendous progress
and led to state-of-the-art performance. In this paper, we construct a novel
probabilistic graphical model that effectively incorporates the low rank
promoting prior into the framework of contrastive learning, referred to as
LORAC. In contrast to the existing conventional self-supervised approaches that
only considers independent learning, our hypothesis explicitly requires that
all the samples belonging to the same instance class lie on the same subspace
with small dimension. This heuristic poses particular joint learning
constraints to reduce the degree of freedom of the problem during the search of
the optimal network parameterization. Most importantly, we argue that the low
rank prior employed here is not unique, and many different priors can be
invoked in a similar probabilistic way, corresponding to different hypotheses
about underlying truth behind the contrastive features. Empirical evidences
show that the proposed algorithm clearly surpasses the state-of-the-art
approaches on multiple benchmarks, including image classification, object
detection, instance segmentation and keypoint detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jingyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Qi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-08-06T00:51:47.351Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding collections of related datasets using dependent MMD coresets. (arXiv:2006.14621v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14621</id>
        <link href="http://arxiv.org/abs/2006.14621"/>
        <updated>2021-08-06T00:51:47.344Z</updated>
        <summary type="html"><![CDATA[Understanding how two datasets differ can help us determine whether one
dataset under-represents certain sub-populations, and provides insights into
how well models will generalize across datasets. Representative points selected
by a maximum mean discrepency (MMD) coreset can provide interpretable summaries
of a single dataset, but are not easily compared across datasets. In this paper
we introduce dependent MMD coresets, a data summarization method for
collections of datasets that facilitates comparison of distributions. We show
that dependent MMD coresets are useful for understanding multiple related
datasets and understanding model generalization between such datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1"&gt;Sinead A. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henderson_J/0/1/0/all/0/1"&gt;Jette Henderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise training convolutional neural networks with smaller filters for human activity recognition using wearable sensors. (arXiv:2005.03948v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03948</id>
        <link href="http://arxiv.org/abs/2005.03948"/>
        <updated>2021-08-06T00:51:47.336Z</updated>
        <summary type="html"><![CDATA[Recently, convolutional neural networks (CNNs) have set latest
state-of-the-art on various human activity recognition (HAR) datasets. However,
deep CNNs often require more computing resources, which limits their
applications in embedded HAR. Although many successful methods have been
proposed to reduce memory and FLOPs of CNNs, they often involve special network
architectures designed for visual tasks, which are not suitable for deep HAR
tasks with time series sensor signals, due to remarkable discrepancy.
Therefore, it is necessary to develop lightweight deep models to perform HAR.
As filter is the basic unit in constructing CNNs, it deserves further research
whether re-designing smaller filters is applicable for deep HAR. In the paper,
inspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.
A set of lower-dimensional filters is used as Lego bricks to be stacked for
conventional filters, which does not rely on any special network structure. The
local loss function is used to train model. To our knowledge, this is the first
paper that proposes lightweight CNN for HAR in ubiquitous and wearable
computing arena. The experiment results on five public HAR datasets, UCI-HAR
dataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM
dataset collected from either smartphones or multiple sensor nodes, indicate
that our novel Lego CNN with local loss can greatly reduce memory and
computation cost over CNN, while achieving higher accuracy. That is to say, the
proposed model is smaller, faster and more accurate. Finally, we evaluate the
actual performance on an Android smartphone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1"&gt;Qi Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_F/0/1/0/all/0/1"&gt;Fuhong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12216</id>
        <link href="http://arxiv.org/abs/2107.12216"/>
        <updated>2021-08-06T00:51:47.329Z</updated>
        <summary type="html"><![CDATA[Policy gradient methods are appealing in deep reinforcement learning but
suffer from high variance of gradient estimate. To reduce the variance, the
state value function is applied commonly. However, the effect of the state
value function becomes limited in stochastic dynamic environments, where the
unexpected state dynamics and rewards will increase the variance. In this
paper, we propose to replace the state value function with a novel hindsight
value function, which leverages the information from the future to reduce the
variance of the gradient estimate for stochastic dynamic environments.

Particularly, to obtain an ideally unbiased gradient estimate, we propose an
information-theoretic approach, which optimizes the embeddings of the future to
be independent of previous actions. In our experiments, we apply the proposed
hindsight value function in stochastic dynamic environments, including
discrete-action environments and continuous-action environments. Compared with
the standard state value function, the proposed hindsight value function
consistently reduces the variance, stabilizes the training, and improves the
eventual policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiaming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xishan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaohui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qi Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zidong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunji Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02456</id>
        <link href="http://arxiv.org/abs/2108.02456"/>
        <updated>2021-08-06T00:51:47.323Z</updated>
        <summary type="html"><![CDATA[Multi-label image recognition is a challenging computer vision task of
practical use. Progresses in this area, however, are often characterized by
complicated methods, heavy computations, and lack of intuitive explanations. To
effectively capture different spatial regions occupied by objects from
different categories, we propose an embarrassingly simple module, named
class-specific residual attention (CSRA). CSRA generates class-specific
features for every category by proposing a simple spatial attention score, and
then combines it with the class-agnostic average pooling feature. CSRA achieves
state-of-the-art results on multilabel recognition, and at the same time is
much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads
to consistent improvement across many diverse pretrained models and datasets
without any extra training. CSRA is both easy to implement and light in
computations, which also enjoys intuitive explanations and visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1"&gt;Ke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation. (arXiv:2010.06107v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06107</id>
        <link href="http://arxiv.org/abs/2010.06107"/>
        <updated>2021-08-06T00:51:47.316Z</updated>
        <summary type="html"><![CDATA[Automatic and accurate tumor segmentation on medical images is in high demand
to assist physicians with diagnosis and treatment. However, it is difficult to
obtain massive amounts of annotated training data required by the deep-learning
models as the manual delineation process is often tedious and expertise
required. Although self-supervised learning (SSL) scheme has been widely
adopted to address this problem, most SSL methods focus only on global
structure information, ignoring the key distinguishing features of tumor
regions: local intensity variation and large size distribution. In this paper,
we propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor
segmentation. Specifically, a novel proxy task, i.e. scale discrimination, is
formulated to pre-train the 3D neural network combined with the
self-restoration task. Thus, the pre-trained model learns multi-level local
representations through multi-scale inputs. Moreover, an adversarial learning
module is further introduced to learn modality invariant representations from
multiple unlabeled source datasets. We demonstrate the effectiveness of our
methods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas
tumor segmentation. Compared with the state-of-the-art 3D SSL methods, our
proposed approach can significantly improve the segmentation accuracy. Besides,
we analyze its advantages from multiple perspectives such as data efficiency,
performance, and convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shixiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild. (arXiv:2108.02452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02452</id>
        <link href="http://arxiv.org/abs/2108.02452"/>
        <updated>2021-08-06T00:51:47.276Z</updated>
        <summary type="html"><![CDATA[We present VoxelTrack for multi-person 3D pose estimation and tracking from a
few cameras which are separated by wide baselines. It employs a multi-branch
network to jointly estimate 3D poses and re-identification (Re-ID) features for
all people in the environment. In contrast to previous efforts which require to
establish cross-view correspondence based on noisy 2D pose estimates, it
directly estimates and tracks 3D poses from a 3D voxel-based representation
constructed from multi-view images. We first discretize the 3D space by regular
voxels and compute a feature vector for each voxel by averaging the body joint
heatmaps that are inversely projected from all views. We estimate 3D poses from
the voxel representation by predicting whether each voxel contains a particular
body joint. Similarly, a Re-ID feature is computed for each voxel which is used
to track the estimated 3D poses over time. The main advantage of the approach
is that it avoids making any hard decisions based on individual images. The
approach can robustly estimate and track 3D poses even when people are severely
occluded in some cameras. It outperforms the state-of-the-art methods by a
large margin on three public datasets including Shelf, Campus and CMU Panoptic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification. (arXiv:1912.05949v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.05949</id>
        <link href="http://arxiv.org/abs/1912.05949"/>
        <updated>2021-08-06T00:51:47.265Z</updated>
        <summary type="html"><![CDATA[We propose a novel online multi-object visual tracker using a Gaussian
mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance
learning. The GM-PHD filter has a linear complexity with the number of objects
and observations while estimating the states and cardinality of time-varying
number of objects, however, it is susceptible to miss-detections and does not
include the identity of objects. We use visual-spatio-temporal information
obtained from object bounding boxes and deeply learned appearance
representations to perform estimates-to-tracks data association for target
labeling as well as formulate an augmented likelihood and then integrate into
the update step of the GM-PHD filter. We also employ additional unassigned
tracks prediction after the data association step to overcome the
susceptibility of the GM-PHD filter towards miss-detections caused by
occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark datasets
show that our tracker significantly outperforms several state-of-the-art
trackers in terms of tracking accuracy and identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Normalized Representation Learning for Generalizable Face Anti-Spoofing. (arXiv:2108.02667v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02667</id>
        <link href="http://arxiv.org/abs/2108.02667"/>
        <updated>2021-08-06T00:51:47.250Z</updated>
        <summary type="html"><![CDATA[With various face presentation attacks arising under unseen scenarios, face
anti-spoofing (FAS) based on domain generalization (DG) has drawn growing
attention due to its robustness. Most existing methods utilize DG frameworks to
align the features to seek a compact and generalized feature space. However,
little attention has been paid to the feature extraction process for the FAS
task, especially the influence of normalization, which also has a great impact
on the generalization of the learned representation. To address this issue, we
propose a novel perspective of face anti-spoofing that focuses on the
normalization selection in the feature extraction process. Concretely, an
Adaptive Normalized Representation Learning (ANRL) framework is devised, which
adaptively selects feature normalization methods according to the inputs,
aiming to learn domain-agnostic and discriminative representation. Moreover, to
facilitate the representation learning, Dual Calibration Constraints are
designed, including Inter-Domain Compatible loss and Inter-Class Separable
loss, which provide a better optimization direction for generalizable
representation. Extensive experiments and visualizations are presented to
demonstrate the effectiveness of our method against the SOTA competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shubao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke-Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Taiping Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_M/0/1/0/all/0/1"&gt;Mingwei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02566</id>
        <link href="http://arxiv.org/abs/2108.02566"/>
        <updated>2021-08-06T00:51:47.242Z</updated>
        <summary type="html"><![CDATA[Despite tremendous progress in missing data imputation task, designing new
imputation models has become more and more cumbersome but the corresponding
gains are relatively small. Is there any simple but general approach that can
exploit the existing models to further improve the quality of the imputation?
In this article, we aim to respond to this concern and propose a novel general
data augmentation method called Missingness Augmentation (MA), which can be
applied in many existing generative imputation frameworks to further improve
the performance of these models. For MA, before each training epoch, we use the
outputs of the generator to expand the incomplete samples on the fly, and then
determine a special reconstruction loss for these augmented samples. This
reconstruction loss plus the original loss constitutes the final optimization
objective of the model. It is noteworthy that MA is very efficient and does not
need to change the structure of the original model. Experimental results
demonstrate that MA can significantly improve the performance of many recently
developed generative imputation models on a variety of datasets. Our code is
available at https://github.com/WYu-Feng/Missingness-Augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High dimensional Bayesian Optimization Algorithm for Complex System in Time Series. (arXiv:2108.02289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02289</id>
        <link href="http://arxiv.org/abs/2108.02289"/>
        <updated>2021-08-06T00:51:47.223Z</updated>
        <summary type="html"><![CDATA[At present, high-dimensional global optimization problems with time-series
models have received much attention from engineering fields. Since it was
proposed, Bayesian optimization has quickly become a popular and promising
approach for solving global optimization problems. However, the standard
Bayesian optimization algorithm is insufficient to solving the global optimal
solution when the model is high-dimensional. Hence, this paper presents a novel
high dimensional Bayesian optimization algorithm by considering dimension
reduction and different dimension fill-in strategies. Most existing literature
about Bayesian optimization algorithms did not discuss the sampling strategies
to optimize the acquisition function. This study proposed a new sampling method
based on both the multi-armed bandit and random search methods while optimizing
the acquisition function. Besides, based on the time-dependent or
dimension-dependent characteristics of the model, the proposed algorithm can
reduce the dimension evenly. Then, five different dimension fill-in strategies
were discussed and compared in this study. Finally, to increase the final
accuracy of the optimal solution, the proposed algorithm adds a local search
based on a series of Adam-based steps at the final stage. Our computational
experiments demonstrated that the proposed Bayesian optimization algorithm
could achieve reasonable solutions with excellent performances for high
dimensional global optimization problems with a time-series optimal control
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Kaiming Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chih-Hang J. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Arieh_D/0/1/0/all/0/1"&gt;David Ben-Arieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Ashesh Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02703</id>
        <link href="http://arxiv.org/abs/2101.02703"/>
        <updated>2021-08-06T00:51:47.208Z</updated>
        <summary type="html"><![CDATA[While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1"&gt;Lihua Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection. (arXiv:2108.02455v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02455</id>
        <link href="http://arxiv.org/abs/2108.02455"/>
        <updated>2021-08-06T00:51:47.189Z</updated>
        <summary type="html"><![CDATA[Ocean fronts can cause the accumulation of nutrients and affect the
propagation of underwater sound, so high-precision ocean front detection is of
great significance to the marine fishery and national defense fields. However,
the current ocean front detection methods either have low detection accuracy or
most can only detect the occurrence of ocean front by binary classification,
rarely considering the differences of the characteristics of multiple ocean
fronts in different sea areas. In order to solve the above problems, we propose
a semantic segmentation network called location and seasonality enhanced
network (LSENet) for multi-class ocean fronts detection at pixel level. In this
network, we first design a channel supervision unit structure, which integrates
the seasonal characteristics of the ocean front itself and the contextual
information to improve the detection accuracy. We also introduce a location
attention mechanism to adaptively assign attention weights to the fronts
according to their frequently occurred sea area, which can further improve the
accuracy of multi-class ocean front detection. Compared with other semantic
segmentation methods and current representative ocean front detection method,
the experimental results demonstrate convincingly that our method is more
effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cui Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Classification Methods and Portfolio Allocation: An Examination of Market Efficiency. (arXiv:2108.02283v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02283</id>
        <link href="http://arxiv.org/abs/2108.02283"/>
        <updated>2021-08-06T00:51:47.182Z</updated>
        <summary type="html"><![CDATA[We design a novel framework to examine market efficiency through
out-of-sample (OOS) predictability. We frame the asset pricing problem as a
machine learning classification problem and construct classification models to
predict return states. The prediction-based portfolios beat the market with
significant OOS economic gains. We measure prediction accuracies directly. For
each model, we introduce a novel application of binomial test to test the
accuracy of 3.34 million return state predictions. The tests show that our
models can extract useful contents from historical information to predict
future return states. We provide unique economic insights about OOS
predictability and machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Pukthuanthong_K/0/1/0/all/0/1"&gt;Kuntara Pukthuanthong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling the Knowledge from Conditional Normalizing Flows. (arXiv:2106.12699v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12699</id>
        <link href="http://arxiv.org/abs/2106.12699"/>
        <updated>2021-08-06T00:51:47.176Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a powerful class of generative models demonstrating
strong performance in several speech and vision problems. In contrast to other
generative models, normalizing flows are latent variable models with tractable
likelihoods and allow for stable training. However, they have to be carefully
designed to represent invertible functions with efficient Jacobian determinant
calculation. In practice, these requirements lead to overparameterized and
sophisticated architectures that are inferior to alternative feed-forward
models in terms of inference time and memory consumption. In this work, we
investigate whether one can distill flow-based models into more efficient
alternatives. We provide a positive answer to this question by proposing a
simple distillation approach and demonstrating its effectiveness on
state-of-the-art conditional flow-based models for image super-resolution and
speech synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1"&gt;Dmitry Baranchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1"&gt;Vladimir Aliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective. (arXiv:2108.02656v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02656</id>
        <link href="http://arxiv.org/abs/2108.02656"/>
        <updated>2021-08-06T00:51:47.169Z</updated>
        <summary type="html"><![CDATA[Objective: We develop a computer-aided diagnosis (CAD) system using deep
learning approaches for lesion detection and classification on whole-slide
images (WSIs) with breast cancer. The deep features being distinguishing in
classification from the convolutional neural networks (CNN) are demonstrated in
this study to provide comprehensive interpretability for the proposed CAD
system using pathological knowledge. Methods: In the experiment, a total of 186
slides of WSIs were collected and classified into three categories:
Non-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma
(IDC). Instead of conducting pixel-wise classification into three classes
directly, we designed a hierarchical framework with the multi-view scheme that
performs lesion detection for region proposal at higher magnification first and
then conducts lesion classification at lower magnification for each detected
lesion. Results: The slide-level accuracy rate for three-category
classification reaches 90.8% (99/109) through 5-fold cross-validation and
achieves 94.8% (73/77) on the testing set. The experimental results show that
the morphological characteristics and co-occurrence properties learned by the
deep learning models for lesion classification are accordant with the clinical
rules in diagnosis. Conclusion: The pathological interpretability of the deep
features not only enhances the reliability of the proposed CAD system to gain
acceptance from medical specialists, but also facilitates the development of
deep learning frameworks for various tasks in pathology. Significance: This
paper presents a CAD system for pathological image analysis, which fills the
clinical requirements and can be accepted by medical specialists with providing
its interpretability from the pathological perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Wen Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongfang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Chang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yu-Ling Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yun Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xueli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Yanhong Tai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On spectral algorithms for community detection in stochastic blockmodel graphs with vertex covariates. (arXiv:2007.02156v3 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2007.02156</id>
        <link href="http://arxiv.org/abs/2007.02156"/>
        <updated>2021-08-06T00:51:47.161Z</updated>
        <summary type="html"><![CDATA[In network inference applications, it is often desirable to detect community
structure, namely to cluster vertices into groups, or blocks, according to some
measure of similarity. Beyond mere adjacency matrices, many real networks also
involve vertex covariates that carry key information about underlying block
structure in graphs. To assess the effects of such covariates on block
recovery, we present a comparative analysis of two model-based spectral
algorithms for clustering vertices in stochastic blockmodel graphs with vertex
covariates. The first algorithm uses only the adjacency matrix, and directly
estimates the block assignments. The second algorithm incorporates both the
adjacency matrix and the vertex covariates into the estimation of block
assignments, and moreover quantifies the explicit impact of the vertex
covariates on the resulting estimate of the block assignments. We employ
Chernoff information to analytically compare the algorithms' performance and
derive the information-theoretic Chernoff ratio for certain models of interest.
Analytic results and simulations suggest that the second algorithm is often
preferred: we can often better estimate the induced block assignments by first
estimating the effect of vertex covariates. In addition, real data examples
also indicate that the second algorithm has the advantages of revealing
underlying block structure and taking observed vertex heterogeneity into
account in real applications. Our findings emphasize the importance of
distinguishing between observed and unobserved factors that can affect block
structure in graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1"&gt;Cong Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mele_A/0/1/0/all/0/1"&gt;Angelo Mele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1"&gt;Lingxin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cape_J/0/1/0/all/0/1"&gt;Joshua Cape&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athreya_A/0/1/0/all/0/1"&gt;Avanti Athreya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:47.141Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention. (arXiv:2108.02347v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02347</id>
        <link href="http://arxiv.org/abs/2108.02347"/>
        <updated>2021-08-06T00:51:47.134Z</updated>
        <summary type="html"><![CDATA[We propose FMMformers, a class of efficient and flexible transformers
inspired by the celebrated fast multipole method (FMM) for accelerating
interacting particle simulation. FMM decomposes particle-particle interaction
into near-field and far-field components and then performs direct and
coarse-grained computation, respectively. Similarly, FMMformers decompose the
attention into near-field and far-field attention, modeling the near-field
attention by a banded matrix and the far-field attention by a low-rank matrix.
Computing the attention matrix for FMMformers requires linear complexity in
computational time and memory footprint with respect to the sequence length. In
contrast, standard transformers suffer from quadratic complexity. We analyze
and validate the advantage of FMMformers over the standard transformer on the
Long Range Arena and language modeling benchmarks. FMMformers can even
outperform the standard transformer in terms of accuracy by a significant
margin. For instance, FMMformers achieve an average classification accuracy of
$60.74\%$ over the five Long Range Arena tasks, which is significantly better
than the standard transformer's average accuracy of $58.70\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tan M. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suliafu_V/0/1/0/all/0/1"&gt;Vai Suliafu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley J. Osher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples. (arXiv:2108.02756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02756</id>
        <link href="http://arxiv.org/abs/2108.02756"/>
        <updated>2021-08-06T00:51:47.128Z</updated>
        <summary type="html"><![CDATA[The design of additive imperceptible perturbations to the inputs of deep
classifiers to maximize their misclassification rates is a central focus of
adversarial machine learning. An alternative approach is to synthesize
adversarial examples from scratch using GAN-like structures, albeit with the
use of large amounts of training data. By contrast, this paper considers
one-shot synthesis of adversarial examples; the inputs are synthesized from
scratch to induce arbitrary soft predictions at the output of pre-trained
models, while simultaneously maintaining high similarity to specified inputs.
To this end, we present a problem that encodes objectives on the distance
between the desired and output distributions of the trained model and the
similarity between such inputs and the synthesized examples. We prove that the
formulated problem is NP-complete. Then, we advance a generative approach to
the solution in which the adversarial examples are obtained as the output of a
generative network whose parameters are iteratively updated by optimizing
surrogate loss functions for the dual-objective. We demonstrate the generality
and versatility of the framework and approach proposed through applications to
the design of targeted adversarial attacks, generation of decision boundary
samples, and synthesis of low confidence classification inputs. The approach is
further extended to an ensemble of models with different soft output
specifications. The experimental results verify that the targeted and
confidence reduction attack methods developed perform on par with
state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alkhouri_I/0/1/0/all/0/1"&gt;Ismail Alkhouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1"&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1"&gt;George Atia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02676</id>
        <link href="http://arxiv.org/abs/2108.02676"/>
        <updated>2021-08-06T00:51:47.121Z</updated>
        <summary type="html"><![CDATA[The automated segmentation of cancer tissue in histopathology images can help
clinicians to detect, diagnose, and analyze such disease. Different from other
natural images used in many convolutional networks for benchmark,
histopathology images can be extremely large, and the cancerous patterns can
reach beyond 1000 pixels. Therefore, the well-known networks in the literature
were never conceived to handle these peculiarities. In this work, we propose a
Fully Convolutional DenseUNet that is particularly designed to solve
histopathology problems. We evaluated our network in two public pathology
datasets published as challenges in the recent MICCAI 2019: binary segmentation
in colon cancer images (DigestPath2019), and multi-class segmentation in
prostate cancer images (Gleason2019), achieving similar and better results than
the winners of the challenges, respectively. Furthermore, we discussed some
good practices in the training setup to yield the best performance and the main
challenges in these histopathology datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02432</id>
        <link href="http://arxiv.org/abs/2108.02432"/>
        <updated>2021-08-06T00:51:47.114Z</updated>
        <summary type="html"><![CDATA[Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.

This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yanbin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory AMP. (arXiv:2012.10861v4 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10861</id>
        <link href="http://arxiv.org/abs/2012.10861"/>
        <updated>2021-08-06T00:51:47.077Z</updated>
        <summary type="html"><![CDATA[Approximate message passing (AMP) is a low-cost iterative
parameter-estimation technique for certain high-dimensional linear systems with
non-Gaussian distributions. However, AMP only applies to independent
identically distributed (IID) transform matrices, but may become unreliable
(e.g. perform poorly or even diverge) for other matrix ensembles, especially
for ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for
general right-unitarily-invariant matrices to handle this difficulty. However,
the Bayes-optimal OAMP/VAMP requires a high-complexity linear minimum mean
square error (MMSE) estimator. This limits the application of OAMP/VAMP to
large-scale systems.

To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory
AMP (MAMP) framework under an orthogonality principle, which guarantees the
asymptotic IID Gaussianity of estimation errors in MAMP. We present an
orthogonalization procedure for the local memory estimators to realize the
required orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP
(BO-MAMP), in which a long-memory matched filter is proposed for interference
suppression. The complexity of BO-MAMP is comparable to AMP. A state evolution
is derived to asymptotically characterize the performance of BO-MAMP. Based on
state evolution, the relaxation parameters and damping vector in BO-MAMP are
optimized. For all right-unitarily-invariant matrices, the optimized BO-MAMP
converges to the high-complexity OAMP/VAMP, and thus is Bayes-optimal if it has
a unique fixed point. Finally, simulations are provided to verify the validity
and accuracy of the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shunqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1"&gt;Brian M. Kurkoski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Role-based lateral movement detection with unsupervised learning. (arXiv:2108.02713v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02713</id>
        <link href="http://arxiv.org/abs/2108.02713"/>
        <updated>2021-08-06T00:51:47.013Z</updated>
        <summary type="html"><![CDATA[Adversarial lateral movement via compromised accounts remains difficult to
discover via traditional rule-based defenses because it generally lacks
explicit indicators of compromise. We propose a behavior-based, unsupervised
framework comprising two methods of lateral movement detection on enterprise
networks: one aimed at generic lateral movement via either exploit or
authenticated connections, and one targeting the specific techniques of process
injection and hijacking. The first method is based on the premise that the role
of a system---the functions it performs on the network---determines the roles
of the systems it should make connections with. The adversary meanwhile might
move between any systems whatever, possibly seeking out systems with unusual
roles that facilitate certain accesses. We use unsupervised learning to cluster
systems according to role and identify connections to systems with novel roles
as potentially malicious. The second method is based on the premise that the
temporal patterns of inter-system processes that facilitate these connections
depend on the roles of the systems involved. If a process is compromised by an
attacker, these normal patterns might be disrupted in discernible ways. We
apply frequent-itemset mining to process sequences to establish regular
patterns of communication between systems based on role, and identify rare
process sequences as signalling potentially malicious connections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powell_B/0/1/0/all/0/1"&gt;Brian A. Powell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-08-06T00:51:47.007Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10234</id>
        <link href="http://arxiv.org/abs/2107.10234"/>
        <updated>2021-08-06T00:51:47.000Z</updated>
        <summary type="html"><![CDATA[Deep learning's performance has been extensively recognized recently. Graph
neural networks (GNNs) are designed to deal with graph-structural data that
classical deep learning does not easily manage. Since most GNNs were created
using distinct theories, direct comparisons are impossible. Prior research has
primarily concentrated on categorizing existing models, with little attention
paid to their intrinsic connections. The purpose of this study is to establish
a unified framework that integrates GNNs based on spectral graph and
approximation theory. The framework incorporates a strong integration between
spatial- and spectral-based GNNs while tightly associating approaches that
exist within each respective domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1"&gt;Taoran Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Kaiqun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02704</id>
        <link href="http://arxiv.org/abs/2108.02704"/>
        <updated>2021-08-06T00:51:46.986Z</updated>
        <summary type="html"><![CDATA[Regularization in convolutional neural networks (CNNs) is usually addressed
with dropout layers. However, dropout is sometimes detrimental in the
convolutional part of a CNN as it simply sets to zero a percentage of pixels in
the feature maps, adding unrepresentative examples during training. Here, we
propose a CNN layer that performs regularization by applying random rotations
of reflections to a small percentage of feature maps after every convolutional
layer. We prove how this concept is beneficial for images with orientational
symmetries, such as in medical images, as it provides a certain degree of
rotational invariance. We tested this method in two datasets, a patch-based set
of histopathology images (PatchCamelyon) to perform classification using a
generic DenseNet, and a set of specular microscopy images of the corneal
endothelium to perform segmentation using a tailored U-net, improving the
performance in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Subspaces. (arXiv:2106.00001v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00001</id>
        <link href="http://arxiv.org/abs/2106.00001"/>
        <updated>2021-08-06T00:51:46.961Z</updated>
        <summary type="html"><![CDATA[Private data analysis suffers a costly curse of dimensionality. However, the
data often has an underlying low-dimensional structure. For example, when
optimizing via gradient descent, the gradients often lie in or near a
low-dimensional subspace. If that low-dimensional structure can be identified,
then we can avoid paying (in terms of privacy or accuracy) for the high ambient
dimension.

We present differentially private algorithms that take input data sampled
from a low-dimensional linear subspace (possibly with a small amount of error)
and output that subspace (or an approximation to it). These algorithms can
serve as a pre-processing step for other procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1"&gt;Vikrant Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00531</id>
        <link href="http://arxiv.org/abs/2104.00531"/>
        <updated>2021-08-06T00:51:46.953Z</updated>
        <summary type="html"><![CDATA[While most neural video codecs address P-frame coding (predicting each frame
from past ones), in this paper we address B-frame compression (predicting
frames using both past and future reference frames). Our B-frame solution is
based on the existing P-frame methods. As a result, B-frame coding capability
can easily be added to an existing neural codec. The basic idea of our B-frame
coding method is to interpolate the two reference frames to generate a single
reference frame and then use it together with an existing P-frame codec to
encode the input B-frame. Our studies show that the interpolated frame is a
much better reference for the P-frame codec compared to using the previous
frame as is usually done. Our results show that using the proposed method with
an existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG
dataset compared to the P-frame codec while generating the same video quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco S Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GIFAIR-FL: An Approach for Group and Individual Fairness in Federated Learning. (arXiv:2108.02741v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02741</id>
        <link href="http://arxiv.org/abs/2108.02741"/>
        <updated>2021-08-06T00:51:46.945Z</updated>
        <summary type="html"><![CDATA[In this paper we propose \texttt{GIFAIR-FL}: an approach that imposes group
and individual fairness to federated learning settings. By adding a
regularization term, our algorithm penalizes the spread in the loss of client
groups to drive the optimizer to fair solutions. Theoretically, we show
convergence in non-convex and strongly convex settings. Our convergence
guarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the
empirical performance of our algorithm, we apply our method on image
classification and text prediction tasks. Compared to existing algorithms, our
method shows improved fairness results while retaining superior or similar
prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xubo Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nouiehed_M/0/1/0/all/0/1"&gt;Maher Nouiehed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1"&gt;Raed Al Kontar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lyapunov Robust Constrained-MDPs: Soft-Constrained Robustly Stable Policy Optimization under Model Uncertainty. (arXiv:2108.02701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02701</id>
        <link href="http://arxiv.org/abs/2108.02701"/>
        <updated>2021-08-06T00:51:46.938Z</updated>
        <summary type="html"><![CDATA[Safety and robustness are two desired properties for any reinforcement
learning algorithm. CMDPs can handle additional safety constraints and RMDPs
can perform well under model uncertainties. In this paper, we propose to unite
these two frameworks resulting in robust constrained MDPs (RCMDPs). The
motivation is to develop a framework that can satisfy safety constraints while
also simultaneously offer robustness to model uncertainties. We develop the
RCMDP objective, derive gradient update formula to optimize this objective and
then propose policy gradient based algorithms. We also independently propose
Lyapunov based reward shaping for RCMDPs, yielding better stability and
convergence properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russel_R/0/1/0/all/0/1"&gt;Reazul Hasan Russel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1"&gt;Mouhacine Benosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1"&gt;Jeroen Van Baar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1"&gt;Radu Corcodel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-08-06T00:51:46.931Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test Score Algorithms for Budgeted Stochastic Utility Maximization. (arXiv:2012.15194v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15194</id>
        <link href="http://arxiv.org/abs/2012.15194"/>
        <updated>2021-08-06T00:51:46.910Z</updated>
        <summary type="html"><![CDATA[Motivated by recent developments in designing algorithms based on individual
item scores for solving utility maximization problems, we study the framework
of using test scores, defined as a statistic of observed individual item
performance data, for solving the budgeted stochastic utility maximization
problem. We extend an existing scoring mechanism, namely the replication test
scores, to incorporate heterogeneous item costs as well as item values. We show
that a natural greedy algorithm that selects items solely based on their
replication test scores outputs solutions within a constant factor of the
optimum for a broad class of utility functions. Our algorithms and
approximation guarantees assume that test scores are noisy estimates of certain
expected values with respect to marginal distributions of individual item
values, thus making our algorithms practical and extending previous work that
assumes noiseless estimates. Moreover, we show how our algorithm can be adapted
to the setting where items arrive in a streaming fashion while maintaining the
same approximation guarantee. We present numerical results, using synthetic
data and data sets from the Academia.StackExchange Q&A forum, which show that
our test score algorithm can achieve competitiveness, and in some cases better
performance than a benchmark algorithm that requires access to a value oracle
to evaluate function values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dabeen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1"&gt;Milan Vojnovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13838</id>
        <link href="http://arxiv.org/abs/2012.13838"/>
        <updated>2021-08-06T00:51:46.903Z</updated>
        <summary type="html"><![CDATA[Pretrained transformers achieve the state of the art across tasks in natural
language processing, motivating researchers to investigate their inner
mechanisms. One common direction is to understand what features are important
for prediction. In this paper, we apply information bottlenecks to analyze the
attribution of each feature for prediction on a black-box model. We use BERT as
the example and evaluate our approach both quantitatively and qualitatively. We
show the effectiveness of our method in terms of attribution and the ability to
provide insight into how information flows through layers. We demonstrate that
our technique outperforms two competitive methods in degradation tests on four
datasets. Code is available at https://github.com/bazingagin/IBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Raphael Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Ji Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural Network Acceleration. (arXiv:2108.02569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02569</id>
        <link href="http://arxiv.org/abs/2108.02569"/>
        <updated>2021-08-06T00:51:46.896Z</updated>
        <summary type="html"><![CDATA[The increasing popularity of deep neural network (DNN) applications demands
high computing power and efficient hardware accelerator architecture. DNN
accelerators use a large number of processing elements (PEs) and on-chip memory
for storing weights and other parameters. As the communication backbone of a
DNN accelerator, networks-on-chip (NoC) play an important role in supporting
various dataflow patterns and enabling processing with communication
parallelism in a DNN accelerator. However, the widely used mesh-based NoC
architectures inherently cannot support the efficient one-to-many and
many-to-one traffic largely existing in DNN workloads. In this paper, we
propose a modified mesh architecture with a one-way/two-way streaming bus to
speedup one-to-many (multicast) traffic, and the use of gather packets to
support many-to-one (gather) traffic. The analysis of the runtime latency of a
convolutional layer shows that the two-way streaming architecture achieves
better improvement than the one-way streaming architecture for an Output
Stationary (OS) dataflow architecture. The simulation results demonstrate that
the gather packets can help to reduce the runtime latency up to 1.8 times and
network power consumption up to 1.7 times, compared with the repetitive unicast
method on modified mesh architectures supporting two-way streaming.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1"&gt;Binayak Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yingtao Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities. (arXiv:2108.02594v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02594</id>
        <link href="http://arxiv.org/abs/2108.02594"/>
        <updated>2021-08-06T00:51:46.890Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating potential revenue or demand at business
facilities and understanding its generating mechanism. This problem arises in
different fields such as operation research or urban science, and more
generally, it is crucial for businesses' planning and decision making. We
develop a Bayesian spatial interaction model, henceforth BSIM, which provides
probabilistic predictions about revenues generated by a particular business
location provided their features and the potential customers' characteristics
in a given region. BSIM explicitly accounts for the competition among the
competitive facilities through a probability value determined by evaluating a
store-specific Gaussian distribution at a given customer location. We propose a
scalable variational inference framework that, while being significantly faster
than competing Markov Chain Monte Carlo inference schemes, exhibits comparable
performances in terms of parameters identification and uncertainty
quantification. We demonstrate the benefits of BSIM in various synthetic
settings characterised by an increasing number of stores and customers.
Finally, we construct a real-world, large spatial dataset for pub activities in
London, UK, which includes over 1,500 pubs and 150,000 customer regions. We
demonstrate how BSIM outperforms competing approaches on this large dataset in
terms of prediction performances while providing results that are both
interpretable and consistent with related indicators observed for the London
region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perera_S/0/1/0/all/0/1"&gt;Shanaka Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aglietti_V/0/1/0/all/0/1"&gt;Virginia Aglietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1"&gt;Theodoros Damoulas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06337</id>
        <link href="http://arxiv.org/abs/2105.06337"/>
        <updated>2021-08-06T00:51:46.883Z</updated>
        <summary type="html"><![CDATA[Recently, denoising diffusion probabilistic models and generative score
matching have shown high potential in modelling complex data distributions
while stochastic calculus has provided a unified point of view on these
techniques allowing for flexible inference schemes. In this paper we introduce
Grad-TTS, a novel text-to-speech model with score-based decoder producing
mel-spectrograms by gradually transforming noise predicted by encoder and
aligned with text input by means of Monotonic Alignment Search. The framework
of stochastic differential equations helps us to generalize conventional
diffusion probabilistic models to the case of reconstructing data from noise
with different parameters and allows to make this reconstruction flexible by
explicitly controlling trade-off between sound quality and inference speed.
Subjective human evaluation shows that Grad-TTS is competitive with
state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We
will make the code publicly available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1"&gt;Vadim Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1"&gt;Ivan Vovk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1"&gt;Vladimir Gogoryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1"&gt;Tasnima Sadekova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1"&gt;Mikhail Kudinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Elect. (arXiv:2108.02768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02768</id>
        <link href="http://arxiv.org/abs/2108.02768"/>
        <updated>2021-08-06T00:51:46.864Z</updated>
        <summary type="html"><![CDATA[Voting systems have a wide range of applications including recommender
systems, web search, product design and elections. Limited by the lack of
general-purpose analytical tools, it is difficult to hand-engineer desirable
voting rules for each use case. For this reason, it is appealing to
automatically discover voting rules geared towards each scenario. In this
paper, we show that set-input neural network architectures such as Set
Transformers, fully-connected graph networks and DeepSets are both
theoretically and empirically well-suited for learning voting rules. In
particular, we show that these network models can not only mimic a number of
existing voting rules to compelling accuracy --- both position-based (such as
Plurality and Borda) and comparison-based (such as Kemeny, Copeland and
Maximin) --- but also discover near-optimal voting rules that maximize
different social welfare functions. Furthermore, the learned voting rules
generalize well to different voter utility distributions and election sizes
unseen during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1"&gt;Cem Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1"&gt;Xuchan Bao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonperturbative renormalization for the neural network-QFT correspondence. (arXiv:2108.01403v1 [hep-th] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2108.01403</id>
        <link href="http://arxiv.org/abs/2108.01403"/>
        <updated>2021-08-06T00:51:46.858Z</updated>
        <summary type="html"><![CDATA[In a recent work arXiv:2008.08601, Halverson, Maiti and Stoner proposed a
description of neural networks in terms of a Wilsonian effective field theory.
The infinite-width limit is mapped to a free field theory, while finite $N$
corrections are taken into account by interactions (non-Gaussian terms in the
action). In this paper, we study two related aspects of this correspondence.
First, we comment on the concepts of locality and power-counting in this
context. Indeed, these usual space-time notions may not hold for neural
networks (since inputs can be arbitrary), however, the renormalization group
provides natural notions of locality and scaling. Moreover, we comment on
several subtleties, for example, that data components may not have a
permutation symmetry: in that case, we argue that random tensor field theories
could provide a natural generalization. Second, we improve the perturbative
Wilsonian renormalization from arXiv:2008.08601 by providing an analysis in
terms of the nonperturbative renormalization group using the Wetterich-Morris
equation. An important difference with usual nonperturbative RG analysis is
that only the effective (IR) 2-point function is known, which requires setting
the problem with care. Our aim is to provide a useful formalism to investigate
neural networks behavior beyond the large-width limit (i.e.~far from Gaussian
limit) in a nonperturbative fashion. A major result of our analysis is that
changing the standard deviation of the neural network weight distribution can
be interpreted as a renormalization flow in the space of networks. We focus on
translations invariant kernels and provide preliminary numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1"&gt;Harold Erbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Lahoche_V/0/1/0/all/0/1"&gt;Vincent Lahoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Samary_D/0/1/0/all/0/1"&gt;Dine Ousmane Samary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks and PIDE discretizations. (arXiv:2108.02430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02430</id>
        <link href="http://arxiv.org/abs/2108.02430"/>
        <updated>2021-08-06T00:51:46.850Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose neural networks that tackle the problems of
stability and field-of-view of a Convolutional Neural Network (CNN). As an
alternative to increasing the network's depth or width to improve performance,
we propose integral-based spatially nonlocal operators which are related to
global weighted Laplacian, fractional Laplacian and inverse fractional
Laplacian operators that arise in several problems in the physical sciences.
The forward propagation of such networks is inspired by partial
integro-differential equations (PIDEs). We test the effectiveness of the
proposed neural architectures on benchmark image classification datasets and
semantic segmentation tasks in autonomous driving. Moreover, we investigate the
extra computational costs of these dense operators and the stability of forward
propagation of the proposed neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohn_B/0/1/0/all/0/1"&gt;Bastian Bohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griebel_M/0/1/0/all/0/1"&gt;Michael Griebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannan_D/0/1/0/all/0/1"&gt;Dinesh Kannan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach. (arXiv:2108.02731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02731</id>
        <link href="http://arxiv.org/abs/2108.02731"/>
        <updated>2021-08-06T00:51:46.844Z</updated>
        <summary type="html"><![CDATA[One of the challenges for multi-agent reinforcement learning (MARL) is
designing efficient learning algorithms for a large system in which each agent
has only limited or partial information of the entire system. In this system,
it is desirable to learn policies of a decentralized type. A recent and
promising paradigm to analyze such decentralized MARL is to take network
structures into consideration. While exciting progress has been made to analyze
decentralized MARL with the network of agents, often found in social networks
and team video games, little is known theoretically for decentralized MARL with
the network of states, frequently used for modeling self-driving vehicles,
ride-sharing, and data and traffic routing.

This paper proposes a framework called localized training and decentralized
execution to study MARL with network of states, with homogeneous (a.k.a.
mean-field type) agents. Localized training means that agents only need to
collect local information in their neighboring states during the training
phase; decentralized execution implies that, after the training stage, agents
can execute the learned decentralized policies, which only requires knowledge
of the agents' current states. The key idea is to utilize the homogeneity of
agents and regroup them according to their states, thus the formulation of a
networked Markov decision process with teams of agents, enabling the update of
the Q-function in a localized fashion. In order to design an efficient and
scalable reinforcement learning algorithm under such a framework, we adopt the
actor-critic approach with over-parameterized neural networks, and establish
the convergence and sample complexity for our algorithm, shown to be scalable
with respect to the size of both agents and states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1"&gt;Haotian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaoli Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-08-06T00:51:46.836Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://torchio.rtfd.io/. The package can be installed from the
Python Package Index running 'pip install torchio'. It includes a command-line
interface which allows users to apply transforms to image files without using
Python. Additionally, we provide a graphical interface within a TorchIO
extension in 3D Slicer to visualize the effects of transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Consensus-based Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems. (arXiv:2108.02551v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02551</id>
        <link href="http://arxiv.org/abs/2108.02551"/>
        <updated>2021-08-06T00:51:46.809Z</updated>
        <summary type="html"><![CDATA[Hybrid FSO/RF system requires an efficient FSO and RF link switching
mechanism to improve the system capacity by realizing the complementary
benefits of both the links. The dynamics of network conditions, such as fog,
dust, and sand storms compound the link switching problem and control
complexity. To address this problem, we initiate the study of deep
reinforcement learning (DRL) for link switching of hybrid FSO/RF systems.
Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF
and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under
atmospheric turbulences. To formulate the problem, we define the state, action,
and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates
the deployed policy that interacts with the environment in a hybrid FSO/RF
system, resulting in high switching costs. To overcome this, we lift this
problem to ensemble consensus-based representation learning for deep
reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF
DRL approach uses consensus learned features representations based on an
ensemble of asynchronous threads to update the deployed policy. Experimental
results corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned
features switching achieves better performance than Actor/Critic-FSO/RF,
DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching
cost significantly low.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04005</id>
        <link href="http://arxiv.org/abs/2006.04005"/>
        <updated>2021-08-06T00:51:46.799Z</updated>
        <summary type="html"><![CDATA[In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks, but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., classification accuracy
drop, additional hyperparameters, slower inferences, and collecting extra
data). In the opposite direction, we propose replacing SoftMax loss with a
novel loss function that does not suffer from the mentioned weaknesses. The
proposed IsoMax loss is isotropic (exclusively distance-based) and provides
high entropy posterior probability distributions. Replacing the SoftMax loss by
IsoMax loss requires no model or training changes. Additionally, the models
trained with IsoMax loss produce as fast and energy-efficient inferences as
those trained using SoftMax loss. Moreover, no classification accuracy drop is
observed. The proposed method does not rely on outlier/background data,
hyperparameter tuning, temperature calibration, feature extraction, metric
learning, adversarial training, ensemble procedures, or generative models. Our
experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in
replacement that significantly improves neural networks' OOD detection
performance. Hence, it may be used as a baseline OOD detection approach to be
combined with current or future OOD detection techniques to achieve even higher
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04767</id>
        <link href="http://arxiv.org/abs/2010.04767"/>
        <updated>2021-08-06T00:51:46.783Z</updated>
        <summary type="html"><![CDATA[In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1"&gt;Tanmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1"&gt;Chinmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1"&gt;Sivanathan Kandhasamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Abstraction Error and a Category of Causal Models. (arXiv:2103.15758v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15758</id>
        <link href="http://arxiv.org/abs/2103.15758"/>
        <updated>2021-08-06T00:51:46.730Z</updated>
        <summary type="html"><![CDATA[Interventional causal models describe several joint distributions over some
variables used to describe a system, one for each intervention setting. They
provide a formal recipe for how to move between the different joint
distributions and make predictions about the variables upon intervening on the
system. Yet, it is difficult to formalise how we may change the underlying
variables used to describe the system, say moving from fine-grained to
coarse-grained variables. Here, we argue that compositionality is a desideratum
for such model transformations and the associated errors: When abstracting a
reference model M iteratively, first obtaining M' and then further simplifying
that to obtain M'', we expect the composite transformation from M to M'' to
exist and its error to be bounded by the errors incurred by each individual
transformation step. Category theory, the study of mathematical objects via
compositional transformations between them, offers a natural language to
develop our framework for model transformations and abstractions. We introduce
a category of finite interventional causal models and, leveraging theory of
enriched categories, prove the desired compositionality properties for our
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rischel_E/0/1/0/all/0/1"&gt;Eigil F. Rischel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weichwald_S/0/1/0/all/0/1"&gt;Sebastian Weichwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-08-06T00:51:46.714Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-08-06T00:51:46.693Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;Mohammad Javad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for inverse problems with unknown operator. (arXiv:2108.02744v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02744</id>
        <link href="http://arxiv.org/abs/2108.02744"/>
        <updated>2021-08-06T00:51:46.686Z</updated>
        <summary type="html"><![CDATA[We consider ill-posed inverse problems where the forward operator $T$ is
unknown, and instead we have access to training data consisting of functions
$f_i$ and their noisy images $Tf_i$. This is a practically relevant and
challenging problem which current methods are able to solve only under strong
assumptions on the training set. Here we propose a new method that requires
minimal assumptions on the data, and prove reconstruction rates that depend on
the number of training points and the noise level. We show that, in the regime
of "many" training data, the method is minimax optimal. The proposed method
employs a type of convolutional neural networks (U-nets) and empirical risk
minimization in order to "fit" the unknown operator. In a nutshell, our
approach is based on two ideas: the first is to relate U-nets to multiscale
decompositions such as wavelets, thereby linking them to the existing theory,
and the second is to use the hierarchical structure of U-nets and the low
number of parameters of convolutional neural nets to prove entropy bounds that
are practically useful. A significant difference with the existing works on
neural networks in nonparametric statistics is that we use them to approximate
operators and not functions, which we argue is mathematically more natural and
technically more convenient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alamo_M/0/1/0/all/0/1"&gt;Miguel del Alamo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:46.679Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Trajectory Optimization for Space Vehicle Control. (arXiv:2108.02335v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02335</id>
        <link href="http://arxiv.org/abs/2108.02335"/>
        <updated>2021-08-06T00:51:46.672Z</updated>
        <summary type="html"><![CDATA[Space mission design places a premium on cost and operational efficiency. The
search for new science and life beyond Earth calls for spacecraft that can
deliver scientific payloads to geologically rich yet hazardous landing sites.
At the same time, the last four decades of optimization research have put a
suite of powerful optimization tools at the fingertips of the controls
engineer. As we enter the new decade, optimization theory, algorithms, and
software tooling have reached a critical mass to start seeing serious
application in space vehicle guidance and control systems. This survey paper
provides a detailed overview of recent advances, successes, and promising
directions for optimization-based space vehicle control. The considered
applications include planetary landing, rendezvous and proximity operations,
small body landing, constrained reorientation, endo-atmospheric flight
including ascent and re-entry, and orbit transfer and injection. The primary
focus is on the last ten years of progress, which have seen a veritable rise in
the number of applications using three core technologies: lossless
convexification, sequential convex programming, and model predictive control.
The reader will come away with a well-rounded understanding of the
state-of-the-art in each space vehicle control application, and will be well
positioned to tackle important current open problems using convex optimization
as a core technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Malyuta_D/0/1/0/all/0/1"&gt;Danylo Malyuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Elango_P/0/1/0/all/0/1"&gt;Purnanand Elango&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Acikmese_B/0/1/0/all/0/1"&gt;Behcet Acikmese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Linearized Assignment Flows for Image Labeling. (arXiv:2108.02571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02571</id>
        <link href="http://arxiv.org/abs/2108.02571"/>
        <updated>2021-08-06T00:51:46.665Z</updated>
        <summary type="html"><![CDATA[We introduce a novel algorithm for estimating optimal parameters of
linearized assignment flows for image labeling. An exact formula is derived for
the parameter gradient of any loss function that is constrained by the linear
system of ODEs determining the linearized assignment flow. We show how to
efficiently evaluate this formula using a Krylov subspace and a low-rank
approximation. This enables us to perform parameter learning by Riemannian
gradient descent in the parameter space, without the need to backpropagate
errors or to solve an adjoint equation, in less than 10 seconds for a
$512\times 512$ image using just about $0.5$ GB memory. Experiments demonstrate
that our method performs as good as highly-tuned machine learning software
using automatic differentiation. Unlike methods employing automatic
differentiation, our approach yields a low-dimensional representation of
internal parameters and their dynamics which helps to understand how networks
work and perform that realize assignment flows and generalizations thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeilmann_A/0/1/0/all/0/1"&gt;Alexander Zeilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petra_S/0/1/0/all/0/1"&gt;Stefania Petra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1"&gt;Christoph Schn&amp;#xf6;rr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05231</id>
        <link href="http://arxiv.org/abs/2101.05231"/>
        <updated>2021-08-06T00:51:46.647Z</updated>
        <summary type="html"><![CDATA[This paper considers the use of Robust PCA in a CUR decomposition framework
and applications thereof. Our main algorithms produce a robust version of
column-row factorizations of matrices $\mathbf{D}=\mathbf{L}+\mathbf{S}$ where
$\mathbf{L}$ is low-rank and $\mathbf{S}$ contains sparse outliers. These
methods yield interpretable factorizations at low computational cost, and
provide new CUR decompositions that are robust to sparse outliers, in contrast
to previous methods. We consider two key imaging applications of Robust PCA:
video foreground-background separation and face modeling. This paper examines
the qualitative behavior of our Robust CUR decompositions on the benchmark
videos and face datasets, and find that our method works as well as standard
Robust PCA while being significantly faster. Additionally, we consider hybrid
randomized and deterministic sampling methods which produce a compact CUR
decomposition of a given matrix, and apply this to video sequences to produce
canonical frames thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longxiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1"&gt;Deanna Needell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metric Transforms and Low Rank Matrices via Representation Theory of the Real Hyperrectangle. (arXiv:2011.11503v2 [cs.CG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11503</id>
        <link href="http://arxiv.org/abs/2011.11503"/>
        <updated>2021-08-06T00:51:46.641Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a new technique which we call representation theory
of the real hyperrectangle, which describes how to compute the eigenvectors and
eigenvalues of certain matrices arising from hyperrectangles. We show that
these matrices arise naturally when analyzing a number of different algorithmic
tasks such as kernel methods, neural network training, natural language
processing, and the design of algorithms using the polynomial method. We then
use our new technique along with these connections to prove several new
structural results in these areas, including:

$\bullet$ A function is a positive definite Manhattan kernel if and only if
it is a completely monotone function. These kernels are widely used across
machine learning; one example is the Laplace kernel which is widely used in
machine learning for chemistry.

$\bullet$ A function transforms Manhattan distances to Manhattan distances if
and only if it is a Bernstein function. This completes the theory of Manhattan
to Manhattan metric transforms initiated by Assouad in 1980.

$\bullet$ A function applied entry-wise to any square matrix of rank $r$
always results in a matrix of rank $< 2^{r-1}$ if and only if it is a
polynomial of sufficiently low degree. This gives a converse to a key lemma
used by the polynomial method in algorithm design.

Our work includes a sophisticated combination of techniques from different
fields, including metric embeddings, the polynomial method, and group
representation theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1"&gt;Josh Alman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1"&gt;Timothy Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_G/0/1/0/all/0/1"&gt;Gary Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-08-06T00:51:46.634Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Performance of a NoC-based CNN Accelerator with Gather Support. (arXiv:2108.02567v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02567</id>
        <link href="http://arxiv.org/abs/2108.02567"/>
        <updated>2021-08-06T00:51:46.627Z</updated>
        <summary type="html"><![CDATA[The increasing application of deep learning technology drives the need for an
efficient parallel computing architecture for Convolutional Neural Networks
(CNNs). A significant challenge faced when designing a many-core CNN
accelerator is to handle the data movement between the processing elements. The
CNN workload introduces many-to-one traffic in addition to one-to-one and
one-to-many traffic. As the de-facto standard for on-chip communication,
Network-on-Chip (NoC) can support various unicast and multicast traffic. For
many-to-one traffic, repetitive unicast is employed which is not an efficient
way. In this paper, we propose to use the gather packet on mesh-based NoCs
employing output stationary systolic array in support of many-to-one traffic.
The gather packet will collect the data from the intermediate nodes eventually
leading to the destination efficiently. This method is evaluated using the
traffic traces generated from the convolution layer of AlexNet and VGG-16 with
improvement in the latency and power than the repetitive unicast method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1"&gt;Binayak Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yingtao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthukumar_V/0/1/0/all/0/1"&gt;Venkatesan Muthukumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02563</id>
        <link href="http://arxiv.org/abs/2108.02563"/>
        <updated>2021-08-06T00:51:46.621Z</updated>
        <summary type="html"><![CDATA[This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,
Cheese and Fish based on Image Processing using Computer Vision and Deep
Learning: A Review. It consists of a comprehensive review of image processing,
computer vision and deep learning techniques applied to carry out analysis of
fruits, vegetables, cheese and fish.This part also serves as a literature
review for Part II.Part II: GuavaNet: A deep neural network architecture for
automatic sensory evaluation to predict degree of acceptability for Guava by a
consumer. This part introduces to an end-to-end deep neural network
architecture that can predict the degree of acceptability by the consumer for a
guava based on sensory evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1"&gt;Vipul Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-08-06T00:51:46.615Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VBridge: Connecting the Dots Between Features, Explanations, and Data for Healthcare Models. (arXiv:2108.02550v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.02550</id>
        <link href="http://arxiv.org/abs/2108.02550"/>
        <updated>2021-08-06T00:51:46.597Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is increasingly applied to Electronic Health Records
(EHRs) to solve clinical prediction tasks. Although many ML models perform
promisingly, issues with model transparency and interpretability limit their
adoption in clinical practice. Directly using existing explainable ML
techniques in clinical settings can be challenging. Through literature surveys
and collaborations with six clinicians with an average of 17 years of clinical
experience, we identified three key challenges, including clinicians'
unfamiliarity with ML features, lack of contextual information, and the need
for cohort-level evidence. Following an iterative design process, we further
designed and developed VBridge, a visual analytics tool that seamlessly
incorporates ML explanations into clinicians' decision-making workflow. The
system includes a novel hierarchical display of contribution-based feature
explanations and enriched interactions that connect the dots between ML
features, explanations, and data. We demonstrated the effectiveness of VBridge
through two case studies and expert interviews with four clinicians, showing
that visually associating model explanations with patients' situational records
can help clinicians better interpret and use model predictions when making
clinician decisions. We further derived a list of design implications for
developing future explainable ML tools to support clinical decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1"&gt;Furui Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dongyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_F/0/1/0/all/0/1"&gt;Fan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yanna Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zytek_A/0/1/0/all/0/1"&gt;Alexandra Zytek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haomin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Metamorphic Relations to Verify and Enhance Artcode Classification. (arXiv:2108.02694v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02694</id>
        <link href="http://arxiv.org/abs/2108.02694"/>
        <updated>2021-08-06T00:51:46.590Z</updated>
        <summary type="html"><![CDATA[Software testing is often hindered where it is impossible or impractical to
determine the correctness of the behaviour or output of the software under test
(SUT), a situation known as the oracle problem. An example of an area facing
the oracle problem is automatic image classification, using machine learning to
classify an input image as one of a set of predefined classes. An approach to
software testing that alleviates the oracle problem is metamorphic testing
(MT). While traditional software testing examines the correctness of individual
test cases, MT instead examines the relations amongst multiple executions of
test cases and their outputs. These relations are called metamorphic relations
(MRs): if an MR is found to be violated, then a fault must exist in the SUT.
This paper examines the problem of classifying images containing visually
hidden markers called Artcodes, and applies MT to verify and enhance the
trained classifiers. This paper further examines two MRs, Separation and
Occlusion, and reports on their capability in verifying the image
classification using one-way analysis of variance (ANOVA) in conjunction with
three other statistical analysis methods: t-test (for unequal variances),
Kruskal-Wallis test, and Dunnett's test. In addition to our previously-studied
classifier, that used Random Forests, we introduce a new classifier that uses a
support vector machine, and present its MR-augmented version. Experimental
evaluations across a number of performance metrics show that the augmented
classifiers can achieve better performance than non-augmented classifiers. This
paper also analyses how the enhanced performance is obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Towey_D/0/1/0/all/0/1"&gt;Dave Towey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+French_A/0/1/0/all/0/1"&gt;Andrew French&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benford_S/0/1/0/all/0/1"&gt;Steve Benford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tsong Yueh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02722</id>
        <link href="http://arxiv.org/abs/2108.02722"/>
        <updated>2021-08-06T00:51:46.583Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has revolutionized self-supervised image representation
learning field, and recently been adapted to video domain. One of the greatest
advantages of contrastive learning is that it allows us to flexibly define
powerful loss objectives as long as we can find a reasonable way to formulate
positive and negative samples to contrast. However, existing approaches rely
heavily on the short-range spatiotemporal salience to form clip-level
contrastive signals, thus limit themselves from using global context. In this
paper, we propose a new video-level contrastive learning method based on
segments to formulate positive pairs. Our formulation is able to capture global
context in a video, thus robust to temporal content change. We also incorporate
a temporal order regularization term to enforce the inherent sequential
structure of videos. Extensive experiments show that our video-level
contrastive learning framework (VCLR) is able to outperform previous
state-of-the-arts on five video datasets for downstream action classification,
action localization and video retrieval. Code is available at
https://github.com/amazon-research/video-contrastive-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1"&gt;Haofei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schwertfeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1"&gt;Cyrill Stachniss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLAMP: Stochastic Latent Appearance and Motion Prediction. (arXiv:2108.02760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02760</id>
        <link href="http://arxiv.org/abs/2108.02760"/>
        <updated>2021-08-06T00:51:46.567Z</updated>
        <summary type="html"><![CDATA[Motion is an important cue for video prediction and often utilized by
separating video content into static and dynamic components. Most of the
previous work utilizing motion is deterministic but there are stochastic
methods that can model the inherent uncertainty of the future. Existing
stochastic models either do not reason about motion explicitly or make limiting
assumptions about the static part. In this paper, we reason about appearance
and motion in the video stochastically by predicting the future based on the
motion history. Explicit reasoning about motion without history already reaches
the performance of current stochastic models. The motion history further
improves the results by allowing to predict consistent dynamics several frames
into the future. Our model performs comparably to the state-of-the-art models
on the generic video prediction datasets, however, significantly outperforms
them on two challenging real-world autonomous driving datasets with complex
motion and dynamic background.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1"&gt;Adil Kaan Akan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1"&gt;Fatma G&amp;#xfc;ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02646</id>
        <link href="http://arxiv.org/abs/2108.02646"/>
        <updated>2021-08-06T00:51:46.527Z</updated>
        <summary type="html"><![CDATA[This paper proposes a hypothesis for the aesthetic appreciation that
aesthetic images make a neural network strengthen salient concepts and discard
inessential concepts. In order to verify this hypothesis, we use multi-variate
interactions to represent salient concepts and inessential concepts contained
in images. Furthermore, we design a set of operations to revise images towards
more beautiful ones. In experiments, we find that the revised images are more
aesthetic than the original ones to some extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Haotian Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhengyang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02696</id>
        <link href="http://arxiv.org/abs/2108.02696"/>
        <updated>2021-08-06T00:51:46.521Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning is just at a tipping point where it could really take
off. Among these approaches, contrastive learning has seen tremendous progress
and led to state-of-the-art performance. In this paper, we construct a novel
probabilistic graphical model that effectively incorporates the low rank
promoting prior into the framework of contrastive learning, referred to as
LORAC. In contrast to the existing conventional self-supervised approaches that
only considers independent learning, our hypothesis explicitly requires that
all the samples belonging to the same instance class lie on the same subspace
with small dimension. This heuristic poses particular joint learning
constraints to reduce the degree of freedom of the problem during the search of
the optimal network parameterization. Most importantly, we argue that the low
rank prior employed here is not unique, and many different priors can be
invoked in a similar probabilistic way, corresponding to different hypotheses
about underlying truth behind the contrastive features. Empirical evidences
show that the proposed algorithm clearly surpasses the state-of-the-art
approaches on multiple benchmarks, including image classification, object
detection, instance segmentation and keypoint detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jingyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Qi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures. (arXiv:2108.02565v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02565</id>
        <link href="http://arxiv.org/abs/2108.02565"/>
        <updated>2021-08-06T00:51:46.514Z</updated>
        <summary type="html"><![CDATA[With edge-AI finding an increasing number of real-world applications,
especially in industry, the question of functionally safe applications using AI
has begun to be asked. In this body of work, we explore the issue of achieving
dependable operation of neural networks. We discuss the issue of dependability
in general implementation terms before examining lockstep solutions. We intuit
that it is not necessarily a given that two similar neural networks generate
results at precisely the same time and that synchronization between the
platforms will be required. We perform some preliminary measurements that may
support this intuition and introduce some work in implementing lockstep neural
network engines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doran_H/0/1/0/all/0/1"&gt;Hans Dermot Doran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ielpo_G/0/1/0/all/0/1"&gt;Gianluca Ielpo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganz_D/0/1/0/all/0/1"&gt;David Ganz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zapke_M/0/1/0/all/0/1"&gt;Michael Zapke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Continuous Docking Control of Autonomous Underwater Vehicles: A Benchmarking Study. (arXiv:2108.02665v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02665</id>
        <link href="http://arxiv.org/abs/2108.02665"/>
        <updated>2021-08-06T00:51:46.507Z</updated>
        <summary type="html"><![CDATA[Docking control of an autonomous underwater vehicle (AUV) is a task that is
integral to achieving persistent long term autonomy. This work explores the
application of state-of-the-art model-free deep reinforcement learning (DRL)
approaches to the task of AUV docking in the continuous domain. We provide a
detailed formulation of the reward function, utilized to successfully dock the
AUV onto a fixed docking platform. A major contribution that distinguishes our
work from the previous approaches is the usage of a physics simulator to define
and simulate the underwater environment as well as the DeepLeng AUV. We propose
a new reward function formulation for the docking task, incorporating several
components, that outperforms previous reward formulations. We evaluate proximal
policy optimization (PPO), twin delayed deep deterministic policy gradients
(TD3) and soft actor-critic (SAC) in combination with our reward function. Our
evaluation yielded results that conclusively show the TD3 agent to be most
efficient and consistent in terms of docking the AUV, over multiple evaluation
runs it achieved a 100% success rate and episode return of 10667.1 +- 688.8. We
also show how our reward function formulation improves over the state of the
art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_M/0/1/0/all/0/1"&gt;Mihir Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1"&gt;Bilal Wehbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1"&gt;Matias Valdenegro-Toro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02776</id>
        <link href="http://arxiv.org/abs/2108.02776"/>
        <updated>2021-08-06T00:51:46.498Z</updated>
        <summary type="html"><![CDATA[This paper presents Sinsy, a deep neural network (DNN)-based singing voice
synthesis (SVS) system. In recent years, DNNs have been utilized in statistical
parametric SVS systems, and DNN-based SVS systems have demonstrated better
performance than conventional hidden Markov model-based ones. SVS systems are
required to synthesize a singing voice with pitch and timing that strictly
follow a given musical score. Additionally, singing expressions that are not
described on the musical score, such as vibrato and timing fluctuations, should
be reproduced. The proposed system is composed of four modules: a time-lag
model, a duration model, an acoustic model, and a vocoder, and singing voices
can be synthesized taking these characteristics of singing voices into account.
To better model a singing voice, the proposed system incorporates improved
approaches to modeling pitch and vibrato and better training criteria into the
acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive
neural vocoder with robustness for the pitch, into our systems to generate a
high-fidelity singing voice waveform. Moreover, we propose automatic pitch
correction techniques for DNN-based SVS to synthesize singing voices with
correct pitch even if the training data has out-of-tune phrases. Experimental
results show our system can synthesize a singing voice with better timing, more
natural vibrato, and correct pitch, and it can achieve better mean opinion
scores in subjective evaluation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1"&gt;Yukiya Hono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kei Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1"&gt;Keiichiro Oura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1"&gt;Yoshihiko Nankaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1"&gt;Keiichi Tokuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New State-of-the-Art Transformers-Based Load Forecaster on the Smart Grid Domain. (arXiv:2108.02628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02628</id>
        <link href="http://arxiv.org/abs/2108.02628"/>
        <updated>2021-08-06T00:51:46.475Z</updated>
        <summary type="html"><![CDATA[Meter-level load forecasting is crucial for efficient energy management and
power system planning for Smart Grids (SGs), in tasks associated with
regulation, dispatching, scheduling, and unit commitment of power grids.
Although a variety of algorithms have been proposed and applied on the field,
more accurate and robust models are still required: the overall utility cost of
operations in SGs increases 10 million currency units if the load forecasting
error increases 1%, and the mean absolute percentage error (MAPE) in
forecasting is still much higher than 1%. Transformers have become the new
state-of-the-art in a variety of tasks, including the ones in computer vision,
natural language processing and time series forecasting, surpassing alternative
neural models such as convolutional and recurrent neural networks. In this
letter, we present a new state-of-the-art Transformer-based algorithm for the
meter-level load forecasting task, which has surpassed the former
state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all
experiments by a margin of at least 13% in MAPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Novaes_A/0/1/0/all/0/1"&gt;Andre Luiz Farias Novaes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_R/0/1/0/all/0/1"&gt;Rui Alexandre de Matos Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1"&gt;Jose Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavanelli_L/0/1/0/all/0/1"&gt;Lucas Aguiar Pavanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02671</id>
        <link href="http://arxiv.org/abs/2108.02671"/>
        <updated>2021-08-06T00:51:46.467Z</updated>
        <summary type="html"><![CDATA[Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1"&gt;Julia Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1"&gt;Lazaros Nalpantidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepScanner: a Robotic System for Automated 2D Object Dataset Collection with Annotations. (arXiv:2108.02555v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02555</id>
        <link href="http://arxiv.org/abs/2108.02555"/>
        <updated>2021-08-06T00:51:46.445Z</updated>
        <summary type="html"><![CDATA[In the proposed study, we describe the possibility of automated dataset
collection using an articulated robot. The proposed technology reduces the
number of pixel errors on a polygonal dataset and the time spent on manual
labeling of 2D objects. The paper describes a novel automatic dataset
collection and annotation system, and compares the results of automated and
manual dataset labeling. Our approach increases the speed of data labeling
240-fold, and improves the accuracy compared to manual labeling 13-fold. We
also present a comparison of metrics for training a neural network on a
manually annotated and an automatically collected dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_V/0/1/0/all/0/1"&gt;Valery Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinov_I/0/1/0/all/0/1"&gt;Ivan Kalinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1"&gt;Pavel Karpyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1"&gt;Dzmitry Tsetserukou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement. (arXiv:2108.02505v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.02505</id>
        <link href="http://arxiv.org/abs/2108.02505"/>
        <updated>2021-08-06T00:51:46.419Z</updated>
        <summary type="html"><![CDATA[The evaluation of the impact of using Machine Learning in the management of
softwarized networks is considered in multiple research works. Beyond that, we
propose to evaluate the robustness of online learning for optimal network slice
placement. A major assumption to this study is to consider that slice request
arrivals are non-stationary. In this context, we simulate unpredictable network
load variations and compare two Deep Reinforcement Learning (DRL) algorithms: a
pure DRL-based algorithm and a heuristically controlled DRL as a hybrid
DRL-heuristic algorithm, to assess the impact of these unpredictable changes of
traffic load on the algorithms performance. We conduct extensive simulations of
a large-scale operator infrastructure. The evaluation results show that the
proposed hybrid DRL-heuristic approach is more robust and reliable in case of
unpredictable network load changes than pure DRL as it reduces the performance
degradation. These results are follow-ups for a series of recent research we
have performed showing that the proposed hybrid DRL-heuristic approach is
efficient and more adapted to real network scenarios than pure DRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1"&gt;Jose Jurandir Alves Esteves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1"&gt;Amina Boubendir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1"&gt;Fabrice Guillemin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1"&gt;Pierre Sens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Model-Free Reinforcement Learning for the Automatic Control of a Flexible Wing Aircraft. (arXiv:2108.02393v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.02393</id>
        <link href="http://arxiv.org/abs/2108.02393"/>
        <updated>2021-08-06T00:51:46.400Z</updated>
        <summary type="html"><![CDATA[The control problem of the flexible wing aircraft is challenging due to the
prevailing and high nonlinear deformations in the flexible wing system. This
urged for new control mechanisms that are robust to the real-time variations in
the wing's aerodynamics. An online control mechanism based on a value iteration
reinforcement learning process is developed for flexible wing aerial
structures. It employs a model-free control policy framework and a guaranteed
convergent adaptive learning architecture to solve the system's Bellman
optimality equation. A Riccati equation is derived and shown to be equivalent
to solving the underlying Bellman equation. The online reinforcement learning
solution is implemented using means of an adaptive-critic mechanism. The
controller is proven to be asymptotically stable in the Lyapunov sense. It is
assessed through computer simulations and its superior performance is
demonstrated on two scenarios under different operating conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1"&gt;Mohammed Abouheaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1"&gt;Wail Gueaieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lewis_F/0/1/0/all/0/1"&gt;Frank Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRL-based Slice Placement Under Non-Stationary Conditions. (arXiv:2108.02495v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.02495</id>
        <link href="http://arxiv.org/abs/2108.02495"/>
        <updated>2021-08-06T00:51:46.393Z</updated>
        <summary type="html"><![CDATA[We consider online learning for optimal network slice placement under the
assumption that slice requests arrive according to a non-stationary Poisson
process. We propose a framework based on Deep Reinforcement Learning (DRL)
combined with a heuristic to design algorithms. We specifically design two
pure-DRL algorithms and two families of hybrid DRL-heuristic algorithms. To
validate their performance, we perform extensive simulations in the context of
a large-scale operator infrastructure. The evaluation results show that the
proposed hybrid DRL-heuristic algorithms require three orders of magnitude of
learning episodes less than pure-DRL to achieve convergence. This result
indicates that the proposed hybrid DRL-heuristic approach is more reliable than
pure-DRL in a real non-stationary network scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1"&gt;Jose Jurandir Alves Esteves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1"&gt;Amina Boubendir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1"&gt;Fabrice Guillemin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1"&gt;Pierre Sens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02646</id>
        <link href="http://arxiv.org/abs/2108.02646"/>
        <updated>2021-08-06T00:51:46.386Z</updated>
        <summary type="html"><![CDATA[This paper proposes a hypothesis for the aesthetic appreciation that
aesthetic images make a neural network strengthen salient concepts and discard
inessential concepts. In order to verify this hypothesis, we use multi-variate
interactions to represent salient concepts and inessential concepts contained
in images. Furthermore, we design a set of operations to revise images towards
more beautiful ones. In experiments, we find that the revised images are more
aesthetic than the original ones to some extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Haotian Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhengyang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PI3NN: Prediction intervals from three independently trained neural networks. (arXiv:2108.02327v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02327</id>
        <link href="http://arxiv.org/abs/2108.02327"/>
        <updated>2021-08-06T00:51:46.379Z</updated>
        <summary type="html"><![CDATA[We propose a novel prediction interval method to learn prediction mean
values, lower and upper bounds of prediction intervals from three independently
trained neural networks only using the standard mean squared error (MSE) loss,
for uncertainty quantification in regression tasks. Our method requires no
distributional assumption on data, does not introduce unusual hyperparameters
to either the neural network models or the loss function. Moreover, our method
can effectively identify out-of-distribution samples and reasonably quantify
their uncertainty. Numerical experiments on benchmark regression problems show
that our method outperforms the state-of-the-art methods with respect to
predictive uncertainty quality, robustness, and identification of
out-of-distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Dan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guannan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction. (arXiv:2108.02424v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.02424</id>
        <link href="http://arxiv.org/abs/2108.02424"/>
        <updated>2021-08-06T00:51:46.373Z</updated>
        <summary type="html"><![CDATA[Accurate forecasting of traffic conditions is critical for improving safety,
stability, and efficiency of a city transportation system. In reality, it is
challenging to produce accurate traffic forecasts due to the complex and
dynamic spatiotemporal correlations. Most existing works only consider partial
characteristics and features of traffic data, and result in unsatisfactory
performances on modeling and forecasting. In this paper, we propose a periodic
spatial-temporal deep neural network (PSTN) with three pivotal modules to
improve the forecasting performance of traffic conditions through a novel
integration of three types of information. First, the historical traffic
information is folded and fed into a module consisting of a graph convolutional
network and a temporal convolutional network. Second, the recent traffic
information together with the historical output passes through the second
module consisting of a graph convolutional network and a gated recurrent unit
framework. Finally, a multi-layer perceptron is applied to process the
auxiliary road attributes and output the final predictions. Experimental
results on two publicly accessible real-world urban traffic data sets show that
the proposed PSTN outperforms the state-of-the-art benchmarks by significant
margins for short-term traffic conditions forecasting]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redatuming physical systems using symmetric autoencoders. (arXiv:2108.02537v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2108.02537</id>
        <link href="http://arxiv.org/abs/2108.02537"/>
        <updated>2021-08-06T00:51:46.367Z</updated>
        <summary type="html"><![CDATA[This paper considers physical systems described by hidden states and
indirectly observed through repeated measurements corrupted by unmodeled
nuisance parameters. A network-based representation learns to disentangle the
coherent information (relative to the state) from the incoherent nuisance
information (relative to the sensing). Instead of physical models, the
representation uses symmetry and stochastic regularization to inform an
autoencoder architecture called SymAE. It enables redatuming, i.e., creating
virtual data instances where the nuisances are uniformized across measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bharadwaj_P/0/1/0/all/0/1"&gt;Pawan Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Li_M/0/1/0/all/0/1"&gt;Matthew Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Demanet_L/0/1/0/all/0/1"&gt;Laurent Demanet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations. (arXiv:2108.02318v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02318</id>
        <link href="http://arxiv.org/abs/2108.02318"/>
        <updated>2021-08-06T00:51:46.348Z</updated>
        <summary type="html"><![CDATA[Deep learning has an increasing impact to assist research, allowing, for
example, the discovery of novel materials. Until now, however, these artificial
intelligence techniques have fallen short of discovering the full differential
equation of an experimental physical system. Here we show that a dynamical
neural network, trained on a minimal amount of data, can predict the behavior
of spintronic devices with high accuracy and an extremely efficient simulation
time, compared to the micromagnetic simulations that are usually employed to
model them. For this purpose, we re-frame the formalism of Neural Ordinary
Differential Equations (ODEs) to the constraints of spintronics: few measured
outputs, multiple inputs and internal parameters. We demonstrate with
Spin-Neural ODEs an acceleration factor over 200 compared to micromagnetic
simulations for a complex problem -- the simulation of a reservoir computer
made of magnetic skyrmions (20 minutes compared to three days). In a second
realization, we show that we can predict the noisy response of experimental
spintronic nano-oscillators to varying inputs after training Spin-Neural ODEs
on five milliseconds of their measured response to different excitations.
Spin-Neural ODE is a disruptive tool for developing spintronic applications in
complement to micromagnetic simulations, which are time-consuming and cannot
fit experiments when noise or imperfections are present. Spin-Neural ODE can
also be generalized to other electronic devices involving dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1"&gt;Flavio Abreu Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riou_M/0/1/0/all/0/1"&gt;Mathieu Riou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torrejon_J/0/1/0/all/0/1"&gt;Jacob Torrejon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravelosona_D/0/1/0/all/0/1"&gt;Dafin&amp;#xe9; Ravelosona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weisheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grollier_J/0/1/0/all/0/1"&gt;Julie Grollier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1"&gt;Damien Querlioz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks. (arXiv:2108.02517v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.02517</id>
        <link href="http://arxiv.org/abs/2108.02517"/>
        <updated>2021-08-06T00:51:46.337Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has evolved as a promising technique to handle
distributed machine learning across edge devices. A single neural network (NN)
that optimises a global objective is generally learned in most work in FL,
which could be suboptimal for edge devices. Although works finding a NN
personalised for edge device specific tasks exist, they lack generalisation
and/or convergence guarantees. In this paper, a novel communication efficient
FL algorithm for personalised learning in a wireless setting with guarantees is
presented. The algorithm relies on finding a ``better`` empirical estimate of
losses at each device, using a weighted average of the losses across different
devices. It is devised from a Probably Approximately Correct (PAC) bound on the
true loss in terms of the proposed empirical loss and is bounded by (i) the
Rademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a
signed gradient feedback to find a personalised NN at each device, it is also
proven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate
of the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed
algorithm outperforms locally trained devices as well as the conventionally
used FedAvg and FedSGD algorithms under practical SNR regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahara_S/0/1/0/all/0/1"&gt;Sawan Singh Mahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M%2E_S/0/1/0/all/0/1"&gt;Shruti M.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharath_B/0/1/0/all/0/1"&gt;B. N. Bharath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors. (arXiv:2001.04627v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.04627</id>
        <link href="http://arxiv.org/abs/2001.04627"/>
        <updated>2021-08-06T00:51:46.237Z</updated>
        <summary type="html"><![CDATA[In this paper, we build on a concept of self-supervision by taking RGB frames
as input to learn to predict both action concepts and auxiliary descriptors
e.g., object descriptors. So-called hallucination streams are trained to
predict auxiliary cues, simultaneously fed into classification layers, and then
hallucinated at the testing stage to aid network. We design and hallucinate two
descriptors, one leveraging four popular object detectors applied to training
videos, and the other leveraging image- and video-level saliency detectors. The
first descriptor encodes the detector- and ImageNet-wise class prediction
scores, confidence scores, and spatial locations of bounding boxes and frame
indexes to capture the spatio-temporal distribution of features per video.
Another descriptor encodes spatio-angular gradient distributions of saliency
maps and intensity patterns. Inspired by the characteristic function of the
probability distribution, we capture four statistical moments on the above
intermediate descriptors. As numbers of coefficients in the mean, covariance,
coskewness and cokurtotsis grow linearly, quadratically, cubically and
quartically w.r.t. the dimension of feature vectors, we describe the covariance
matrix by its leading n' eigenvectors (so-called subspace) and we capture
skewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of
the art on five popular datasets such as Charades and EPIC-Kitchens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Analysis of Learning-Based MPC with Partially-Unknown Cost Function. (arXiv:2108.02307v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02307</id>
        <link href="http://arxiv.org/abs/2108.02307"/>
        <updated>2021-08-06T00:51:46.229Z</updated>
        <summary type="html"><![CDATA[The exploration/exploitation trade-off is an inherent challenge in
data-driven and adaptive control. Though this trade-off has been studied for
multi-armed bandits, reinforcement learning (RL) for finite Markov chains, and
RL for linear control systems; it is less well-studied for learning-based
control of nonlinear control systems. A significant theoretical challenge in
the nonlinear setting is that, unlike the linear case, there is no explicit
characterization of an optimal controller for a given set of cost and system
parameters. We propose in this paper the use of a finite-horizon oracle
controller with perfect knowledge of all system parameters as a reference for
optimal control actions. First, this allows us to propose a new regret notion
with respect to this oracle finite-horizon controller. Second, this allows us
to develop learning-based policies that we prove achieve low regret (i.e.,
square-root regret up to a log-squared factor) with respect to this oracle
finite-horizon controller. This policy is developed in the context of
learning-based model predictive control (LBMPC). We conduct a statistical
analysis to prove finite sample concentration bounds for the estimation step of
our policy, and then we perform a control-theoretic analysis using techniques
from MPC- and optimization-theory to show this policy ensures closed-loop
stability and achieves low regret. We conclude with numerical experiments on a
model of heating, ventilation, and air-conditioning (HVAC) systems that show
the low regret of our policy in a setting where the cost function is
partially-unknown to the controller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Dogan_I/0/1/0/all/0/1"&gt;Ilgin Dogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuo-Jun Max Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Aswani_A/0/1/0/all/0/1"&gt;Anil Aswani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-phase Liver Tumor Segmentation with Spatial Aggregation and Uncertain Region Inpainting. (arXiv:2108.00911v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00911</id>
        <link href="http://arxiv.org/abs/2108.00911"/>
        <updated>2021-08-06T00:51:46.221Z</updated>
        <summary type="html"><![CDATA[Multi-phase computed tomography (CT) images provide crucial complementary
information for accurate liver tumor segmentation (LiTS). State-of-the-art
multi-phase LiTS methods usually fused cross-phase features through
phase-weighted summation or channel-attention based concatenation. However,
these methods ignored the spatial (pixel-wise) relationships between different
phases, hence leading to insufficient feature integration. In addition, the
performance of existing methods remains subject to the uncertainty in
segmentation, which is particularly acute in tumor boundary regions. In this
work, we propose a novel LiTS method to adequately aggregate multi-phase
information and refine uncertain region segmentation. To this end, we introduce
a spatial aggregation module (SAM), which encourages per-pixel interactions
between different phases, to make full use of cross-phase information.
Moreover, we devise an uncertain region inpainting module (URIM) to refine
uncertain pixels using neighboring discriminative features. Experiments on an
in-house multi-phase CT dataset of focal liver lesions (MPCT-FLLs) demonstrate
that our method achieves promising liver tumor segmentation and outperforms
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huimin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1"&gt;Ruofeng Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lanfen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingsong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongjie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiyi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in Multimodal Language Learning from Simulation. (arXiv:2108.02319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02319</id>
        <link href="http://arxiv.org/abs/2108.02319"/>
        <updated>2021-08-06T00:51:46.200Z</updated>
        <summary type="html"><![CDATA[Neural networks can be powerful function approximators, which are able to
model high-dimensional feature distributions from a subset of examples drawn
from the target distribution. Naturally, they perform well at generalizing
within the limits of their target function, but they often fail to generalize
outside of the explicitly learned feature space. It is therefore an open
research topic whether and how neural network-based architectures can be
deployed for systematic reasoning. Many studies have shown evidence for poor
generalization, but they often work with abstract data or are limited to
single-channel input. Humans, however, learn and interact through a combination
of multiple sensory modalities, and rarely rely on just one. To investigate
compositional generalization in a multimodal setting, we generate an extensible
dataset with multimodal input sequences from simulation. We investigate the
influence of the underlying training data distribution on compostional
generalization in a minimal LSTM-based network trained in a supervised, time
continuous setting. We find compositional generalization to fail in simple
setups while improving with the number of objects, actions, and particularly
with a lot of color overlaps between objects. Furthermore, multimodality
strongly improves compositional generalization in settings where a pure vision
model struggles to generalize.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eisermann_A/0/1/0/all/0/1"&gt;Aaron Eisermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Cornelius Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Stable neural networks: large-width asymptotics and convergence rates. (arXiv:2108.02316v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02316</id>
        <link href="http://arxiv.org/abs/2108.02316"/>
        <updated>2021-08-06T00:51:46.193Z</updated>
        <summary type="html"><![CDATA[In modern deep learning, there is a recent and growing literature on the
interplay between large-width asymptotics for deep Gaussian neural networks
(NNs), i.e. deep NNs with Gaussian-distributed weights, and classes of Gaussian
stochastic processes (SPs). Such an interplay has proved to be critical in
several contexts of practical interest, e.g. Bayesian inference under Gaussian
SP priors, kernel regression for infinite-wide deep NNs trained via gradient
descent, and information propagation within infinite-wide NNs. Motivated by
empirical analysis, showing the potential of replacing Gaussian distributions
with Stable distributions for the NN's weights, in this paper we investigate
large-width asymptotics for (fully connected) feed-forward deep Stable NNs,
i.e. deep NNs with Stable-distributed weights. First, we show that as the width
goes to infinity jointly over the NN's layers, a suitable rescaled deep Stable
NN converges weakly to a Stable SP whose distribution is characterized
recursively through the NN's layers. Because of the non-triangular NN's
structure, this is a non-standard asymptotic problem, to which we propose a
novel and self-contained inductive approach, which may be of independent
interest. Then, we establish sup-norm convergence rates of a deep Stable NN to
a Stable SP, quantifying the critical difference between the settings of
``joint growth" and ``sequential growth" of the width over the NN's layers. Our
work extends recent results on infinite-wide limits for deep Gaussian NNs to
the more general deep Stable NNs, providing the first result on convergence
rates for infinite-wide deep NNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1"&gt;Stefano Favaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortini_S/0/1/0/all/0/1"&gt;Sandra Fortini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peluchetti_S/0/1/0/all/0/1"&gt;Stefano Peluchetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aspis: A Robust Detection System for Distributed Learning. (arXiv:2108.02416v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02416</id>
        <link href="http://arxiv.org/abs/2108.02416"/>
        <updated>2021-08-06T00:51:46.184Z</updated>
        <summary type="html"><![CDATA[State of the art machine learning models are routinely trained on large scale
distributed clusters. Crucially, such systems can be compromised when some of
the computing devices exhibit abnormal (Byzantine) behavior and return
arbitrary results to the parameter server (PS). This behavior may be attributed
to a plethora of reasons including system failures and orchestrated attacks.
Existing work suggests robust aggregation and/or computational redundancy to
alleviate the effect of distorted gradients. However, most of these schemes are
ineffective when an adversary knows the task assignment and can judiciously
choose the attacked workers to induce maximal damage. Our proposed method Aspis
assigns gradient computations to worker nodes using a subset-based assignment
which allows for multiple consistency checks on the behavior of a worker node.
Examination of the calculated gradients and post-processing (clique-finding in
an appropriately constructed graph) by the central node allows for efficient
detection and subsequent exclusion of adversaries from the training process. We
prove the Byzantine resilience and detection guarantees of Aspis under weak and
strong attacks and extensively evaluate the system on various large-scale
training scenarios. The main metric for our experiments is the test accuracy
for which we demonstrate significant improvement of about 30% compared to many
state-of-the-art approaches on the CIFAR-10 dataset. The corresponding
reduction of the fraction of corrupted gradients ranges from 16% to 98%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinidis_K/0/1/0/all/0/1"&gt;Konstantinos Konstantinidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1"&gt;Aditya Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Reinforcement Learning over MDPs. (arXiv:2108.02323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02323</id>
        <link href="http://arxiv.org/abs/2108.02323"/>
        <updated>2021-08-06T00:51:46.175Z</updated>
        <summary type="html"><![CDATA[The past decade has seen the rapid development of Reinforcement Learning,
which acquires impressive performance with numerous training resources.
However, one of the greatest challenges in RL is generalization efficiency
(i.e., generalization performance in a unit time). This paper proposes a
framework of Active Reinforcement Learning (ARL) over MDPs to improve
generalization efficiency in a limited resource by instance selection. Given a
number of instances, the algorithm chooses out valuable instances as training
sets while training the policy, thereby costing fewer resources. Unlike
existing approaches, we attempt to actively select and use training data rather
than train on all the given data, thereby costing fewer resources. Furthermore,
we introduce a general instance evaluation metrics and selection mechanism into
the framework. Experiments results reveal that the proposed framework with
Proximal Policy Optimization as policy optimizer can effectively improve
generalization efficiency than unselect-ed and unbiased selected methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration. (arXiv:2108.02313v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02313</id>
        <link href="http://arxiv.org/abs/2108.02313"/>
        <updated>2021-08-06T00:51:46.168Z</updated>
        <summary type="html"><![CDATA[Modern hardware design trends have shifted towards specialized hardware
acceleration for computationally intensive tasks like machine learning and
computer vision. While these complex workloads can be accelerated by commercial
GPUs, domain-specific hardware is far more optimal when needing to meet the
stringent memory, throughput, and power constraints of mobile and embedded
devices. This paper proposes and evaluates a Binary-Enabled Architecture for
Neural Network Acceleration (BEANNA), a neural network hardware accelerator
capable of processing both floating point and binary network layers. Through
the use of a novel 16x16 systolic array based matrix multiplier with processing
elements that compute both floating point and binary multiply-adds, BEANNA
seamlessly switches between high precision floating point and binary neural
network layers. Running at a clock speed of 100MHz, BEANNA achieves a peak
throughput of 52.8 GigaOps/second when operating in high precision mode, and
820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was
performed by comparing a hybrid network with floating point outer layers and
binary hidden layers to a network with only floating point layers. The hybrid
network accelerated using BEANNA achieved a 194% throughput increase, a 68%
memory usage decrease, and a 66% energy consumption decrease per inference, all
this at the cost of a mere 0.23% classification accuracy decrease on the MNIST
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terrill_C/0/1/0/all/0/1"&gt;Caleb Terrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1"&gt;Fred Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design. (arXiv:2108.02548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02548</id>
        <link href="http://arxiv.org/abs/2108.02548"/>
        <updated>2021-08-06T00:51:46.148Z</updated>
        <summary type="html"><![CDATA[Head shapes play an important role in 3D character design. In this work, we
propose SimpModeling, a novel sketch-based system for helping users, especially
amateur users, easily model 3D animalmorphic heads - a prevalent kind of heads
in character design. Although sketching provides an easy way to depict desired
shapes, it is challenging to infer dense geometric information from sparse line
drawings. Recently, deepnet-based approaches have been taken to address this
challenge and try to produce rich geometric details from very few strokes.
However, while such methods reduce users' workload, they would cause less
controllability of target shapes. This is mainly due to the uncertainty of the
neural prediction. Our system tackles this issue and provides good
controllability from three aspects: 1) we separate coarse shape design and
geometric detail specification into two stages and respectively provide
different sketching means; 2) in coarse shape designing, sketches are used for
both shape inference and geometric constraints to determine global geometry,
and in geometric detail crafting, sketches are used for carving surface
details; 3) in both stages, we use the advanced implicit-based shape inference
methods, which have strong ability to handle the domain gap between freehand
sketches and synthetic ones used for training. Experimental results confirm the
effectiveness of our method and the usability of our interactive system. We
also contribute to a dataset of high-quality 3D animal heads, which are
manually created by artists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhongjin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Heming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1"&gt;Dong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongbo Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02223</id>
        <link href="http://arxiv.org/abs/2108.02223"/>
        <updated>2021-08-06T00:51:46.030Z</updated>
        <summary type="html"><![CDATA[Deep learning based analysis of histopathology images shows promise in
advancing the understanding of tumor progression, tumor micro-environment, and
their underpinning biological processes. So far, these approaches have focused
on extracting information associated with annotations. In this work, we ask how
much information can be learned from the tissue architecture itself.

We present an adversarial learning model to extract feature representations
of cancer tissue, without the need for manual annotations. We show that these
representations are able to identify a variety of morphological characteristics
across three cancer types: Breast, colon, and lung. This is supported by 1) the
separation of morphologic characteristics in the latent space; 2) the ability
to classify tissue type with logistic regression using latent representations,
with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)
the ability to predict the presence of tumor in Whole Slide Images (WSIs) using
multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.

Our results show that our model captures distinct phenotypic characteristics
of real tissue samples, paving the way for further understanding of tumor
progression and tumor micro-environment, and ultimately refining
histopathological classification for diagnosis and treatment. The code and
pretrained models are available at:
https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1"&gt;Adalberto Claudio Quiros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1"&gt;Nicolas Coudray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1"&gt;Anna Yeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1"&gt;Wisuwat Sunhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1"&gt;Roderick Murray-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1"&gt;Aristotelis Tsirigos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Ke Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12014</id>
        <link href="http://arxiv.org/abs/2106.12014"/>
        <updated>2021-08-06T00:51:46.021Z</updated>
        <summary type="html"><![CDATA[Detailed analysis of seizure semiology, the symptoms and signs which occur
during a seizure, is critical for management of epilepsy patients. Inter-rater
reliability using qualitative visual analysis is often poor for semiological
features. Therefore, automatic and quantitative analysis of video-recorded
seizures is needed for objective assessment.

We present GESTURES, a novel architecture combining convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) to learn deep
representations of arbitrarily long videos of epileptic seizures.

We use a spatiotemporal CNN (STCNN) pre-trained on large human action
recognition (HAR) datasets to extract features from short snippets (approx. 0.5
s) sampled from seizure videos. We then train an RNN to learn seizure-level
representations from the sequence of features.

We curated a dataset of seizure videos from 68 patients and evaluated
GESTURES on its ability to classify seizures into focal onset seizures (FOSs)
(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),
obtaining an accuracy of 98.9% using bidirectional long short-term memory
(BLSTM) units.

We demonstrate that an STCNN trained on a HAR dataset can be used in
combination with an RNN to accurately represent arbitrarily long videos of
seizures. GESTURES can provide accurate seizure classification by modeling
sequences of semiologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Catherine Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1"&gt;Beate Diehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04005</id>
        <link href="http://arxiv.org/abs/2006.04005"/>
        <updated>2021-08-06T00:51:46.014Z</updated>
        <summary type="html"><![CDATA[In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks, but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., classification accuracy
drop, additional hyperparameters, slower inferences, and collecting extra
data). In the opposite direction, we propose replacing SoftMax loss with a
novel loss function that does not suffer from the mentioned weaknesses. The
proposed IsoMax loss is isotropic (exclusively distance-based) and provides
high entropy posterior probability distributions. Replacing the SoftMax loss by
IsoMax loss requires no model or training changes. Additionally, the models
trained with IsoMax loss produce as fast and energy-efficient inferences as
those trained using SoftMax loss. Moreover, no classification accuracy drop is
observed. The proposed method does not rely on outlier/background data,
hyperparameter tuning, temperature calibration, feature extraction, metric
learning, adversarial training, ensemble procedures, or generative models. Our
experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in
replacement that significantly improves neural networks' OOD detection
performance. Hence, it may be used as a baseline OOD detection approach to be
combined with current or future OOD detection techniques to achieve even higher
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-06T00:51:46.007Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully-connected structure makes LSTM networks a
memory-bounded algorithm. Previous work in LSTM accelerators either exploited
weight spatial sparsity or temporal sparsity. In this paper, we present a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity was induced using our
proposed pruning method called Column-Balanced Targeted Dropout (CBTD) that
leads to structured sparse weight matrices benefiting workload balance. It
achieved up to 96% weight sparsity with negligible accuracy difference for an
LSTM network trained on a TIMIT phone recognition task. To induce temporal
sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU
method to the LSTM network. This combined sparsity saves on weight memory
access and associated arithmetic operations simultaneously. Spartus was
implemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single
DeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved
9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which
are respectively 4X and 7X higher than the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction. (arXiv:2103.12091v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12091</id>
        <link href="http://arxiv.org/abs/2103.12091"/>
        <updated>2021-08-06T00:51:45.988Z</updated>
        <summary type="html"><![CDATA[While convolutional neural networks have shown a tremendous impact on various
computer vision tasks, they generally demonstrate limitations in explicitly
modeling long-range dependencies due to the intrinsic locality of the
convolution operation. Initially designed for natural language processing
tasks, Transformers have emerged as alternative architectures with innate
global self-attention mechanisms to capture long-range dependencies. In this
paper, we propose TransDepth, an architecture that benefits from both
convolutional neural networks and transformers. To avoid the network losing its
ability to capture local-level details due to the adoption of transformers, we
propose a novel decoder that employs attention mechanisms based on gates.
Notably, this is the first paper that applies transformers to pixel-wise
prediction problems involving continuous labels (i.e., monocular depth
prediction and surface normal estimation). Extensive experiments demonstrate
that the proposed TransDepth achieves state-of-the-art performance on three
challenging datasets. Our code is available at:
https://github.com/ygjwd12345/TransDepth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanglei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Mingli Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-08-06T00:51:45.914Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-08-06T00:51:45.884Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://torchio.rtfd.io/. The package can be installed from the
Python Package Index running 'pip install torchio'. It includes a command-line
interface which allows users to apply transforms to image files without using
Python. Additionally, we provide a graphical interface within a TorchIO
extension in 3D Slicer to visualize the effects of transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-08-06T00:51:45.876Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00531</id>
        <link href="http://arxiv.org/abs/2104.00531"/>
        <updated>2021-08-06T00:51:45.858Z</updated>
        <summary type="html"><![CDATA[While most neural video codecs address P-frame coding (predicting each frame
from past ones), in this paper we address B-frame compression (predicting
frames using both past and future reference frames). Our B-frame solution is
based on the existing P-frame methods. As a result, B-frame coding capability
can easily be added to an existing neural codec. The basic idea of our B-frame
coding method is to interpolate the two reference frames to generate a single
reference frame and then use it together with an existing P-frame codec to
encode the input B-frame. Our studies show that the interpolated frame is a
much better reference for the P-frame codec compared to using the previous
frame as is usually done. Our results show that using the proposed method with
an existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG
dataset compared to the P-frame codec while generating the same video quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco S Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XVFI: eXtreme Video Frame Interpolation. (arXiv:2103.16206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16206</id>
        <link href="http://arxiv.org/abs/2103.16206"/>
        <updated>2021-08-06T00:51:45.850Z</updated>
        <summary type="html"><![CDATA[In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000
fps with the extreme motion to the research community for video frame
interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that
first handles the VFI for 4K videos with large motion. The XVFI-Net is based on
a recursive multi-scale shared structure that consists of two cascaded modules
for bidirectional optical flow learning between two input frames (BiOF-I) and
for bidirectional optical flow learning from target to input frames (BiOF-T).
The optical flows are stably approximated by a complementary flow reversal
(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start
at any scale of input while the BiOF-T module only operates at the original
input scale so that the inference can be accelerated while maintaining highly
accurate VFI performance. Extensive experimental results show that our XVFI-Net
can successfully capture the essential information of objects with extremely
large motions and complex textures while the state-of-the-art methods exhibit
poor performance. Furthermore, our XVFI-Net framework also performs comparably
on the previous lower resolution benchmark dataset, which shows a robustness of
our algorithm as well. All source codes, pre-trained models, and proposed
X4K1000FPS datasets are publicly available at
https://github.com/JihyongOh/XVFI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sim_H/0/1/0/all/0/1"&gt;Hyeonjun Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jihyong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Munchurl Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.10861</id>
        <link href="http://arxiv.org/abs/2101.10861"/>
        <updated>2021-08-06T00:51:45.844Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) learn representation from data with an impressive
capability, and brought important breakthroughs for processing images,
time-series, natural language, audio, video, and many others. In the remote
sensing field, surveys and literature revisions specifically involving DNNs
algorithms' applications have been conducted in an attempt to summarize the
amount of information produced in its subfields. Recently, Unmanned Aerial
Vehicles (UAV) based applications have dominated aerial sensing research.
However, a literature revision that combines both "deep learning" and "UAV
remote sensing" thematics has not yet been conducted. The motivation for our
work was to present a comprehensive review of the fundamentals of Deep Learning
(DL) applied in UAV-based imagery. We focused mainly on describing
classification and regression techniques used in recent applications with
UAV-acquired data. For that, a total of 232 papers published in international
scientific journal databases was examined. We gathered the published material
and evaluated their characteristics regarding application, sensor, and
technique used. We relate how DL presents promising results and has the
potential for processing tasks associated with UAV-based image data. Lastly, we
project future perspectives, commentating on prominent DL paths to be explored
in the UAV remote sensing field. Our revision consists of a friendly-approach
to introduce, commentate, and summarize the state-of-the-art in UAV-based image
applications with DNNs algorithms in diverse subfields of remote sensing,
grouping it in the environmental, urban, and agricultural contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osco_L/0/1/0/all/0/1"&gt;Lucas Prado Osco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Marcato Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1"&gt;Ana Paula Marques Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorge_L/0/1/0/all/0/1"&gt;L&amp;#xfa;cio Andr&amp;#xe9; de Castro Jorge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatholahi_S/0/1/0/all/0/1"&gt;Sarah Narges Fatholahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jonathan de Andrade Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_E/0/1/0/all/0/1"&gt;Edson Takashi Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pistori_H/0/1/0/all/0/1"&gt;Hemerson Pistori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goncalves_W/0/1/0/all/0/1"&gt;Wesley Nunes Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jonathan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02743</id>
        <link href="http://arxiv.org/abs/2108.02743"/>
        <updated>2021-08-06T00:51:45.837Z</updated>
        <summary type="html"><![CDATA[Recent developments in fluorescence microscopy allow capturing
high-resolution 3D images over time for living model organisms. To be able to
image even large specimens, techniques like multi-view light-sheet imaging
record different orientations at each time point that can then be fused into a
single high-quality volume. Based on measured point spread functions (PSF),
deconvolution and content fusion are able to largely revert the inevitable
degradation occurring during the imaging process. Classical multi-view
deconvolution and fusion methods mainly use iterative procedures and
content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been
deployed to approach 3D single-view deconvolution microscopy, but the
multi-view case waits to be studied. We investigated the efficacy of CNN-based
multi-view deconvolution and fusion with two synthetic data sets that mimic
developing embryos and involve either two or four complementary 3D views.
Compared with classical state-of-the-art methods, the proposed semi- and
self-supervised models achieve competitive and superior deconvolution and
fusion quality in the two-view and quad-view cases, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Canyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1"&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1"&gt;Johannes Stegmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08905</id>
        <link href="http://arxiv.org/abs/2106.08905"/>
        <updated>2021-08-06T00:51:45.826Z</updated>
        <summary type="html"><![CDATA[Recent deep generative models have achieved promising performance in image
inpainting. However, it is still very challenging for a neural network to
generate realistic image details and textures, due to its inherent spectral
bias. By our understanding of how artists work, we suggest to adopt a
`structure first detail next' workflow for image inpainting. To this end, we
propose to build a Pyramid Generator by stacking several sub-generators, where
lower-layer sub-generators focus on restoring image structures while the
higher-layer sub-generators emphasize image details. Given an input image, it
will be gradually restored by going through the entire pyramid in a bottom-up
fashion. Particularly, our approach has a learning scheme of progressively
increasing hole size, which allows it to restore large-hole images. In
addition, our method could fully exploit the benefits of learning with
high-resolution images, and hence is suitable for high-resolution image
inpainting. Extensive experimental results on benchmark datasets have validated
the effectiveness of our approach compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuyi Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1"&gt;Gadi Zimerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05231</id>
        <link href="http://arxiv.org/abs/2101.05231"/>
        <updated>2021-08-06T00:51:45.810Z</updated>
        <summary type="html"><![CDATA[This paper considers the use of Robust PCA in a CUR decomposition framework
and applications thereof. Our main algorithms produce a robust version of
column-row factorizations of matrices $\mathbf{D}=\mathbf{L}+\mathbf{S}$ where
$\mathbf{L}$ is low-rank and $\mathbf{S}$ contains sparse outliers. These
methods yield interpretable factorizations at low computational cost, and
provide new CUR decompositions that are robust to sparse outliers, in contrast
to previous methods. We consider two key imaging applications of Robust PCA:
video foreground-background separation and face modeling. This paper examines
the qualitative behavior of our Robust CUR decompositions on the benchmark
videos and face datasets, and find that our method works as well as standard
Robust PCA while being significantly faster. Additionally, we consider hybrid
randomized and deterministic sampling methods which produce a compact CUR
decomposition of a given matrix, and apply this to video sequences to produce
canonical frames thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longxiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1"&gt;Deanna Needell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02703</id>
        <link href="http://arxiv.org/abs/2101.02703"/>
        <updated>2021-08-06T00:51:45.778Z</updated>
        <summary type="html"><![CDATA[While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1"&gt;Lihua Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Visual Pretraining with Contrastive Detection. (arXiv:2103.10957v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10957</id>
        <link href="http://arxiv.org/abs/2103.10957"/>
        <updated>2021-08-06T00:51:45.764Z</updated>
        <summary type="html"><![CDATA[Self-supervised pretraining has been shown to yield powerful representations
for transfer learning. These performance gains come at a large computational
cost however, with state-of-the-art methods requiring an order of magnitude
more computation than supervised pretraining. We tackle this computational
bottleneck by introducing a new self-supervised objective, contrastive
detection, which tasks representations with identifying object-level features
across augmentations. This objective extracts a rich learning signal per image,
leading to state-of-the-art transfer accuracy on a variety of downstream tasks,
while requiring up to 10x less pretraining. In particular, our strongest
ImageNet-pretrained model performs on par with SEER, one of the largest
self-supervised systems to date, which uses 1000x more pretraining data.
Finally, our objective seamlessly handles pretraining on more complex images
such as those in COCO, closing the gap with supervised transfer learning from
COCO to PASCAL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1"&gt;Olivier J. H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1"&gt;Skanda Koppula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1"&gt;Jean-Baptiste Alayrac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1"&gt;Aaron van den Oord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Similarity Learning for Unsupervised Feature Representation. (arXiv:2108.02721v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02721</id>
        <link href="http://arxiv.org/abs/2108.02721"/>
        <updated>2021-08-06T00:51:45.757Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an instance similarity learning (ISL) method for
unsupervised feature representation. Conventional methods assign close instance
pairs in the feature space with high similarity, which usually leads to wrong
pairwise relationship for large neighborhoods because the Euclidean distance
fails to depict the true semantic similarity on the feature manifold. On the
contrary, our method mines the feature manifold in an unsupervised manner,
through which the semantic similarity among instances is learned in order to
obtain discriminative representations. Specifically, we employ the Generative
Adversarial Networks (GAN) to mine the underlying feature manifold, where the
generated features are applied as the proxies to progressively explore the
feature manifold so that the semantic similarity among instances is acquired as
reliable pseudo supervision. Extensive experiments on image classification
demonstrate the superiority of our method compared with the state-of-the-art
methods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunsong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDesc: Unsupervised Point Descriptor Learning for Robust Registration. (arXiv:2108.02740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02740</id>
        <link href="http://arxiv.org/abs/2108.02740"/>
        <updated>2021-08-06T00:51:45.695Z</updated>
        <summary type="html"><![CDATA[In this work, we propose UPDesc, an unsupervised method to learn point
descriptors for robust point cloud registration. Our work builds upon a recent
supervised 3D CNN-based descriptor extraction framework, namely, 3DSmoothNet,
which leverages a voxel-based representation to parameterize the surrounding
geometry of interest points. Instead of using a predefined fixed-size local
support in voxelization, which potentially limits the access of richer local
geometry information, we propose to learn the support size in a data-driven
manner. To this end, we design a differentiable voxelization module that can
back-propagate gradients to the support size optimization. To optimize
descriptor similarity, the prior 3D CNN work and other supervised methods
require abundant correspondence labels or pose annotations of point clouds for
crafting metric learning losses. Differently, we show that unsupervised
learning of descriptor similarity can be achieved by performing geometric
registration in networks. Our learning objectives consider descriptor
similarity both across and within point clouds without supervision. Through
extensive experiments on point cloud registration benchmarks, we show that our
learned descriptors yield superior performance over existing unsupervised
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongbo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1"&gt;Maks Ovsjanikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02644</id>
        <link href="http://arxiv.org/abs/2108.02644"/>
        <updated>2021-08-06T00:51:45.676Z</updated>
        <summary type="html"><![CDATA[Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arijit Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Global-Local Representations in Salient Object Detection with Transformer. (arXiv:2108.02759v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02759</id>
        <link href="http://arxiv.org/abs/2108.02759"/>
        <updated>2021-08-06T00:51:45.669Z</updated>
        <summary type="html"><![CDATA[The fully convolutional network (FCN) has dominated salient object detection
for a long period. However, the locality of CNN requires the model deep enough
to have a global receptive field and such a deep model always leads to the loss
of local details. In this paper, we introduce a new attention-based encoder,
vision transformer, into salient object detection to ensure the globalization
of the representations from shallow to deep layers. With the global view in
very shallow layers, the transformer encoder preserves more local
representations to recover the spatial details in final saliency maps. Besides,
as each layer can capture a global view of its previous layer, adjacent layers
can implicitly maximize the representation differences and minimize the
redundant features, making that every output feature of transformer layers
contributes uniquely for final prediction. To decode features from the
transformer, we propose a simple yet effective deeply-transformed decoder. The
decoder densely decodes and upsamples the transformer features, generating the
final saliency map with less noise injection. Experimental results demonstrate
that our method significantly outperforms other FCN-based and transformer-based
methods in five benchmarks by a large margin, with an average of 12.17%
improvement in terms of Mean Absolute Error (MAE). Code will be available at
https://github.com/OliverRensu/GLSTR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1"&gt;Qiang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1"&gt;Nanxuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1"&gt;Guoqiang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12469</id>
        <link href="http://arxiv.org/abs/2107.12469"/>
        <updated>2021-08-06T00:51:45.663Z</updated>
        <summary type="html"><![CDATA[Access to high resolution satellite imagery has dramatically increased in
recent years as several new constellations have entered service. High revisit
frequencies as well as improved resolution has widened the use cases of
satellite imagery to areas such as humanitarian relief and even Search and
Rescue (SaR). We propose a novel remote sensing object detection dataset for
deep learning assisted SaR. This dataset contains only small objects that have
been identified as potential targets as part of a live SaR response. We
evaluate the application of popular object detection models to this dataset as
a baseline to inform further research. We also propose a novel object detection
metric, specifically designed to be used in a deep learning assisted SaR
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1"&gt;Michael Thoreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1"&gt;Frazer Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06610</id>
        <link href="http://arxiv.org/abs/2010.06610"/>
        <updated>2021-08-06T00:51:45.656Z</updated>
        <summary type="html"><![CDATA[Recent approaches to efficiently ensemble neural networks have shown that
strong robustness and uncertainty performance can be achieved with a negligible
gain in parameters over the original network. However, these methods still
require multiple forward passes for prediction, leading to a significant
computational cost. In this work, we show a surprising result: the benefits of
using multiple predictions can be achieved `for free' under a single model's
forward pass. In particular, we show that, using a multi-input multi-output
(MIMO) configuration, one can utilize a single model's capacity to train
multiple subnetworks that independently learn the task at hand. By ensembling
the predictions made by the subnetworks, we improve model robustness without
increasing compute. We observe a significant improvement in negative
log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,
and their out-of-distribution variants compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02725</id>
        <link href="http://arxiv.org/abs/2108.02725"/>
        <updated>2021-08-06T00:51:45.649Z</updated>
        <summary type="html"><![CDATA[Numerous online stock image libraries offer high quality yet copyright free
images for use in marketing campaigns. To assist advertisers in navigating such
third party libraries, we study the problem of automatically fetching relevant
ad images given the ad text (via a short textual query for images). Motivated
by our observations in logged data on ad image search queries (given ad text),
we formulate a keyword extraction problem, where a keyword extracted from the
ad text (or its augmented version) serves as the ad image query. In this
context, we propose VisualTextRank: an unsupervised method to (i) augment input
ad text using semantically similar ads, and (ii) extract the image query from
the augmented ad text. VisualTextRank builds on prior work on graph based
context extraction (biased TextRank in particular) by leveraging both the text
and image of similar ads for better keyword extraction, and using advertiser
category specific biasing with sentence-BERT embeddings. Using data collected
from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search
feature for onboarding advertisers, we demonstrate the superiority of
VisualTextRank compared to competitive keyword extraction baselines (including
an $11\%$ accuracy lift over biased TextRank). For the case when the stock
image library is restricted to English queries, we show the effectiveness of
VisualTextRank on multilingual ads (translated to English) while leveraging
semantically similar English ads. Online tests with a simplified version of
VisualTextRank led to a 28.7% increase in the usage of stock image search, and
a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native
ad platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1"&gt;Mikhail Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1"&gt;Gaurav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02722</id>
        <link href="http://arxiv.org/abs/2108.02722"/>
        <updated>2021-08-06T00:51:45.641Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has revolutionized self-supervised image representation
learning field, and recently been adapted to video domain. One of the greatest
advantages of contrastive learning is that it allows us to flexibly define
powerful loss objectives as long as we can find a reasonable way to formulate
positive and negative samples to contrast. However, existing approaches rely
heavily on the short-range spatiotemporal salience to form clip-level
contrastive signals, thus limit themselves from using global context. In this
paper, we propose a new video-level contrastive learning method based on
segments to formulate positive pairs. Our formulation is able to capture global
context in a video, thus robust to temporal content change. We also incorporate
a temporal order regularization term to enforce the inherent sequential
structure of videos. Extensive experiments show that our video-level
contrastive learning framework (VCLR) is able to outperform previous
state-of-the-arts on five video datasets for downstream action classification,
action localization and video retrieval. Code is available at
https://github.com/amazon-research/video-contrastive-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1"&gt;Haofei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schwertfeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1"&gt;Cyrill Stachniss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02563</id>
        <link href="http://arxiv.org/abs/2108.02563"/>
        <updated>2021-08-06T00:51:45.622Z</updated>
        <summary type="html"><![CDATA[This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,
Cheese and Fish based on Image Processing using Computer Vision and Deep
Learning: A Review. It consists of a comprehensive review of image processing,
computer vision and deep learning techniques applied to carry out analysis of
fruits, vegetables, cheese and fish.This part also serves as a literature
review for Part II.Part II: GuavaNet: A deep neural network architecture for
automatic sensory evaluation to predict degree of acceptability for Guava by a
consumer. This part introduces to an end-to-end deep neural network
architecture that can predict the degree of acceptability by the consumer for a
guava based on sensory evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1"&gt;Vipul Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPMs: Neural Parametric Models for 3D Deformable Shapes. (arXiv:2104.00702v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00702</id>
        <link href="http://arxiv.org/abs/2104.00702"/>
        <updated>2021-08-06T00:51:45.614Z</updated>
        <summary type="html"><![CDATA[Parametric 3D models have enabled a wide variety of tasks in computer
graphics and vision, such as modeling human bodies, faces, and hands. However,
the construction of these parametric models is often tedious, as it requires
heavy manual tweaking, and they struggle to represent additional complexity and
details such as wrinkles or clothing. To this end, we propose Neural Parametric
Models (NPMs), a novel, learned alternative to traditional, parametric 3D
models, which does not require hand-crafted, object-specific constraints. In
particular, we learn to disentangle 4D dynamics into latent-space
representations of shape and pose, leveraging the flexibility of recent
developments in learned implicit functions. Crucially, once learned, our neural
parametric models of shape and pose enable optimization over the learned spaces
to fit to new observations, similar to the fitting of a traditional parametric
model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate
and detailed representation of observed deformable sequences. We show that NPMs
improve notably over both parametric and non-parametric state of the art in
reconstruction and tracking of monocular depth sequences of clothed humans and
hands. Latent-space interpolation as well as shape/pose transfer experiments
further demonstrate the usefulness of NPMs. Code is publicly available at
https://pablopalafox.github.io/npms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1"&gt;Pablo Palafox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1"&gt;Alja&amp;#x17e; Bo&amp;#x17e;i&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1"&gt;Justus Thies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Angela Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-Augmented RGB-D SLAM for Wide-Disparity Relocalisation. (arXiv:2108.02522v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02522</id>
        <link href="http://arxiv.org/abs/2108.02522"/>
        <updated>2021-08-06T00:51:45.607Z</updated>
        <summary type="html"><![CDATA[We propose a novel object-augmented RGB-D SLAM system that is capable of
constructing a consistent object map and performing relocalisation based on
centroids of objects in the map. The approach aims to overcome the view
dependence of appearance-based relocalisation methods using point features or
images. During the map construction, we use a pre-trained neural network to
detect objects and estimate 6D poses from RGB-D data. An incremental
probabilistic model is used to aggregate estimates over time to create the
object map. Then in relocalisation, we use the same network to extract
objects-of-interest in the `lost' frames. Pairwise geometric matching finds
correspondences between map and frame objects, and probabilistic absolute
orientation followed by application of iterative closest point to dense depth
maps and object centroids gives relocalisation. Results of experiments in
desktop environments demonstrate very high success rates even for frames with
widely different viewpoints from those used to construct the map, significantly
outperforming two appearance-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yuhang Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingrui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1"&gt;Andrew Calway&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02613</id>
        <link href="http://arxiv.org/abs/2108.02613"/>
        <updated>2021-08-06T00:51:45.599Z</updated>
        <summary type="html"><![CDATA[Point cloud registration is a fundamental problem in 3D computer vision. In
this paper, we cast point cloud registration into a planning problem in
reinforcement learning, which can seek the transformation between the source
and target point clouds through trial and error. By modeling the point cloud
registration process as a Markov decision process (MDP), we develop a latent
dynamic model of point clouds, consisting of a transformation network and
evaluation network. The transformation network aims to predict the new
transformed feature of the point cloud after performing a rigid transformation
(i.e., action) on it while the evaluation network aims to predict the alignment
precision between the transformed source point cloud and target point cloud as
the reward signal. Once the dynamic model of the point cloud is trained, we
employ the cross-entropy method (CEM) to iteratively update the planning policy
by maximizing the rewards in the point cloud registration process. Thus, the
optimal policy, i.e., the transformation between the source and target point
clouds, can be obtained via gradually narrowing the search space of the
transformation. Experimental results on ModelNet40 and 7Scene benchmark
datasets demonstrate that our method can yield good registration performance in
an unsupervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haobo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianjun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02707</id>
        <link href="http://arxiv.org/abs/2108.02707"/>
        <updated>2021-08-06T00:51:45.592Z</updated>
        <summary type="html"><![CDATA[The proliferation of automated facial recognition in various commercial and
government sectors has caused significant privacy concerns for individuals. A
recent and popular approach to address these privacy concerns is to employ
evasion attacks against the metric embedding networks powering facial
recognition systems. Face obfuscation systems generate imperceptible
perturbations, when added to an image, cause the facial recognition system to
misidentify the user. The key to these approaches is the generation of
perturbations using a pre-trained metric embedding network followed by their
application to an online system, whose model might be proprietary. This
dependence of face obfuscation on metric embedding networks, which are known to
be unfair in the context of facial recognition, surfaces the question of
demographic fairness -- \textit{are there demographic disparities in the
performance of face obfuscation systems?} To address this question, we perform
an analytical and empirical exploration of the performance of recent face
obfuscation systems that rely on deep embedding networks. We find that metric
embedding networks are demographically aware; they cluster faces in the
embedding space based on their demographic attributes. We observe that this
effect carries through to the face obfuscation systems: faces belonging to
minority groups incur reduced utility compared to those from majority groups.
For example, the disparity in average obfuscation success rate on the online
Face++ API can reach up to 20 percentage points. Further, for some demographic
groups, the average perturbation size increases by up to 17\% when choosing a
target identity belonging to a different demographic group versus the same
demographic group. Finally, we present a simple analytical model to provide
insights into these phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1"&gt;Harrison Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Brian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1"&gt;Kassem Fawaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences. (arXiv:2103.06028v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06028</id>
        <link href="http://arxiv.org/abs/2103.06028"/>
        <updated>2021-08-06T00:51:45.576Z</updated>
        <summary type="html"><![CDATA[Estimating the states of surrounding traffic participants stays at the core
of autonomous driving. In this paper, we study a novel setting of this problem:
model-free single-object tracking (SOT), which takes the object state in the
first frame as input, and jointly solves state estimation and tracking in
subsequent frames. The main purpose for this new setting is to break the strong
limitation of the popular "detection and tracking" scheme in multi-object
tracking. Moreover, we notice that shape completion by overlaying the point
clouds, which is a by-product of our proposed task, not only improves the
performance of state estimation but also has numerous applications. As no
benchmark for this task is available so far, we construct a new dataset
LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open
dataset. We then propose an optimization-based algorithm called SOTracker
involving point cloud registration, vehicle shapes, correspondence, and motion
priors. Our quantitative and qualitative results prove the effectiveness of our
SOTracker and reveal the challenging cases for SOT in point clouds, including
the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also
explore how the proposed task and algorithm may benefit other autonomous
driving applications, including simulating LiDAR scans, generating motion data,
and annotating optical flow. The code and protocols for our benchmark and
algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video
demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1"&gt;Ziqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Naiyan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizable Mixed-Precision Quantization via Attribution Rank Preservation. (arXiv:2108.02720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02720</id>
        <link href="http://arxiv.org/abs/2108.02720"/>
        <updated>2021-08-06T00:51:45.568Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a generalizable mixed-precision quantization (GMPQ)
method for efficient inference. Conventional methods require the consistency of
datasets for bitwidth search and model deployment to guarantee the policy
optimality, leading to heavy search cost on challenging largescale datasets in
realistic applications. On the contrary, our GMPQ searches the
mixed-quantization policy that can be generalized to largescale datasets with
only a small amount of data, so that the search cost is significantly reduced
without performance degradation. Specifically, we observe that locating network
attribution correctly is general ability for accurate visual analysis across
different data distribution. Therefore, despite of pursuing higher model
accuracy and complexity, we preserve attribution rank consistency between the
quantized models and their full-precision counterparts via efficient
capacity-aware attribution imitation for generalizable mixed-precision
quantization strategy search. Extensive experiments show that our method
obtains competitive accuracy-complexity trade-off compared with the
state-of-the-art mixed-precision networks in significantly reduced search cost.
The code is available at https://github.com/ZiweiWangTHU/GMPQ.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Han Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02774</id>
        <link href="http://arxiv.org/abs/2108.02774"/>
        <updated>2021-08-06T00:51:45.561Z</updated>
        <summary type="html"><![CDATA[Can a user create a deep generative model by sketching a single example?
Traditionally, creating a GAN model has required the collection of a
large-scale dataset of exemplars and specialized knowledge in deep learning. In
contrast, sketching is possibly the most universally accessible way to convey a
visual concept. In this work, we present a method, GAN Sketching, for rewriting
GANs with one or more sketches, to make GANs training easier for novice users.
In particular, we change the weights of an original GAN model according to user
sketches. We encourage the model's output to match the user sketches through a
cross-domain adversarial loss. Furthermore, we explore different regularization
methods to preserve the original model's diversity and image quality.
Experiments have shown that our method can mold GANs to match shapes and poses
specified by sketches while maintaining realism and diversity. Finally, we
demonstrate a few applications of the resulting GAN, including latent space
interpolation and image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng-Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1"&gt;David Bau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07158</id>
        <link href="http://arxiv.org/abs/1911.07158"/>
        <updated>2021-08-06T00:51:45.555Z</updated>
        <summary type="html"><![CDATA[Current state-of-the-art object detectors can have significant performance
drop when deployed in the wild due to domain gaps with training data.
Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models
for new domains/environments without any expensive label cost. However, without
ground truth labels, most prior works on UDA for object detection tasks can
only perform coarse image-level and/or feature-level adaptation by using
adversarial learning methods. In this work, we show that such adversarial-based
methods can only reduce the domain style gap, but cannot address the domain
content distribution gap that is shown to be important for object detectors. To
overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning
(CDSSL) framework by leveraging high-quality pseudo labels to learn better
representations from the target domain directly. To enable SSL for cross-domain
object detection, we propose fine-grained domain transfer,
progressive-confidence-based label sharpening and imbalanced sampling strategy
to address two challenges: (i) non-identical distribution between source and
target domain data, (ii) error amplification/accumulation due to noisy pseudo
labeling on the target domain. Experiment results show that our proposed
approach consistently achieves new state-of-the-art performance (2.2% - 9.5%
better than prior best work on mAP) under various domain gap scenarios. The
code will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fuxun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karianakis_N/0/1/0/all/0/1"&gt;Nikolaos Karianakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lymberopoulos_D/0/1/0/all/0/1"&gt;Dimitrios Lymberopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Sidi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weisong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:45.531Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02502</id>
        <link href="http://arxiv.org/abs/2108.02502"/>
        <updated>2021-08-06T00:51:45.519Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks have been shown to be vulnerable to various kinds of
adversarial perturbations. In addition to widely studied additive noise based
perturbations, adversarial examples can also be created by applying a per pixel
spatial drift on input images. While spatial transformation based adversarial
examples look more natural to human observers due to absence of additive noise,
they still possess visible distortions caused by spatial transformations. Since
the human vision is more sensitive to the distortions in the luminance compared
to those in chrominance channels, which is one of the main ideas behind the
lossy visual multimedia compression standards, we propose a spatial
transformation based perturbation method to create adversarial examples by only
modifying the color components of an input image. While having competitive
fooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,
examples created with the proposed method have better scores with regards to
various perceptual quality metrics. Human visual perception studies validate
that the examples are more natural looking and often indistinguishable from
their original counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1"&gt;Ayberk Aydin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1"&gt;Deniz Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1"&gt;Berat Tuna Karli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1"&gt;Oguz Hanoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1"&gt;Alptekin Temizel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Wake-up: 3-D Object Reconstruction, Animation, and in-situ Rendering from a Single Image. (arXiv:2108.02708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02708</id>
        <link href="http://arxiv.org/abs/2108.02708"/>
        <updated>2021-08-06T00:51:45.501Z</updated>
        <summary type="html"><![CDATA[Given a picture of a chair, could we extract the 3-D shape of the chair,
animate its plausible articulations and motions, and render in-situ in its
original image space? The above question prompts us to devise an automated
approach to extract and manipulate articulated objects in single images.
Comparing with previous efforts on object manipulation, our work goes beyond
2-D manipulation and focuses on articulable objects, thus introduces greater
flexibility for possible object deformations. The pipeline of our approach
starts by reconstructing and refining a 3-D mesh representation of the object
of interest from an input image; its control joints are predicted by exploiting
the semantic part segmentation information; the obtained object 3-D mesh is
then rigged \& animated by non-rigid deformation, and rendered to perform
in-situ motions in its original image space. Quantitative evaluations are
carried out on 3-D reconstruction from single images, an established task that
is related to our pipeline, where our results surpass those of the SOTAs by a
noticeable margin. Extensive visual results also demonstrate the applicability
of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Ji Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhenbo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Minglun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Li Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02607</id>
        <link href="http://arxiv.org/abs/2108.02607"/>
        <updated>2021-08-06T00:51:45.493Z</updated>
        <summary type="html"><![CDATA[We introduce a new efficient framework, the Unified Context Network (UniCon),
for robust active speaker detection (ASD). Traditional methods for ASD usually
operate on each candidate's pre-cropped face track separately and do not
sufficiently consider the relationships among the candidates. This potentially
limits performance, especially in challenging scenarios with low-resolution
faces, multiple candidates, etc. Our solution is a novel, unified framework
that focuses on jointly modeling multiple types of contextual information:
spatial context to indicate the position and scale of each candidate's face,
relational context to capture the visual relationships among the candidates and
contrast audio-visual affinities with each other, and temporal context to
aggregate long-term information and smooth out local uncertainties. Based on
such information, our model optimizes all candidates in a unified process for
robust and reliable ASD. A thorough ablation study is performed on several
challenging ASD benchmarks under different settings. In particular, our method
outperforms the state-of-the-art by a large margin of about 15% mean Average
Precision (mAP) absolute on two challenging subsets: one with three candidate
speakers, and the other with faces smaller than 64 pixels. Together, our UniCon
achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for
the first time on this challenging dataset at the time of submission. Project
website: https://unicon-asd.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuanhang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Susan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Aggregation for 3D Instance Segmentation. (arXiv:2108.02350v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02350</id>
        <link href="http://arxiv.org/abs/2108.02350"/>
        <updated>2021-08-06T00:51:45.483Z</updated>
        <summary type="html"><![CDATA[Instance segmentation on point clouds is a fundamental task in 3D scene
perception. In this work, we propose a concise clustering-based framework named
HAIS, which makes full use of spatial relation of points and point sets.
Considering clustering-based methods may result in over-segmentation or
under-segmentation, we introduce the hierarchical aggregation to progressively
generate instance proposals, i.e., point aggregation for preliminarily
clustering points to sets and set aggregation for generating complete instances
from sets. Once the complete 3D instances are obtained, a sub-network of
intra-instance prediction is adopted for noisy points filtering and mask
quality scoring. HAIS is fast (only 410ms per frame) and does not require
non-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving
the highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods
by a large margin. Besides, the SOTA results on the S3DIS dataset validate the
good generalization ability. Code will be available at
https://github.com/hustvl/HAIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02561</id>
        <link href="http://arxiv.org/abs/2108.02561"/>
        <updated>2021-08-06T00:51:45.466Z</updated>
        <summary type="html"><![CDATA[Single online handwritten Chinese character recognition~(single OLHCCR) has
achieved prominent performance. However, in real application scenarios, users
always write multiple Chinese characters to form one complete sentence and the
contextual information within these characters holds the significant potential
to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In
this work, we first propose a simple and straightforward end-to-end network,
namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.
It couples convolutional neural network with sequence modeling architecture to
exploit the handwritten character's previous contextual information. Although
VCN performs much better than the state-of-the-art single OLHCCR model, it
exposes high fragility when confronting with not well written characters such
as sloppy writing, missing or broken strokes. To improve the robustness of
sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion
network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the
backbone component, which projects each Chinese character into word embeddings,
and integrates the spatial glyph features of handwritten characters and their
contextual information multiple times at multi-layer fusion module. We also
construct a large-scale sentence-level handwriting dataset, named as CSOHD to
evaluate models. Extensive experiment results demonstrate that DSTFN achieves
the state-of-the-art performance, which presents strong robustness compared
with VCN and exiting single OLHCCR models. The in-depth empirical analysis and
case studies indicate that DSTFN can significantly improve the efficiency of
handwriting input, with the handwritten Chinese character with incomplete
strokes being recognized precisely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Baotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuxin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Colorectal Polyp Classification from White-light Colonoscopy Images via Domain Alignment. (arXiv:2108.02476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02476</id>
        <link href="http://arxiv.org/abs/2108.02476"/>
        <updated>2021-08-06T00:51:45.458Z</updated>
        <summary type="html"><![CDATA[Differentiation of colorectal polyps is an important clinical examination. A
computer-aided diagnosis system is required to assist accurate diagnosis from
colonoscopy images. Most previous studies at-tempt to develop models for polyp
differentiation using Narrow-Band Imaging (NBI) or other enhanced images.
However, the wide range of these models' applications for clinical work has
been limited by the lagging of imaging techniques. Thus, we propose a novel
framework based on a teacher-student architecture for the accurate colorectal
polyp classification (CPC) through directly using white-light (WL) colonoscopy
images in the examination. In practice, during training, the auxiliary NBI
images are utilized to train a teacher network and guide the student network to
acquire richer feature representation from WL images. The feature transfer is
realized by domain alignment and contrastive learning. Eventually the final
student network has the ability to extract aligned features from only WL images
to facilitate the CPC task. Besides, we release the first public-available
paired CPC dataset containing WL-NBI pairs for the alignment training.
Quantitative and qualitative evaluation indicates that the proposed method
outperforms the previous methods in CPC, improving the accuracy by 5.6%with
very fast speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1"&gt;Hui Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weizhen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Li Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Lossless Image Formats. (arXiv:2108.02557v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02557</id>
        <link href="http://arxiv.org/abs/2108.02557"/>
        <updated>2021-08-06T00:51:45.451Z</updated>
        <summary type="html"><![CDATA[In recent years, a bag with image and video compression formats has been
torn. However, most of them are focused on lossy compression and only
marginally support the lossless mode. In this paper, I will focus on lossless
formats and the critical question: "Which one is the most efficient?" It turned
out that FLIF is currently the most efficient format for lossless image
compression. This finding is in contrast to that FLIF developers stopped its
development in favor of JPEG XL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barina_D/0/1/0/all/0/1"&gt;David Barina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection of Rail Components via A Deep Convolutional Transformer Network. (arXiv:2108.02423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02423</id>
        <link href="http://arxiv.org/abs/2108.02423"/>
        <updated>2021-08-06T00:51:45.444Z</updated>
        <summary type="html"><![CDATA[Automatic detection of rail track and its fasteners via using continuously
collected railway images is important to maintenance as it can significantly
improve maintenance efficiency and better ensure system safety. Dominant
computer vision-based detection models typically rely on convolutional neural
networks that utilize local image features and cumbersome prior settings to
generate candidate boxes. In this paper, we propose a deep convolutional
transformer network based method to detect multi-class rail components
including the rail, clip, and bolt. We effectively synergize advantages of the
convolutional structure on extracting latent features from raw images as well
as advantages of transformers on selectively determining valuable latent
features to achieve an efficient and accurate performance on rail component
detections. Our proposed method simplifies the detection pipeline by
eliminating the need of prior settings, such as anchor box, aspect ratio,
default coordinates, and post-processing, such as the threshold for non-maximum
suppression; as well as allows users to trade off the quality and complexity of
the detector with limited training data. Results of a comprehensive
computational study show that our proposed method outperforms a set of existing
state-of-art approaches with large margins]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fangfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-06T00:51:45.436Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully-connected structure makes LSTM networks a
memory-bounded algorithm. Previous work in LSTM accelerators either exploited
weight spatial sparsity or temporal sparsity. In this paper, we present a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity was induced using our
proposed pruning method called Column-Balanced Targeted Dropout (CBTD) that
leads to structured sparse weight matrices benefiting workload balance. It
achieved up to 96% weight sparsity with negligible accuracy difference for an
LSTM network trained on a TIMIT phone recognition task. To induce temporal
sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU
method to the LSTM network. This combined sparsity saves on weight memory
access and associated arithmetic operations simultaneously. Spartus was
implemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single
DeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved
9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which
are respectively 4X and 7X higher than the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02508</id>
        <link href="http://arxiv.org/abs/2108.02508"/>
        <updated>2021-08-06T00:51:45.429Z</updated>
        <summary type="html"><![CDATA[The advancements in deep learning technologies have produced immense
contribution to biomedical image analysis applications. With breast cancer
being the common deadliest disease among women, early detection is the key
means to improve survivability. Medical imaging like ultrasound presents an
excellent visual representation of the functioning of the organs; however, for
any radiologist analysing such scans is challenging and time consuming which
delays the diagnosis process. Although various deep learning based approaches
are proposed that achieved promising results, the present article introduces an
efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)
model with minimal training parameters for tumor segmentation using breast
ultrasound imaging to further improve the segmentation performance of varying
tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception
depth-wise separable convolution and hybrid pooling (max pooling and spectral
pooling) layers. In addition, cross-spatial attention filters are added to
suppress the irrelevant features and focus on the target structure. The
segmentation performance of the proposed model is validated on two publicly
available datasets using standard segmentation evaluation metrics, where it
outperformed the other state-of-the-art segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Hashing with Similarity Learning. (arXiv:2108.02560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02560</id>
        <link href="http://arxiv.org/abs/2108.02560"/>
        <updated>2021-08-06T00:51:45.421Z</updated>
        <summary type="html"><![CDATA[Online hashing methods usually learn the hash functions online, aiming to
efficiently adapt to the data variations in the streaming environment. However,
when the hash functions are updated, the binary codes for the whole database
have to be updated to be consistent with the hash functions, resulting in the
inefficiency in the online image retrieval process. In this paper, we propose a
novel online hashing framework without updating binary codes. In the proposed
framework, the hash functions are fixed and a parametric similarity function
for the binary codes is learnt online to adapt to the streaming data.
Specifically, a parametric similarity function that has a bilinear form is
adopted and a metric learning algorithm is proposed to learn the similarity
function online based on the characteristics of the hashing methods. The
experiments on two multi-label image datasets show that our method is
competitive or outperforms the state-of-the-art online hashing methods in terms
of both accuracy and efficiency for multi-label image retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1"&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuesheng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Structure Consistency for Deep Model Watermarking. (arXiv:2108.02360v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02360</id>
        <link href="http://arxiv.org/abs/2108.02360"/>
        <updated>2021-08-06T00:51:45.414Z</updated>
        <summary type="html"><![CDATA[The intellectual property (IP) of Deep neural networks (DNNs) can be easily
``stolen'' by surrogate model attack. There has been significant progress in
solutions to protect the IP of DNN models in classification tasks. However,
little attention has been devoted to the protection of DNNs in image processing
tasks. By utilizing consistent invisible spatial watermarks, one recent work
first considered model watermarking for deep image processing networks and
demonstrated its efficacy in many downstream tasks. Nevertheless, it highly
depends on the hypothesis that the embedded watermarks in the network outputs
are consistent. When the attacker uses some common data augmentation attacks
(e.g., rotate, crop, and resize) during surrogate model training, it will
totally fail because the underlying watermark consistency is destroyed. To
mitigate this issue, we propose a new watermarking methodology, namely
``structure consistency'', based on which a new deep structure-aligned model
watermarking algorithm is designed. Specifically, the embedded watermarks are
designed to be aligned with physically consistent image structures, such as
edges or semantic regions. Experiments demonstrate that our method is much more
robust than the baseline method in resisting data augmentation attacks for
model IP protection. Besides that, we further test the generalization ability
and robustness of our method to a broader range of circumvention attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zehua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02488</id>
        <link href="http://arxiv.org/abs/2108.02488"/>
        <updated>2021-08-06T00:51:45.389Z</updated>
        <summary type="html"><![CDATA[Recent research shows deep neural networks are vulnerable to different types
of attacks, such as adversarial attack, data poisoning attack and backdoor
attack. Among them, backdoor attack is the most cunning one and can occur in
almost every stage of deep learning pipeline. Therefore, backdoor attack has
attracted lots of interests from both academia and industry. However, most
existing backdoor attack methods are either visible or fragile to some
effortless pre-processing such as common data transformations. To address these
limitations, we propose a robust and invisible backdoor attack called ``Poison
Ink''. Concretely, we first leverage the image structures as target poisoning
areas, and fill them with poison ink (information) to generate the trigger
pattern. As the image structure can keep its semantic meaning during the data
transformation, such trigger pattern is inherently robust to data
transformations. Then we leverage a deep injection network to embed such
trigger pattern into the cover image to achieve stealthiness. Compared to
existing popular backdoor attack methods, Poison Ink outperforms both in
stealthiness and robustness. Through extensive experiments, we demonstrate
Poison Ink is not only general to different datasets and network architectures,
but also flexible for different attack scenarios. Besides, it also has very
strong resistance against many state-of-the-art defense techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+zhang_J/0/1/0/all/0/1"&gt;Jie zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qidong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02359</id>
        <link href="http://arxiv.org/abs/2108.02359"/>
        <updated>2021-08-06T00:51:45.370Z</updated>
        <summary type="html"><![CDATA[Video captioning combines video understanding and language generation.
Different from image captioning that describes a static image with details of
almost every object, video captioning usually considers a sequence of frames
and biases towards focused objects, e.g., the objects that stay in focus
regardless of the changing background. Therefore, detecting and properly
accommodating focused objects is critical in video captioning. To enforce the
description of focused objects and achieve controllable video captioning, we
propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs
caption generation in three steps: 1) identify the focused objects and predict
their locations in the target caption; 2) generate the related attribute words
and relation words of these focused objects to form a draft caption; and 3)
combine video information to refine the draft caption to a fluent final
caption. Since the focused objects are generated and located ahead of other
words, it is difficult to apply the word-by-word autoregressive generation
process; instead, we adopt a non-autoregressive approach. The experiments on
two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness
of O2NA, which achieves results competitive with the state-of-the-arts but with
both higher diversity and higher inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02574</id>
        <link href="http://arxiv.org/abs/2108.02574"/>
        <updated>2021-08-06T00:51:45.355Z</updated>
        <summary type="html"><![CDATA[Recently, much progress has been made in unsupervised restoration learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised restoration learning without
any prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
restoration learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we show that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zeyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rendong Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peilin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2IOSR: Maximal Mutual Information Open Set Recognition. (arXiv:2108.02373v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02373</id>
        <link href="http://arxiv.org/abs/2108.02373"/>
        <updated>2021-08-06T00:51:45.348Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to address the challenging task of open set recognition
(OSR). Many recent OSR methods rely on auto-encoders to extract class-specific
features by a reconstruction strategy, requiring the network to restore the
input image on pixel-level. This strategy is commonly over-demanding for OSR
since class-specific features are generally contained in target objects, not in
all pixels. To address this shortcoming, here we discard the pixel-level
reconstruction strategy and pay more attention to improving the effectiveness
of class-specific feature extraction. We propose a mutual information-based
method with a streamlined architecture, Maximal Mutual Information Open Set
Recognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract
class-specific features by maximizing the mutual information between the given
input and its latent features across multiple scales. Meanwhile, to further
reduce the open space risk, latent features are constrained to class
conditional Gaussian distributions by a KL-divergence loss function. In this
way, a strong function is learned to prevent the network from mapping different
observations to similar latent features and help the network extract
class-specific features with desired statistical characteristics. The proposed
method significantly improves the performance of baselines and achieves new
state-of-the-art results on several benchmarks consistently. Source codes are
uploaded in supplementary materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1"&gt;Keck-Voon Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MS-KD: Multi-Organ Segmentation with Multiple Binary-Labeled Datasets. (arXiv:2108.02559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02559</id>
        <link href="http://arxiv.org/abs/2108.02559"/>
        <updated>2021-08-06T00:51:45.325Z</updated>
        <summary type="html"><![CDATA[Annotating multiple organs in 3D medical images is time-consuming and costly.
Meanwhile, there exist many single-organ datasets with one specific organ
annotated. This paper investigates how to learn a multi-organ segmentation
model leveraging a set of binary-labeled datasets. A novel Multi-teacher
Single-student Knowledge Distillation (MS-KD) framework is proposed, where the
teacher models are pre-trained single-organ segmentation networks, and the
student model is a multi-organ segmentation network. Considering that each
teacher focuses on different organs, a region-based supervision method,
consisting of logits-wise supervision and feature-wise supervision, is
proposed. Each teacher supervises the student in two regions, the organ region
where the teacher is considered as an expert and the background region where
all teachers agree. Extensive experiments on three public single-organ datasets
and a multi-organ dataset have demonstrated the effectiveness of the proposed
MS-KD framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shixiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixLacune: Segmentation of lacunes of presumed vascular origin. (arXiv:2108.02483v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02483</id>
        <link href="http://arxiv.org/abs/2108.02483"/>
        <updated>2021-08-06T00:51:45.318Z</updated>
        <summary type="html"><![CDATA[Lacunes of presumed vascular origin are fluid-filled cavities of between 3 -
15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes
relies on manual annotation or semi-automatic / interactive approaches; and
almost no automatic methods exist for this task. In this work, we present a
two-stage approach to segment lacunes of presumed vascular origin: (1)
detection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data
originates from Task 3 of the "Where is VALDO?" challenge and consists of 40
training subjects. We report the mean DICE on the training set of 0.83 and on
the validation set of 0.84. Source code is available at:
https://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune
can be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1"&gt;Denis Kutnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1"&gt;Bas H.M. van der Velden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1"&gt;Marta Girones Sanguesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geerlings_M/0/1/0/all/0/1"&gt;Mirjam I. Geerlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biesbroek_J/0/1/0/all/0/1"&gt;J. Matthijs Biesbroek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02451</id>
        <link href="http://arxiv.org/abs/2108.02451"/>
        <updated>2021-08-06T00:51:45.311Z</updated>
        <summary type="html"><![CDATA[The nonlocal-based blocks are designed for capturing long-range
spatial-temporal dependencies in computer vision tasks. Although having shown
excellent performance, they still lack the mechanism to encode the rich,
structured information among elements in an image or video. In this paper, to
theoretically analyze the property of these nonlocal-based blocks, we provide a
new perspective to interpret them, where we view them as a set of graph filters
generated on a fully-connected graph. Specifically, when choosing the Chebyshev
graph filter, a unified formulation can be derived for explaining and analyzing
the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,
double attention block). Furthermore, by concerning the property of spectral,
we propose an efficient and robust spectral nonlocal block, which can be more
robust and flexible to catch long-range dependencies when inserted into deep
neural networks than the existing nonlocal blocks. Experimental results
demonstrate the clear-cut improvements and practical applicabilities of our
method on image classification, action recognition, semantic segmentation, and
person re-identification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qi She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Duo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yanye Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1"&gt;Xuejing Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach. (arXiv:2108.02399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02399</id>
        <link href="http://arxiv.org/abs/2108.02399"/>
        <updated>2021-08-06T00:51:45.264Z</updated>
        <summary type="html"><![CDATA[Learning from the web can ease the extreme dependence of deep learning on
large-scale manually labeled datasets. Especially for fine-grained recognition,
which targets at distinguishing subordinate categories, it will significantly
reduce the labeling costs by leveraging free web data. Despite its significant
practical and research value, the webly supervised fine-grained recognition
problem is not extensively studied in the computer vision community, largely
due to the lack of high-quality datasets. To fill this gap, in this paper we
construct two new benchmark webly supervised fine-grained datasets, termed
WebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of
three sub-datasets containing a total of 53,339 web training images with 200
species of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196
models of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and
more than 1.1 million web training images, which is the largest webly
supervised fine-grained dataset ever. As a minor contribution, we also propose
a novel webly supervised method (termed ``{Peer-learning}'') for benchmarking
these datasets.~Comprehensive experimental results and analyses on two new
benchmark datasets demonstrate that the proposed method achieves superior
performance over the competing baseline models and states-of-the-art. Our
benchmark datasets and the source codes of Peer-learning have been made
available at
{\url{https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zeren Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yazhou Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongshun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1"&gt;Fumin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng-Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion. (arXiv:2108.02448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02448</id>
        <link href="http://arxiv.org/abs/2108.02448"/>
        <updated>2021-08-06T00:51:45.258Z</updated>
        <summary type="html"><![CDATA[We design a multiscopic vision system that utilizes a low-cost monocular RGB
camera to acquire accurate depth estimation. Unlike multi-view stereo with
images captured at unconstrained camera poses, the proposed system controls the
motion of a camera to capture a sequence of images in horizontally or
vertically aligned positions with the same parallax. In this system, we propose
a new heuristic method and a robust learning-based method to fuse multiple cost
volumes between the reference image and its surrounding images. To obtain
training data, we build a synthetic dataset with multiscopic images. The
experiments on the real-world Middlebury dataset and real robot demonstration
show that our multiscopic vision system outperforms traditional two-frame
stereo matching methods in depth estimation. Our code and dataset are available
at \url{https://sites.google.com/view/multiscopic]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weihao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Rui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Michael Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID. (arXiv:2108.02413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02413</id>
        <link href="http://arxiv.org/abs/2108.02413"/>
        <updated>2021-08-06T00:51:45.235Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptive person re-identification (UDA re-ID) aims at
transferring the labeled source domain's knowledge to improve the model's
discriminability on the unlabeled target domain. From a novel perspective, we
argue that the bridging between the source and target domains can be utilized
to tackle the UDA re-ID task, and we focus on explicitly modeling appropriate
intermediate domains to characterize this bridging. Specifically, we propose an
Intermediate Domain Module (IDM) to generate intermediate domains'
representations on-the-fly by mixing the source and target domains' hidden
representations using two domain factors. Based on the "shortest geodesic path"
definition, i.e., the intermediate domains along the shortest geodesic path
between the two extreme domains can play a better bridging role, we propose two
properties that these intermediate domains should satisfy. To ensure these two
properties to better characterize appropriate intermediate domains, we enforce
the bridge losses on intermediate domains' prediction space and feature space,
and enforce a diversity loss on the two domain factors. The bridge losses aim
at guiding the distribution of appropriate intermediate domains to keep the
right distance to the source and target domains. The diversity loss serves as a
regularization to prevent the generated intermediate domains from being
over-fitting to either of the source and target domains. Our proposed method
outperforms the state-of-the-arts by a large margin in all the common UDA re-ID
tasks, and the mAP gain is up to 7.7% on the challenging MSMT17 benchmark. Code
is available at https://github.com/SikaStar/IDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yongxing Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yifan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1"&gt;Zekun Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Ling-Yu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Design and Construct Bridge without Blueprint. (arXiv:2108.02439v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02439</id>
        <link href="http://arxiv.org/abs/2108.02439"/>
        <updated>2021-08-06T00:51:45.210Z</updated>
        <summary type="html"><![CDATA[Autonomous assembly has been a desired functionality of many intelligent
robot systems. We study a new challenging assembly task, designing and
constructing a bridge without a blueprint. In this task, the robot needs to
first design a feasible bridge architecture for arbitrarily wide cliffs and
then manipulate the blocks reliably to construct a stable bridge according to
the proposed design. In this paper, we propose a bi-level approach to tackle
this task. At the high level, the system learns a bridge blueprint policy in a
physical simulator using deep reinforcement learning and curriculum learning. A
policy is represented as an attention-based neural network with object-centric
input, which enables generalization to different numbers of blocks and cliff
widths. For low-level control, we implement a motion-planning-based policy for
real-robot motion control, which can be directly combined with a trained
blueprint policy for real-world bridge construction without tuning. In our
field study, our bi-level robot system demonstrates the capability of
manipulating blocks to construct a diverse set of bridges with different
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunfei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02425</id>
        <link href="http://arxiv.org/abs/2108.02425"/>
        <updated>2021-08-06T00:51:45.203Z</updated>
        <summary type="html"><![CDATA[Grasping in cluttered scenes has always been a great challenge for robots,
due to the requirement of the ability to well understand the scene and object
information. Previous works usually assume that the geometry information of the
objects is available, or utilize a step-wise, multi-stage strategy to predict
the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF
grasp pose estimation as a simultaneous multi-task learning problem. In a
unified framework, we jointly predict the feasible 6-DoF grasp poses, instance
semantic segmentation, and collision information. The whole framework is
jointly optimized and end-to-end differentiable. Our model is evaluated on
large-scale benchmarks as well as the real robot system. On the public dataset,
our method outperforms prior state-of-the-art methods by a large margin (+4.08
AP). We also demonstrate the implementation of our model on a real robotic
platform and show that the robot can accurately grasp target objects in
cluttered scenarios with a high success rate. Project link:
https://openbyterobotics.github.io/sscl]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1"&gt;Ruihang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02356</id>
        <link href="http://arxiv.org/abs/2108.02356"/>
        <updated>2021-08-06T00:51:45.188Z</updated>
        <summary type="html"><![CDATA[Video abnormal event detection (VAD) is a vital semi-supervised task that
requires learning with only roughly labeled normal videos, as anomalies are
often practically unavailable. Although deep neural networks (DNNs) enable
great progress in VAD, existing solutions typically suffer from two issues: (1)
The precise and comprehensive localization of video events is ignored. (2) The
video semantics and temporal context are under-explored. To address those
issues, we are motivated by the prevalent cloze test in education and propose a
novel approach named visual cloze completion (VCC), which performs VAD by
learning to complete "visual cloze tests" (VCTs). Specifically, VCC first
localizes each video event and encloses it into a spatio-temporal cube (STC).
To achieve both precise and comprehensive localization, appearance and motion
are used as mutually complementary cues to mark the object region associated
with each video event. For each marked region, a normalized patch sequence is
extracted from temporally adjacent frames and stacked into the STC. By
comparing each patch and the patch sequence of a STC to a visual "word" and
"sentence" respectively, we can deliberately erase a certain "word" (patch) to
yield a VCT. DNNs are then trained to infer the erased patch by video
semantics, so as to complete the VCT. To fully exploit the temporal context,
each patch in STC is alternatively erased to create multiple VCTs, and the
erased patch's optical flow is also inferred to integrate richer motion clues.
Meanwhile, a new DNN architecture is designed as a model-level solution to
utilize video semantics and temporal context. Extensive experiments demonstrate
that VCC achieves state-of-the-art VAD performance. Our codes and results are
open at \url{https://github.com/yuguangnudt/VEC_VAD/tree/VCC}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiping Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1"&gt;En Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Convergence of DETR with Spatially Modulated Co-Attention. (arXiv:2108.02404v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02404</id>
        <link href="http://arxiv.org/abs/2108.02404"/>
        <updated>2021-08-06T00:51:45.179Z</updated>
        <summary type="html"><![CDATA[The recently proposed Detection Transformer (DETR) model successfully applies
Transformer to objects detection and achieves comparable performance with
two-stage object detection frameworks, such as Faster-RCNN. However, DETR
suffers from its slow convergence. Training DETR from scratch needs 500 epochs
to achieve a high accuracy. To accelerate its convergence, we propose a simple
yet effective scheme for improving the DETR framework, namely Spatially
Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct
location-aware co-attention in DETR by constraining co-attention responses to
be high near initially estimated bounding box locations. Our proposed SMCA
increases DETR's convergence speed by replacing the original co-attention
mechanism in the decoder while keeping other operations in DETR unchanged.
Furthermore, by integrating multi-head and scale-selection attention designs
into SMCA, our fully-fledged SMCA can achieve better performance compared to
DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3
mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to
validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Minghang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution. (arXiv:2108.02348v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02348</id>
        <link href="http://arxiv.org/abs/2108.02348"/>
        <updated>2021-08-06T00:51:45.161Z</updated>
        <summary type="html"><![CDATA[For deep learning methods of image super-resolution, the most critical issue
is whether the paired low and high resolution images for training accurately
reflect the sampling process of real cameras. Low and high resolution
(LR$\sim$HR) image pairs synthesized by existing degradation models (\eg,
bicubic downsampling) deviate from those in reality; thus the super-resolution
CNN trained by these synthesized LR$\sim$HR image pairs does not perform well
when being applied to real images. In this paper, we propose a novel method to
capture a large set of realistic LR$\sim$HR image pairs using real cameras.The
data acquisition is carried out under controllable lab conditions with minimum
human intervention and at high throughput (about 500 image pairs per hour). The
high level of automation makes it easy to produce a set of real LR$\sim$HR
training image pairs for each camera. Our innovation is to shoot images
displayed on an ultra-high quality screen at different resolutions.There are
three distinctive advantages with our method that allow us to collect
high-quality training datasets for image super-resolution. First, as the LR and
HR images are taken of a 3D planar surface (the screen) the registration
problem fits exactly to a homography model. Second, we can display special
markers on the image margin to further improve the registration
precision.Third, the displayed digital image file can be exploited as a
reference to optimize the high frequency content of the restored image.
Experimental results show that training a super-resolution CNN by our
LR$\sim$HR dataset has superior restoration performance than training it by
existing datasets on real world images at the inference stage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanhui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiao Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaolin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pan-Cancer Integrative Histology-Genomic Analysis via Interpretable Multimodal Deep Learning. (arXiv:2108.02278v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02278</id>
        <link href="http://arxiv.org/abs/2108.02278"/>
        <updated>2021-08-06T00:51:45.144Z</updated>
        <summary type="html"><![CDATA[The rapidly emerging field of deep learning-based computational pathology has
demonstrated promise in developing objective prognostic models from histology
whole slide images. However, most prognostic models are either based on
histology or genomics alone and do not address how histology and genomics can
be integrated to develop joint image-omic prognostic models. Additionally
identifying explainable morphological and molecular descriptors from these
models that govern such prognosis is of interest. We used multimodal deep
learning to integrate gigapixel whole slide pathology images, RNA-seq
abundance, copy number variation, and mutation data from 5,720 patients across
14 major cancer types. Our interpretable, weakly-supervised, multimodal deep
learning algorithm is able to fuse these heterogeneous modalities for
predicting outcomes and discover prognostic features from these modalities that
corroborate with poor and favorable outcomes via multimodal interpretability.
We compared our model with unimodal deep learning models trained on histology
slides and molecular profiles alone, and demonstrate performance increase in
risk stratification on 9 out of 14 cancers. In addition, we analyze morphologic
and molecular markers responsible for prognostic predictions across all cancer
types. All analyzed data, including morphological and molecular correlates of
patient prognosis across the 14 cancer types at a disease and patient level are
presented in an interactive open-access database
(this http URL) to allow for further exploration and
prognostic biomarker discovery. To validate that these model explanations are
prognostic, we further analyzed high attention morphological regions in WSIs,
which indicates that tumor-infiltrating lymphocyte presence corroborates with
favorable cancer prognosis on 9 out of 14 cancer types studied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Richard J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F. K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1"&gt;Jana Lipkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaban_M/0/1/0/all/0/1"&gt;Muhammad Shaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shady_M/0/1/0/all/0/1"&gt;Maha Shady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1"&gt;Mane Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_B/0/1/0/all/0/1"&gt;Bumjin Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noor_Z/0/1/0/all/0/1"&gt;Zahra Noor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval. (arXiv:2108.02417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02417</id>
        <link href="http://arxiv.org/abs/2108.02417"/>
        <updated>2021-08-06T00:51:45.136Z</updated>
        <summary type="html"><![CDATA[The current state-of-the-art image-sentence retrieval methods implicitly
align the visual-textual fragments, like regions in images and words in
sentences, and adopt attention modules to highlight the relevance of
cross-modal semantic correspondences. However, the retrieval performance
remains unsatisfactory due to a lack of consistent representation in both
semantics and structural spaces. In this work, we propose to address the above
issue from two aspects: (i) constructing intrinsic structure (along with
relations) among the fragments of respective modalities, e.g., "dog $\to$ play
$\to$ ball" in semantic structure for an image, and (ii) seeking explicit
inter-modal structural and semantic correspondence between the visual and
textual modalities. In this paper, we propose a novel Structured Multi-modal
Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In
order to jointly and explicitly learn the visual-textual embedding and the
cross-modal alignment, SMFEA creates a novel multi-modal structured module with
a shared context-aware referral tree. In particular, the relations of the
visual and textual fragments are modeled by constructing Visual Context-aware
Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree
encoder (TCS-Tree) with shared labels, from which visual and textual features
can be jointly learned and optimized. We utilize the multi-modal tree structure
to explicitly align the heterogeneous image-sentence data by maximizing the
semantic and structural similarity between corresponding inter-modal tree
nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks
demonstrate the superiority of the proposed model in comparison to the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xuri Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fuhai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1"&gt;Joemon M. Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhilong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Railway Foreign Object Detection: A Semi-supervised Convolutional Autoencoder Based Method. (arXiv:2108.02421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02421</id>
        <link href="http://arxiv.org/abs/2108.02421"/>
        <updated>2021-08-06T00:51:45.129Z</updated>
        <summary type="html"><![CDATA[Automated inspection and detection of foreign objects on railways is
important for rail transportation safety as it helps prevent potential
accidents and trains derailment. Most existing vision-based approaches focus on
the detection of frontal intrusion objects with prior labels, such as
categories and locations of the objects. In reality, foreign objects with
unknown categories can appear anytime on railway tracks. In this paper, we
develop a semi-supervised convolutional autoencoder based framework that only
requires railway track images without prior knowledge on the foreign objects in
the training process. It consists of three different modules, a bottleneck
feature generator as encoder, a photographic image generator as decoder, and a
reconstruction discriminator developed via adversarial learning. In the
proposed framework, the problem of detecting the presence, location, and shape
of foreign objects is addressed by comparing the input and reconstructed images
as well as setting thresholds based on reconstruction errors. The proposed
method is evaluated through comprehensive studies under different performance
criteria. The results show that the proposed method outperforms some well-known
benchmarking methods. The proposed framework is useful for data analytics via
the train Internet-of-Things (IoT) systems]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fangfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02235</id>
        <link href="http://arxiv.org/abs/2108.02235"/>
        <updated>2021-08-06T00:51:45.120Z</updated>
        <summary type="html"><![CDATA[Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods based on meta-learning have achieved promising performance, such as
Meta R-CNN series. However, only a single category of support data is used as
the attention to guide the detecting of query images each time. Their relevance
to each other remains unexploited. Moreover, a lot of recent works treat the
support data and query images as independent branch without considering the
relationship between them. To address this issue, we propose a dynamic
relevance learning model, which utilizes the relationship between all support
images and Region of Interest (RoI) on the query images to construct a dynamic
graph convolutional network (GCN). By adjusting the prediction distribution of
the base detector using the output of this GCN, the proposed model can guide
the detector to improve the class representation implicitly. Comprehensive
experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed
model achieves the best overall performance, which shows its effectiveness of
learning more generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1"&gt;Chong Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haohe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shenghao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Song Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xulun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiafei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Transformer for Scalable Inference in Open-domain Question Answering. (arXiv:2108.02765v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02765</id>
        <link href="http://arxiv.org/abs/2108.02765"/>
        <updated>2021-08-06T00:51:45.090Z</updated>
        <summary type="html"><![CDATA[Large transformer models, such as BERT, achieve state-of-the-art results in
machine reading comprehension (MRC) for open-domain question answering (QA).
However, transformers have a high computational cost for inference which makes
them hard to apply to online QA systems for applications like voice assistants.
To reduce computational cost and latency, we propose decoupling the transformer
MRC model into input-component and cross-component. The decoupling allows for
part of the representation computation to be performed offline and cached for
online use. To retain the decoupled transformer accuracy, we devised a
knowledge distillation objective from a standard transformer model. Moreover,
we introduce learned representation compression layers which help reduce by
four times the storage requirement for the cache. In experiments on the SQUAD
2.0 dataset, a decoupled transformer reduces the computational cost and latency
of open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a
standard transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+ElFadeel_H/0/1/0/all/0/1"&gt;Haytham ElFadeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1"&gt;Stan Peshterliev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Few-shot Semantic Segmentation with Transformers. (arXiv:2108.02266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02266</id>
        <link href="http://arxiv.org/abs/2108.02266"/>
        <updated>2021-08-06T00:51:45.083Z</updated>
        <summary type="html"><![CDATA[Due to the fact that fully supervised semantic segmentation methods require
sufficient fully-labeled data to work well and can not generalize to unseen
classes, few-shot segmentation has attracted lots of research attention.
Previous arts extract features from support and query images, which are
processed jointly before making predictions on query images. The whole process
is based on convolutional neural networks (CNN), leading to the problem that
only local information is used. In this paper, we propose a TRansformer-based
Few-shot Semantic segmentation method (TRFS). Specifically, our model consists
of two modules: Global Enhancement Module (GEM) and Local Enhancement Module
(LEM). GEM adopts transformer blocks to exploit global information, while LEM
utilizes conventional convolutions to exploit local information, across query
and support features. Both GEM and LEM are complementary, helping to learn
better feature representations for segmenting query images. Extensive
experiments on PASCAL-5i and COCO datasets show that our approach achieves new
state-of-the-art performance, demonstrating its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jingyun Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06337</id>
        <link href="http://arxiv.org/abs/2105.06337"/>
        <updated>2021-08-06T00:51:45.076Z</updated>
        <summary type="html"><![CDATA[Recently, denoising diffusion probabilistic models and generative score
matching have shown high potential in modelling complex data distributions
while stochastic calculus has provided a unified point of view on these
techniques allowing for flexible inference schemes. In this paper we introduce
Grad-TTS, a novel text-to-speech model with score-based decoder producing
mel-spectrograms by gradually transforming noise predicted by encoder and
aligned with text input by means of Monotonic Alignment Search. The framework
of stochastic differential equations helps us to generalize conventional
diffusion probabilistic models to the case of reconstructing data from noise
with different parameters and allows to make this reconstruction flexible by
explicitly controlling trade-off between sound quality and inference speed.
Subjective human evaluation shows that Grad-TTS is competitive with
state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We
will make the code publicly available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1"&gt;Vadim Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1"&gt;Ivan Vovk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1"&gt;Vladimir Gogoryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1"&gt;Tasnima Sadekova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1"&gt;Mikhail Kudinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02446</id>
        <link href="http://arxiv.org/abs/2108.02446"/>
        <updated>2021-08-06T00:51:45.066Z</updated>
        <summary type="html"><![CDATA[Text variational autoencoders (VAEs) are notorious for posterior collapse, a
phenomenon where the model's decoder learns to ignore signals from the encoder.
Because posterior collapse is known to be exacerbated by expressive decoders,
Transformers have seen limited adoption as components of text VAEs. Existing
studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et
al., 2021) mitigate posterior collapse using massive pretraining, a technique
unavailable to most of the research community without extensive computing
resources. We present a simple two-phase training scheme to convert a
sequence-to-sequence Transformer into a VAE with just finetuning. The resulting
language model is competitive with massively pretrained Transformer-based VAEs
in some internal metrics while falling short on others. To facilitate training
we comprehensively explore the impact of common posterior collapse alleviation
techniques in the literature. We release our code for reproducability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihwa Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02725</id>
        <link href="http://arxiv.org/abs/2108.02725"/>
        <updated>2021-08-06T00:51:45.047Z</updated>
        <summary type="html"><![CDATA[Numerous online stock image libraries offer high quality yet copyright free
images for use in marketing campaigns. To assist advertisers in navigating such
third party libraries, we study the problem of automatically fetching relevant
ad images given the ad text (via a short textual query for images). Motivated
by our observations in logged data on ad image search queries (given ad text),
we formulate a keyword extraction problem, where a keyword extracted from the
ad text (or its augmented version) serves as the ad image query. In this
context, we propose VisualTextRank: an unsupervised method to (i) augment input
ad text using semantically similar ads, and (ii) extract the image query from
the augmented ad text. VisualTextRank builds on prior work on graph based
context extraction (biased TextRank in particular) by leveraging both the text
and image of similar ads for better keyword extraction, and using advertiser
category specific biasing with sentence-BERT embeddings. Using data collected
from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search
feature for onboarding advertisers, we demonstrate the superiority of
VisualTextRank compared to competitive keyword extraction baselines (including
an $11\%$ accuracy lift over biased TextRank). For the case when the stock
image library is restricted to English queries, we show the effectiveness of
VisualTextRank on multilingual ads (translated to English) while leveraging
semantically similar English ads. Online tests with a simplified version of
VisualTextRank led to a 28.7% increase in the usage of stock image search, and
a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native
ad platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1"&gt;Mikhail Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1"&gt;Gaurav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding. (arXiv:2108.02388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02388</id>
        <link href="http://arxiv.org/abs/2108.02388"/>
        <updated>2021-08-06T00:51:45.027Z</updated>
        <summary type="html"><![CDATA[Recently proposed fine-grained 3D visual grounding is an essential and
challenging task, whose goal is to identify the 3D object referred by a natural
language sentence from other distractive objects of the same category. Existing
works usually adopt dynamic graph networks to indirectly model the
intra/inter-modal interactions, making the model difficult to distinguish the
referred object from distractors due to the monolithic representations of
visual and linguistic contents. In this work, we exploit Transformer for its
natural suitability on permutation-invariant 3D point clouds data and propose a
TransRefer3D network to extract entity-and-relation aware multimodal context
among objects for more discriminative feature learning. Concretely, we devise
an Entity-aware Attention (EA) module and a Relation-aware Attention (RA)
module to conduct fine-grained cross-modal feature matching. Facilitated by
co-attention operation, our EA module matches visual entity features with
linguistic entity features while RA module matches pair-wise visual relation
features with linguistic relation features, respectively. We further integrate
EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and
stack several ERCBs to form our TransRefer3D for hierarchical multimodal
context modeling. Extensive experiments on both Nr3D and Sr3D datasets
demonstrate that our proposed model significantly outperforms existing
approaches by up to 10.6% and claims the new state-of-the-art. To the best of
our knowledge, this is the first work investigating Transformer architecture
for fine-grained 3D visual grounding task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+he_D/0/1/0/all/0/1"&gt;Dailan he&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yusheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Junyu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1"&gt;Tianrui Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaofei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aixi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02605</id>
        <link href="http://arxiv.org/abs/2108.02605"/>
        <updated>2021-08-06T00:51:45.020Z</updated>
        <summary type="html"><![CDATA[This report presents the results of the EENLP project, done as a part of EEML
2021 summer school.

It presents a broad index of NLP resources for Eastern European languages,
which, we hope, could be helpful for the NLP community; several new
hand-crafted cross-lingual datasets focused on Eastern European languages, and
a sketch evaluation of cross-lingual transfer learning abilities of several
modern multilingual Transformer-based models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1"&gt;Alex Malkhasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1"&gt;Andrey Manoshin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1"&gt;George Dima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1"&gt;R&amp;#xe9;ka Cserh&amp;#xe1;ti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;Md.Sadek Hossain Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1"&gt;Matt S&amp;#xe1;rdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02561</id>
        <link href="http://arxiv.org/abs/2108.02561"/>
        <updated>2021-08-06T00:51:45.013Z</updated>
        <summary type="html"><![CDATA[Single online handwritten Chinese character recognition~(single OLHCCR) has
achieved prominent performance. However, in real application scenarios, users
always write multiple Chinese characters to form one complete sentence and the
contextual information within these characters holds the significant potential
to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In
this work, we first propose a simple and straightforward end-to-end network,
namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.
It couples convolutional neural network with sequence modeling architecture to
exploit the handwritten character's previous contextual information. Although
VCN performs much better than the state-of-the-art single OLHCCR model, it
exposes high fragility when confronting with not well written characters such
as sloppy writing, missing or broken strokes. To improve the robustness of
sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion
network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the
backbone component, which projects each Chinese character into word embeddings,
and integrates the spatial glyph features of handwritten characters and their
contextual information multiple times at multi-layer fusion module. We also
construct a large-scale sentence-level handwriting dataset, named as CSOHD to
evaluate models. Extensive experiment results demonstrate that DSTFN achieves
the state-of-the-art performance, which presents strong robustness compared
with VCN and exiting single OLHCCR models. The in-depth empirical analysis and
case studies indicate that DSTFN can significantly improve the efficiency of
handwriting input, with the handwritten Chinese character with incomplete
strokes being recognized precisely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Baotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuxin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security and Privacy Enhanced Gait Authentication with Random Representation Learning and Digital Lockers. (arXiv:2108.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02400</id>
        <link href="http://arxiv.org/abs/2108.02400"/>
        <updated>2021-08-06T00:51:45.006Z</updated>
        <summary type="html"><![CDATA[Gait data captured by inertial sensors have demonstrated promising results on
user authentication. However, most existing approaches stored the enrolled gait
pattern insecurely for matching with the validating pattern, thus, posed
critical security and privacy issues. In this study, we present a gait
cryptosystem that generates from gait data the random key for user
authentication, meanwhile, secures the gait pattern. First, we propose a
revocable and random binary string extraction method using a deep neural
network followed by feature-wise binarization. A novel loss function for
network optimization is also designed, to tackle not only the intrauser
stability but also the inter-user randomness. Second, we propose a new
biometric key generation scheme, namely Irreversible Error Correct and
Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,
to securely generate from the binary string the random and irreversible key.
The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We
showed that our model could generate the key of 139 bits from 5-second data
sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)
smaller than 5.441%. In addition, the security and user privacy analyses showed
that our model was secure against existing attacks on biometric template
protection, and fulfilled irreversibility and unlinkability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1"&gt;Lam Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunil Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Deokjai Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Out-of-Distribution Generalization in Text Classifiers Trained on Tobacco-3482 and RVL-CDIP. (arXiv:2108.02684v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02684</id>
        <link href="http://arxiv.org/abs/2108.02684"/>
        <updated>2021-08-06T00:51:44.999Z</updated>
        <summary type="html"><![CDATA[To be robust enough for widespread adoption, document analysis systems
involving machine learning models must be able to respond correctly to inputs
that fall outside of the data distribution that was used to generate the data
on which the models were trained. This paper explores the ability of text
classifiers trained on standard document classification datasets to generalize
to out-of-distribution documents at inference time. We take the Tobacco-3482
and RVL-CDIP datasets as a starting point and generate new out-of-distribution
evaluation datasets in order to analyze the generalization performance of
models trained on these standard datasets. We find that models trained on the
smaller Tobacco-3482 dataset perform poorly on our new out-of-distribution
data, while text classification models trained on the larger RVL-CDIP exhibit
smaller performance drops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1"&gt;Stefan Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navtej Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1"&gt;Saarthak Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_S/0/1/0/all/0/1"&gt;Shanti Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_U/0/1/0/all/0/1"&gt;Uma Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02365</id>
        <link href="http://arxiv.org/abs/2108.02365"/>
        <updated>2021-08-06T00:51:44.981Z</updated>
        <summary type="html"><![CDATA[The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weijiang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuejian Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1"&gt;Nong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation. (arXiv:2108.02376v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02376</id>
        <link href="http://arxiv.org/abs/2108.02376"/>
        <updated>2021-08-06T00:51:44.972Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a crucial image understanding task, where each pixel
of image is categorized into a corresponding label. Since the pixel-wise
labeling for ground-truth is tedious and labor intensive, in practical
applications, many works exploit the synthetic images to train the model for
real-word image semantic segmentation, i.e., Synthetic-to-Real Semantic
Segmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained
on the source synthetic data may not generalize well to the target real-world
data. In this work, we propose two simple yet effective texture randomization
mechanisms, Global Texture Randomization (GTR) and Local Texture Randomization
(LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the
texture of source images into diverse unreal texture styles. It aims to
alleviate the reliance of the network on texture while promoting the learning
of the domain-invariant cues. In addition, we find the texture difference is
not always occurred in entire image and may only appear in some local areas.
Therefore, we further propose a LTR mechanism to generate diverse local regions
for partially stylizing the source images. Finally, we implement a
regularization of Consistency between GTR and LTR (CGL) aiming to harmonize the
two proposed mechanisms during training. Extensive experiments on five publicly
available datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with
various SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary)
demonstrate that the proposed method is superior to the state-of-the-art
methods for domain generalization based SRSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Duo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot. (arXiv:2108.02385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02385</id>
        <link href="http://arxiv.org/abs/2108.02385"/>
        <updated>2021-08-06T00:51:44.960Z</updated>
        <summary type="html"><![CDATA[One-stage long-tailed recognition methods improve the overall performance in
a "seesaw" manner, i.e., either sacrifice the head's accuracy for better tail
classification or elevate the head's accuracy even higher but ignore the tail.
Existing algorithms bypass such trade-off by a multi-stage training process:
pre-training on imbalanced set and fine-tuning on balanced set. Though
achieving promising performance, not only are they sensitive to the
generalizability of the pre-trained model, but also not easily integrated into
other computer vision tasks like detection and segmentation, where pre-training
of classifiers solely is not applicable. In this paper, we propose a one-stage
long-tailed recognition scheme, ally complementary experts (ACE), where the
expert is the most knowledgeable specialist in a sub-set that dominates its
training, and is complementary to other experts in the less-seen categories
without being disturbed by what it has never seen. We design a
distribution-adaptive optimizer to adjust the learning pace of each expert to
avoid over-fitting. Without special bells and whistles, the vanilla ACE
outperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,
CIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the
first one to break the "seesaw" trade-off by improving the accuracy of the
majority and minority categories simultaneously in only one stage. Code and
trained models are at https://github.com/jrcai/ACE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jiarui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jenq-Neng Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02352</id>
        <link href="http://arxiv.org/abs/2108.02352"/>
        <updated>2021-08-06T00:51:44.953Z</updated>
        <summary type="html"><![CDATA[Aspect-level sentiment classification (ASC) aims to predict the fine-grained
sentiment polarity towards a given aspect mentioned in a review. Despite recent
advances in ASC, enabling machines to preciously infer aspect sentiments is
still challenging. This paper tackles two challenges in ASC: (1) due to lack of
aspect knowledge, aspect representation derived in prior works is inadequate to
represent aspect's exact meaning and property information; (2) prior works only
capture either local syntactic information or global relational information,
thus missing either one of them leads to insufficient syntactic information. To
tackle these challenges, we propose a novel ASC model which not only end-to-end
embeds and leverages aspect knowledge but also marries the two kinds of
syntactic information and lets them compensate for each other. Our model
includes three key components: (1) a knowledge-aware gated recurrent memory
network recurrently integrates dynamically summarized aspect knowledge; (2) a
dual syntax graph network combines both kinds of syntactic information to
comprehensively capture sufficient syntactic information; (3) a knowledge
integrating gate re-enhances the final representation with further needed
aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the
aspect-related semantics from all hidden states into the final representation.
Experimental results on several benchmark datasets demonstrate the
effectiveness of our model, which overpass previous state-of-the-art models by
large margins in terms of both Accuracy and Macro-F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1"&gt;Bowen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W. Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning. (arXiv:2108.02366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02366</id>
        <link href="http://arxiv.org/abs/2108.02366"/>
        <updated>2021-08-06T00:51:44.946Z</updated>
        <summary type="html"><![CDATA[Existing image captioning methods just focus on understanding the
relationship between objects or instances in a single image, without exploring
the contextual correlation existed among contextual image. In this paper, we
propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and
curriculum learning for image captioning. In particular, we not only use an
object-level GCN to capture the object to object spatial relation within a
single image, but also adopt an image-level GCN to capture the feature
information provided by similar images. With the well-designed Dual-GCN, we can
make the linguistic transformer better understand the relationship between
different objects in a single image and make full use of similar images as
auxiliary information to generate a reasonable caption description for a single
image. Meanwhile, with a cross-review strategy introduced to determine
difficulty levels, we adopt curriculum learning as the training strategy to
increase the robustness and generalization of our proposed model. We conduct
extensive experiments on the large-scale MS COCO dataset, and the experimental
results powerfully demonstrate that our proposed method outperforms recent
state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2
score of 67.6. Our source code is available at {\em
\color{magenta}{\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinzhi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenju Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[dp-GAN : Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02353</id>
        <link href="http://arxiv.org/abs/2108.02353"/>
        <updated>2021-08-06T00:51:44.896Z</updated>
        <summary type="html"><![CDATA[The vanilla GAN [5] suffers from mode collapse deeply, which usually
manifests as that the images generated by generators tend to have a high
similarity amongst them, even though their corresponding latent vectors have
been very different. In this paper, we introduce a pluggable block called
diversity penalty (dp) to alleviate mode collapse of GANs. It is used to reduce
the similarity of image pairs in feature space, i.e., if two latent vectors are
different, then we enforce the generator to generate two images with different
features. The normalized Gram Matrix is used to measure the similarity. We
compare the proposed method with Unrolled GAN [17], BourGAN [26], PacGAN [14],
VEEGAN [23] and ALI [4] on 2D synthetic dataset, and results show that our
proposed method can help GAN capture more modes of the data distribution.
Further, we apply this penalty term into image data augmentation on MNIST,
Fashion-MNIST and CIFAR-10, and the testing accuracy is improved by 0.24%,
1.34% and 0.52% compared with WGAN GP [6], respectively. Finally, we
quantitatively evaluate the proposed method with IS and FID on CelebA,
CIFAR-10, MNIST and Fashion-MNIST. Results show that our method gets much
higher IS and lower FID compared with some current GAN architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1"&gt;Sen Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Richard Yi Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1"&gt;Gaofeng Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02234</id>
        <link href="http://arxiv.org/abs/2108.02234"/>
        <updated>2021-08-06T00:51:44.857Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel hand-based person recognition method for
the purpose of criminal investigations since the hand image is often the only
available information in cases of serious crime such as sexual abuse. Our
proposed method, Multi-Branch with Attention Network (MBA-Net), incorporates
both channel and spatial attention modules in branches in addition to a global
(without attention) branch to capture global structural information for
discriminative feature learning. The attention modules focus on the relevant
features of the hand image while suppressing the irrelevant backgrounds. In
order to overcome the weakness of the attention mechanisms, equivariant to
pixel shuffling, we integrate relative positional encodings into the spatial
attention module to capture the spatial positions of pixels. Extensive
evaluations on two large multi-ethnic and publicly available hand datasets
demonstrate that our proposed method achieves state-of-the-art performance,
surpassing the existing hand-based identification methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Bryan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1"&gt;Plamen Angelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1"&gt;Sue Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Fourier single-pixel imaging with Gaussian random sampling. (arXiv:2108.02317v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02317</id>
        <link href="http://arxiv.org/abs/2108.02317"/>
        <updated>2021-08-06T00:51:44.849Z</updated>
        <summary type="html"><![CDATA[Fourier single-pixel imaging (FSI) is a branch of single-pixel imaging
techniques. It uses Fourier basis patterns as structured patterns for spatial
information acquisition in the Fourier domain. However, the spatial resolution
of the image reconstructed by FSI mainly depends on the number of Fourier
coefficients sampled. The reconstruction of a high-resolution image typically
requires a number of Fourier coefficients to be sampled, and therefore takes a
long data acquisition time. Here we propose a new sampling strategy for FSI. It
allows FSI to reconstruct a clear and sharp image with a reduced number of
measurements. The core of the proposed sampling strategy is to perform a
variable density sampling in the Fourier space and, more importantly, the
density with respect to the importance of Fourier coefficients is subject to a
one-dimensional Gaussian function. Combined with compressive sensing, the
proposed sampling strategy enables better reconstruction quality than
conventional sampling strategies, especially when the sampling ratio is low. We
experimentally demonstrate compressive FSI combined with the proposed sampling
strategy is able to reconstruct a sharp and clear image of 256-by-256 pixels
with a sampling ratio of 10%. The proposed method enables fast single-pixel
imaging and provides a new approach for efficient spatial information
acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Ziheng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xinyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tianao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1"&gt;Pan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zibang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1"&gt;Jingang Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of Machine Learning on 2D/3D Registration for Image-guided Interventions: A Systematic Review and Perspective. (arXiv:2108.02238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02238</id>
        <link href="http://arxiv.org/abs/2108.02238"/>
        <updated>2021-08-06T00:51:44.842Z</updated>
        <summary type="html"><![CDATA[Image-based navigation is widely considered the next frontier of minimally
invasive surgery. It is believed that image-based navigation will increase the
access to reproducible, safe, and high-precision surgery as it may then be
performed at acceptable costs and effort. This is because image-based
techniques avoid the need of specialized equipment and seamlessly integrate
with contemporary workflows. Further, it is expected that image-based
navigation will play a major role in enabling mixed reality environments and
autonomous, robotic workflows. A critical component of image guidance is 2D/3D
registration, a technique to estimate the spatial relationships between 3D
structures, e.g., volumetric imagery or tool models, and 2D images thereof,
such as fluoroscopy or endoscopy. While image-based 2D/3D registration is a
mature technique, its transition from the bench to the bedside has been
restrained by well-known challenges, including brittleness of the optimization
objective, hyperparameter selection, and initialization, difficulties around
inconsistencies or multiple objects, and limited single-view performance. One
reason these challenges persist today is that analytical solutions are likely
inadequate considering the complexity, variability, and high-dimensionality of
generic 2D/3D registration problems. The recent advent of machine
learning-based approaches to imaging problems that, rather than specifying the
desired functional mapping, approximate it using highly expressive parametric
models holds promise for solving some of the notorious challenges in 2D/3D
registration. In this manuscript, we review the impact of machine learning on
2D/3D registration to systematically summarize the recent advances made by
introduction of this novel technology. Grounded in these insights, we then
offer our perspective on the most pressing needs, significant open problems,
and possible next steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Cong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yicheng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judish_M/0/1/0/all/0/1"&gt;Max Judish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armand_M/0/1/0/all/0/1"&gt;Mehran Armand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grupp_R/0/1/0/all/0/1"&gt;Robert Grupp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12452</id>
        <link href="http://arxiv.org/abs/2102.12452"/>
        <updated>2021-08-06T00:51:44.821Z</updated>
        <summary type="html"><![CDATA[Probing classifiers have emerged as one of the prominent methodologies for
interpreting and analyzing deep neural network models of natural language
processing. The basic idea is simple---a classifier is trained to predict some
linguistic property from a model's representations---and has been used to
examine a wide variety of models and properties. However, recent studies have
demonstrated various methodological weaknesses of this approach. This article
critically reviews the probing classifiers framework, highlighting their
promises, shortcomings, and advances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1"&gt;Yonatan Belinkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Terabyte-scale supervised 3D training and benchmarking dataset of the mouse kidney. (arXiv:2108.02226v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02226</id>
        <link href="http://arxiv.org/abs/2108.02226"/>
        <updated>2021-08-06T00:51:44.814Z</updated>
        <summary type="html"><![CDATA[The performance of machine learning algorithms used for the segmentation of
3D biomedical images lags behind that of the algorithms employed in the
classification of 2D photos. This may be explained by the comparative lack of
high-volume, high-quality training datasets, which require state-of-the art
imaging facilities, domain experts for annotation and large computational and
personal resources to create. The HR-Kidney dataset presented in this work
bridges this gap by providing 1.7 TB of artefact-corrected synchrotron
radiation-based X-ray phase-contrast microtomography images of whole mouse
kidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2
orders of magnitude increase over currently available biomedical datasets. The
dataset further contains the underlying raw data, classical segmentations of
renal vasculature and uriniferous tubules, as well as true 3D manual
annotations. By removing limits currently imposed by small training datasets,
the provided data open up the possibility for disruptions in machine learning
for biomedical image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1"&gt;Willy Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossinelli_D/0/1/0/all/0/1"&gt;Diego Rossinelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_G/0/1/0/all/0/1"&gt;Georg Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenger_R/0/1/0/all/0/1"&gt;Roland H. Wenger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hieber_S/0/1/0/all/0/1"&gt;Simone Hieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1"&gt;Bert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurtcuoglu_V/0/1/0/all/0/1"&gt;Vartan Kurtcuoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03095</id>
        <link href="http://arxiv.org/abs/2105.03095"/>
        <updated>2021-08-06T00:51:44.807Z</updated>
        <summary type="html"><![CDATA[Having numerous potential applications and great impact, end-to-end speech
translation (ST) has long been treated as an independent task, failing to fully
draw strength from the rapid advances of its sibling - text machine translation
(MT). With text and audio inputs represented differently, the modality gap has
rendered MT data and its end-to-end models incompatible with their ST
counterparts. In observation of this obstacle, we propose to bridge this
representation gap with Chimera. By projecting audio and text features to a
common semantic representation, Chimera unifies MT and ST tasks and boosts the
performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new
state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,
improving the SOTA by a +1.9 BLEU margin. Further experimental analyses
demonstrate that the shared semantic space indeed conveys common knowledge
between these two tasks and thus paves a new way for augmenting training
resources across modalities. Code, data, and resources are available at
https://github.com/Glaciohound/Chimera-ST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02223</id>
        <link href="http://arxiv.org/abs/2108.02223"/>
        <updated>2021-08-06T00:51:44.800Z</updated>
        <summary type="html"><![CDATA[Deep learning based analysis of histopathology images shows promise in
advancing the understanding of tumor progression, tumor micro-environment, and
their underpinning biological processes. So far, these approaches have focused
on extracting information associated with annotations. In this work, we ask how
much information can be learned from the tissue architecture itself.

We present an adversarial learning model to extract feature representations
of cancer tissue, without the need for manual annotations. We show that these
representations are able to identify a variety of morphological characteristics
across three cancer types: Breast, colon, and lung. This is supported by 1) the
separation of morphologic characteristics in the latent space; 2) the ability
to classify tissue type with logistic regression using latent representations,
with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)
the ability to predict the presence of tumor in Whole Slide Images (WSIs) using
multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.

Our results show that our model captures distinct phenotypic characteristics
of real tissue samples, paving the way for further understanding of tumor
progression and tumor micro-environment, and ultimately refining
histopathological classification for diagnosis and treatment. The code and
pretrained models are available at:
https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1"&gt;Adalberto Claudio Quiros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1"&gt;Nicolas Coudray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1"&gt;Anna Yeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1"&gt;Wisuwat Sunhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1"&gt;Roderick Murray-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1"&gt;Aristotelis Tsirigos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Ke Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02233</id>
        <link href="http://arxiv.org/abs/2108.02233"/>
        <updated>2021-08-06T00:51:44.792Z</updated>
        <summary type="html"><![CDATA[Lung nodules are commonly missed in chest radiographs. We propose and
evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules
in radiographs. P-AnoGAN modifies the fast anomaly detection generative
adversarial network (f-AnoGAN) by utilizing a progressive GAN and a
convolutional encoder-decoder-encoder pipeline. Model training uses only
unlabelled healthy lung patches extracted from the Indiana University Chest
X-Ray Collection. External validation and testing are performed using healthy
and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for
Radiological Technology datasets, respectively. Our model robustly identifies
patches containing lung nodules in external validation and test data with
ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised
methods may be useful in challenging tasks such as lung nodule detection in
radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nitish Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1"&gt;David Ramon Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1"&gt;Nedim Hodzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1"&gt;Christos Karanassios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04082</id>
        <link href="http://arxiv.org/abs/2107.04082"/>
        <updated>2021-08-06T00:51:44.765Z</updated>
        <summary type="html"><![CDATA[Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 25 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1"&gt;Diptanu Gon Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kritika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1"&gt;Alexis Conneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1"&gt;Assaf Sela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02776</id>
        <link href="http://arxiv.org/abs/2108.02776"/>
        <updated>2021-08-06T00:51:44.726Z</updated>
        <summary type="html"><![CDATA[This paper presents Sinsy, a deep neural network (DNN)-based singing voice
synthesis (SVS) system. In recent years, DNNs have been utilized in statistical
parametric SVS systems, and DNN-based SVS systems have demonstrated better
performance than conventional hidden Markov model-based ones. SVS systems are
required to synthesize a singing voice with pitch and timing that strictly
follow a given musical score. Additionally, singing expressions that are not
described on the musical score, such as vibrato and timing fluctuations, should
be reproduced. The proposed system is composed of four modules: a time-lag
model, a duration model, an acoustic model, and a vocoder, and singing voices
can be synthesized taking these characteristics of singing voices into account.
To better model a singing voice, the proposed system incorporates improved
approaches to modeling pitch and vibrato and better training criteria into the
acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive
neural vocoder with robustness for the pitch, into our systems to generate a
high-fidelity singing voice waveform. Moreover, we propose automatic pitch
correction techniques for DNN-based SVS to synthesize singing voices with
correct pitch even if the training data has out-of-tune phrases. Experimental
results show our system can synthesize a singing voice with better timing, more
natural vibrato, and correct pitch, and it can achieve better mean opinion
scores in subjective evaluation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1"&gt;Yukiya Hono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kei Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1"&gt;Keiichiro Oura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1"&gt;Yoshihiko Nankaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1"&gt;Keiichi Tokuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recipes for Safety in Open-domain Chatbots. (arXiv:2010.07079v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07079</id>
        <link href="http://arxiv.org/abs/2010.07079"/>
        <updated>2021-08-06T00:51:44.713Z</updated>
        <summary type="html"><![CDATA[Models trained on large unlabeled corpora of human interactions will learn
patterns and mimic behaviors therein, which include offensive or otherwise
toxic behavior and unwanted biases. We investigate a variety of methods to
mitigate these issues in the context of open-domain generative dialogue models.
We introduce a new human-and-model-in-the-loop framework for both training
safer models and for evaluating them, as well as a novel method to distill
safety considerations inside generative models without the use of an external
classifier at deployment time. We conduct experiments comparing these methods
and find our new techniques are (i) safer than existing models as measured by
automatic and human evaluations while (ii) maintaining usability metrics such
as engagingness relative to the state of the art. We then discuss the
limitations of this work by analyzing failure cases of our models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Margaret Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1"&gt;Y-Lan Boureau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1"&gt;Emily Dinan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference Resolution: Are the eliminated spans totally worthless?. (arXiv:2101.00737v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00737</id>
        <link href="http://arxiv.org/abs/2101.00737"/>
        <updated>2021-08-06T00:51:44.672Z</updated>
        <summary type="html"><![CDATA[Various neural-based methods have been proposed so far for joint mention
detection and coreference resolution. However, existing works on coreference
resolution are mainly dependent on filtered mention representation, while other
spans are largely neglected. In this paper, we aim at increasing the
utilization rate of data and investigating whether those eliminated spans are
totally useless, or to what extent they can improve the performance of
coreference resolution. To achieve this, we propose a mention representation
refining strategy where spans highly related to mentions are well leveraged
using a pointer network for representation enhancing. Notably, we utilize an
additional loss term in this work to encourage the diversity between entity
clusters. Experimental results on the document-level CoNLL-2012 Shared Task
English dataset show that eliminated spans are indeed much effective and our
approach can achieve competitive results when compared with previous
state-of-the-art in coreference resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Longyin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guodong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02607</id>
        <link href="http://arxiv.org/abs/2108.02607"/>
        <updated>2021-08-06T00:51:44.662Z</updated>
        <summary type="html"><![CDATA[We introduce a new efficient framework, the Unified Context Network (UniCon),
for robust active speaker detection (ASD). Traditional methods for ASD usually
operate on each candidate's pre-cropped face track separately and do not
sufficiently consider the relationships among the candidates. This potentially
limits performance, especially in challenging scenarios with low-resolution
faces, multiple candidates, etc. Our solution is a novel, unified framework
that focuses on jointly modeling multiple types of contextual information:
spatial context to indicate the position and scale of each candidate's face,
relational context to capture the visual relationships among the candidates and
contrast audio-visual affinities with each other, and temporal context to
aggregate long-term information and smooth out local uncertainties. Based on
such information, our model optimizes all candidates in a unified process for
robust and reliable ASD. A thorough ablation study is performed on several
challenging ASD benchmarks under different settings. In particular, our method
outperforms the state-of-the-art by a large margin of about 15% mean Average
Precision (mAP) absolute on two challenging subsets: one with three candidate
speakers, and the other with faces smaller than 64 pixels. Together, our UniCon
achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for
the first time on this challenging dataset at the time of submission. Project
website: https://unicon-asd.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuanhang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Susan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification. (arXiv:2108.02598v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02598</id>
        <link href="http://arxiv.org/abs/2108.02598"/>
        <updated>2021-08-06T00:51:44.641Z</updated>
        <summary type="html"><![CDATA[End-to-end intent classification using speech has numerous advantages
compared to the conventional pipeline approach using automatic speech
recognition (ASR), followed by natural language processing modules. It attempts
to predict intent from speech without using an intermediate ASR module.
However, such end-to-end framework suffers from the unavailability of large
speech resources with higher acoustic variation in spoken language
understanding. In this work, we exploit the scope of the transformer
distillation method that is specifically designed for knowledge distillation
from a transformer based language model to a transformer based speech model. In
this regard, we leverage the reliable and widely used bidirectional encoder
representations from transformers (BERT) model as a language model and transfer
the knowledge to build an acoustic model for intent classification using the
speech. In particular, a multilevel transformer based teacher-student model is
designed, and knowledge distillation is performed across attention and hidden
sub-layers of different transformer layers of the student and teacher models.
We achieve an intent classification accuracy of 99.10% and 88.79% for Fluent
speech corpus and ATIS database, respectively. Further, the proposed method
demonstrates better performance and robustness in acoustically degraded
condition compared to the baseline method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yidi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1"&gt;Bidisha Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1"&gt;Maulik Madhavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-clue reconstruction of sharing chains for social media images. (arXiv:2108.02515v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.02515</id>
        <link href="http://arxiv.org/abs/2108.02515"/>
        <updated>2021-08-06T00:51:44.632Z</updated>
        <summary type="html"><![CDATA[The amount of multimedia content shared everyday, combined with the level of
realism reached by recent fake-generating technologies, threatens to impair the
trustworthiness of online information sources. The process of uploading and
sharing data tends to hinder standard media forensic analyses, since multiple
re-sharing steps progressively hide the traces of past manipulations. At the
same time though, new traces are introduced by the platforms themselves,
enabling the reconstruction of the sharing history of digital objects, with
possible applications in information flow monitoring and source identification.
In this work, we propose a supervised framework for the reconstruction of image
sharing chains on social media platforms. The system is structured as a cascade
of backtracking blocks, each of them tracing back one step of the sharing chain
at a time. Blocks are designed as ensembles of classifiers trained to analyse
the input image independently from one another by leveraging different feature
representations that describe both content and container of the media object.
Individual decisions are then properly combined by a late fusion strategy.
Results highlight the advantages of employing multiple clues, which allow
accurately tracing back up to three steps along the sharing chain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verde_S/0/1/0/all/0/1"&gt;Sebastiano Verde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1"&gt;Cecilia Pasquini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lago_F/0/1/0/all/0/1"&gt;Federica Lago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goller_A/0/1/0/all/0/1"&gt;Alessandro Goller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1"&gt;Francesco GB De Natale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piva_A/0/1/0/all/0/1"&gt;Alessandro Piva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1"&gt;Giulia Boato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02625</id>
        <link href="http://arxiv.org/abs/2108.02625"/>
        <updated>2021-08-06T00:51:44.625Z</updated>
        <summary type="html"><![CDATA[This paper makes several contributions to automatic lyrics transcription
(ALT) research. Our main contribution is a novel variant of the Multistreaming
Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which
processes the temporal information using multiple streams in parallel with
varying resolutions keeping the network more compact, and thus with a faster
inference and an improved recognition rate than having identical TDNN streams.
In addition, two novel preprocessing steps prior to training the acoustic model
are proposed. First, we suggest using recordings from both monophonic and
polyphonic domains during training the acoustic model. Second, we tag
monophonic and polyphonic recordings with distinct labels for discriminating
non-vocal silence and music instances during alignment. Moreover, we present a
new test set with a considerably larger size and a higher musical variability
compared to the existing datasets used in ALT literature, while maintaining the
gender balance of the singers. Our best performing model sets the
state-of-the-art in lyrics transcription by a large margin. For
reproducibility, we publicly share the identifiers to retrieve the data used in
this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1"&gt;Emir Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02314</id>
        <link href="http://arxiv.org/abs/2108.02314"/>
        <updated>2021-08-06T00:51:44.610Z</updated>
        <summary type="html"><![CDATA[Enormous hope in the efficacy of vaccines became recently a successful
reality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,
fueled by exposure to social media misinformation about COVID-19 vaccines
became a major hurdle. Therefore, it is essential to automatically detect where
misinformation about COVID-19 vaccines on social media is spread and what kind
of misinformation is discussed, such that inoculation interventions can be
delivered at the right time and in the right place, in addition to
interventions designed to address vaccine hesitancy. This paper is addressing
the first step in tackling hesitancy against COVID-19 vaccines, namely the
automatic detection of misinformation about the vaccines on Twitter, the social
media platform that has the highest volume of conversations about COVID-19 and
its vaccines. We present CoVaxLies, a new dataset of tweets judged relevant to
several misinformation targets about COVID-19 vaccines on which a novel method
of detecting misinformation was developed. Our method organizes CoVaxLies in a
Misinformation Knowledge Graph as it casts misinformation detection as a graph
link prediction problem. The misinformation detection method detailed in this
paper takes advantage of the link scoring functions provided by several
knowledge embedding methods. The experimental results demonstrate the
superiority of this method when compared with classification-based methods,
widely used currently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1"&gt;Maxwell A. Weinzierl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1"&gt;Sanda M. Harabagiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13838</id>
        <link href="http://arxiv.org/abs/2012.13838"/>
        <updated>2021-08-06T00:51:44.601Z</updated>
        <summary type="html"><![CDATA[Pretrained transformers achieve the state of the art across tasks in natural
language processing, motivating researchers to investigate their inner
mechanisms. One common direction is to understand what features are important
for prediction. In this paper, we apply information bottlenecks to analyze the
attribution of each feature for prediction on a black-box model. We use BERT as
the example and evaluate our approach both quantitatively and qualitatively. We
show the effectiveness of our method in terms of attribution and the ability to
provide insight into how information flows through layers. We demonstrate that
our technique outperforms two competitive methods in degradation tests on four
datasets. Code is available at https://github.com/bazingagin/IBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Raphael Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Ji Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:44.588Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bambara Language Dataset for Sentiment Analysis. (arXiv:2108.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02524</id>
        <link href="http://arxiv.org/abs/2108.02524"/>
        <updated>2021-08-06T00:51:44.560Z</updated>
        <summary type="html"><![CDATA[For easier communication, posting, or commenting on each others posts, people
use their dialects. In Africa, various languages and dialects exist. However,
they are still underrepresented and not fully exploited for analytical studies
and research purposes. In order to perform approaches like Machine Learning and
Deep Learning, datasets are required. One of the African languages is Bambara,
used by citizens in different countries. However, no previous work on datasets
for this language was performed for Sentiment Analysis. In this paper, we
present the first common-crawl-based Bambara dialectal dataset dedicated for
Sentiment Analysis, available freely for Natural Language Processing research
purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_M/0/1/0/all/0/1"&gt;Mountaga Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fourati_C/0/1/0/all/0/1"&gt;Chayma Fourati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_H/0/1/0/all/0/1"&gt;Hatem Haddad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02401</id>
        <link href="http://arxiv.org/abs/2108.02401"/>
        <updated>2021-08-06T00:51:44.470Z</updated>
        <summary type="html"><![CDATA[This paper introduces WeChat AI's participation in WMT 2021 shared news
translation task on English->Chinese, English->Japanese, Japanese->English and
English->German. Our systems are based on the Transformer (Vaswani et al.,
2017) with several novel and effective variants. In our experiments, we employ
data filtering, large-scale synthetic data generation (i.e., back-translation,
knowledge distillation, forward-translation, iterative in-domain knowledge
transfer), advanced finetuning approaches, and boosted Self-BLEU based model
ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3
case-sensitive BLEU scores on English->Chinese, English->Japanese,
Japanese->English and English->German, respectively. The BLEU scores of
English->Chinese, English->Japanese and Japanese->English are the highest among
all submissions, and that of English->German is the highest among all
constrained submissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianfeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1"&gt;Ernan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1"&gt;Qiu Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02365</id>
        <link href="http://arxiv.org/abs/2108.02365"/>
        <updated>2021-08-06T00:51:44.457Z</updated>
        <summary type="html"><![CDATA[The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weijiang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuejian Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1"&gt;Nong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zipf Matrix Factorization : Matrix Factorization with Matthew Effect Reduction. (arXiv:2106.07347v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07347</id>
        <link href="http://arxiv.org/abs/2106.07347"/>
        <updated>2021-08-06T00:51:44.435Z</updated>
        <summary type="html"><![CDATA[Recommender system recommends interesting items to users based on users' past
information history. Researchers have been paying attention to improvement of
algorithmic performance such as MAE and precision@K. Major techniques such as
matrix factorization and learning to rank are optimized based on such
evaluation metrics. However, the intrinsic Matthew Effect problem poses great
threat to the fairness of the recommender system, and the unfairness problem
cannot be resolved by optimization of traditional metrics. In this paper, we
propose a novel algorithm that incorporates Matthew Effect reduction with the
matrix factorization framework. We demonstrate that our approach can boost the
fairness of the algorithm and enhances performance evaluated by traditional
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:44.416Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of UMLS Concept Extraction from Clinical Notes using Boolean Combination Ensembles. (arXiv:2108.02255v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02255</id>
        <link href="http://arxiv.org/abs/2108.02255"/>
        <updated>2021-08-06T00:51:44.405Z</updated>
        <summary type="html"><![CDATA[Our objective in this study is to investigate the behavior of Boolean
operators on combining annotation output from multiple Natural Language
Processing (NLP) systems across multiple corpora and to assess how filtering by
aggregation of Unified Medical Language System (UMLS) Metathesaurus concepts
affects system performance for Named Entity Recognition (NER) of UMLS concepts.
We used three corpora annotated for UMLS concepts: 2010 i2b2 VA challenge set
(31,161 annotations), Multi-source Integrated Platform for Answering Clinical
Questions (MiPACQ) corpus (17,457 annotations including UMLS concept unique
identifiers), and Fairview Health Services corpus (44,530 annotations). Our
results showed that for UMLS concept matching, Boolean ensembling of the MiPACQ
corpus trended towards higher performance over individual systems. Use of an
approximate grid-search can help optimize the precision-recall tradeoff and can
provide a set of heuristics for choosing an optimal set of ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1"&gt;Greg M. Silverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1"&gt;Raymond L. Finzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinz_M/0/1/0/all/0/1"&gt;Michael V. Heinz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solinsky_J/0/1/0/all/0/1"&gt;Jacob C. Solinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEwan_R/0/1/0/all/0/1"&gt;Reed McEwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_B/0/1/0/all/0/1"&gt;Benjamin C. Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher J. Tignanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1"&gt;Serguei VS Pakhomov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Geometry and Color Projection-based Point Cloud Quality Metric. (arXiv:2108.02481v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02481</id>
        <link href="http://arxiv.org/abs/2108.02481"/>
        <updated>2021-08-06T00:51:44.395Z</updated>
        <summary type="html"><![CDATA[Point cloud coding solutions have been recently standardized to address the
needs of multiple application scenarios. The design and assessment of point
cloud coding methods require reliable objective quality metrics to evaluate the
level of degradation introduced by compression or any other type of processing.
Several point cloud objective quality metrics has been recently proposed to
reliable estimate human perceived quality, including the so-called
projection-based metrics. In this context, this paper proposes a joint geometry
and color projection-based point cloud objective quality metric which solves
the critical weakness of this type of quality metrics, i.e., the misalignment
between the reference and degraded projected images. Moreover, the proposed
point cloud quality metric exploits the best performing 2D quality metrics in
the literature to assess the quality of the projected images. The experimental
results show that the proposed projection-based quality metric offers the best
subjective-objective correlation performance in comparison with other metrics
in the literature. The Pearson correlation gains regarding D1-PSNR and D2-PSNR
metrics are 17% and 14.2 when data with all coding degradations is considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Javaheri_A/0/1/0/all/0/1"&gt;Alireza Javaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brites_C/0/1/0/all/0/1"&gt;Catarina Brites&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Fernando Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ascenso_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Ascenso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Transfer Learning with Pretrained Language Models through Adapters. (arXiv:2108.02340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02340</id>
        <link href="http://arxiv.org/abs/2108.02340"/>
        <updated>2021-08-06T00:51:44.369Z</updated>
        <summary type="html"><![CDATA[Transfer learning with large pretrained transformer-based language models
like BERT has become a dominating approach for most NLP tasks. Simply
fine-tuning those large language models on downstream tasks or combining it
with task-specific pretraining is often not robust. In particular, the
performance considerably varies as the random seed changes or the number of
pretraining and/or fine-tuning iterations varies, and the fine-tuned model is
vulnerable to adversarial attack. We propose a simple yet effective
adapter-based approach to mitigate these issues. Specifically, we insert small
bottleneck layers (i.e., adapter) within each layer of a pretrained model, then
fix the pretrained layers and train the adapter layers on the downstream task
data, with (1) task-specific unsupervised pretraining and then (2)
task-specific supervised training (e.g., classification, sequence labeling).
Our experiments demonstrate that such a training scheme leads to improved
stability and adversarial robustness in transfer learning to various downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wenjuan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yingnian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02359</id>
        <link href="http://arxiv.org/abs/2108.02359"/>
        <updated>2021-08-06T00:51:44.356Z</updated>
        <summary type="html"><![CDATA[Video captioning combines video understanding and language generation.
Different from image captioning that describes a static image with details of
almost every object, video captioning usually considers a sequence of frames
and biases towards focused objects, e.g., the objects that stay in focus
regardless of the changing background. Therefore, detecting and properly
accommodating focused objects is critical in video captioning. To enforce the
description of focused objects and achieve controllable video captioning, we
propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs
caption generation in three steps: 1) identify the focused objects and predict
their locations in the target caption; 2) generate the related attribute words
and relation words of these focused objects to form a draft caption; and 3)
combine video information to refine the draft caption to a fluent final
caption. Since the focused objects are generated and located ahead of other
words, it is difficult to apply the word-by-word autoregressive generation
process; instead, we adopt a non-autoregressive approach. The experiments on
two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness
of O2NA, which achieves results competitive with the state-of-the-arts but with
both higher diversity and higher inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06416</id>
        <link href="http://arxiv.org/abs/2107.06416"/>
        <updated>2021-08-06T00:51:44.129Z</updated>
        <summary type="html"><![CDATA[Recommendations with personalized explanations have been shown to increase
user trust and perceived quality and help users make better decisions.
Moreover, such explanations allow users to provide feedback by critiquing them.
Several algorithms for recommender systems with multi-step critiquing have
therefore been developed. However, providing a user-friendly interface based on
personalized explanations and critiquing has not been addressed in the last
decade. In this paper, we introduce four different web interfaces (available
under https://lia.epfl.ch/critiquing/) helping users making decisions and
finding their ideal item. We have chosen the hotel recommendation domain as a
use case even though our approach is trivially adaptable for other domains.
Moreover, our system is model-agnostic (for both recommender systems and
critiquing models) allowing a great flexibility and further extensions. Our
interfaces are above all a useful tool to help research in recommendation with
critiquing. They allow to test such systems on a real use case and also to
highlight some limitations of these approaches to find solutions to overcome
them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1"&gt;Diana Petrescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02576</id>
        <link href="http://arxiv.org/abs/2108.02576"/>
        <updated>2021-08-06T00:51:44.089Z</updated>
        <summary type="html"><![CDATA[Music Performers have their own idiosyncratic way of interpreting a musical
piece. A group of skilled performers playing the same piece of music would
likely to inject their unique artistic styles in their performances. The
variations of the tempo, timing, dynamics, articulation etc. from the actual
notated music are what make the performers unique in their performances. This
study presents a dataset consisting of four movements of Schubert's ``Sonata in
B-flat major, D.960" performed by nine virtuoso pianists individually. We
proposed and extracted a set of expressive features that are able to capture
the characteristics of an individual performer's style. We then present a
performer identification method based on the similarity of feature
distribution, given a set of piano performances. The identification is done
considering each feature individually as well as a fusion of the features.
Results show that the proposed method achieved a precision of 0.903 using
fusion features. Moreover, the onset time deviation feature shows promising
result when considered individually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1"&gt;Syed Rifat Mahmud Rafee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gyorgy Fazekas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1"&gt;Geraint A.~Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-aware Path Reasoning on Knowledge Graph for Recommendation. (arXiv:2108.02634v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02634</id>
        <link href="http://arxiv.org/abs/2108.02634"/>
        <updated>2021-08-06T00:51:44.076Z</updated>
        <summary type="html"><![CDATA[Reasoning on knowledge graph (KG) has been studied for explainable
recommendation due to it's ability of providing explicit explanations. However,
current KG-based explainable recommendation methods unfortunately ignore the
temporal information (such as purchase time, recommend time, etc.), which may
result in unsuitable explanations. In this work, we propose a novel Time-aware
Path reasoning for Recommendation (TPRec for short) method, which leverages the
potential of temporal information to offer better recommendation with plausible
explanations. First, we present an efficient time-aware interaction relation
extraction component to construct collaborative knowledge graph with time-aware
interactions (TCKG for short), and then introduce a novel time-aware path
reasoning method for recommendation. We conduct extensive experiments on three
real-world datasets. The results demonstrate that the proposed TPRec could
successfully employ TCKG to achieve substantial gains and improve the quality
of explainable recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuyue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yashen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haiyong Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02625</id>
        <link href="http://arxiv.org/abs/2108.02625"/>
        <updated>2021-08-06T00:51:44.047Z</updated>
        <summary type="html"><![CDATA[This paper makes several contributions to automatic lyrics transcription
(ALT) research. Our main contribution is a novel variant of the Multistreaming
Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which
processes the temporal information using multiple streams in parallel with
varying resolutions keeping the network more compact, and thus with a faster
inference and an improved recognition rate than having identical TDNN streams.
In addition, two novel preprocessing steps prior to training the acoustic model
are proposed. First, we suggest using recordings from both monophonic and
polyphonic domains during training the acoustic model. Second, we tag
monophonic and polyphonic recordings with distinct labels for discriminating
non-vocal silence and music instances during alignment. Moreover, we present a
new test set with a considerably larger size and a higher musical variability
compared to the existing datasets used in ALT literature, while maintaining the
gender balance of the singers. Our best performing model sets the
state-of-the-art in lyrics transcription by a large margin. For
reproducibility, we publicly share the identifiers to retrieve the data used in
this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1"&gt;Emir Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Itinerary-aware Personalized Deep Matching at Fliggy. (arXiv:2108.02343v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02343</id>
        <link href="http://arxiv.org/abs/2108.02343"/>
        <updated>2021-08-06T00:51:44.001Z</updated>
        <summary type="html"><![CDATA[Matching items for a user from a travel item pool of large cardinality have
been the most important technology for increasing the business at Fliggy, one
of the most popular online travel platforms (OTPs) in China. There are three
major challenges facing OTPs: sparsity, diversity, and implicitness. In this
paper, we present a novel Fliggy ITinerary-aware deep matching NETwork (FitNET)
to address these three challenges. FitNET is designed based on the popular deep
matching network, which has been successfully employed in many industrial
recommendation systems, due to its effectiveness. The concept itinerary is
firstly proposed under the context of recommendation systems for OTPs, which is
defined as the list of unconsumed orders of a user. All orders in a user
itinerary are learned as a whole, based on which the implicit travel intention
of each user can be more accurately inferred. To alleviate the sparsity
problem, users' profiles are incorporated into FitNET. Meanwhile, a series of
itinerary-aware attention mechanisms that capture the vital interactions
between user's itinerary and other input categories are carefully designed.
These mechanisms are very helpful in inferring a user's travel intention or
preference, and handling the diversity in a user's need. Further, two training
objectives, i.e., prediction accuracy of user's travel intention and prediction
accuracy of user's click behavior, are utilized by FitNET, so that these two
objectives can be optimized simultaneously. An offline experiment on Fliggy
production dataset with over 0.27 million users and 1.55 million travel items,
and an online A/B test both show that FitNET effectively learns users' travel
intentions, preferences, and diverse needs, based on their itineraries and
gains superior performance compared with state-of-the-art methods. FitNET now
has been successfully deployed at Fliggy, serving major online traffic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1"&gt;Detao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chuanfei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02432</id>
        <link href="http://arxiv.org/abs/2108.02432"/>
        <updated>2021-08-06T00:51:43.978Z</updated>
        <summary type="html"><![CDATA[Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.

This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yanbin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LHRM: A LBS based Heterogeneous Relations Model for User Cold Start Recommendation in Online Travel Platform. (arXiv:2108.02344v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02344</id>
        <link href="http://arxiv.org/abs/2108.02344"/>
        <updated>2021-08-06T00:51:43.933Z</updated>
        <summary type="html"><![CDATA[Most current recommender systems used the historical behaviour data of user
to predict user' preference. However, it is difficult to recommend items to new
users accurately. To alleviate this problem, existing user cold start methods
either apply deep learning to build a cross-domain recommender system or map
user attributes into the space of user behaviour. These methods are more
challenging when applied to online travel platform (e.g., Fliggy), because it
is hard to find a cross-domain that user has similar behaviour with travel
scenarios and the Location Based Services (LBS) information of users have not
been paid sufficient attention. In this work, we propose a LBS-based
Heterogeneous Relations Model (LHRM) for user cold start recommendation, which
utilizes user's LBS information and behaviour information in related domains
and user's behaviour information in travel platforms (e.g., Fliggy) to
construct the heterogeneous relations between users and items. Moreover, an
attention-based multi-layer perceptron is applied to extract latent factors of
users and items. Through this way, LHRM has better generalization performance
than existing methods. Experimental results on real data from Fliggy's offline
log illustrate the effectiveness of LHRM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wendong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhi Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08928</id>
        <link href="http://arxiv.org/abs/2107.08928"/>
        <updated>2021-08-05T01:56:21.727Z</updated>
        <summary type="html"><![CDATA[A significant impediment to progress in research on bias in machine learning
(ML) is the availability of relevant datasets. This situation is unlikely to
change much given the sensitivity of such data. For this reason, there is a
role for synthetic data in this research. In this short paper, we present one
such family of synthetic data sets. We provide an overview of the data,
describe how the level of bias can be varied, and present a simple example of
an experiment on the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1"&gt;William Blanzeisky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1"&gt;P&amp;#xe1;draig Cunningham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1"&gt;Kenneth Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Modeling: A Review. (arXiv:2104.12556v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12556</id>
        <link href="http://arxiv.org/abs/2104.12556"/>
        <updated>2021-08-05T01:56:21.720Z</updated>
        <summary type="html"><![CDATA[The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming demand, challenges and opportunities to domain, model and data
driven modeling. This paper provides a comprehensive review of the challenges,
tasks, methods, progress, gaps and opportunities in relation to modeling
COVID-19 problems, data and objectives. It constructs a research landscape of
COVID-19 modeling tasks and methods, and further categorizes, summarizes,
compares and discusses the related methods and progress of modeling COVID-19
epidemic transmission processes and dynamics, case identification and tracing,
infection diagnosis and medical treatments, non-pharmaceutical interventions
and their effects, drug and vaccine development, psychological, economic and
social influence and impact, and misinformation, etc. The modeling methods
involve mathematical and statistical models, domain-driven modeling by
epidemiological compartmental models, medical and biomedical analysis, AI and
data science in particular shallow and deep machine learning, simulation
modeling, social science methods, and hybrid modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-08-05T01:56:21.693Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15715</id>
        <link href="http://arxiv.org/abs/2012.15715"/>
        <updated>2021-08-05T01:56:21.672Z</updated>
        <summary type="html"><![CDATA[Recent research on cross-lingual word embeddings has been dominated by
unsupervised mapping approaches that align monolingual embeddings. Such methods
critically rely on those embeddings having a similar structure, but it was
recently shown that the separate training in different languages causes
departures from this assumption. In this paper, we propose an alternative
approach that does not have this limitation, while requiring a weak seed
dictionary (e.g., a list of identical words) as the only form of supervision.
Rather than aligning two fixed embedding spaces, our method works by fixing the
target language embeddings, and learning a new set of embeddings for the source
language that are aligned with them. To that end, we use an extension of
skip-gram that leverages translated context words as anchor points, and
incorporates self-learning and iterative restarts to reduce the dependency on
the initial dictionary. Our approach outperforms conventional mapping methods
on bilingual lexicon induction, and obtains competitive results in the
downstream XNLI task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1"&gt;Aitor Ormazabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1"&gt;Aitor Soroa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05901</id>
        <link href="http://arxiv.org/abs/2107.05901"/>
        <updated>2021-08-05T01:56:21.665Z</updated>
        <summary type="html"><![CDATA[The Jeffreys divergence is a renown symmetrization of the statistical
Kullback-Leibler divergence which is often used in statistics, machine
learning, signal processing, and information sciences in general. Since the
Jeffreys divergence between the ubiquitous Gaussian Mixture Models are not
available in closed-form, many techniques with various pros and cons have been
proposed in the literature to either (i) estimate, (ii) approximate, or (iii)
lower and/or upper bound this divergence. In this work, we propose a simple yet
fast heuristic to approximate the Jeffreys divergence between two univariate
GMMs of arbitrary number of components. The heuristic relies on converting GMMs
into pairs of dually parameterized probability densities belonging to
exponential families. In particular, we consider Exponential-Polynomial
Densities, and design a goodness-of-fit criterion to measure the dissimilarity
between a GMM and a EPD which is a generalization of the Hyv\"arinen
divergence. This criterion allows one to select the orders of the EPDs to
approximate the GMMs. We demonstrate experimentally that the computational time
of our heuristic improves over the stochastic Monte Carlo estimation baseline
by several orders of magnitude while approximating reasonably well the Jeffreys
divergence, specially when the univariate mixtures have a small number of
modes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-08-05T01:56:21.657Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insta-RS: Instance-wise Randomized Smoothing for Improved Robustness and Accuracy. (arXiv:2103.04436v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04436</id>
        <link href="http://arxiv.org/abs/2103.04436"/>
        <updated>2021-08-05T01:56:21.651Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing (RS) is an effective and scalable technique for
constructing neural network classifiers that are certifiably robust to
adversarial perturbations. Most RS works focus on training a good base model
that boosts the certified robustness of the smoothed model. However, existing
RS techniques treat every data point the same, i.e., the variance of the
Gaussian noise used to form the smoothed model is preset and universal for all
training and test data. This preset and universal Gaussian noise variance is
suboptimal since different data points have different margins and the local
properties of the base model vary across the input examples. In this paper, we
examine the impact of customized handling of examples and propose Instance-wise
Randomized Smoothing (Insta-RS) -- a multiple-start search algorithm that
assigns customized Gaussian variances to test examples. We also design Insta-RS
Train -- a novel two-stage training algorithm that adaptively adjusts and
customizes the noise level of each training example for training a base model
that boosts the certified robustness of the instance-wise Gaussian smoothed
model. Through extensive experiments on CIFAR-10 and ImageNet, we show that our
method significantly enhances the average certified radius (ACR) as well as the
clean data accuracy compared to existing state-of-the-art provably robust
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peihong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1"&gt;Juan Luque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12822</id>
        <link href="http://arxiv.org/abs/2104.12822"/>
        <updated>2021-08-05T01:56:21.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1"&gt;Martin Milenkoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLPerf Tiny Benchmark. (arXiv:2106.07597v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07597</id>
        <link href="http://arxiv.org/abs/2106.07597"/>
        <updated>2021-08-05T01:56:21.625Z</updated>
        <summary type="html"><![CDATA[Advancements in ultra-low-power tiny machine learning (TinyML) systems
promise to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted and easily
reproducible benchmark for these systems. To meet this need, we present MLPerf
Tiny, the first industry-standard benchmark suite for ultra-low-power tiny
machine learning systems. The benchmark suite is the collaborative effort of
more than 50 organizations from industry and academia and reflects the needs of
the community. MLPerf Tiny measures the accuracy, latency, and energy of
machine learning inference to properly evaluate the tradeoffs between systems.
Additionally, MLPerf Tiny implements a modular design that enables benchmark
submitters to show the benefits of their product, regardless of where it falls
on the ML deployment stack, in a fair and reproducible manner. The suite
features four benchmarks: keyword spotting, visual wake words, image
classification, and anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1"&gt;Colby Banbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1"&gt;Peter Torelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1"&gt;Jeremy Holleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1"&gt;Nat Jeffries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1"&gt;Csaba Kiraly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1"&gt;Pietro Montino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1"&gt;David Kanter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sebastian Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1"&gt;Danilo Pau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1"&gt;Urmish Thakker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1"&gt;Antonio Torrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1"&gt;Peter Warden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1"&gt;Jay Cordaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1"&gt;Giuseppe Di Guglielmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1"&gt;Stephen Gibellini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1"&gt;Videet Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Honson Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"&gt;Nhan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1"&gt;Niu Wenxu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1"&gt;Xu Xuesong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13434</id>
        <link href="http://arxiv.org/abs/2106.13434"/>
        <updated>2021-08-05T01:56:21.618Z</updated>
        <summary type="html"><![CDATA[Binary matrix factorisation is an essential tool for identifying discrete
patterns in binary data. In this paper we consider the rank-k binary matrix
factorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m
binary matrix X with possibly missing entries and need to find two binary
matrices A and B of dimension n x k and k x m respectively, which minimise the
distance between X and the Boolean product of A and B in the squared Frobenius
distance. We present a compact and two exponential size integer programs (IPs)
for k-BMF and show that the compact IP has a weak LP relaxation, while the
exponential size IPs have a stronger equivalent LP relaxation. We introduce a
new objective function, which differs from the traditional squared Frobenius
objective in attributing a weight to zero entries of the input matrix that is
proportional to the number of times the zero is erroneously covered in a rank-k
factorisation. For one of the exponential size IPs we describe a computational
approach based on column generation. Experimental results on synthetic and real
word datasets suggest that our integer programming approach is competitive
against available methods for k-BMF and provides accurate low-error
factorisations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v2 [q-bio.OT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10332</id>
        <link href="http://arxiv.org/abs/2107.10332"/>
        <updated>2021-08-05T01:56:21.612Z</updated>
        <summary type="html"><![CDATA[The early detection of cancer is a challenging problem in medicine. The blood
sera of cancer patients are enriched with heterogeneous secretory lipid bound
extracellular vesicles (EVs), which present a complex repertoire of information
and biomarkers, representing their cell of origin, that are being currently
studied in the field of liquid biopsy and cancer screening. Vibrational
spectroscopies provide non-invasive approaches for the assessment of structural
and biophysical properties in complex biological samples. In this study,
multiple Raman spectroscopy measurements were performed on the EVs extracted
from the blood sera of 9 patients consisting of four different cancer subtypes
(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic
cancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)
spectroscopy measurements were performed as a complementary approach to Raman
analysis, on two of the four cancer subtypes.

The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector
Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs
from those of healthy controls (18 spectra) with a classification accuracy of
greater than 90% when reduced to a spectral frequency range of 1800 to 1940
inverse cm, and subjected to a 0.5 training/testing split. FTIR classification
accuracy on 14 spectra showed an 80% classification accuracy. Our findings
demonstrate that basic machine learning algorithms are powerful tools to
distinguish the complex vibrational spectra of cancer patient EVs from those of
healthy patients. These experimental methods hold promise as valid and
efficient liquid biopsy for machine intelligence-assisted early cancer
screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1"&gt;Abicumaran Uthamacumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1"&gt;Samir Elouatik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1"&gt;Mohamed Abdouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1"&gt;Michael Berteau-Rainville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhu- Hua Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1"&gt;Goffredo Arena&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-08-05T01:56:21.605Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12045</id>
        <link href="http://arxiv.org/abs/2107.12045"/>
        <updated>2021-08-05T01:56:21.587Z</updated>
        <summary type="html"><![CDATA[Context: Machine Learning (ML) has been at the heart of many innovations over
the past years. However, including it in so-called 'safety-critical' systems
such as automotive or aeronautic has proven to be very challenging, since the
shift in paradigm that ML brings completely changes traditional certification
approaches.

Objective: This paper aims to elucidate challenges related to the
certification of ML-based safety-critical systems, as well as the solutions
that are proposed in the literature to tackle them, answering the question 'How
to Certify Machine Learning Based Safety-critical Systems?'.

Method: We conduct a Systematic Literature Review (SLR) of research papers
published between 2015 to 2020, covering topics related to the certification of
ML systems. In total, we identified 217 papers covering topics considered to be
the main pillars of ML certification: Robustness, Uncertainty, Explainability,
Verification, Safe Reinforcement Learning, and Direct Certification. We
analyzed the main trends and problems of each sub-field and provided summaries
of the papers extracted.

Results: The SLR results highlighted the enthusiasm of the community for this
subject, as well as the lack of diversity in terms of datasets and type of
models. It also emphasized the need to further develop connections between
academia and industries to deepen the domain study. Finally, it also
illustrated the necessity to build connections between the above mention main
pillars that are for now mainly studied separately.

Conclusion: We highlighted current efforts deployed to enable the
certification of ML based software systems, and discuss some future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1"&gt;Gabriel Laberge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1"&gt;Le An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1"&gt;Paulina Stevia Nouwou Mindom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1"&gt;Yann Pequignot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giulio Antoniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1"&gt;Ettore Merlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Laviolette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Generalization of Graph Autoencoders with Adversarial Training. (arXiv:2107.02658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02658</id>
        <link href="http://arxiv.org/abs/2107.02658"/>
        <updated>2021-08-05T01:56:21.579Z</updated>
        <summary type="html"><![CDATA[Adversarial training is an approach for increasing model's resilience against
adversarial perturbations. Such approaches have been demonstrated to result in
models with feature representations that generalize better. However, limited
works have been done on adversarial training of models on graph data. In this
paper, we raise such a question { does adversarial training improve the
generalization of graph representations. We formulate L2 and L1 versions of
adversarial training in two powerful node embedding methods: graph autoencoder
(GAE) and variational graph autoencoder (VGAE). We conduct extensive
experiments on three main applications, i.e. link prediction, node clustering,
graph anomaly detection of GAE and VGAE, and demonstrate that both L2 and L1
adversarial training boost the generalization of GAE and VGAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianjin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yulong Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical evaluation of active inference in multi-armed bandits. (arXiv:2101.08699v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08699</id>
        <link href="http://arxiv.org/abs/2101.08699"/>
        <updated>2021-08-05T01:56:21.562Z</updated>
        <summary type="html"><![CDATA[A key feature of sequential decision making under uncertainty is a need to
balance between exploiting--choosing the best action according to the current
knowledge, and exploring--obtaining information about values of other actions.
The multi-armed bandit problem, a classical task that captures this trade-off,
served as a vehicle in machine learning for developing bandit algorithms that
proved to be useful in numerous industrial applications. The active inference
framework, an approach to sequential decision making recently developed in
neuroscience for understanding human and animal behaviour, is distinguished by
its sophisticated strategy for resolving the exploration-exploitation
trade-off. This makes active inference an exciting alternative to already
established bandit algorithms. Here we derive an efficient and scalable
approximate active inference algorithm and compare it to two state-of-the-art
bandit algorithms: Bayesian upper confidence bound and optimistic Thompson
sampling. This comparison is done on two types of bandit problems: a stationary
and a dynamic switching bandit. Our empirical evaluation shows that the active
inference algorithm does not produce efficient long-term behaviour in
stationary bandits. However, in the more challenging switching bandit problem
active inference performs substantially better than the two state-of-the-art
bandit algorithms. The results open exciting venues for further research in
theoretical and applied machine learning, as well as lend additional
credibility to active inference as a general framework for studying human and
animal behaviour.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dimitrije Markovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stojic_H/0/1/0/all/0/1"&gt;Hrvoje Stojic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwoebel_S/0/1/0/all/0/1"&gt;Sarah Schwoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiebel_S/0/1/0/all/0/1"&gt;Stefan J. Kiebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Sap Flow Measurements with Earth Observations. (arXiv:2108.01290v1 [stat.AP] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2108.01290</id>
        <link href="http://arxiv.org/abs/2108.01290"/>
        <updated>2021-08-05T01:56:21.555Z</updated>
        <summary type="html"><![CDATA[While single-tree transpiration is challenging to compare with earth
observation, canopy scale data are suitable for this purpose. To test the
potentialities of the second approach, we equipped the trees at two measurement
sites with sap flow sensors in spruce forests. The sites have contrasting
topography. The measurement period covered the months between June 2020 and
January 2021. To link plot scale transpiration with earth observations, we
utilized Sentinel-2 and local meteorological data. Within a machine learning
framework, we have tested the suitability of earth observations for modelling
canopy transpiration. The R2 of the cross-validated trained models at the
measurement sites was between 0.57 and 0.80. These results demonstrate the
relevance of Sentinel-2 data for the data-driven upscaling of ecosystem fluxes
from plot scale sap flow data. If applied to a broader network of sites and
climatic conditions, such an approach could offer unprecedented possibilities
for investigating our forests' resilience and resistance capacity to an
intensified hydrological cycle in the contest of a changing climate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tomelleri_E/0/1/0/all/0/1"&gt;Enrico Tomelleri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tonon_G/0/1/0/all/0/1"&gt;Giustino Tonon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Pipeline: Synthesizing Complex Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13861</id>
        <link href="http://arxiv.org/abs/2106.13861"/>
        <updated>2021-08-05T01:56:21.536Z</updated>
        <summary type="html"><![CDATA[Recent work has made significant progress in helping users to automate single
data preparation steps, such as string-transformations and table-manipulation
operators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to
automate multiple such steps end-to-end, by synthesizing complex data pipelines
with both string transformations and table-manipulation operators. We propose a
novel "by-target" paradigm that allows users to easily specify the desired
pipeline, which is a significant departure from the traditional by-example
paradigm. Using by-target, users would provide input tables (e.g., csv or json
files), and point us to a "target table" (e.g., an existing database table or
BI dashboard) to demonstrate how the output from the desired pipeline would
schematically "look like". While the problem is seemingly underspecified, our
unique insight is that implicit table constraints such as FDs and keys can be
exploited to significantly constrain the space to make the problem tractable.
We develop an Auto-Pipeline system that learns to synthesize pipelines using
reinforcement learning and search. Experiments on large numbers of real
pipelines crawled from GitHub suggest that Auto-Pipeline can successfully
synthesize 60-70% of these complex pipelines with up to 10 steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Surajit Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:21.490Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00411</id>
        <link href="http://arxiv.org/abs/2104.00411"/>
        <updated>2021-08-05T01:56:21.474Z</updated>
        <summary type="html"><![CDATA[Neural networks have demonstrated remarkable performance in classification
and regression tasks on chest X-rays. In order to establish trust in the
clinical routine, the networks' prediction mechanism needs to be interpretable.
One principal approach to interpretation is feature attribution. Feature
attribution methods identify the importance of input features for the output
prediction. Building on Information Bottleneck Attribution (IBA) method, for
each prediction we identify the chest X-ray regions that have high mutual
information with the network's output. Original IBA identifies input regions
that have sufficient predictive information. We propose Inverse IBA to identify
all informative regions. Thus all predictive cues for pathologies are
highlighted on the X-rays, a desirable property for chest X-ray diagnosis.
Moreover, we propose Regression IBA for explaining regression models. Using
Regression IBA we observe that a model trained on cumulative severity score
labels implicitly learns the severity of different X-ray regions. Finally, we
propose Multi-layer IBA to generate higher resolution and more detailed
attribution/saliency maps. We evaluate our methods using both human-centric
(ground-truth-based) interpretability metrics, and human-independent feature
importance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1"&gt;Wejdene Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuezhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02481</id>
        <link href="http://arxiv.org/abs/2104.02481"/>
        <updated>2021-08-05T01:56:21.451Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks are showing promise in the automatic diagnosis
of thoracic pathologies on chest x-rays. Their black-box nature has sparked
many recent works to explain the prediction via input feature attribution
methods (aka saliency methods). However, input feature attribution methods
merely identify the importance of input regions for the prediction and lack
semantic interpretation of model behavior. In this work, we first identify the
semantics associated with internal units (feature maps) of the network. We
proceed to investigate the following questions; Does a regression model that is
only trained with COVID-19 severity scores implicitly learn visual patterns
associated with thoracic pathologies? Does a network that is trained on weakly
labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,
we investigate the effect of pretraining and data imbalance on the
interpretability of learned features. In addition to the analysis, we propose
semantic attribution to semantically explain each prediction. We present our
findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)
and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).
The Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1"&gt;Sabrina Musatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1"&gt;Jonas Buchberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1"&gt;Icxel Valeriano Quiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1"&gt;Nikolaus Pinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1"&gt;Soroosh Baselizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11905</id>
        <link href="http://arxiv.org/abs/2012.11905"/>
        <updated>2021-08-05T01:56:21.431Z</updated>
        <summary type="html"><![CDATA[With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. Especially in medical contexts, where relevant information often
consists of textural and structural information, high-quality counterfactual
images have the potential to give meaningful insights into decision processes.
In this work, we present GANterfactual, an approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in an exemplary medical use case. Our results show that, in the chosen
medical use-case, counterfactual explanations lead to significantly better
results regarding mental models, explanation satisfaction, trust, emotions, and
self-efficacy than two state-of-the-art systems that work with saliency maps,
namely LIME and LRP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1"&gt;Katharina Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1"&gt;Alexander Heimerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08978</id>
        <link href="http://arxiv.org/abs/2009.08978"/>
        <updated>2021-08-05T01:56:21.422Z</updated>
        <summary type="html"><![CDATA[Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models: we introduce it in a multi-objective approach to
traditionally time-unaware recommender systems and confirm its advantage via
the proposed evaluation protocol. Finally, we validate that the Pareto Fronts
obtained with the added objective dominate those produced by state-of-the-art
models that are only optimized for accuracy on three real-world publicly
available datasets. The results show that including our temporal objective can
improve recall@20 by up to 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spacetime Neural Network for High Dimensional Quantum Dynamics. (arXiv:2108.02200v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2108.02200</id>
        <link href="http://arxiv.org/abs/2108.02200"/>
        <updated>2021-08-05T01:56:21.405Z</updated>
        <summary type="html"><![CDATA[We develop a spacetime neural network method with second order optimization
for solving quantum dynamics from the high dimensional Schr\"{o}dinger
equation. In contrast to the standard iterative first order optimization and
the time-dependent variational principle, our approach utilizes the implicit
mid-point method and generates the solution for all spatial and temporal values
simultaneously after optimization. We demonstrate the method in the
Schr\"{o}dinger equation with a self-normalized autoregressive spacetime neural
network construction. Future explorations for solving different high
dimensional differential equations are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiangran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Luo_D/0/1/0/all/0/1"&gt;Di Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhizhen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Hur_V/0/1/0/all/0/1"&gt;Vera Mikyoung Hur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Clark_B/0/1/0/all/0/1"&gt;Bryan K. Clark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05150</id>
        <link href="http://arxiv.org/abs/2010.05150"/>
        <updated>2021-08-05T01:56:21.399Z</updated>
        <summary type="html"><![CDATA[While safe reinforcement learning (RL) holds great promise for many practical
applications like robotics or autonomous cars, current approaches require
specifying constraints in mathematical form. Such specifications demand domain
expertise, limiting the adoption of safe RL. In this paper, we propose learning
to interpret natural language constraints for safe RL. To this end, we first
introduce HazardWorld, a new multi-task benchmark that requires an agent to
optimize reward while not violating constraints specified in free-form text. We
then develop an agent with a modular architecture that can interpret and adhere
to such textual constraints while learning new tasks. Our model consists of (1)
a constraint interpreter that encodes textual constraints into spatial and
temporal representations of forbidden states, and (2) a policy network that
uses these representations to produce a policy achieving minimal constraint
violations during training. Across different domains in HazardWorld, we show
that our method achieves higher rewards (up to11x) and fewer constraint
violations (by 1.8x) compared to existing approaches. However, in terms of
absolute performance, HazardWorld still poses significant challenges for agents
to learn efficiently, motivating the need for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Michael Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1"&gt;Yinlam Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\times$ Compression and 2.7$\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02191</id>
        <link href="http://arxiv.org/abs/2108.02191"/>
        <updated>2021-08-05T01:56:21.391Z</updated>
        <summary type="html"><![CDATA[Deep learning for recommendation data is the one of the most pervasive and
challenging AI workload in recent times. State-of-the-art recommendation models
are one of the largest models rivalling the likes of GPT-3 and Switch
Transformer. Challenges in deep learning recommendation models (DLRM) stem from
learning dense embeddings for each of the categorical values. These embedding
tables in industrial scale models can be as large as hundreds of terabytes.
Such large models lead to a plethora of engineering challenges, not to mention
prohibitive communication overheads, and slower training and inference times.
Of these, slower inference time directly impacts user experience. Model
compression for DLRM is gaining traction and the community has recently shown
impressive compression results. In this paper, we present Random Offset Block
Embedding Array (ROBE) as a low memory alternative to embedding tables which
provide orders of magnitude reduction in memory usage while maintaining
accuracy and boosting execution speed. ROBE is a simple fundamental approach in
improving both cache performance and the variance of randomized hashing, which
could be of independent interest in itself. We demonstrate that we can
successfully train DLRM models with same accuracy while using $1000 \times$
less memory. A $1000\times$ compressed model directly results in faster
inference without any engineering. In particular, we show that we can train
DLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of
0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model
of 100GB while achieving about $2.7\times$ (170\%) improvement in inference
throughput.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1"&gt;Aditya Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1"&gt;Li Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Matrix Factorization. (arXiv:2010.02469v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02469</id>
        <link href="http://arxiv.org/abs/2010.02469"/>
        <updated>2021-08-05T01:56:21.384Z</updated>
        <summary type="html"><![CDATA[Unmeasured or latent variables are often the cause of correlations between
multivariate measurements and are studied in a variety of fields such as
psychology, ecology, and medicine. For Gaussian measurements, there are
classical tools such as factor analysis or principal component analysis with a
well-established theory and fast algorithms. Generalized Linear Latent Variable
models (GLLVM) generalize such factor models to non-Gaussian responses.
However, current algorithms for estimating model parameters in GLLVMs require
intensive computation and do not scale to large datasets with thousands of
observational units or responses. In this article, we propose a new approach
for fitting GLLVMs to such high-volume, high-dimensional datasets. We
approximate the likelihood using penalized quasi-likelihood and use a Newton
method and Fisher scoring to learn the model parameters. Our method greatly
reduces the computation time and can be easily parallelized, enabling
factorization at unprecedented scale using commodity hardware. We illustrate
application of our method on a dataset of 48,000 observational units with over
2,000 observed species in each unit, finding that most of the variability can
be explained with a handful of factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidzinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_F/0/1/0/all/0/1"&gt;Francis K.C. Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warton_D/0/1/0/all/0/1"&gt;David I. Warton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hastie_T/0/1/0/all/0/1"&gt;Trevor Hastie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling longitudinal data using matrix completion. (arXiv:1809.08771v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1809.08771</id>
        <link href="http://arxiv.org/abs/1809.08771"/>
        <updated>2021-08-05T01:56:21.372Z</updated>
        <summary type="html"><![CDATA[In clinical practice and biomedical research, measurements are often
collected sparsely and irregularly in time while the data acquisition is
expensive and inconvenient. Examples include measurements of spine bone mineral
density, cancer growth through mammography or biopsy, a progression of
defective vision, or assessment of gait in patients with neurological
disorders. Since the data collection is often costly and inconvenient,
estimation of progression from sparse observations is of great interest for
practitioners.

From the statistical standpoint, such data is often analyzed in the context
of a mixed-effect model where time is treated as both a fixed-effect
(population progression curve) and a random-effect (individual variability).
Alternatively, researchers analyze Gaussian processes or functional data where
observations are assumed to be drawn from a certain distribution of processes.
These models are flexible but rely on probabilistic assumptions, require very
careful implementation, specific to the given problem, and tend to be slow in
practice.

In this study, we propose an alternative elementary framework for analyzing
longitudinal data, relying on matrix completion. Our method yields estimates of
progression curves by iterative application of the Singular Value
Decomposition. Our framework covers multivariate longitudinal data, regression,
and can be easily extended to other settings. As it relies on existing tools
for matrix algebra it is efficient and easy to implement.

We apply our methods to understand trends of progression of motor impairment
in children with Cerebral Palsy. Our model approximates individual progression
curves and explains 30% of the variability. Low-rank representation of
progression trends enables identification of different progression trends in
subtypes of Cerebral Palsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kidzinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hastie_T/0/1/0/all/0/1"&gt;Trevor Hastie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06028</id>
        <link href="http://arxiv.org/abs/2007.06028"/>
        <updated>2021-08-05T01:56:21.362Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised speech pre-training method called TERA, which
stands for Transformer Encoder Representations from Alteration. Recent
approaches often learn by using a single auxiliary task like contrastive
prediction, autoregressive prediction, or masked reconstruction. Unlike
previous methods, we use alteration along three orthogonal axes to pre-train
Transformer Encoders on a large amount of unlabeled speech. The model learns
through the reconstruction of acoustic frames from their altered counterpart,
where we use a stochastic policy to alter along various dimensions: time,
frequency, and magnitude. TERA can be used for speech representations
extraction or fine-tuning with downstream models. We evaluate TERA on several
downstream tasks, including phoneme classification, keyword spotting, speaker
recognition, and speech recognition. We present a large-scale comparison of
various self-supervised models. TERA achieves strong performance in the
comparison by improving upon surface features and outperforming previous
models. In our experiments, we study the effect of applying different
alteration techniques, pre-training on more data, and pre-training on various
features. We analyze different model sizes and find that smaller models are
strong representation learners than larger models, while larger models are more
effective for downstream fine-tuning than smaller models. Furthermore, we show
the proposed method is transferable to downstream datasets not used in
pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences. (arXiv:2006.14091v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14091</id>
        <link href="http://arxiv.org/abs/2006.14091"/>
        <updated>2021-08-05T01:56:21.343Z</updated>
        <summary type="html"><![CDATA[Reward functions are a common way to specify the objective of a robot. As
designing reward functions can be extremely challenging, a more promising
approach is to directly learn reward functions from human teachers.
Importantly, data from human teachers can be collected either passively or
actively in a variety of forms: passive data sources include demonstrations,
(e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings)
are actively elicited. Prior research has independently applied reward learning
to these different data sources. However, there exist many domains where
multiple sources are complementary and expressive. Motivated by this general
problem, we present a framework to integrate multiple sources of information,
which are either passively or actively collected from human users. In
particular, we present an algorithm that first utilizes user demonstrations to
initialize a belief about the reward function, and then actively probes the
user with preference queries to zero-in on their true reward. This algorithm
not only enables us combine multiple data sources, but it also informs the
robot when it should leverage each type of information. Further, our approach
accounts for the human's ability to provide data: yielding user-friendly
preference queries which are also theoretically optimal. Our extensive
simulated experiments and user studies on a Fetch mobile manipulator
demonstrate the superiority and the usability of our integrated framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1"&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Losey_D/0/1/0/all/0/1"&gt;Dylan P. Losey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palan_M/0/1/0/all/0/1"&gt;Malayandi Palan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landolfi_N/0/1/0/all/0/1"&gt;Nicholas C. Landolfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shevchuk_G/0/1/0/all/0/1"&gt;Gleb Shevchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1"&gt;Dorsa Sadigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01248</id>
        <link href="http://arxiv.org/abs/1810.01248"/>
        <updated>2021-08-05T01:56:21.336Z</updated>
        <summary type="html"><![CDATA[Deep learning researches on the transformation problems for image and text
have raised great attention. However, present methods for music feature
transfer using neural networks are far from practical application. In this
paper, we initiate a novel system for transferring the texture of music, and
release it as an open source project. Its core algorithm is composed of a
converter which represents sounds as texture spectra, a corresponding
reconstructor and a feed-forward transfer network. We evaluate this system from
multiple perspectives, and experimental results reveal that it achieves
convincing results in both sound effects and computational performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xutan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Faqiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yidan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Potential of Using Vision Videos for CrowdRE: Video Comments as a Source of Feedback. (arXiv:2108.02076v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02076</id>
        <link href="http://arxiv.org/abs/2108.02076"/>
        <updated>2021-08-05T01:56:21.329Z</updated>
        <summary type="html"><![CDATA[Vision videos are established for soliciting feedback and stimulating
discussions in requirements engineering (RE) practices, such as focus groups.
Different researchers motivated the transfer of these benefits into crowd-based
RE (CrowdRE) by using vision videos on social media platforms. So far, however,
little research explored the potential of using vision videos for CrowdRE in
detail. In this paper, we analyze and assess this potential, in particular,
focusing on video comments as a source of feedback. In a case study, we
analyzed 4505 comments on a vision video from YouTube. We found that the video
solicited 2770 comments from 2660 viewers in four days. This is more than 50%
of all comments the video received in four years. Even though only a certain
fraction of these comments are relevant to RE, the relevant comments address
typical intentions and topics of user feedback, such as feature request or
problem report. Besides the typical user feedback categories, we found more
than 300 comments that address the topic safety, which has not appeared in
previous analyses of user feedback. In an automated analysis, we compared the
performance of three machine learning algorithms on classifying the video
comments. Despite certain differences, the algorithms classified the video
comments well. Based on these findings, we conclude that the use of vision
videos for CrowdRE has a large potential. Despite the preliminary nature of the
case study, we are optimistic that vision videos can motivate stakeholders to
actively participate in a crowd and solicit numerous of video comments as a
valuable source of feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_O/0/1/0/all/0/1"&gt;Oliver Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristo_E/0/1/0/all/0/1"&gt;Eklekta Kristo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klunder_J/0/1/0/all/0/1"&gt;Jil Kl&amp;#xfc;nder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06822</id>
        <link href="http://arxiv.org/abs/2008.06822"/>
        <updated>2021-08-05T01:56:21.322Z</updated>
        <summary type="html"><![CDATA[This paper focuses on high-transferable adversarial attacks on detectors,
which are hard to attack in a black-box manner, because of their
multiple-output characteristics and the diversity across architectures. To
pursue a high attack transferability, one plausible way is to find a common
property across detectors, which facilitates the discovery of common
weaknesses. We are the first to suggest that the relevance map from
interpreters for detectors is such a property. Based on it, we design a
Relevance Attack on Detectors (RAD), which achieves a state-of-the-art
transferability, exceeding existing results by above 20%. On MS COCO, the
detection mAPs for all 8 black-box architectures are more than halved and the
segmentation mAPs are also significantly influenced. Given the great
transferability of RAD, we generate the first adversarial dataset for object
detection and instance segmentation, i.e., Adversarial Objects in COntext
(AOCO), which helps to quickly evaluate and improve the robustness of
detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07088</id>
        <link href="http://arxiv.org/abs/2002.07088"/>
        <updated>2021-08-05T01:56:21.306Z</updated>
        <summary type="html"><![CDATA[The physical, black-box hard-label setting is arguably the most realistic
threat model for cyber-physical vision systems. In this setting, the attacker
only has query access to the model and only receives the top-1 class label
without confidence information. Creating small physical stickers that are
robust to environmental variation is difficult in the discrete and
discontinuous hard-label space because the attack must both design a small
shape to perturb within and find robust noise to fill it with. Unfortunately,
we find that existing $\ell_2$ or $\ell_\infty$ minimizing hard-label attacks
do not easily extend to finding such robust physical perturbation attacks.
Thus, we propose GRAPHITE, the first algorithm for hard-label physical attacks
on computer vision models. We show that "survivability", an estimate of
physical variation robustness, can be used in new ways to generate small masks
and is a sufficiently smooth function to optimize with gradient-free
optimization. We use GRAPHITE to attack a traffic sign classifier and a
publicly-available Automatic License Plate Recognition (ALPR) tool using only
query access. We evaluate both tools in real-world field tests to measure its
physical-world robustness. We successfully cause a Stop sign to be
misclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and
cause errors in 75% of physical images for the ALPR tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1"&gt;Earlence Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping. (arXiv:2108.02137v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.02137</id>
        <link href="http://arxiv.org/abs/2108.02137"/>
        <updated>2021-08-05T01:56:21.296Z</updated>
        <summary type="html"><![CDATA[Humanitarian mapping from space with machine learning helps policy-makers to
timely and accurately identify people in need. However, recent concerns around
fairness and transparency of algorithmic decision-making are a significant
obstacle for applying these methods in practice. In this paper, we study if
humanitarian mapping approaches from space are prone to bias in their
predictions. We map village-level poverty and electricity rates in India based
on nighttime lights (NTLs) with linear regression and random forest and analyze
if the predictions systematically show prejudice against scheduled caste or
tribe communities. To achieve this, we design a causal approach to measure
counterfactual fairness based on propensity score matching. This allows to
compare villages within a community of interest to synthetic counterfactuals.
Our findings indicate that poverty is systematically overestimated and
electricity systematically underestimated for scheduled tribes in comparison to
a synthetic counterfactual group of villages. The effects have the opposite
direction for scheduled castes where poverty is underestimated and
electrification overestimated. These results are a warning sign for a variety
of applications in humanitarian mapping where fairness issues would compromise
policy goals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kondmann_L/0/1/0/all/0/1"&gt;Lukas Kondmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02122</id>
        <link href="http://arxiv.org/abs/2108.02122"/>
        <updated>2021-08-05T01:56:21.289Z</updated>
        <summary type="html"><![CDATA[We explore the value of weak labels in learning transferable representations
for medical images. Compared to hand-labeled datasets, weak or inexact labels
can be acquired in large quantities at significantly lower cost and can provide
useful training signals for data-hungry models such as deep neural networks. We
consider weak labels in the form of pseudo-labels and propose a semi-weakly
supervised contrastive learning (SWCL) framework for representation learning
using semi-weakly annotated images. Specifically, we train a semi-supervised
model to propagate labels from a small dataset consisting of diverse
image-level annotations to a large unlabeled dataset. Using the propagated
labels, we generate a patch-level dataset for pretraining and formulate a
multi-label contrastive learning objective to capture position-specific
features encoded in each patch. We empirically validate the transfer learning
performance of SWCL on seven public retinal fundus datasets, covering three
disease classification tasks and two anatomical structure segmentation tasks.
Our experiment results suggest that, under very low data regime, large-scale
ImageNet pretraining on improved architecture remains a very strong baseline,
and recently proposed self-supervised methods falter in segmentation tasks,
possibly due to the strong invariant constraint imposed. Our method surpasses
all prior self-supervised methods and standard cross-entropy training, while
closing the gaps with ImageNet pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1"&gt;Boon Peng Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1"&gt;Beng Koon Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Analysis of Wasserstein Distributionally Robust Estimators. (arXiv:2108.02120v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2108.02120</id>
        <link href="http://arxiv.org/abs/2108.02120"/>
        <updated>2021-08-05T01:56:21.282Z</updated>
        <summary type="html"><![CDATA[We consider statistical methods which invoke a min-max distributionally
robust formulation to extract good out-of-sample performance in data-driven
optimization and learning problems. Acknowledging the distributional
uncertainty in learning from limited samples, the min-max formulations
introduce an adversarial inner player to explore unseen covariate data. The
resulting Distributionally Robust Optimization (DRO) formulations, which
include Wasserstein DRO formulations (our main focus), are specified using
optimal transportation phenomena. Upon describing how these
infinite-dimensional min-max problems can be approached via a
finite-dimensional dual reformulation, the tutorial moves into its main
component, namely, explaining a generic recipe for optimally selecting the size
of the adversary's budget. This is achieved by studying the limit behavior of
an optimal transport projection formulation arising from an inquiry on the
smallest confidence region that includes the unknown population risk minimizer.
Incidentally, this systematic prescription coincides with those in specific
examples in high-dimensional statistics and results in error bounds that are
free from the curse of dimensions. Equipped with this prescription, we present
a central limit theorem for the DRO estimator and provide a recipe for
constructing compatible confidence regions that are useful for uncertainty
quantification. The rest of the tutorial is devoted to insights into the nature
of the optimizers selected by the min-max formulations and additional
applications of optimal transport projections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering outliers in the Mars Express thermal power consumption patterns. (arXiv:2108.02067v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2108.02067</id>
        <link href="http://arxiv.org/abs/2108.02067"/>
        <updated>2021-08-05T01:56:21.275Z</updated>
        <summary type="html"><![CDATA[The Mars Express (MEX) spacecraft has been orbiting Mars since 2004. The
operators need to constantly monitor its behavior and handle sporadic
deviations (outliers) from the expected patterns of measurements of quantities
that the satellite is sending to Earth. In this paper, we analyze the patterns
of the electrical power consumption of MEX's thermal subsystem, that maintains
the spacecraft's temperature at the desired level. The consumption is not
constant, but should be roughly periodic in the short term, with the period
that corresponds to one orbit around Mars. By using long short-term memory
neural networks, we show that the consumption pattern is more irregular than
expected, and successfully detect such irregularities, opening possibility for
automatic outlier detection on MEX in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Petkovic_M/0/1/0/all/0/1"&gt;Matej Petkovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lucas_L/0/1/0/all/0/1"&gt;Luke Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Stepisnik_T/0/1/0/all/0/1"&gt;Toma&amp;#x17e; Stepi&amp;#x161;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Panov_P/0/1/0/all/0/1"&gt;Pan&amp;#x10d;e Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Simidjievski_N/0/1/0/all/0/1"&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kocev_D/0/1/0/all/0/1"&gt;Dragi Kocev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal to-do list gamification. (arXiv:2008.05228v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05228</id>
        <link href="http://arxiv.org/abs/2008.05228"/>
        <updated>2021-08-05T01:56:21.268Z</updated>
        <summary type="html"><![CDATA[What should I work on first? What can wait until later? Which projects should
I prioritize and which tasks are not worth my time? These are challenging
questions that many people face every day. People's intuitive strategy is to
prioritize their immediate experience over the long-term consequences. This
leads to procrastination and the neglect of important long-term projects in
favor of seemingly urgent tasks that are less important. Optimal gamification
strives to help people overcome these problems by incentivizing each task by a
number of points that communicates how valuable it is in the long-run.
Unfortunately, computing the optimal number of points with standard dynamic
programming methods quickly becomes intractable as the number of a person's
projects and the number of tasks required by each project increase. Here, we
introduce and evaluate a scalable method for identifying which tasks are most
important in the long run and incentivizing each task according to its
long-term value. Our method makes it possible to create to-do list gamification
apps that can handle the size and complexity of people's to-do lists in the
real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojcheski_J/0/1/0/all/0/1"&gt;Jugoslav Stojcheski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felso_V/0/1/0/all/0/1"&gt;Valkyrie Felso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lieder_F/0/1/0/all/0/1"&gt;Falk Lieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02148</id>
        <link href="http://arxiv.org/abs/2108.02148"/>
        <updated>2021-08-05T01:56:21.262Z</updated>
        <summary type="html"><![CDATA[Due to the mass advancement in ubiquitous technologies nowadays, new
pervasive methods have come into the practice to provide new innovative
features and stimulate the research on new human-computer interactions. This
paper presents a hand gesture recognition method that utilizes the smartphone's
built-in speakers and microphones. The proposed system emits an ultrasonic
sonar-based signal (inaudible sound) from the smartphone's stereo speakers,
which is then received by the smartphone's microphone and processed via a
Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data
augmentation techniques are proposed to improve the detection accuracy and
three dual-channel input fusion methods are compared. The first method merges
the dual-channel audio as a single input spectrogram image. The second method
adopts early fusion by concatenating the dual-channel spectrograms. The third
method adopts late fusion by having two convectional input branches processing
each of the dual-channel spectrograms and then the outputs are merged by the
last layers. Our experimental results demonstrate a promising detection
accuracy for the six gestures presented in our publicly available dataset with
an accuracy of 93.58\% as a baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Ahmed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1"&gt;Ayman El-Refai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1"&gt;Mariam Aboul-Ela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1"&gt;Mohamed Moustafa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Slice Sampling: Parallel, black-box and gradient-free inference for correlated & multimodal distributions. (arXiv:2002.06212v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06212</id>
        <link href="http://arxiv.org/abs/2002.06212"/>
        <updated>2021-08-05T01:56:21.255Z</updated>
        <summary type="html"><![CDATA[Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm
that adapts to the characteristics of the target distribution with minimal
hand-tuning. However, Slice Sampling's performance is highly sensitive to the
user-specified initial length scale hyperparameter and the method generally
struggles with poorly scaled or strongly correlated distributions. This paper
introduces Ensemble Slice Sampling (ESS), a new class of algorithms that
bypasses such difficulties by adaptively tuning the initial length scale and
utilising an ensemble of parallel walkers in order to efficiently handle strong
correlations between parameters. These affine-invariant algorithms are trivial
to construct, require no hand-tuning, and can easily be implemented in parallel
computing environments. Empirical tests show that Ensemble Slice Sampling can
improve efficiency by more than an order of magnitude compared to conventional
MCMC methods on a broad range of highly correlated target distributions. In
cases of strongly multimodal target distributions, Ensemble Slice Sampling can
sample efficiently even in high dimensions. We argue that the parallel,
black-box and gradient-free nature of the method renders it ideal for use in
scientific fields such as physics, astrophysics and cosmology which are
dominated by a wide variety of computationally expensive and non-differentiable
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Karamanis_M/0/1/0/all/0/1"&gt;Minas Karamanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Beutler_F/0/1/0/all/0/1"&gt;Florian Beutler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10582</id>
        <link href="http://arxiv.org/abs/2012.10582"/>
        <updated>2021-08-05T01:56:21.236Z</updated>
        <summary type="html"><![CDATA[Previous neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yining Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1"&gt;Daniel Ciao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02155</id>
        <link href="http://arxiv.org/abs/2108.02155"/>
        <updated>2021-08-05T01:56:21.216Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in medical image segmentation applications is
essential, as it is often connected to vital decision-making. Compelling
attempts have been made in quantifying the uncertainty in image segmentation
architectures, e.g. to learn a density segmentation model conditioned on the
input image. Typical work in this field restricts these learnt densities to be
strictly Gaussian. In this paper, we propose to use a more flexible approach by
introducing Normalizing Flows (NFs), which enables the learnt densities to be
more complex and facilitate more accurate modeling for uncertainty. We prove
this hypothesis by adopting the Probabilistic U-Net and augmenting the
posterior density with an NF, allowing it to be more expressive. Our
qualitative as well as quantitative (GED and IoU) evaluations on the
multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation
datasets, respectively, show a clear improvement. This is mostly apparent in
the quantification of aleatoric uncertainty and the increased predictive
performance of up to 14 percent. This result strongly indicates that a more
flexible density model should be seriously considered in architectures that
attempt to capture segmentation ambiguity through density modeling. The benefit
of this improved modeling will increase human confidence in annotation and
segmentation, and enable eager adoption of the technology in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1"&gt;M.M.A. Valiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1"&gt;C.G.A. Viviers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;R.J.G. van Sloun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1"&gt;P.H.N. de With&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1"&gt;F. van der Sommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09962</id>
        <link href="http://arxiv.org/abs/2012.09962"/>
        <updated>2021-08-05T01:56:21.191Z</updated>
        <summary type="html"><![CDATA[Contrastive learning is effective at learning useful representations without
supervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it
may learn shortcut features irrelevant to the downstream task and discard
relevant information. Past work has addressed this limitation via handcrafted
data augmentations that eliminate the shortcut. However, handcrafted
augmentations are infeasible for data modalities that are not interpretable by
humans (e.g., radio signals). Further, even when the modality is interpretable
(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.
For example, in multi-attribute classification, information related to one
attribute may act as a shortcut around other attributes. This paper presents
reconstructive contrastive learning (RCL), a framework for learning
unsupervised representations that are robust to shortcuts. The key idea is to
force the learned representation to reconstruct the input, which naturally
counters potential shortcuts. Extensive experiments verify that RCL is highly
robust to shortcuts and outperforms state-of-the-art contrastive learning
methods on both RGB and RF datasets for a variety of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lijie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1"&gt;Dina Katabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02032</id>
        <link href="http://arxiv.org/abs/2108.02032"/>
        <updated>2021-08-05T01:56:21.184Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1"&gt;Cosmin Octavian Pene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1"&gt;Amirmasoud Ghiassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1"&gt;Taraneh Younesian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y.Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04331</id>
        <link href="http://arxiv.org/abs/2010.04331"/>
        <updated>2021-08-05T01:56:21.177Z</updated>
        <summary type="html"><![CDATA[Real world traffic sign recognition is an important step towards building
autonomous vehicles, most of which highly dependent on Deep Neural Networks
(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to
adversarial examples. Many attack methods have been proposed to understand and
generate adversarial examples, such as gradient based attack, score based
attack, decision based attack, and transfer based attacks. However, most of
these algorithms are ineffective in real-world road sign attack, because (1)
iteratively learning perturbations for each frame is not realistic for a fast
moving car and (2) most optimization algorithms traverse all pixels equally
without considering their diverse contribution. To alleviate these problems,
this paper proposes the targeted attention attack (TAA) method for real world
road sign attack. Specifically, we have made the following contributions: (1)
we leverage the soft attention map to highlight those important pixels and skip
those zero-contributed areas - this also helps to generate natural
perturbations, (2) we design an efficient universal attack that optimizes a
single perturbation/noise based on a set of training images under the guidance
of the pre-trained attention map, (3) we design a simple objective function
that can be easily optimized, (4) we evaluate the effectiveness of TAA on real
world data sets. Experimental results validate that the TAA method improves the
attack successful rate (nearly 10%) and reduces the perturbation loss (about a
quarter) compared with the popular RP2 method. Additionally, our TAA also
provides good properties, e.g., transferability and generalization capability.
We provide code and data to ensure the reproducibility:
https://github.com/AdvAttack/RoadSignAttack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MIT Supercloud Dataset. (arXiv:2108.02037v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.02037</id>
        <link href="http://arxiv.org/abs/2108.02037"/>
        <updated>2021-08-05T01:56:21.170Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) and Machine learning (ML) workloads are an
increasingly larger share of the compute workloads in traditional
High-Performance Computing (HPC) centers and commercial cloud systems. This has
led to changes in deployment approaches of HPC clusters and the commercial
cloud, as well as a new focus on approaches to optimized resource usage,
allocations and deployment of new AI frame- works, and capabilities such as
Jupyter notebooks to enable rapid prototyping and deployment. With these
changes, there is a need to better understand cluster/datacenter operations
with the goal of developing improved scheduling policies, identifying
inefficiencies in resource utilization, energy/power consumption, failure
prediction, and identifying policy violations. In this paper we introduce the
MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the
analysis of large scale HPC and datacenter/cloud operations. We provide
detailed monitoring logs from the MIT Supercloud system, which include CPU and
GPU usage by jobs, memory usage, file system logs, and physical monitoring
data. This paper discusses the details of the dataset, collection methodology,
data availability, and discusses potential challenge problems being developed
using this data. Datasets and future challenge announcements will be available
via https://dcc.mit.edu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1"&gt;Siddharth Samsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1"&gt;Matthew L Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bestor_D/0/1/0/all/0/1"&gt;David Bestor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1"&gt;Michael Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuther_A/0/1/0/all/0/1"&gt;Albert Reuther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edelman_D/0/1/0/all/0/1"&gt;Daniel Edelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcand_W/0/1/0/all/0/1"&gt;William Arcand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_C/0/1/0/all/0/1"&gt;Chansup Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holodnack_J/0/1/0/all/0/1"&gt;John Holodnack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubbell_M/0/1/0/all/0/1"&gt;Matthew Hubbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1"&gt;Jeremy Kepner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1"&gt;Anna Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1"&gt;Joseph McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michaleas_A/0/1/0/all/0/1"&gt;Adam Michaleas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michaleas_P/0/1/0/all/0/1"&gt;Peter Michaleas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milechin_L/0/1/0/all/0/1"&gt;Lauren Milechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mullen_J/0/1/0/all/0/1"&gt;Julia Mullen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1"&gt;Charles Yee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Benjamin Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prout_A/0/1/0/all/0/1"&gt;Andrew Prout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_A/0/1/0/all/0/1"&gt;Antonio Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanterpool_A/0/1/0/all/0/1"&gt;Allan Vanterpool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEvoy_L/0/1/0/all/0/1"&gt;Lindsey McEvoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1"&gt;Anson Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_D/0/1/0/all/0/1"&gt;Devesh Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1"&gt;Vijay Gadepally&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-encoder based Model for High-dimensional Imbalanced Industrial Data. (arXiv:2108.02083v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02083</id>
        <link href="http://arxiv.org/abs/2108.02083"/>
        <updated>2021-08-05T01:56:21.163Z</updated>
        <summary type="html"><![CDATA[With the proliferation of IoT devices, the distributed control systems are
now capturing and processing more sensors at higher frequency than ever before.
These new data, due to their volume and novelty, cannot be effectively consumed
without the help of data-driven techniques. Deep learning is emerging as a
promising technique to analyze these data, particularly in soft sensor
modeling. The strong representational capabilities of complex data and the
flexibility it offers from an architectural perspective make it a topic of
active applied research in industrial settings. However, the successful
applications of deep learning in soft sensing are still not widely integrated
in factory control systems, because most of the research on soft sensing do not
have access to large scale industrial data which are varied, noisy and
incomplete. The results published in most research papers are therefore not
easily reproduced when applied to the variety of data in industrial settings.
Here we provide manufacturing data sets that are much larger and more complex
than public open soft sensor data. Moreover, the data sets are from Seagate
factories on active service with only necessary anonymization, so that they
reflect the complex and noisy nature of real-world data. We introduce a
variance weighted multi-headed auto-encoder classification model that fits well
into the high-dimensional and highly imbalanced data. Besides the use of
weighting or sampling methods to handle the highly imbalanced data, the model
also simultaneously predicts multiple outputs by exploiting output-supervised
representation learning and multi-task weighting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang1_C/0/1/0/all/0/1"&gt;Chao Zhang1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bom_S/0/1/0/all/0/1"&gt;Sthitie Bom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of gradient descent for learning linear neural networks. (arXiv:2108.02040v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02040</id>
        <link href="http://arxiv.org/abs/2108.02040"/>
        <updated>2021-08-05T01:56:21.142Z</updated>
        <summary type="html"><![CDATA[We study the convergence properties of gradient descent for training deep
linear neural networks, i.e., deep matrix factorizations, by extending a
previous analysis for the related gradient flow. We show that under suitable
conditions on the step sizes gradient descent converges to a critical point of
the loss function, i.e., the square loss in this article. Furthermore, we
demonstrate that for almost all initializations gradient descent converges to a
global minimum in the case of two layers. In the case of three or more layers
we show that gradient descent converges to a global minimum on the manifold
matrices of some fixed rank, where the rank cannot be determined a priori.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguegnang_G/0/1/0/all/0/1"&gt;Gabin Maxime Nguegnang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1"&gt;Holger Rauhut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terstiege_U/0/1/0/all/0/1"&gt;Ulrich Terstiege&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradients Incorporating the Future. (arXiv:2108.02096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02096</id>
        <link href="http://arxiv.org/abs/2108.02096"/>
        <updated>2021-08-05T01:56:21.136Z</updated>
        <summary type="html"><![CDATA[Reasoning about the future -- understanding how decisions in the present time
affect outcomes in the future -- is one of the central challenges for
reinforcement learning (RL), especially in highly-stochastic or partially
observable environments. While predicting the future directly is hard, in this
work we introduce a method that allows an agent to "look into the future"
without explicitly predicting it. Namely, we propose to allow an agent, during
its training on past experience, to observe what \emph{actually} happened in
the future at that time, while enforcing an information bottleneck to avoid the
agent overly relying on this privileged information. This gives our agent the
opportunity to utilize rich and useful information about the future trajectory
dynamics in addition to the present. Our method, Policy Gradients Incorporating
the Future (PGIF), is easy to implement and versatile, being applicable to
virtually any policy gradient algorithm. We apply our proposed method to a
number of off-the-shelf RL algorithms and show that PGIF is able to achieve
higher reward faster in a variety of online and offline RL domains, as well as
sparse-reward and partially observable environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venuto_D/0/1/0/all/0/1"&gt;David Venuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1"&gt;Elaine Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1"&gt;Ofir Nachum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelized Reverse Curriculum Generation. (arXiv:2108.02128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02128</id>
        <link href="http://arxiv.org/abs/2108.02128"/>
        <updated>2021-08-05T01:56:21.130Z</updated>
        <summary type="html"><![CDATA[For reinforcement learning (RL), it is challenging for an agent to master a
task that requires a specific series of actions due to sparse rewards. To solve
this problem, reverse curriculum generation (RCG) provides a reverse expansion
approach that automatically generates a curriculum for the agent to learn. More
specifically, RCG adapts the initial state distribution from the neighborhood
of a goal to a distance as training proceeds. However, the initial state
distribution generated for each iteration might be biased, thus making the
policy overfit or slowing down the reverse expansion rate. While training RCG
for actor-critic (AC) based RL algorithms, this poor generalization and slow
convergence might be induced by the tight coupling between an AC pair.
Therefore, we propose a parallelized approach that simultaneously trains
multiple AC pairs and periodically exchanges their critics. We empirically
demonstrate that this proposed approach can improve RCG in performance and
convergence, and it can also be applied to other AC based RL algorithms with
adapted initial state distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1"&gt;Zih-Yun Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1"&gt;Yi-Lin Tuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01699</id>
        <link href="http://arxiv.org/abs/2012.01699"/>
        <updated>2021-08-05T01:56:21.122Z</updated>
        <summary type="html"><![CDATA[To remove the effects of adversarial perturbations, preprocessing defenses
such as pixel discretization are appealing due to their simplicity but have so
far been shown to be ineffective except on simple datasets such as MNIST,
leading to the belief that pixel discretization approaches are doomed to
failure as a defense technique. This paper revisits the pixel discretization
approaches. We hypothesize that the reason why existing approaches have failed
is that they have used a fixed codebook for the entire dataset. In particular,
we find that can lead to situations where images become more susceptible to
adversarial perturbations and also suffer significant loss of accuracy after
discretization. We propose a novel image preprocessing technique called
Essential Features that uses an adaptive codebook that is based on per-image
content and threat model. Essential Features adaptively selects a separable set
of color clusters for each image to reduce the color space while preserving the
pertinent features of the original image, maximizing both separability and
representation of colors. Additionally, to limit the adversary's ability to
influence the chosen color clusters, Essential Features takes advantage of
spatial correlation with an adaptive blur that moves pixels closer to their
original value without destroying original edge information. We design several
adaptive attacks and find that our approach is more robust than previous
baselines on $L_\infty$ and $L_2$ bounded attacks for several challenging
datasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wu-chi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter-free and Explainable Whole Graph Embedding. (arXiv:2108.02113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02113</id>
        <link href="http://arxiv.org/abs/2108.02113"/>
        <updated>2021-08-05T01:56:21.115Z</updated>
        <summary type="html"><![CDATA[Many real-world complex systems can be described as graphs. For a large-scale
graph with low sparsity, a node's adjacency vector is a long and sparse
representation, limiting the practical utilization of existing machine learning
methods on nodal features. In practice, graph embedding (graph representation
learning) attempts to learn a lower-dimensional representation vector for each
node or the whole graph while maintaining the most basic information of graph.
Since various machine learning methods can efficiently process
lower-dimensional vectors, graph embedding has recently attracted a lot of
attention. However, most node embedding or whole graph embedding methods suffer
from the problem of having more sophisticated methodology, hyperparameter
optimization, and low explainability. This paper proposes a
hyperparameter-free, extensible, and explainable whole graph embedding method,
combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy
(E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a
trade-off between the simplicity and the quality under some supervised
classification learning tasks, using molecular, social, and brain networks. In
addition, the proposed approach has a good performance in lower-dimensional
graph visualization. The new methodology is overall simple,
hyperparameter-free, extensible, and explainable for whole graph embedding with
promising potential for exploring graph classification, prediction, and
lower-dimensional graph visualization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yue Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Linyuan L&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanrong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation via Column Generation. (arXiv:2011.04457v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04457</id>
        <link href="http://arxiv.org/abs/2011.04457"/>
        <updated>2021-08-05T01:56:21.092Z</updated>
        <summary type="html"><![CDATA[Identifying discrete patterns in binary data is an important dimensionality
reduction tool in machine learning and data mining. In this paper, we consider
the problem of low-rank binary matrix factorisation (BMF) under Boolean
arithmetic. Due to the hardness of this problem, most previous attempts rely on
heuristic techniques. We formulate the problem as a mixed integer linear
program and use a large scale optimisation technique of column generation to
solve it without the need of heuristic pattern mining. Our approach focuses on
accuracy and on the provision of optimality guarantees. Experimental results on
real world datasets demonstrate that our proposed method is effective at
producing highly accurate factorisations and improves on the previously
available best known results for 15 out of 24 problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weaker Faithfulness Assumption based on Triple Interactions. (arXiv:2010.14265v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14265</id>
        <link href="http://arxiv.org/abs/2010.14265"/>
        <updated>2021-08-05T01:56:21.085Z</updated>
        <summary type="html"><![CDATA[One of the core assumptions in causal discovery is the faithfulness
assumption, i.e., assuming that independencies found in the data are due to
separations in the true causal graph. This assumption can, however, be violated
in many ways, including xor connections, deterministic functions or cancelling
paths. In this work, we propose a weaker assumption that we call $2$-adjacency
faithfulness. In contrast to adjacency faithfulness, which assumes that there
is no conditional independence between each pair of variables that are
connected in the causal graph, we only require no conditional independence
between a node and a subset of its Markov blanket that can contain up to two
nodes. Equivalently, we adapt orientation faithfulness to this setting. We
further propose a sound orientation rule for causal discovery that applies
under weaker assumptions. As a proof of concept, we derive a modified Grow and
Shrink algorithm that recovers the Markov blanket of a target node and prove
its correctness under strictly weaker assumptions than the standard
faithfulness assumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Marx_A/0/1/0/all/0/1"&gt;Alexander Marx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mooij_J/0/1/0/all/0/1"&gt;Joris M. Mooij&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedJAX: Federated learning simulation with JAX. (arXiv:2108.02117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02117</id>
        <link href="http://arxiv.org/abs/2108.02117"/>
        <updated>2021-08-05T01:56:21.063Z</updated>
        <summary type="html"><![CDATA[Federated learning is a machine learning technique that enables training
across decentralized data. Recently, federated learning has become an active
area of research due to the increased concerns over privacy and security. In
light of this, a variety of open source federated learning libraries have been
developed and released. We introduce FedJAX, a JAX-based open source library
for federated learning simulations that emphasizes ease-of-use in research.
With its simple primitives for implementing federated learning algorithms,
prepackaged datasets, models and algorithms, and fast simulation speed, FedJAX
aims to make developing and evaluating federated algorithms faster and easier
for researchers. Our benchmark results show that FedJAX can be used to train
models with federated averaging on the EMNIST dataset in a few minutes and the
Stack Overflow dataset in roughly an hour with standard hyperparmeters using
TPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1"&gt;Jae Hun Ro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Ke Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A purely data-driven framework for prediction, optimization, and control of networked processes: application to networked SIS epidemic model. (arXiv:2108.02005v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.02005</id>
        <link href="http://arxiv.org/abs/2108.02005"/>
        <updated>2021-08-05T01:56:21.055Z</updated>
        <summary type="html"><![CDATA[Networks are landmarks of many complex phenomena where interweaving
interactions between different agents transform simple local rule-sets into
nonlinear emergent behaviors. While some recent studies unveil associations
between the network structure and the underlying dynamical process, identifying
stochastic nonlinear dynamical processes continues to be an outstanding
problem. Here we develop a simple data-driven framework based on
operator-theoretic techniques to identify and control stochastic nonlinear
dynamics taking place over large-scale networks. The proposed approach requires
no prior knowledge of the network structure and identifies the underlying
dynamics solely using a collection of two-step snapshots of the states. This
data-driven system identification is achieved by using the Koopman operator to
find a low dimensional representation of the dynamical patterns that evolve
linearly. Further, we use the global linear Koopman model to solve critical
control problems by applying to model predictive control (MPC)--typically, a
challenging proposition when applied to large networks. We show that our
proposed approach tackles this by converting the original nonlinear programming
into a more tractable optimization problem that is both convex and with far
fewer variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavasoli_A/0/1/0/all/0/1"&gt;Ali Tavasoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Teague Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakeri_H/0/1/0/all/0/1"&gt;Heman Shakeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Convolution Kernels with Multi-Scale Decomposition for Preterm EEG Inter-burst Detection. (arXiv:2108.02039v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02039</id>
        <link href="http://arxiv.org/abs/2108.02039"/>
        <updated>2021-08-05T01:56:21.032Z</updated>
        <summary type="html"><![CDATA[Linear classifiers with random convolution kernels are computationally
efficient methods that need no design or domain knowledge. Unlike deep neural
networks, there is no need to hand-craft a network architecture; the kernels
are randomly generated and only the linear classifier needs training. A
recently proposed method, RandOm Convolutional KErnel Transforms (ROCKETs), has
shown high accuracy across a range of time-series data sets. Here we propose a
multi-scale version of this method, using both high- and low-frequency
components. We apply our methods to inter-burst detection in a cohort of
preterm EEG recorded from 36 neonates <30 weeks gestational age. Two features
from the convolution of 10,000 random kernels are combined using ridge
regression. The proposed multi-scale ROCKET method out-performs the method
without scale: median (interquartile range, IQR) Matthews correlation
coefficient (MCC) of 0.859 (0.815 to 0.874) for multi-scale versus 0.841 (0.807
to 0.865) without scale, p<0.001. The proposed method lags behind an existing
feature-based machine learning method developed with deep domain knowledge, but
is fast to train and can quickly set an initial baseline threshold of
performance for generic and biomedical time-series classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lundy_C/0/1/0/all/0/1"&gt;Christopher Lundy&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/eess/1/au:+OToole_J/0/1/0/all/0/1"&gt;John M. O&amp;#x27;Toole&lt;/a&gt; (1 and 2) ((1) Irish Centre for Maternal and Child Health Research (INFANT), University College Cork, Ireland, (2) Department of Paediatrics and Child Health, University College Cork, Ireland)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personal Devices for Contact Tracing: Smartphones and Wearables to Fight Covid-19. (arXiv:2108.02008v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02008</id>
        <link href="http://arxiv.org/abs/2108.02008"/>
        <updated>2021-08-05T01:56:20.993Z</updated>
        <summary type="html"><![CDATA[Digital contact tracing has emerged as a viable tool supplementing manual
contact tracing. To date, more than 100 contact tracing applications have been
published to slow down the spread of highly contagious Covid-19. Despite subtle
variabilities among these applications, all of them achieve contact tracing by
manipulating the following three components: a) use a personal device to
identify the user while designing a secure protocol to anonymize the user's
identity; b) leverage networking technologies to analyze and store the data; c)
exploit rich sensing features on the user device to detect the interaction
among users and thus estimate the exposure risk. This paper reviews the current
digital contact tracing based on these three components. We focus on two
personal devices that are intimate to the user: smartphones and wearables. We
discuss the centralized and decentralized networking approaches that use to
facilitate the data flow. Lastly, we investigate the sensing feature available
on smartphones and wearables to detect the proximity between any two users and
present experiments comparing the proximity sensing performance between these
two personal devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Pai Chet Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spachos_P/0/1/0/all/0/1"&gt;Petros Spachos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregori_S/0/1/0/all/0/1"&gt;Stefano Gregori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples. (arXiv:2108.02010v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02010</id>
        <link href="http://arxiv.org/abs/2108.02010"/>
        <updated>2021-08-05T01:56:20.979Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are known to be vulnerable to adversarial
examples. Applications of ML to voice biometrics authentication are no
exception. Yet, the implications of audio adversarial examples on these
real-world systems remain poorly understood given that most research targets
limited defenders who can only listen to the audio samples. Conflating
detectability of an attack with human perceptibility, research has focused on
methods that aim to produce imperceptible adversarial examples which humans
cannot distinguish from the corresponding benign samples. We argue that this
perspective is coarse for two reasons: 1. Imperceptibility is impossible to
verify; it would require an experimental process that encompasses variations in
listener training, equipment, volume, ear sensitivity, types of background
noise etc, and 2. It disregards pipeline-based detection clues that realistic
defenders leverage. This results in adversarial examples that are ineffective
in the presence of knowledgeable defenders. Thus, an adversary only needs an
audio sample to be plausible to a human. We thus introduce surreptitious
adversarial examples, a new class of attacks that evades both human and
pipeline controls. In the white-box setting, we instantiate this class with a
joint, multi-stage optimization attack. Using an Amazon Mechanical Turk user
study, we show that this attack produces audio samples that are more
surreptitious than previous attacks that aim solely for imperceptibility.
Lastly we show that surreptitious adversarial examples are challenging to
develop in the black-box setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Travers_A/0/1/0/all/0/1"&gt;Adelin Travers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Licollari_L/0/1/0/all/0/1"&gt;Lorna Licollari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1"&gt;Varun Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1"&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lie_D/0/1/0/all/0/1"&gt;David Lie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Approach to Estimate Early Worst-Case Execution Time. (arXiv:2108.02001v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02001</id>
        <link href="http://arxiv.org/abs/2108.02001"/>
        <updated>2021-08-05T01:56:20.958Z</updated>
        <summary type="html"><![CDATA[Estimating Worst-Case Execution Time (WCET) is of utmost importance for
developing Cyber-Physical and Safety-Critical Systems. The system's scheduler
uses the estimated WCET to schedule each task of these systems, and failure may
lead to catastrophic events. It is thus imperative to build provably reliable
systems. WCET is available to us in the last stage of systems development when
the hardware is available and the application code is compiled on it. Different
methodologies measure the WCET, but none of them give early insights on WCET,
which is crucial for system development. If the system designers overestimate
WCET in the early stage, then it would lead to the overqualified system, which
will increase the cost of the final product, and if they underestimate WCET in
the early stage, then it would lead to financial loss as the system would not
perform as expected. This paper estimates early WCET using Deep Neural Networks
as an approximate predictor model for hardware architecture and compiler. This
model predicts the WCET based on the source code without compiling and running
on the hardware architecture. Our WCET prediction model is created using the
Pytorch framework. The resulting WCET is too erroneous to be used as an upper
bound on the WCET. However, getting these results in the early stages of system
development is an essential prerequisite for the system's dimensioning and
configuration of the hardware setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikash Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of convolutional neural networks to physiological ECG noise. (arXiv:2108.01995v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01995</id>
        <link href="http://arxiv.org/abs/2108.01995"/>
        <updated>2021-08-05T01:56:20.951Z</updated>
        <summary type="html"><![CDATA[The electrocardiogram (ECG) is one of the most widespread diagnostic tools in
healthcare and supports the diagnosis of cardiovascular disorders. Deep
learning methods are a successful and popular technique to detect indications
of disorders from an ECG signal. However, there are open questions around the
robustness of these methods to various factors, including physiological ECG
noise. In this study we generate clean and noisy versions of an ECG dataset
before applying Symmetric Projection Attractor Reconstruction (SPAR) and
scalogram image transformations. A pretrained convolutional neural network is
trained using transfer learning to classify these image transforms. For the
clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were
0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the
noisy ECG datasets. Notably, when the network trained on clean data was used to
classify the noisy datasets, performance decreases of up to 0.18 in F1 scores
were seen. However, when the network trained on the noisy data was used to
classify the clean dataset, the performance decrease was less than 0.05. We
conclude that physiological ECG noise impacts classification using deep
learning methods and careful consideration should be given to the inclusion of
noisy ECG signals in the training data when developing supervised networks for
ECG classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Venton_J/0/1/0/all/0/1"&gt;J. Venton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harris_P/0/1/0/all/0/1"&gt;P. M. Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundar_A/0/1/0/all/0/1"&gt;A. Sundar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smith_N/0/1/0/all/0/1"&gt;N. A. S. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aston_P/0/1/0/all/0/1"&gt;P. J. Aston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01998</id>
        <link href="http://arxiv.org/abs/2108.01998"/>
        <updated>2021-08-05T01:56:20.944Z</updated>
        <summary type="html"><![CDATA[Energy disaggregation, also known as non-intrusive load monitoring (NILM),
challenges the problem of separating the whole-home electricity usage into
appliance-specific individual consumptions, which is a typical application of
data analysis. {NILM aims to help households understand how the energy is used
and consequently tell them how to effectively manage the energy, thus allowing
energy efficiency which is considered as one of the twin pillars of sustainable
energy policy (i.e., energy efficiency and renewable energy).} Although NILM is
unidentifiable, it is widely believed that the NILM problem can be addressed by
data science. Most of the existing approaches address the energy disaggregation
problem by conventional techniques such as sparse coding, non-negative matrix
factorization, and hidden Markov model. Recent advances reveal that deep neural
networks (DNNs) can get favorable performance for NILM since DNNs can
inherently learn the discriminative signatures of the different appliances. In
this paper, we propose a novel method named adversarial energy disaggregation
(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,
which is new for the energy disaggregation task. Our method trains a generator
and multiple discriminators via an adversarial fashion. The proposed method not
only learns shard representations for different appliances, but captures the
specific multimode structures of each appliance. Extensive experiments on
real-world datasets verify that our method can achieve new state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01938</id>
        <link href="http://arxiv.org/abs/2108.01938"/>
        <updated>2021-08-05T01:56:20.926Z</updated>
        <summary type="html"><![CDATA[Graph neural networks are increasingly becoming the go-to approach in various
fields such as computer vision, computational biology and chemistry, where data
are naturally explained by graphs. However, unlike traditional convolutional
neural networks, deep graph networks do not necessarily yield better
performance than shallow graph networks. This behavior usually stems from the
over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical
methods for solving Partial Differential Equations (PDEs) on manifolds, and as
such, their behavior can be explained by similar analysis. Moreover, as we
demonstrate using an extensive set of experiments, our PDE-motivated networks
can generalize and be effective for various types of problems from different
fields. Our architectures obtain better or on par with the current
state-of-the-art results for problems that are typically approached using
different architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1"&gt;Eldad Haber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure and Privacy-Preserving Federated Learning via Co-Utility. (arXiv:2108.01913v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01913</id>
        <link href="http://arxiv.org/abs/2108.01913"/>
        <updated>2021-08-05T01:56:20.919Z</updated>
        <summary type="html"><![CDATA[The decentralized nature of federated learning, that often leverages the
power of edge devices, makes it vulnerable to attacks against privacy and
security. The privacy risk for a peer is that the model update she computes on
her private data may, when sent to the model manager, leak information on those
private data. Even more obvious are security attacks, whereby one or several
malicious peers return wrong model updates in order to disrupt the learning
process and lead to a wrong model being learned. In this paper we build a
federated learning framework that offers privacy to the participating peers as
well as security against Byzantine and poisoning attacks. Our framework
consists of several protocols that provide strong privacy to the participating
peers via unlinkable anonymity and that are rationally sustainable based on the
co-utility property. In other words, no rational party is interested in
deviating from the proposed protocols. We leverage the notion of co-utility to
build a decentralized co-utile reputation management system that provides
incentives for parties to adhere to the protocols. Unlike privacy protection
via differential privacy, our approach preserves the values of model updates
and hence the accuracy of plain federated learning; unlike privacy protection
via update aggregation, our approach preserves the ability to detect bad model
updates while substantially reducing the computational overhead compared to
methods based on homomorphic encryption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingo_Ferrer_J/0/1/0/all/0/1"&gt;Josep Domingo-Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanco_Justicia_A/0/1/0/all/0/1"&gt;Alberto Blanco-Justicia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjon_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Manj&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1"&gt;David S&amp;#xe1;nchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01997</id>
        <link href="http://arxiv.org/abs/2108.01997"/>
        <updated>2021-08-05T01:56:20.913Z</updated>
        <summary type="html"><![CDATA[Early detection of the coronavirus disease 2019 (COVID-19) helps to treat
patients timely and increase the cure rate, thus further suppressing the spread
of the disease. In this study, we propose a novel deep learning based detection
and similar case recommendation network to help control the epidemic. Our
proposed network contains two stages: the first one is a lung region
segmentation step and is used to exclude irrelevant factors, and the second is
a detection and recommendation stage. Under this framework, in the second
stage, we develop a dual-children network (DuCN) based on a pre-trained
ResNet-18 to simultaneously realize the disease diagnosis and similar case
recommendation. Besides, we employ triplet loss and intrapulmonary distance
maps to assist the detection, which helps incorporate tiny differences between
two images and is conducive to improving the diagnostic accuracy. For each
confirmed COVID-19 case, we give similar cases to provide radiologists with
diagnosis and treatment references. We conduct experiments on a large publicly
available dataset (CC-CCII) and compare the proposed model with
state-of-the-art COVID-19 detection methods. The results show that our proposed
model achieves a promising clinical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Senhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1"&gt;Dandan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Attention Network For Microwave Imaging of Brain Anomaly. (arXiv:2108.01965v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01965</id>
        <link href="http://arxiv.org/abs/2108.01965"/>
        <updated>2021-08-05T01:56:20.906Z</updated>
        <summary type="html"><![CDATA[So far, numerous learned models have been pressed to use in microwave imaging
problems. These models however, are oblivious to the imaging geometry. It has
always been hard to bake the physical setup of the imaging array into the
structure of the network, resulting in a data-intensive models that are not
practical. This work put forward a graph formulation of the microwave imaging
array. The architectures proposed is made cognizant of the physical setup,
allowing it to incorporate the symmetries, resulting in a less data
requirements. Graph convolution and attention mechanism is deployed to handle
the cases of fully-connected graphs corresponding to multi-static arrays. The
graph-treatment of the problem is evaluated on experimental setup in context of
brain anomaly localization with microwave imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1"&gt;A. Al-Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;L. Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1"&gt;A. Abbosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Sound Classification Using Co-tuning and Stochastic Normalization. (arXiv:2108.01991v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.01991</id>
        <link href="http://arxiv.org/abs/2108.01991"/>
        <updated>2021-08-05T01:56:20.900Z</updated>
        <summary type="html"><![CDATA[In this paper, we use pre-trained ResNet models as backbone architectures for
classification of adventitious lung sounds and respiratory diseases. The
knowledge of the pre-trained model is transferred by using vanilla fine-tuning,
co-tuning, stochastic normalization and the combination of the co-tuning and
stochastic normalization techniques. Furthermore, data augmentation in both
time domain and time-frequency domain is used to account for the class
imbalance of the ICBHI and our multi-channel lung sound dataset. Additionally,
we apply spectrum correction to consider the variations of the recording device
properties on the ICBHI dataset. Empirically, our proposed systems mostly
outperform all state-of-the-art lung sound classification systems for the
adventitious lung sounds and respiratory diseases of both datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Truc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pernkopf_F/0/1/0/all/0/1"&gt;Franz Pernkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.01952</id>
        <link href="http://arxiv.org/abs/2108.01952"/>
        <updated>2021-08-05T01:56:20.879Z</updated>
        <summary type="html"><![CDATA[Existing libraries for supervised classification implement techniques that
are based on empirical risk minimization and utilize surrogate losses. We
present MRCpy library that implements minimax risk classifiers (MRCs) that are
based on robust risk minimization and can utilize 0-1-loss. Such techniques
give rise to a manifold of classification methods that can provide tight bounds
on the expected loss. MRCpy provides a unified interface for different variants
of MRCs and follows the standards of popular Python libraries. The presented
library also provides implementation for popular techniques that can be seen as
MRCs such as L1-regularized logistic regression, zero-one adversarial, and
maximum entropy machines. In addition, MRCpy implements recent feature mappings
such as Fourier, ReLU, and threshold features. The library is designed with an
object-oriented approach that facilitates collaborators and users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bondugula_K/0/1/0/all/0/1"&gt;Kartheek Bondugula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazuelas_S/0/1/0/all/0/1"&gt;Santiago Mazuelas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1"&gt;Aritz P&amp;#xe9;rez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex Factorization and Manifold Formulations are Almost Equivalent in Low-rank Matrix Optimization. (arXiv:2108.01772v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.01772</id>
        <link href="http://arxiv.org/abs/2108.01772"/>
        <updated>2021-08-05T01:56:20.872Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the geometric landscape connection of the widely
studied manifold and factorization formulations in low-rank positive
semidefinite (PSD) and general matrix optimization. We establish an equivalence
on the set of first-order stationary points (FOSPs) and second-order stationary
points (SOSPs) between the manifold and the factorization formulations. We
further give a sandwich inequality on the spectrum of Riemannian and Euclidean
Hessians at FOSPs, which can be used to transfer more geometric properties from
one formulation to another. Similarities and differences on the landscape
connection under the PSD case and the general case are discussed. To the best
of our knowledge, this is the first geometric landscape connection between the
manifold and the factorization formulations for handling rank constraints. In
the general low-rank matrix optimization, the landscape connection of two
factorization formulations (unregularized and regularized ones) is also
provided. By applying these geometric landscape connections, we are able to
solve unanswered questions in literature and establish stronger results in the
applications on geometric analysis of phase retrieval, well-conditioned
low-rank matrix optimization, and the role of regularization in factorization
arising from machine learning and signal processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1"&gt;Xudong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.01949</id>
        <link href="http://arxiv.org/abs/2108.01949"/>
        <updated>2021-08-05T01:56:20.866Z</updated>
        <summary type="html"><![CDATA[Media is evolving from traditional linear narratives to personalised
experiences, where control over information (or how it is presented) is given
to individual audience members. Measuring and understanding audience engagement
with this media is important in at least two ways: (1) a post-hoc understanding
of how engaged audiences are with the content will help production teams learn
from experience and improve future productions; (2), this type of media has
potential for real-time measures of engagement to be used to enhance the user
experience by adapting content on-the-fly. Engagement is typically measured by
asking samples of users to self-report, which is time consuming and expensive.
In some domains, however, interaction data have been used to infer engagement.
Fortuitously, the nature of interactive media facilitates a much richer set of
interaction data than traditional media; our research aims to understand if
these data can be used to infer audience engagement. In this paper, we report a
study using data captured from audience interactions with an interactive TV
show to model and predict engagement. We find that temporal metrics, including
overall time spent on the experience and the interval between events, are
predictive of engagement. The results demonstrate that interaction data can be
used to infer users' engagement during and after an experience, and the
proposed techniques are relevant to better understand audience preference and
responses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1"&gt;Jonathan Carlton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andy Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1"&gt;Caroline Jay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1"&gt;John Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01828</id>
        <link href="http://arxiv.org/abs/2108.01828"/>
        <updated>2021-08-05T01:56:20.857Z</updated>
        <summary type="html"><![CDATA[Neural agents trained in reinforcement learning settings can learn to
communicate among themselves via discrete tokens, accomplishing as a team what
agents would be unable to do alone. However, the current standard of using
one-hot vectors as discrete communication tokens prevents agents from acquiring
more desirable aspects of communication such as zero-shot understanding.
Inspired by word embedding techniques from natural language processing, we
propose neural agent architectures that enables them to communicate via
discrete tokens derived from a learned, continuous space. We show in a decision
theoretic framework that our technique optimizes communication over a wide
range of scenarios, whereas one-hot tokens are only optimal under restrictive
assumptions. In self-play experiments, we validate that our trained agents
learn to cluster tokens in semantically-meaningful ways, allowing them
communicate in noisy environments where other techniques fail. Lastly, we
demonstrate both that agents using our method can effectively respond to novel
human communication and that humans can understand unlabeled emergent agent
communication, outperforming the use of one-hot communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1"&gt;Mycal Tucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Siddharth Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1"&gt;Dana Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1"&gt;Katia Sycara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Michael Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Julie Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:20.849Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations. (arXiv:2108.01846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01846</id>
        <link href="http://arxiv.org/abs/2108.01846"/>
        <updated>2021-08-05T01:56:20.821Z</updated>
        <summary type="html"><![CDATA[Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pragmatic Look at Deep Imitation Learning. (arXiv:2108.01867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01867</id>
        <link href="http://arxiv.org/abs/2108.01867"/>
        <updated>2021-08-05T01:56:20.813Z</updated>
        <summary type="html"><![CDATA[The introduction of the generative adversarial imitation learning (GAIL)
algorithm has spurred the development of scalable imitation learning approaches
using deep neural networks. The GAIL objective can be thought of as 1) matching
the expert policy's state distribution; 2) penalising the learned policy's
state distribution; and 3) maximising entropy. While theoretically motivated,
in practice GAIL can be difficult to apply, not least due to the instabilities
of adversarial training. In this paper, we take a pragmatic look at GAIL and
related imitation learning algorithms. We implement and automatically tune a
range of algorithms in a unified experimental setup, presenting a fair
evaluation between the competing methods. From our results, our primary
recommendation is to consider non-adversarial methods. Furthermore, we discuss
the common components of imitation learning objectives, and present promising
avenues for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1"&gt;Kai Arulkumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lillrank_D/0/1/0/all/0/1"&gt;Dan Ogawa Lillrank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing a dynamical system and forecasting time series by self-consistent deep learning. (arXiv:2108.01862v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01862</id>
        <link href="http://arxiv.org/abs/2108.01862"/>
        <updated>2021-08-05T01:56:20.807Z</updated>
        <summary type="html"><![CDATA[We introduce a self-consistent deep-learning framework which, for a noisy
deterministic time series, provides unsupervised filtering, state-space
reconstruction, identification of the underlying differential equations and
forecasting. Without a priori information on the signal, we embed the time
series in a state space, where deterministic structures, i.e. attractors, are
revealed. Under the assumption that the evolution of solution trajectories is
described by an unknown dynamical system, we filter out stochastic outliers.
The embedding function, the solution trajectories and the dynamical systems are
constructed using deep neural networks, respectively. By exploiting the
differentiability of the neural solution trajectory, the neural dynamical
system is defined locally at each time, mitigating the need for propagating
gradients through numerical solvers. On a chaotic time series masked by
additive Gaussian noise, we demonstrate the filtering ability and the
predictive power of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guet_C/0/1/0/all/0/1"&gt;Claude Guet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Task Agnostic Skills with Data-driven Guidance. (arXiv:2108.01869v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01869</id>
        <link href="http://arxiv.org/abs/2108.01869"/>
        <updated>2021-08-05T01:56:20.801Z</updated>
        <summary type="html"><![CDATA[To increase autonomy in reinforcement learning, agents need to learn useful
behaviours without reliance on manually designed reward functions. To that end,
skill discovery methods have been used to learn the intrinsic options available
to an agent using task-agnostic objectives. However, without the guidance of
task-specific rewards, emergent behaviours are generally useless due to the
under-constrained problem of skill discovery in complex and high-dimensional
spaces. This paper proposes a framework for guiding the skill discovery towards
the subset of expert-visited states using a learned state projection. We apply
our method in various reinforcement learning (RL) tasks and show that such a
projection results in more useful behaviours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klemsdal_E/0/1/0/all/0/1"&gt;Even Klemsdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herland_S/0/1/0/all/0/1"&gt;Sverre Herland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murad_A/0/1/0/all/0/1"&gt;Abdulmajid Murad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01887</id>
        <link href="http://arxiv.org/abs/2108.01887"/>
        <updated>2021-08-05T01:56:20.794Z</updated>
        <summary type="html"><![CDATA[Despite the success of multilingual sequence-to-sequence pretraining, most
existing approaches rely on monolingual corpora, and do not make use of the
strong cross-lingual signal contained in parallel data. In this paper, we
present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence
models), which extends the conventional denoising objective used to train these
models by (i) replacing words in the noised sequence according to a
multilingual dictionary, and (ii) predicting the reference translation
according to a parallel corpus instead of recovering the original sequence. Our
experiments on machine translation and cross-lingual natural language inference
show an average improvement of 2.0 BLEU points and 6.7 accuracy points from
integrating parallel data into pretraining, respectively, obtaining results
that are competitive with several popular models at a fraction of their
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1"&gt;Machel Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Across Two Atari Paddle Games Using the Same Perceptual Control Architecture Without Training. (arXiv:2108.01895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01895</id>
        <link href="http://arxiv.org/abs/2108.01895"/>
        <updated>2021-08-05T01:56:20.785Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning (DRL) requires large samples and a long training
time to operate optimally. Yet humans rarely require long periods training to
perform well on novel tasks, such as computer games, once they are provided
with an accurate program of instructions. We used perceptual control theory
(PCT) to construct a simple closed-loop model which requires no training
samples and training time within a video game study using the Arcade Learning
Environment (ALE). The model was programmed to parse inputs from the
environment into hierarchically organised perceptual signals, and it computed a
dynamic error signal by subtracting the incoming signal for each perceptual
variable from a reference signal to drive output signals to reduce this error.
We tested the same model across two different Atari paddle games Breakout and
Pong to achieve performance at least as high as DRL paradigms, and close to
good human performance. Our study shows that perceptual control models, based
on simple assumptions, can perform well without learning. We conclude by
specifying a parsimonious role of learning that may be more similar to
psychological functioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulrez_T/0/1/0/all/0/1"&gt;Tauseef Gulrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansell_W/0/1/0/all/0/1"&gt;Warren Mansell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Continuous Distributions and Fenchel-Young Losses. (arXiv:2108.01988v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01988</id>
        <link href="http://arxiv.org/abs/2108.01988"/>
        <updated>2021-08-05T01:56:20.766Z</updated>
        <summary type="html"><![CDATA[Exponential families are widely used in machine learning; they include many
distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,
Poisson, and categorical distributions via the softmax transformation).
Distributions in each of these families have fixed support. In contrast, for
finite domains, there has been recent works on sparse alternatives to softmax
(e.g. sparsemax, $\alpha$-entmax, and fusedmax) and corresponding losses, which
have varying support.

This paper expands that line of work in several directions: first, it extends
$\Omega$-regularized prediction maps and Fenchel-Young losses to arbitrary
domains (possibly countably infinite or continuous). For linearly parametrized
families, we show that minimization of Fenchel-Young losses is equivalent to
moment matching of the statistics, generalizing a fundamental property of
exponential families. When $\Omega$ is a Tsallis negentropy with parameter
$\alpha$, we obtain "deformed exponential families," which include
$\alpha$-entmax and sparsemax ($\alpha$ = 2) as particular cases. For quadratic
energy functions in continuous domains, the resulting densities are
$\beta$-Gaussians, an instance of elliptical distributions that contain as
particular cases the Gaussian, biweight, triweight and Epanechnikov densities,
and for which we derive closed-form expressions for the variance, Tsallis
entropy, and Fenchel-Young loss. When $\Omega$ is a total variation or Sobolev
regularizer, we obtain a continuous version of the fusedmax. Finally, we
introduce continuous-domain attention mechanisms, deriving efficient gradient
backpropagation algorithms for $\alpha \in \{1, 4/3, 3/2, 2\}$. Using them, we
demonstrate our sparse continuous distributions for attention-based audio
classification and visual question answering, showing that they allow attending
to time intervals and compact regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1"&gt;Marcos Treviso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Farinhas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aguiar_P/0/1/0/all/0/1"&gt;Pedro M. Q. Aguiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1"&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01899</id>
        <link href="http://arxiv.org/abs/2108.01899"/>
        <updated>2021-08-05T01:56:20.754Z</updated>
        <summary type="html"><![CDATA[Most existing neural architecture search (NAS) algorithms are dedicated to
the downstream tasks, e.g., image classification in computer vision. However,
extensive experiments have shown that, prominent neural architectures, such as
ResNet in computer vision and LSTM in natural language processing, are
generally good at extracting patterns from the input data and perform well on
different downstream tasks. These observations inspire us to ask: Is it
necessary to use the performance of specific downstream tasks to evaluate and
search for good neural architectures? Can we perform NAS effectively and
efficiently while being agnostic to the downstream task? In this work, we
attempt to affirmatively answer the above two questions and improve the
state-of-the-art NAS solution by proposing a novel and generic NAS framework,
termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but
instead adopts \textit{regression} on a set of manually designed synthetic
signal bases for architecture evaluation. Such a self-supervised regression
task can effectively evaluate the intrinsic power of an architecture to capture
and transform the input signal patterns, and allow more sufficient usage of
training samples. We then propose an automatic task search to optimize the
combination of synthetic signals using limited downstream-task-specific labels,
further improving the performance of GenNAS. We also thoroughly evaluate
GenNAS's generality and end-to-end NAS performance on all search spaces, which
outperforms almost all existing works with significant speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Deming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staged trees and asymmetry-labeled DAGs. (arXiv:2108.01994v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.01994</id>
        <link href="http://arxiv.org/abs/2108.01994"/>
        <updated>2021-08-05T01:56:20.748Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are a widely-used class of probabilistic graphical models
capable of representing symmetric conditional independence between variables of
interest using the topology of the underlying graph. They can be seen as a
special case of the much more general class of models called staged trees,
which can represent any type of non-symmetric conditional independence. Here we
formalize the relationship between these two models and introduce a minimal
Bayesian network representation of the staged tree, which can be used to read
conditional independences in an intuitive way. Furthermore, we define a new
labeled graph, termed asymmetry-labeled directed acyclic graph, whose edges are
labeled to denote the type of dependence existing between any two random
variables. Various datasets are used to illustrate the methodology,
highlighting the need to construct models which more flexibly encode and
represent non-symmetric structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Varando_G/0/1/0/all/0/1"&gt;Gherardo Varando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carli_F/0/1/0/all/0/1"&gt;Federico Carli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leonelli_M/0/1/0/all/0/1"&gt;Manuele Leonelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximating Attributed Incentive Salience In Large Scale Scenarios. A Representation Learning Approach Based on Artificial Neural Networks. (arXiv:2108.01724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01724</id>
        <link href="http://arxiv.org/abs/2108.01724"/>
        <updated>2021-08-05T01:56:20.739Z</updated>
        <summary type="html"><![CDATA[Incentive salience attribution can be understood as a psychobiological
process ascribing relevance to potentially rewarding objects and actions.
Despite being an important component of the motivational process guiding our
everyday behaviour its study in naturalistic contexts is not straightforward.
Here we propose a methodology based on artificial neural networks (ANNs) for
approximating latent states produced by this process in situations where large
volumes of behavioural data are available but no strict experimental control is
possible. Leveraging knowledge derived from theoretical and computational
accounts of incentive salience attribution we designed an ANN for estimating
duration and intensity of future interactions between individuals and a series
of video games in a large-scale ($N> 3 \times 10^6$) longitudinal dataset.
Through model comparison and inspection we show that our approach outperforms
competing ones while also generating a representation that well approximate
some of the functions of attributed incentive salience. We discuss our findings
with reference to the adopted theoretical and computational frameworks and
suggest how our methodology could be an initial step for estimating attributed
incentive salience in large scale behavioural studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonometti_V/0/1/0/all/0/1"&gt;Valerio Bonometti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_M/0/1/0/all/0/1"&gt;Mathieu J. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drachen_A/0/1/0/all/0/1"&gt;Anders Drachen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1"&gt;Alex Wade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Opponent Modeling. (arXiv:2108.01843v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01843</id>
        <link href="http://arxiv.org/abs/2108.01843"/>
        <updated>2021-08-05T01:56:20.730Z</updated>
        <summary type="html"><![CDATA[When one agent interacts with a multi-agent environment, it is challenging to
deal with various opponents unseen before. Modeling the behaviors, goals, or
beliefs of opponents could help the agent adjust its policy to adapt to
different opponents. In addition, it is also important to consider opponents
who are learning simultaneously or capable of reasoning. However, existing work
usually tackles only one of the aforementioned types of opponent. In this
paper, we propose model-based opponent modeling (MBOM), which employs the
environment model to adapt to all kinds of opponent. MBOM simulates the
recursive reasoning process in the environment model and imagines a set of
improving opponent policies. To effectively and accurately represent the
opponent policy, MBOM further mixes the imagined opponent policies according to
the similarity with the real behaviors of opponents. Empirically, we show that
MBOM achieves more effective adaptation than existing methods in competitive
and cooperative environments, respectively with different types of opponent,
i.e., fixed policy, na\"ive learner, and reasoning learner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaopeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiechuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haobin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zongqing Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01808</id>
        <link href="http://arxiv.org/abs/2108.01808"/>
        <updated>2021-08-05T01:56:20.712Z</updated>
        <summary type="html"><![CDATA[There is a warning light for the loss of plant habitats worldwide that
entails concerted efforts to conserve plant biodiversity. Thus, plant species
classification is of crucial importance to address this environmental
challenge. In recent years, there is a considerable increase in the number of
studies related to plant taxonomy. While some researchers try to improve their
recognition performance using novel approaches, others concentrate on
computational optimization for their framework. In addition, a few studies are
diving into feature extraction to gain significantly in terms of accuracy. In
this paper, we propose an effective method for the leaf recognition problem. In
our proposed approach, a leaf goes through some pre-processing to extract its
refined color image, vein image, xy-projection histogram, handcrafted shape,
texture features, and Fourier descriptors. These attributes are then
transformed into a better representation by neural network-based encoders
before a support vector machine (SVM) model is utilized to classify different
leaves. Overall, our approach performs a state-of-the-art result on the Flavia
leaf dataset, achieving the accuracy of 99.58\% on test sets under random
10-fold cross-validation and bypassing the previous methods. We also release
our codes\footnote{Scripts are available at
\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to
the research community in the leaf classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1"&gt;Boi M. Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1"&gt;Dinh V. Cuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1"&gt;Nhung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dang Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Community Detection via Parallel Correlation Clustering. (arXiv:2108.01731v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01731</id>
        <link href="http://arxiv.org/abs/2108.01731"/>
        <updated>2021-08-05T01:56:20.704Z</updated>
        <summary type="html"><![CDATA[Graph clustering and community detection are central problems in modern data
mining. The increasing need for analyzing billion-scale data calls for faster
and more scalable algorithms for these problems. There are certain trade-offs
between the quality and speed of such clustering algorithms. In this paper, we
design scalable algorithms that achieve high quality when evaluated based on
ground truth. We develop a generalized sequential and shared-memory parallel
framework based on the LambdaCC objective (introduced by Veldt et al.), which
encompasses modularity and correlation clustering. Our framework consists of
highly-optimized implementations that scale to large data sets of billions of
edges and that obtain high-quality clusters compared to ground-truth data, on
both unweighted and weighted graphs. Our empirical evaluation shows that this
framework improves the state-of-the-art trade-offs between speed and quality of
scalable community detection. For example, on a 30-core machine with two-way
hyper-threading, our implementations achieve orders of magnitude speedups over
other correlation clustering baselines, and up to 28.44x speedups over our own
sequential baselines while maintaining or improving quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jessica Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1"&gt;Laxman Dhulipala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstat_D/0/1/0/all/0/1"&gt;David Eisenstat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacki_J/0/1/0/all/0/1"&gt;Jakub &amp;#x141;&amp;#x105;cki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Conditioned Neural Motion Planning. (arXiv:2108.01851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01851</id>
        <link href="http://arxiv.org/abs/2108.01851"/>
        <updated>2021-08-05T01:56:20.698Z</updated>
        <summary type="html"><![CDATA[Risk-bounded motion planning is an important yet difficult problem for
safety-critical tasks. While existing mathematical programming methods offer
theoretical guarantees in the context of constrained Markov decision processes,
they either lack scalability in solving larger problems or produce conservative
plans. Recent advances in deep reinforcement learning improve scalability by
learning policy networks as function approximators. In this paper, we propose
an extension of soft actor critic model to estimate the execution risk of a
plan through a risk critic and produce risk-bounded policies efficiently by
adding an extra risk term in the loss function of the policy network. We define
the execution risk in an accurate form, as opposed to approximating it through
a summation of immediate risks at each time step that leads to conservative
plans. Our proposed model is conditioned on a continuous spectrum of risk
bounds, allowing the user to adjust the risk-averse level of the agent on the
fly. Through a set of experiments, we show the advantage of our model in terms
of both computational time and plan quality, compared to a state-of-the-art
mathematical programming baseline, and validate its performance in more
complicated scenarios, including nonlinear dynamics and larger state space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Meng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasour_A/0/1/0/all/0/1"&gt;Ashkan Jasour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1"&gt;Guy Rosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Brian Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01769</id>
        <link href="http://arxiv.org/abs/2108.01769"/>
        <updated>2021-08-05T01:56:20.690Z</updated>
        <summary type="html"><![CDATA[Previous work has shown that neural architectures are able to perform optical
music recognition (OMR) on monophonic and homophonic music with high accuracy.
However, piano and orchestral scores frequently exhibit polyphonic passages,
which add a second dimension to the task. Monophonic and homophonic music can
be described as homorhythmic, or having a single musical rhythm. Polyphonic
music, on the other hand, can be seen as having multiple rhythmic sequences, or
voices, concurrently. We first introduce a workflow for creating large-scale
polyphonic datasets suitable for end-to-end recognition from sheet music
publicly available on the MuseScore forum. We then propose two novel
formulations for end-to-end polyphonic OMR -- one treating the problem as a
type of multi-task binary classification, and the other treating it as
multi-sequence detection. Building upon the encoder-decoder architecture and an
image encoder proposed in past work on end-to-end OMR, we propose two novel
decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two
formulations. Finally, we compare the empirical performance of these end-to-end
approaches to polyphonic OMR and observe a new state-of-the-art performance
with our multi-sequence detection decoder, RNNDecoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1"&gt;Sachinda Edirisooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01832</id>
        <link href="http://arxiv.org/abs/2108.01832"/>
        <updated>2021-08-05T01:56:20.684Z</updated>
        <summary type="html"><![CDATA[In many real-world multi-agent cooperative tasks, due to high cost and risk,
agents cannot interact with the environment and collect experiences during
learning, but have to learn from offline datasets. However, the transition
probabilities calculated from the dataset can be much different from the
transition probabilities induced by the learned policies of other agents,
creating large errors in value estimates. Moreover, the experience
distributions of agents' datasets may vary wildly due to diverse behavior
policies, causing large difference in value estimates between agents.
Consequently, agents will learn uncoordinated suboptimal policies. In this
paper, we propose MABCQ, which exploits value deviation and transition
normalization to modify the transition probabilities. Value deviation
optimistically increases the transition probabilities of high-value next
states, and transition normalization normalizes the biased transition
probabilities of next states. They together encourage agents to discover
potential optimal and coordinated policies. Mathematically, we prove the
convergence of Q-learning under the non-stationary transition probabilities
after modification. Empirically, we show that MABCQ greatly outperforms
baselines and reduces the difference in value estimates between agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiechuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zongqing Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTTP2vec: Embedding of HTTP Requests for Detection of Anomalous Traffic. (arXiv:2108.01763v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01763</id>
        <link href="http://arxiv.org/abs/2108.01763"/>
        <updated>2021-08-05T01:56:20.678Z</updated>
        <summary type="html"><![CDATA[Hypertext transfer protocol (HTTP) is one of the most widely used protocols
on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use
HTTP as the transport mechanism. Therefore, it is crucial to develop an
intelligent solution that would allow to effectively detect and filter out
anomalies in HTTP traffic. Currently, most of the anomaly detection systems are
either rule-based or trained using manually selected features. We propose
utilizing modern unsupervised language representation model for embedding HTTP
requests and then using it to classify anomalies in the traffic. The solution
is motivated by methods used in Natural Language Processing (NLP) such as
Doc2Vec which could potentially capture the true understanding of HTTP
messages, and therefore improve the efficiency of Intrusion Detection System.
In our work, we not only aim at generating a suitable embedding space, but also
at the interpretability of the proposed model. We decided to use the current
state-of-the-art RoBERTa, which, as far as we know, has never been used in a
similar problem. To verify how the solution would work in real word conditions,
we train the model using only legitimate traffic. We also try to explain the
results based on clusters that occur in the vectorized requests space and a
simple logistic regression classifier. We compared our approach with the
similar, previously proposed methods. We evaluate the feasibility of our method
on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared
ourselves. The results we show are comparable to others or better, and most
importantly - interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gniewkowski_M/0/1/0/all/0/1"&gt;Mateusz Gniewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maciejewski_H/0/1/0/all/0/1"&gt;Henryk Maciejewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surmacz_T/0/1/0/all/0/1"&gt;Tomasz R. Surmacz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walentynowicz_W/0/1/0/all/0/1"&gt;Wiktor Walentynowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I2V-GAN: Unpaired Infrared-to-Visible Video Translation. (arXiv:2108.00913v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00913</id>
        <link href="http://arxiv.org/abs/2108.00913"/>
        <updated>2021-08-05T01:56:20.655Z</updated>
        <summary type="html"><![CDATA[Human vision is often adversely affected by complex environmental factors,
especially in night vision scenarios. Thus, infrared cameras are often
leveraged to help enhance the visual effects via detecting infrared radiation
in the surrounding environment, but the infrared videos are undesirable due to
the lack of detailed semantic information. In such a case, an effective
video-to-video translation method from the infrared domain to the visible light
counterpart is strongly needed by overcoming the intrinsic huge gap between
infrared and visible fields. To address this challenging problem, we propose an
infrared-to-visible (I2V) video translation method I2V-GAN to generate
fine-grained and spatial-temporal consistent visible light videos by given
unpaired infrared videos. Technically, our model capitalizes on three types of
constraints: 1)adversarial constraint to generate synthetic frames that are
similar to the real ones, 2)cyclic consistency with the introduced perceptual
loss for effective content conversion as well as style preservation, and
3)similarity constraints across and within domains to enhance the content and
motion consistency in both spatial and temporal spaces at a fine-grained level.
Furthermore, the current public available infrared and visible light datasets
are mainly used for object detection or tracking, and some are composed of
discontinuous images which are not suitable for video tasks. Thus, we provide a
new dataset for I2V video translation, which is named IRVI. Specifically, it
has 12 consecutive video clips of vehicle and monitoring scenes, and both
infrared and visible light videos could be apart into 24352 frames.
Comprehensive experiments validate that I2V-GAN is superior to the compared
SOTA methods in the translation of I2V videos with higher fluency and finer
semantic details. The code and IRVI dataset are available at
https://github.com/BIT-DA/I2V-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bingfeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhenjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chi Harold Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuigen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study on Herd Behavior Using Sentiment Analysis in Online Social Network. (arXiv:2108.01728v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01728</id>
        <link href="http://arxiv.org/abs/2108.01728"/>
        <updated>2021-08-05T01:56:20.648Z</updated>
        <summary type="html"><![CDATA[Social media platforms are thriving nowadays, so a huge volume of data is
produced. As it includes brief and clear statements, millions of people post
their thoughts on microblogging sites every day. This paper represents and
analyze the capacity of diverse strategies to volumetric, delicate, and social
networks to predict critical opinions from online social networking sites. In
the exploration of certain searching for relevant, the thoughts of people play
a crucial role. Social media becomes a good outlet since the last decades to
share the opinions globally. Sentiment analysis as well as opinion mining is a
tool that is used to extract the opinions or thoughts of the common public. An
occurrence in one place, be it economic, political, or social, may trigger
large-scale chain public reaction across many other sites in an increasingly
interconnected world. This study demonstrates the evaluation of sentiment
analysis techniques using social media contents and creating the association
between subjectivity with herd behavior and clustering coefficient as well as
tries to predict the election result (2021 election in West Bengal). This is an
implementation of sentiment analysis targeted at estimating the results of an
upcoming election by assessing the public's opinion across social media. This
paper also has a short discussion section on the usefulness of the idea in
other fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Suchandra Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1"&gt;Dhrubasish Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Sohom Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kole_D/0/1/0/all/0/1"&gt;Dipak K. Kole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_P/0/1/0/all/0/1"&gt;Premananda Jana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Chromatic and Clique Numbers of Graphs. (arXiv:2108.01810v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01810</id>
        <link href="http://arxiv.org/abs/2108.01810"/>
        <updated>2021-08-05T01:56:20.642Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been applied to a wide range of problems across
different application domains with great success. Recently, research into
combinatorial optimization problems in particular has generated much interest
in the machine learning community. In this work, we develop deep learning
models to predict the chromatic number and maximum clique size of graphs, both
of which represent classical NP-complete combinatorial optimization problems
encountered in graph theory. The neural networks are trained using the most
basic representation of the graph, the adjacency matrix, as opposed to
undergoing complex domain-specific feature engineering. The experimental
results show that deep neural networks, and in particular convolutional neural
networks, obtain strong performance on this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hulse_J/0/1/0/all/0/1"&gt;Jason Van Hulse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1"&gt;Joshua S. Friedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application. (arXiv:2108.01903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01903</id>
        <link href="http://arxiv.org/abs/2108.01903"/>
        <updated>2021-08-05T01:56:20.633Z</updated>
        <summary type="html"><![CDATA[While machine learning techniques are being applied to various fields for
their exceptional ability to find complex relations in large datasets, the
strengthening of regulations on data ownership and privacy is causing
increasing difficulty in its application to medical data. In light of this,
Federated Learning has recently been proposed as a solution to train on private
data without breach of confidentiality. This conservation of privacy is
particularly appealing in the field of healthcare, where patient data is highly
confidential. However, many studies have shown that its assumption of
Independent and Identically Distributed data is unrealistic for medical data.
In this paper, we propose Personalized Federated Cluster Models, a hierarchical
clustering-based FL process, to predict Major Depressive Disorder severity from
Heart Rate Variability. By allowing clients to receive more personalized model,
we address problems caused by non-IID data, showing an accuracy increase in
severity prediction. This increase in performance may be sufficient to use
Personalized Federated Cluster Models in many existing Federated Learning
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Joo Hun Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1"&gt;Ha Min Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hyejun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1"&gt;Eun-Hye Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Ah Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Young Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hong Jin Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1"&gt;Tai-Myoung Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Tree Search for high precision manufacturing. (arXiv:2108.01789v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01789</id>
        <link href="http://arxiv.org/abs/2108.01789"/>
        <updated>2021-08-05T01:56:20.600Z</updated>
        <summary type="html"><![CDATA[Monte Carlo Tree Search (MCTS) has shown its strength for a lot of
deterministic and stochastic examples, but literature lacks reports of
applications to real world industrial processes. Common reasons for this are
that there is no efficient simulator of the process available or there exist
problems in applying MCTS to the complex rules of the process. In this paper,
we apply MCTS for optimizing a high-precision manufacturing process that has
stochastic and partially observable outcomes. We make use of an
expert-knowledge-based simulator and adapt the MCTS default policy to deal with
the manufacturing process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1"&gt;Dorina Weichert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horchler_F/0/1/0/all/0/1"&gt;Felix Horchler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1"&gt;Alexander Kister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trost_M/0/1/0/all/0/1"&gt;Marcus Trost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartung_J/0/1/0/all/0/1"&gt;Johannes Hartung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risse_S/0/1/0/all/0/1"&gt;Stefan Risse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Neural Architecture Search with Performance Prediction. (arXiv:2108.01854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01854</id>
        <link href="http://arxiv.org/abs/2108.01854"/>
        <updated>2021-08-05T01:56:20.593Z</updated>
        <summary type="html"><![CDATA[Neural networks are powerful models that have a remarkable ability to extract
patterns that are too complex to be noticed by humans or other machine learning
models. Neural networks are the first class of models that can train end-to-end
systems with large learning capacities. However, we still have the difficult
challenge of designing the neural network, which requires human experience and
a long process of trial and error. As a solution, we can use a neural
architecture search to find the best network architecture for the task at hand.
Existing NAS algorithms generally evaluate the fitness of a new architecture by
fully training from scratch, resulting in the prohibitive computational cost,
even if operated on high-performance computers. In this paper, an end-to-end
offline performance predictor is proposed to accelerate the evaluation of
sampled architectures.

Index Terms- Learning Curve Prediction, Neural Architecture Search,
Reinforcement Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1"&gt;Ibrahim Alshubaily&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorical EHR Imputation with Generative Adversarial Nets. (arXiv:2108.01701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01701</id>
        <link href="http://arxiv.org/abs/2108.01701"/>
        <updated>2021-08-05T01:56:20.586Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records often suffer from missing data, which poses a major
problem in clinical practice and clinical studies. A novel approach for dealing
with missing data are Generative Adversarial Nets (GANs), which have been
generating huge research interest in image generation and transformation.
Recently, researchers have attempted to apply GANs to missing data generation
and imputation for EHR data: a major challenge here is the categorical nature
of the data. State-of-the-art solutions to the GAN-based generation of
categorical data involve either reinforcement learning, or learning a
bidirectional mapping between the categorical and the real latent feature
space, so that the GANs only need to generate real-valued features. However,
these methods are designed to generate complete feature vectors instead of
imputing only the subsets of missing features. In this paper we propose a
simple and yet effective approach that is based on previous work on GANs for
data imputation. We first motivate our solution by discussing the reason why
adversarial training often fails in case of categorical features. Then we
derive a novel way to re-code the categorical features to stabilize the
adversarial training. Based on experiments on two real-world EHR data with
multiple settings, we show that our imputation approach largely improves the
prediction accuracy, compared to more traditional data imputation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiilang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fasching_P/0/1/0/all/0/1"&gt;Peter A. Fasching&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.01711</id>
        <link href="http://arxiv.org/abs/2108.01711"/>
        <updated>2021-08-05T01:56:20.561Z</updated>
        <summary type="html"><![CDATA[Several automatic approaches for objective music performance assessment (MPA)
have been proposed in the past, however, existing systems are not yet capable
of reliably predicting ratings with the same accuracy as professional judges.
This study investigates contrastive learning as a potential method to improve
existing MPA systems. Contrastive learning is a widely used technique in
representation learning to learn a structured latent space capable of
separately clustering multiple classes. It has been shown to produce state of
the art results for image-based classification problems. We introduce a
weighted contrastive loss suitable for regression tasks applied to a
convolutional neural network and show that contrastive loss results in
performance gains in regression tasks for MPA. Our results show that
contrastive-based methods are able to match and exceed SoTA performance for MPA
regression tasks by creating better class clusters within the latent space of
the neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1"&gt;Pavan Seshadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1"&gt;Alexander Lerch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Zip Code-Level Vaccine Hesitancy in US Metropolitan Areas Using Machine Learning Models on Public Tweets. (arXiv:2108.01699v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01699</id>
        <link href="http://arxiv.org/abs/2108.01699"/>
        <updated>2021-08-05T01:56:20.555Z</updated>
        <summary type="html"><![CDATA[Although the recent rise and uptake of COVID-19 vaccines in the United States
has been encouraging, there continues to be significant vaccine hesitancy in
various geographic and demographic clusters of the adult population. Surveys,
such as the one conducted by Gallup over the past year, can be useful in
determining vaccine hesitancy, but can be expensive to conduct and do not
provide real-time data. At the same time, the advent of social media suggests
that it may be possible to get vaccine hesitancy signals at an aggregate level
(such as at the level of zip codes) by using machine learning models and
socioeconomic (and other) features from publicly available sources. It is an
open question at present whether such an endeavor is feasible, and how it
compares to baselines that only use constant priors. To our knowledge, a proper
methodology and evaluation results using real data has also not been presented.
In this article, we present such a methodology and experimental study, using
publicly available Twitter data collected over the last year. Our goal is not
to devise novel machine learning algorithms, but to evaluate existing and
established models in a comparative framework. We show that the best models
significantly outperform constant priors, and can be set up using open-source
tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melotte_S/0/1/0/all/0/1"&gt;Sara Melotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1"&gt;Mayank Kejriwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factor Representation and Decision Making in Stock Markets Using Deep Reinforcement Learning. (arXiv:2108.01758v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2108.01758</id>
        <link href="http://arxiv.org/abs/2108.01758"/>
        <updated>2021-08-05T01:56:20.546Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement learning is a branch of unsupervised learning in which an
agent learns to act based on environment state in order to maximize its total
reward. Deep reinforcement learning provides good opportunity to model the
complexity of portfolio choice in high-dimensional and data-driven environment
by leveraging the powerful representation of deep neural networks. In this
paper, we build a portfolio management system using direct deep reinforcement
learning to make optimal portfolio choice periodically among S\&P500 underlying
stocks by learning a good factor representation (as input). The result shows
that an effective learning of market conditions and optimal portfolio
allocations can significantly outperform the average market.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhaolu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ma_S/0/1/0/all/0/1"&gt;Simiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yining Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07797</id>
        <link href="http://arxiv.org/abs/2107.07797"/>
        <updated>2021-08-05T01:56:20.488Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks have significantly improved 3D human pose
estimation by representing the human skeleton as an undirected graph. However,
this representation fails to reflect the articulated characteristic of human
skeletons as the hierarchical orders among the joints are not explicitly
presented. In this paper, we propose to represent the human skeleton as a
directed graph with the joints as nodes and bones as edges that are directed
from parent joints to child joints. By so doing, the directions of edges can
explicitly reflect the hierarchical relationships among the nodes. Based on
this representation, we further propose a spatial-temporal conditional directed
graph convolution to leverage varying non-local dependence for different poses
by conditioning the graph topology on input poses. Altogether, we form a
U-shaped network, named U-shaped Conditional Directed Graph Convolutional
Network, for 3D human pose estimation from monocular videos. To evaluate the
effectiveness of our method, we conducted extensive experiments on two
challenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both
quantitative and qualitative results show that our method achieves top
performance. Also, ablation studies show that directed graphs can better
exploit the hierarchy of articulated human skeletons than undirected graphs,
and the conditional connections can yield adaptive graph topologies for
different poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tien-Tsin Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-08-05T01:56:20.481Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12744</id>
        <link href="http://arxiv.org/abs/2107.12744"/>
        <updated>2021-08-05T01:56:20.453Z</updated>
        <summary type="html"><![CDATA[With the rapid increase in digital technologies, most fields of study include
recognition of human activity and intention recognition, which are essential in
smart environments. In this study, we equipped the activity recognition system
with the ability to recognize intentions by affecting the pace of movement of
individuals in the representation of images. Using this technology in various
environments such as elevators and automatic doors will lead to identifying
those who intend to pass the automatic door from those who are passing by. This
system, if applied in elevators and automatic doors, will save energy and
increase efficiency. For this study, data preparation is applied to combine the
spatial and temporal features with the help of digital image processing
principles. Nevertheless, unlike previous studies, only one AlexNet neural
network is used instead of two-stream convolutional neural networks. Our
embedded system was implemented with an accuracy of 98.78% on our intention
recognition dataset. We also examined our data representation approach on other
datasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of
78.48%, 97.95%, and 100%, respectively. The image recognition and neural
network models were simulated and implemented using Xilinx simulators for the
Xilinx ZCU102 board. The operating frequency of this embedded system is 333
MHz, and it works in real-time with 120 frames per second (fps).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1"&gt;Sahar Darafsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1"&gt;Saeed Shiry Ghidary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1"&gt;Morteza Saheb Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-08-05T01:56:20.446Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Inspired Face Recognition: A Computational Framework. (arXiv:2105.07237v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07237</id>
        <link href="http://arxiv.org/abs/2105.07237"/>
        <updated>2021-08-05T01:56:20.438Z</updated>
        <summary type="html"><![CDATA[This paper presents a new proposal of an efficient computational model of
face recognition which uses cues from the distributed face recognition
mechanism of the brain, and by gathering engineering equivalent of these cues
from existing literature. Three distinct and widely used features: Histogram of
Oriented Gradients (HOG), Local Binary Patterns (LBP), and Principal components
(PCs) extracted from target images are used in a manner which is simple, and
yet effective. The HOG and LBP features further undergo principal component
analysis for dimensionality reduction. Our model uses multi-layer perceptrons
(MLP) to classify these three features and fuse them at the decision level
using sum rule. A computational theory is first developed by using concepts
from the information processing mechanism of the brain. Extensive experiments
are carried out using ten publicly available datasets to validate our proposed
model's performance in recognizing faces with extreme variation of
illumination, pose angle, expression, and background. Results obtained are
extremely promising when compared with other face recognition algorithms
including CNN and deep learning-based methods. This highlights that simple
computational processes, if clubbed properly, can produce competing performance
with best algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1"&gt;Pinaki Roy Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadhwa_A/0/1/0/all/0/1"&gt;Angad Wadhwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1"&gt;Antariksha Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1"&gt;Nikhil Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:20.427Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07112</id>
        <link href="http://arxiv.org/abs/2105.07112"/>
        <updated>2021-08-05T01:56:20.420Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an efficient and robust deep learning solution for
novel view synthesis of complex scenes. In our approach, a 3D scene is
represented as a light field, i.e., a set of rays, each of which has a
corresponding color when reaching the image plane. For efficient novel view
rendering, we adopt a 4D parameterization of the light field, where each ray is
characterized by a 4D parameter. We then formulate the light field as a 4D
function that maps 4D coordinates to corresponding color values. We train a
deep fully connected network to optimize this implicit function and memorize
the 3D scene. Then, the scene-specific model is used to synthesize novel views.
Different from previous light field approaches which require dense view
sampling to reliably render novel views, our method can render novel views by
sampling rays and querying the color for each ray from the network directly,
thus enabling high-quality light field rendering with a sparser set of training
images. Our method achieves state-of-the-art novel view synthesis results while
maintaining an interactive frame rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Celong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point Completion. (arXiv:2104.09587v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09587</id>
        <link href="http://arxiv.org/abs/2104.09587"/>
        <updated>2021-08-05T01:56:20.414Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of object completion from point clouds and propose a
novel point cloud completion network employing an Asymmetrical Siamese Feature
Matching strategy, termed as ASFM-Net. Specifically, the Siamese auto-encoder
neural network is adopted to map the partial and complete input point cloud
into a shared latent space, which can capture detailed shape prior. Then we
design an iterative refinement unit to generate complete shapes with
fine-grained details by integrating prior information. Experiments are
conducted on the PCN dataset and the Completion3D benchmark, demonstrating the
state-of-the-art performance of the proposed ASFM-Net. Our method achieves the
1st place in the leaderboard of Completion3D and outperforms existing methods
with a large margin, about 12%. The codes and trained models are released
publicly at https://github.com/Yan-Xia/ASFM-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yaqi Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1"&gt;Kailang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1"&gt;Uwe Stilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-08-05T01:56:20.407Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00411</id>
        <link href="http://arxiv.org/abs/2104.00411"/>
        <updated>2021-08-05T01:56:20.399Z</updated>
        <summary type="html"><![CDATA[Neural networks have demonstrated remarkable performance in classification
and regression tasks on chest X-rays. In order to establish trust in the
clinical routine, the networks' prediction mechanism needs to be interpretable.
One principal approach to interpretation is feature attribution. Feature
attribution methods identify the importance of input features for the output
prediction. Building on Information Bottleneck Attribution (IBA) method, for
each prediction we identify the chest X-ray regions that have high mutual
information with the network's output. Original IBA identifies input regions
that have sufficient predictive information. We propose Inverse IBA to identify
all informative regions. Thus all predictive cues for pathologies are
highlighted on the X-rays, a desirable property for chest X-ray diagnosis.
Moreover, we propose Regression IBA for explaining regression models. Using
Regression IBA we observe that a model trained on cumulative severity score
labels implicitly learns the severity of different X-ray regions. Finally, we
propose Multi-layer IBA to generate higher resolution and more detailed
attribution/saliency maps. We evaluate our methods using both human-centric
(ground-truth-based) interpretability metrics, and human-independent feature
importance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1"&gt;Wejdene Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuezhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-08-05T01:56:20.379Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition. (arXiv:2012.05018v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05018</id>
        <link href="http://arxiv.org/abs/2012.05018"/>
        <updated>2021-08-05T01:56:20.358Z</updated>
        <summary type="html"><![CDATA[In the field of large-scale SLAM for autonomous driving and mobile robotics,
3D point cloud based place recognition has aroused significant research
interest due to its robustness to changing environments with drastic daytime
and weather variance. However, it is time-consuming and effort-costly to obtain
high-quality point cloud data for place recognition model training and ground
truth for registration in the real world. To this end, a novel
registration-aided 3D domain adaptation network for point cloud based place
recognition is proposed. A structure-aware registration network is introduced
to help to learn features with geometric information and a 6-DoFs pose between
two point clouds with partial overlap can be estimated. The model is trained
through a synthetic virtual LiDAR dataset through GTA-V with diverse weather
and daytime conditions and domain adaptation is implemented to the real-world
domain by aligning the global features. Our results outperform state-of-the-art
3D place recognition baselines or achieve comparable on the real-world Oxford
RobotCar dataset with the visualization of registration on the virtual dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09962</id>
        <link href="http://arxiv.org/abs/2012.09962"/>
        <updated>2021-08-05T01:56:20.332Z</updated>
        <summary type="html"><![CDATA[Contrastive learning is effective at learning useful representations without
supervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it
may learn shortcut features irrelevant to the downstream task and discard
relevant information. Past work has addressed this limitation via handcrafted
data augmentations that eliminate the shortcut. However, handcrafted
augmentations are infeasible for data modalities that are not interpretable by
humans (e.g., radio signals). Further, even when the modality is interpretable
(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.
For example, in multi-attribute classification, information related to one
attribute may act as a shortcut around other attributes. This paper presents
reconstructive contrastive learning (RCL), a framework for learning
unsupervised representations that are robust to shortcuts. The key idea is to
force the learned representation to reconstruct the input, which naturally
counters potential shortcuts. Extensive experiments verify that RCL is highly
robust to shortcuts and outperforms state-of-the-art contrastive learning
methods on both RGB and RF datasets for a variety of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lijie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1"&gt;Dina Katabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06154</id>
        <link href="http://arxiv.org/abs/2107.06154"/>
        <updated>2021-08-05T01:56:20.322Z</updated>
        <summary type="html"><![CDATA[Due to the domain discrepancy in visual domain adaptation, the performance of
source model degrades when bumping into the high data density near decision
boundary in target domain. A common solution is to minimize the Shannon Entropy
to push the decision boundary away from the high density area. However, entropy
minimization also leads to severe reduction of prediction diversity, and
unfortunately brings harm to the domain adaptation. In this paper, we
investigate the prediction discriminability and diversity by studying the
structure of the classification output matrix of a randomly selected data
batch. We find by theoretical analysis that the prediction discriminability and
diversity could be separately measured by the Frobenius-norm and rank of the
batch output matrix. The nuclear-norm is an upperbound of the former, and a
convex approximation of the latter. Accordingly, we propose Batch Nuclear-norm
Maximization and Minimization, which performs nuclear-norm maximization on the
target output matrix to enhance the target prediction ability, and nuclear-norm
minimization on the source batch output matrix to increase applicability of the
source domain knowledge. We further approximate the nuclear-norm by
L_{1,2}-norm, and design multi-batch optimization for stable solution on large
number of categories. The fast approximation method achieves O(n^2)
computational complexity and better convergence property. Experiments show that
our method could boost the adaptation accuracy and robustness under three
typical domain adaptation scenarios. The code is available at
https://github.com/cuishuhao/BNM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepChange: A Long-Term Person Re-Identification Benchmark. (arXiv:2105.14685v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14685</id>
        <link href="http://arxiv.org/abs/2105.14685"/>
        <updated>2021-08-05T01:56:20.315Z</updated>
        <summary type="html"><![CDATA[Existing person re-identification (Re-ID) works mostly consider a short-term
search problem assuming unchanged clothes and personal appearance. However, in
real-world we often dress differently across locations, time, dates, seasons,
weather, and events. As a result, the existing methods are unsuitable for
long-term person Re-ID with clothes change involved. Whilst there are several
recent long-term Re-ID attempts, a large realistic dataset with clothes change
is lacking and indispensable for enabling extensive study as already
experienced in short-term Re-ID setting. In this work, we contribute a large,
realistic long-term person identification benchmark. It consists of 178K
bounding boxes from 1.1K person identities, collected and constructed over 12
months. Unique characteristics of this dataset include: (1) Natural/native
personal appearance (e.g., clothes and hair style) variations: The
clothes-change and dressing styles all are highly diverse, with the reappearing
gap in time ranging from minutes, hours, and days to weeks, months, seasons,
and years. (2) Diverse walks of life: Persons across a wide range of ages and
professions appear in different weather conditions (e.g., sunny, cloudy, windy,
rainy, snowy, extremely cold) and events (e.g., working, leisure, daily
activities). (3) Rich camera setups: The raw videos were recorded by 17 outdoor
security cameras with various resolutions operating in a real-world
surveillance system for a wide and dense block. (4) Largest scale: It covers
the largest number of (17) cameras, (1, 121) identities, and (178, 407)
bounding boxes, as compared to alternative datasets. Our dataset and benchmark
codes are available on https://github.com/PengBoXiangShang/deepchange.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-08-05T01:56:20.307Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13093</id>
        <link href="http://arxiv.org/abs/2012.13093"/>
        <updated>2021-08-05T01:56:20.299Z</updated>
        <summary type="html"><![CDATA[Recent progress on salient object detection (SOD) mainly benefits from
multi-scale learning, where the high-level and low-level features collaborate
in locating salient objects and discovering fine details, respectively.
However, most efforts are devoted to low-level feature learning by fusing
multi-scale features or enhancing boundary representations. High-level
features, which although have long proven effective for many other tasks, yet
have been barely studied for SOD. In this paper, we tap into this gap and show
that enhancing high-level features is essential for SOD as well. To this end,
we introduce an Extremely-Downsampled Network (EDN), which employs an extreme
downsampling technique to effectively learn a global view of the whole image,
leading to accurate salient object localization. To accomplish better
multi-level feature fusion, we construct the Scale-Correlated Pyramid
Convolution (SCPC) to build an elegant decoder for recovering object details
from the above extreme downsampling. Extensive experiments demonstrate that EDN
achieves state-of-the-art performance with real-time speed. Our efficient
EDN-Lite also achieves competitive performance with a speed of 316fps. Hence,
this work is expected to spark some new thinking in SOD. Full training and
testing code will be available at https://github.com/yuhuan-wu/EDN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bo Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02481</id>
        <link href="http://arxiv.org/abs/2104.02481"/>
        <updated>2021-08-05T01:56:20.280Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks are showing promise in the automatic diagnosis
of thoracic pathologies on chest x-rays. Their black-box nature has sparked
many recent works to explain the prediction via input feature attribution
methods (aka saliency methods). However, input feature attribution methods
merely identify the importance of input regions for the prediction and lack
semantic interpretation of model behavior. In this work, we first identify the
semantics associated with internal units (feature maps) of the network. We
proceed to investigate the following questions; Does a regression model that is
only trained with COVID-19 severity scores implicitly learn visual patterns
associated with thoracic pathologies? Does a network that is trained on weakly
labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,
we investigate the effect of pretraining and data imbalance on the
interpretability of learned features. In addition to the analysis, we propose
semantic attribution to semantically explain each prediction. We present our
findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)
and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).
The Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1"&gt;Sabrina Musatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1"&gt;Jonas Buchberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1"&gt;Icxel Valeriano Quiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1"&gt;Nikolaus Pinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1"&gt;Soroosh Baselizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery. (arXiv:2103.12434v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12434</id>
        <link href="http://arxiv.org/abs/2103.12434"/>
        <updated>2021-08-05T01:56:20.273Z</updated>
        <summary type="html"><![CDATA[Depleting lake ice can serve as an indicator for climate change, just like
sea level rise or glacial retreat. Several Lake Ice Phenological (LIP) events
serve as sentinels to understand the regional and global climate change. Hence,
it is useful to monitor long-term lake freezing and thawing patterns. In this
paper we report a case study for the Oberengadin region of Switzerland, where
there are several small- and medium-sized mountain lakes. We observe the LIP
events, such as freeze-up, break-up and ice cover duration, across two decades
(2000-2020) from optical satellite images. We analyse time-series of MODIS
imagery by estimating spatially resolved maps of lake ice for these Alpine
lakes with supervised machine learning (and additionally cross-check with VIIRS
data when available). To train the classifier we rely on reference data
annotated manually based on webcam images. From the ice maps we derive
long-term LIP trends. Since the webcam data is only available for two winters,
we also validate our results against the operational MODIS and VIIRS snow
products. We find a change in complete freeze duration of -0.76 and -0.89 days
per annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe
plausible correlations of the LIP trends with climate data measured at nearby
meteorological stations. We notice that mean winter air temperature has
negative correlation with the freeze duration and break-up events, and positive
correlation with the freeze-up events. Additionally, we observe strong negative
correlation of sunshine during the winter months with the freeze duration and
break-up events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1"&gt;Manu Tom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1"&gt;Emmanuel Baltsavias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06822</id>
        <link href="http://arxiv.org/abs/2008.06822"/>
        <updated>2021-08-05T01:56:20.267Z</updated>
        <summary type="html"><![CDATA[This paper focuses on high-transferable adversarial attacks on detectors,
which are hard to attack in a black-box manner, because of their
multiple-output characteristics and the diversity across architectures. To
pursue a high attack transferability, one plausible way is to find a common
property across detectors, which facilitates the discovery of common
weaknesses. We are the first to suggest that the relevance map from
interpreters for detectors is such a property. Based on it, we design a
Relevance Attack on Detectors (RAD), which achieves a state-of-the-art
transferability, exceeding existing results by above 20%. On MS COCO, the
detection mAPs for all 8 black-box architectures are more than halved and the
segmentation mAPs are also significantly influenced. Given the great
transferability of RAD, we generate the first adversarial dataset for object
detection and instance segmentation, i.e., Adversarial Objects in COntext
(AOCO), which helps to quickly evaluate and improve the robustness of
detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02183</id>
        <link href="http://arxiv.org/abs/2108.02183"/>
        <updated>2021-08-05T01:56:20.260Z</updated>
        <summary type="html"><![CDATA[The crux of self-supervised video representation learning is to build general
features from unlabeled videos. However, most recent works have mainly focused
on high-level semantics and neglected lower-level representations and their
temporal relationship which are crucial for general video understanding. To
address these challenges, this paper proposes a multi-level feature
optimization framework to improve the generalization and temporal modeling
ability of learned video representations. Concretely, high-level features
obtained from naive and prototypical contrastive learning are utilized to build
distribution graphs, guiding the process of low-level and mid-level feature
learning. We also devise a simple temporal modeling module from multi-level
features to enhance motion pattern learning. Experiments demonstrate that
multi-level feature optimization with the graph constraint and temporal
modeling can greatly improve the representation ability in video understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shuangrui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04331</id>
        <link href="http://arxiv.org/abs/2010.04331"/>
        <updated>2021-08-05T01:56:20.253Z</updated>
        <summary type="html"><![CDATA[Real world traffic sign recognition is an important step towards building
autonomous vehicles, most of which highly dependent on Deep Neural Networks
(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to
adversarial examples. Many attack methods have been proposed to understand and
generate adversarial examples, such as gradient based attack, score based
attack, decision based attack, and transfer based attacks. However, most of
these algorithms are ineffective in real-world road sign attack, because (1)
iteratively learning perturbations for each frame is not realistic for a fast
moving car and (2) most optimization algorithms traverse all pixels equally
without considering their diverse contribution. To alleviate these problems,
this paper proposes the targeted attention attack (TAA) method for real world
road sign attack. Specifically, we have made the following contributions: (1)
we leverage the soft attention map to highlight those important pixels and skip
those zero-contributed areas - this also helps to generate natural
perturbations, (2) we design an efficient universal attack that optimizes a
single perturbation/noise based on a set of training images under the guidance
of the pre-trained attention map, (3) we design a simple objective function
that can be easily optimized, (4) we evaluate the effectiveness of TAA on real
world data sets. Experimental results validate that the TAA method improves the
attack successful rate (nearly 10%) and reduces the perturbation loss (about a
quarter) compared with the popular RP2 method. Additionally, our TAA also
provides good properties, e.g., transferability and generalization capability.
We provide code and data to ensure the reproducibility:
https://github.com/AdvAttack/RoadSignAttack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11905</id>
        <link href="http://arxiv.org/abs/2012.11905"/>
        <updated>2021-08-05T01:56:20.246Z</updated>
        <summary type="html"><![CDATA[With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. Especially in medical contexts, where relevant information often
consists of textural and structural information, high-quality counterfactual
images have the potential to give meaningful insights into decision processes.
In this work, we present GANterfactual, an approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in an exemplary medical use case. Our results show that, in the chosen
medical use-case, counterfactual explanations lead to significantly better
results regarding mental models, explanation satisfaction, trust, emotions, and
self-efficacy than two state-of-the-art systems that work with saliency maps,
namely LIME and LRP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1"&gt;Katharina Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1"&gt;Alexander Heimerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction. (arXiv:2108.02110v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02110</id>
        <link href="http://arxiv.org/abs/2108.02110"/>
        <updated>2021-08-05T01:56:20.227Z</updated>
        <summary type="html"><![CDATA[A number of deep learning based algorithms have been proposed to recover
high-quality videos from low-quality compressed ones. Among them, some restore
the missing details of each frame via exploring the spatiotemporal information
of neighboring frames. However, these methods usually suffer from a narrow
temporal scope, thus may miss some useful details from some frames outside the
neighboring ones. In this paper, to boost artifact removal, on the one hand, we
propose a Recursive Fusion (RF) module to model the temporal dependency within
a long temporal range. Specifically, RF utilizes both the current reference
frames and the preceding hidden state to conduct better spatiotemporal
compensation. On the other hand, we design an efficient and effective
Deformable Spatiotemporal Attention (DSTA) module such that the model can pay
more effort on restoring the artifact-rich areas like the boundary area of a
moving object. Extensive experiments show that our method outperforms the
existing ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual
effect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Minyi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuigeng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Image Denoising via Class Specific Convolution. (arXiv:2103.01624v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01624</id>
        <link href="http://arxiv.org/abs/2103.01624"/>
        <updated>2021-08-05T01:56:20.220Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been widely used in image denoising during the past
few years. Even though they achieve great success on this problem, they are
computationally inefficient which makes them inappropriate to be implemented in
mobile devices. In this paper, we propose an efficient deep neural network for
image denoising based on pixel-wise classification. Despite using a
computationally efficient network cannot effectively remove the noises from any
content, it is still capable to denoise from a specific type of pattern or
texture. The proposed method follows such a divide and conquer scheme. We first
use an efficient U-net to pixel-wisely classify pixels in the noisy image based
on the local gradient statistics. Then we replace part of the convolution
layers in existing denoising networks by the proposed Class Specific
Convolution layers (CSConv) which use different weights for different classes
of pixels. Quantitative and qualitative evaluations on public datasets
demonstrate that the proposed method can reduce the computational costs without
sacrificing the performance compared to state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuanye Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Feng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores. (arXiv:2010.13418v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13418</id>
        <link href="http://arxiv.org/abs/2010.13418"/>
        <updated>2021-08-05T01:56:20.213Z</updated>
        <summary type="html"><![CDATA[One of the challenges of the Optical Music Recognition task is to transcript
the symbols of the camera-captured images into digital music notations.
Previous end-to-end model which was developed as a Convolutional Recurrent
Neural Network does not explore sufficient contextual information from full
scales and there is still a large room for improvement. We propose an
innovative framework that combines a block of Residual Recurrent Convolutional
Neural Network with a recurrent Encoder-Decoder network to map a sequence of
monophonic music symbols corresponding to the notations present in the image.
The Residual Recurrent Convolutional block can improve the ability of the model
to enrich the context information. The experiment results are benchmarked
against a publicly available dataset called CAMERA-PRIMUS, which demonstrates
that our approach surpass the state-of-the-art end-to-end method using
Convolutional Recurrent Neural Network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Aozhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lipei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1"&gt;Yaqi Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Baoqiang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zifeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaohua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated Adversarial Refinement for Stochastic Semantic Segmentation. (arXiv:2006.13144v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13144</id>
        <link href="http://arxiv.org/abs/2006.13144"/>
        <updated>2021-08-05T01:56:20.203Z</updated>
        <summary type="html"><![CDATA[In semantic segmentation tasks, input images can often have more than one
plausible interpretation, thus allowing for multiple valid labels. To capture
such ambiguities, recent work has explored the use of probabilistic networks
that can learn a distribution over predictions. However, these do not
necessarily represent the empirical distribution accurately. In this work, we
present a strategy for learning a calibrated predictive distribution over
semantic maps, where the probability associated with each prediction reflects
its ground truth correctness likelihood. To this end, we propose a novel
two-stage, cascaded approach for calibrated adversarial refinement: (i) a
standard segmentation network is trained with categorical cross entropy to
predict a pixelwise probability distribution over semantic classes and (ii) an
adversarially trained stochastic network is used to model the inter-pixel
correlations to refine the output of the first network into coherent samples.
Importantly, to calibrate the refinement network and prevent mode collapse, the
expectation of the samples in the second stage is matched to the probabilities
predicted in the first. We demonstrate the versatility and robustness of the
approach by achieving state-of-the-art results on the multigrader LIDC dataset
and on a modified Cityscapes dataset with injected ambiguities. In addition, we
show that the core design can be adapted to other tasks requiring learning a
calibrated predictive distribution by experimenting on a toy regression
dataset. We provide an open source implementation of our method at
https://github.com/EliasKassapis/CARSSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassapis_E/0/1/0/all/0/1"&gt;Elias Kassapis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dikov_G/0/1/0/all/0/1"&gt;Georgi Dikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugteren_C/0/1/0/all/0/1"&gt;Cedric Nugteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Adaptive Filtering Layer for Computer Vision. (arXiv:2010.01177v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01177</id>
        <link href="http://arxiv.org/abs/2010.01177"/>
        <updated>2021-08-05T01:56:20.195Z</updated>
        <summary type="html"><![CDATA[We devise a universal adaptive neural layer to "learn" optimal frequency
filter for each image together with the weights of the base neural network that
performs some computer vision task. The proposed approach takes the source
image in the spatial domain, automatically selects the best frequencies from
the frequency domain, and transmits the inverse-transform image to the main
neural network. Remarkably, such a simple add-on layer dramatically improves
the performance of the main network regardless of its design. We observe that
the light networks gain a noticeable boost in the performance metrics; whereas,
the training of the heavy ones converges faster when our adaptive layer is
allowed to "learn" alongside the main architecture. We validate the idea in
four classical computer vision tasks: classification, segmentation, denoising,
and erasing, considering popular natural and medical data benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shipitsin_V/0/1/0/all/0/1"&gt;Viktor Shipitsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bespalov_I/0/1/0/all/0/1"&gt;Iaroslav Bespalov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1"&gt;Dmitry V. Dylov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02155</id>
        <link href="http://arxiv.org/abs/2108.02155"/>
        <updated>2021-08-05T01:56:20.173Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in medical image segmentation applications is
essential, as it is often connected to vital decision-making. Compelling
attempts have been made in quantifying the uncertainty in image segmentation
architectures, e.g. to learn a density segmentation model conditioned on the
input image. Typical work in this field restricts these learnt densities to be
strictly Gaussian. In this paper, we propose to use a more flexible approach by
introducing Normalizing Flows (NFs), which enables the learnt densities to be
more complex and facilitate more accurate modeling for uncertainty. We prove
this hypothesis by adopting the Probabilistic U-Net and augmenting the
posterior density with an NF, allowing it to be more expressive. Our
qualitative as well as quantitative (GED and IoU) evaluations on the
multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation
datasets, respectively, show a clear improvement. This is mostly apparent in
the quantification of aleatoric uncertainty and the increased predictive
performance of up to 14 percent. This result strongly indicates that a more
flexible density model should be seriously considered in architectures that
attempt to capture segmentation ambiguity through density modeling. The benefit
of this improved modeling will increase human confidence in annotation and
segmentation, and enable eager adoption of the technology in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1"&gt;M.M.A. Valiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1"&gt;C.G.A. Viviers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;R.J.G. van Sloun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1"&gt;P.H.N. de With&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1"&gt;F. van der Sommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07088</id>
        <link href="http://arxiv.org/abs/2002.07088"/>
        <updated>2021-08-05T01:56:20.164Z</updated>
        <summary type="html"><![CDATA[The physical, black-box hard-label setting is arguably the most realistic
threat model for cyber-physical vision systems. In this setting, the attacker
only has query access to the model and only receives the top-1 class label
without confidence information. Creating small physical stickers that are
robust to environmental variation is difficult in the discrete and
discontinuous hard-label space because the attack must both design a small
shape to perturb within and find robust noise to fill it with. Unfortunately,
we find that existing $\ell_2$ or $\ell_\infty$ minimizing hard-label attacks
do not easily extend to finding such robust physical perturbation attacks.
Thus, we propose GRAPHITE, the first algorithm for hard-label physical attacks
on computer vision models. We show that "survivability", an estimate of
physical variation robustness, can be used in new ways to generate small masks
and is a sufficiently smooth function to optimize with gradient-free
optimization. We use GRAPHITE to attack a traffic sign classifier and a
publicly-available Automatic License Plate Recognition (ALPR) tool using only
query access. We evaluate both tools in real-world field tests to measure its
physical-world robustness. We successfully cause a Stop sign to be
misclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and
cause errors in 75% of physical images for the ALPR tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1"&gt;Earlence Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalanced Image Classification with Complement Cross Entropy. (arXiv:2009.02189v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02189</id>
        <link href="http://arxiv.org/abs/2009.02189"/>
        <updated>2021-08-05T01:56:20.156Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning models have achieved great success in computer vision
applications, relying on large-scale class-balanced datasets. However,
imbalanced class distributions still limit the wide applicability of these
models due to degradation in performance. To solve this problem, in this paper,
we concentrate on the study of cross entropy which mostly ignores output scores
on incorrect classes. This work discovers that neutralizing predicted
probabilities on incorrect classes improves the prediction accuracy for
imbalanced image classification. This paper proposes a simple but effective
loss named complement cross entropy based on this finding. The proposed loss
makes the ground truth class overwhelm the other classes in terms of softmax
probability, by neutralizing probabilities of incorrect classes, without
additional training procedures. Along with it, this loss facilitates the models
to learn key information especially from samples on minority classes. It
ensures more accurate and robust classification results on imbalanced
distributions. Extensive experiments on imbalanced datasets demonstrate the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yechan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Younkwan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1"&gt;Moongu Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12077</id>
        <link href="http://arxiv.org/abs/2011.12077"/>
        <updated>2021-08-05T01:56:20.148Z</updated>
        <summary type="html"><![CDATA[Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Muhammad Zaigham Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Arif Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1"&gt;Marcella Astrid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seung-Ik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01699</id>
        <link href="http://arxiv.org/abs/2012.01699"/>
        <updated>2021-08-05T01:56:20.140Z</updated>
        <summary type="html"><![CDATA[To remove the effects of adversarial perturbations, preprocessing defenses
such as pixel discretization are appealing due to their simplicity but have so
far been shown to be ineffective except on simple datasets such as MNIST,
leading to the belief that pixel discretization approaches are doomed to
failure as a defense technique. This paper revisits the pixel discretization
approaches. We hypothesize that the reason why existing approaches have failed
is that they have used a fixed codebook for the entire dataset. In particular,
we find that can lead to situations where images become more susceptible to
adversarial perturbations and also suffer significant loss of accuracy after
discretization. We propose a novel image preprocessing technique called
Essential Features that uses an adaptive codebook that is based on per-image
content and threat model. Essential Features adaptively selects a separable set
of color clusters for each image to reduce the color space while preserving the
pertinent features of the original image, maximizing both separability and
representation of colors. Additionally, to limit the adversary's ability to
influence the chosen color clusters, Essential Features takes advantage of
spatial correlation with an adaptive blur that moves pixels closer to their
original value without destroying original edge information. We design several
adaptive attacks and find that our approach is more robust than previous
baselines on $L_\infty$ and $L_2$ bounded attacks for several challenging
datasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wu-chi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to generate shape from global-local spectra. (arXiv:2108.02161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02161</id>
        <link href="http://arxiv.org/abs/2108.02161"/>
        <updated>2021-08-05T01:56:20.119Z</updated>
        <summary type="html"><![CDATA[In this work, we present a new learning-based pipeline for the generation of
3D shapes. We build our method on top of recent advances on the so called
shape-from-spectrum paradigm, which aims at recovering the full 3D geometric
structure of an object only from the eigenvalues of its Laplacian operator. In
designing our learning strategy, we consider the spectrum as a natural and
ready to use representation to encode variability of the shapes. Therefore, we
propose a simple decoder-only architecture that directly maps spectra to 3D
embeddings; in particular, we combine information from global and local
spectra, the latter being obtained from localized variants of the manifold
Laplacian. This combination captures the relations between the full shape and
its local parts, leading to more accurate generation of geometric details and
an improved semantic control in shape synthesis and novel editing applications.
Our results confirm the improvement of the proposed approach in comparison to
existing and alternative methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1"&gt;Marco Pegoraro&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1"&gt;Umberto Castellani&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1"&gt;Simone Melzi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt; (2) ((1) University of Verona, (2) Sapienza University of Rome)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Discriminative Learning for Unsupervised Representation Learning on 3D Point Clouds. (arXiv:2108.02104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02104</id>
        <link href="http://arxiv.org/abs/2108.02104"/>
        <updated>2021-08-05T01:56:20.111Z</updated>
        <summary type="html"><![CDATA[Recently deep learning has achieved significant progress on point cloud
analysis tasks. Learning good representations is of vital importance to these
tasks. Most current methods rely on massive labelled data for training. We here
propose a point discriminative learning method for unsupervised representation
learning on 3D point clouds, which can learn local and global geometry
features. We achieve this by imposing a novel point discrimination loss on the
middle level and global level point features produced in the backbone network.
This point discrimination loss enforces the features to be consistent with
points belonging to the shape surface and inconsistent with randomly sampled
noisy points. Our method is simple in design, which works by adding an extra
adaptation module and a point consistency module for unsupervised training of
the encoder in the backbone network. Once trained, these two modules can be
discarded during supervised training of the classifier or decoder for
down-stream tasks. We conduct extensive experiments on 3D object
classification, 3D part segmentation and shape reconstruction in various
unsupervised and transfer settings. Both quantitative and qualitative results
show that our method learns powerful representations and achieves new
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fayao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1"&gt;Chuan-Sheng Foo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Situational Fusion of Visual Representation for Visual Navigation. (arXiv:1908.09073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.09073</id>
        <link href="http://arxiv.org/abs/1908.09073"/>
        <updated>2021-08-05T01:56:20.104Z</updated>
        <summary type="html"><![CDATA[A complex visual navigation task puts an agent in different situations which
call for a diverse range of visual perception abilities. For example, to "go to
the nearest chair", the agent might need to identify a chair in a living room
using semantics, follow along a hallway using vanishing point cues, and avoid
obstacles using depth. Therefore, utilizing the appropriate visual perception
abilities based on a situational understanding of the visual environment can
empower these navigation models in unseen visual environments. We propose to
train an agent to fuse a large set of visual representations that correspond to
diverse visual perception abilities. To fully utilize each representation, we
develop an action-level representation fusion scheme, which predicts an action
candidate from each representation and adaptively consolidate these action
candidates into the final action. Furthermore, we employ a data-driven
inter-task affinity regularization to reduce redundancies and improve
generalization. Our approach leads to a significantly improved performance in
novel environments over ImageNet-pretrained baseline and other fusion methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-05T01:56:20.097Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02148</id>
        <link href="http://arxiv.org/abs/2108.02148"/>
        <updated>2021-08-05T01:56:20.088Z</updated>
        <summary type="html"><![CDATA[Due to the mass advancement in ubiquitous technologies nowadays, new
pervasive methods have come into the practice to provide new innovative
features and stimulate the research on new human-computer interactions. This
paper presents a hand gesture recognition method that utilizes the smartphone's
built-in speakers and microphones. The proposed system emits an ultrasonic
sonar-based signal (inaudible sound) from the smartphone's stereo speakers,
which is then received by the smartphone's microphone and processed via a
Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data
augmentation techniques are proposed to improve the detection accuracy and
three dual-channel input fusion methods are compared. The first method merges
the dual-channel audio as a single input spectrogram image. The second method
adopts early fusion by concatenating the dual-channel spectrograms. The third
method adopts late fusion by having two convectional input branches processing
each of the dual-channel spectrograms and then the outputs are merged by the
last layers. Our experimental results demonstrate a promising detection
accuracy for the six gestures presented in our publicly available dataset with
an accuracy of 93.58\% as a baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Ahmed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1"&gt;Ayman El-Refai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1"&gt;Mariam Aboul-Ela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1"&gt;Mohamed Moustafa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Portrait Lighting Enhancement with 3D Guidance. (arXiv:2108.02121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02121</id>
        <link href="http://arxiv.org/abs/2108.02121"/>
        <updated>2021-08-05T01:56:20.080Z</updated>
        <summary type="html"><![CDATA[Despite recent breakthroughs in deep learning methods for image lighting
enhancement, they are inferior when applied to portraits because 3D facial
information is ignored in their models. To address this, we present a novel
deep learning framework for portrait lighting enhancement based on 3D facial
guidance. Our framework consists of two stages. In the first stage, corrected
lighting parameters are predicted by a network from the input bad lighting
image, with the assistance of a 3D morphable model and a differentiable
renderer. Given the predicted lighting parameter, the differentiable renderer
renders a face image with corrected shading and texture, which serves as the 3D
guidance for learning image lighting enhancement in the second stage. To better
exploit the long-range correlations between the input and the guidance, in the
second stage, we design an image-to-image translation network with a novel
transformer architecture, which automatically produces a lighting-enhanced
result. Experimental results on the FFHQ dataset and in-the-wild images show
that the proposed method outperforms state-of-the-art methods in terms of both
quantitative metrics and visual quality. We will publish our dataset along with
more results on https://cassiepython.github.io/egsr/index.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fangzhou Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Hao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-based Noise Modeling for Extreme Low-light Photography. (arXiv:2108.02158v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02158</id>
        <link href="http://arxiv.org/abs/2108.02158"/>
        <updated>2021-08-05T01:56:20.061Z</updated>
        <summary type="html"><![CDATA[Enhancing the visibility in extreme low-light environments is a challenging
task. Under nearly lightless condition, existing image denoising methods could
easily break down due to significantly low SNR. In this paper, we
systematically study the noise statistics in the imaging pipeline of CMOS
photosensors, and formulate a comprehensive noise model that can accurately
characterize the real noise structures. Our novel model considers the noise
sources caused by digital camera electronics which are largely overlooked by
existing methods yet have significant influence on raw measurement in the dark.
It provides a way to decouple the intricate noise structure into different
statistical distributions with physical interpretations. Moreover, our noise
model can be used to synthesize realistic training data for learning-based
low-light denoising algorithms. In this regard, although promising results have
been shown recently with deep convolutional neural networks, the success
heavily depends on abundant noisy clean image pairs for training, which are
tremendously difficult to obtain in practice. Generalizing their trained models
to images from new devices is also problematic. Extensive experiments on
multiple low-light denoising datasets -- including a newly collected one in
this work covering various devices -- show that a deep neural network trained
with our proposed noise formation model can reach surprisingly-high accuracy.
The results are on par with or sometimes even outperform training with paired
real data, opening a new door to real-world extreme low-light photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kaixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yinqiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaolong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02122</id>
        <link href="http://arxiv.org/abs/2108.02122"/>
        <updated>2021-08-05T01:56:20.054Z</updated>
        <summary type="html"><![CDATA[We explore the value of weak labels in learning transferable representations
for medical images. Compared to hand-labeled datasets, weak or inexact labels
can be acquired in large quantities at significantly lower cost and can provide
useful training signals for data-hungry models such as deep neural networks. We
consider weak labels in the form of pseudo-labels and propose a semi-weakly
supervised contrastive learning (SWCL) framework for representation learning
using semi-weakly annotated images. Specifically, we train a semi-supervised
model to propagate labels from a small dataset consisting of diverse
image-level annotations to a large unlabeled dataset. Using the propagated
labels, we generate a patch-level dataset for pretraining and formulate a
multi-label contrastive learning objective to capture position-specific
features encoded in each patch. We empirically validate the transfer learning
performance of SWCL on seven public retinal fundus datasets, covering three
disease classification tasks and two anatomical structure segmentation tasks.
Our experiment results suggest that, under very low data regime, large-scale
ImageNet pretraining on improved architecture remains a very strong baseline,
and recently proposed self-supervised methods falter in segmentation tasks,
possibly due to the strong invariant constraint imposed. Our method surpasses
all prior self-supervised methods and standard cross-entropy training, while
closing the gaps with ImageNet pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1"&gt;Boon Peng Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1"&gt;Beng Koon Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations. (arXiv:2108.02016v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02016</id>
        <link href="http://arxiv.org/abs/2108.02016"/>
        <updated>2021-08-05T01:56:20.040Z</updated>
        <summary type="html"><![CDATA[FDG PET/CT imaging is a resource intensive examination critical for managing
malignant disease and is particularly important for longitudinal assessment
during therapy. Approaches to automate longtudinal analysis present many
challenges including lack of available longitudinal datasets, managing complex
large multimodal imaging examinations, and need for detailed annotations for
traditional supervised machine learning. In this work we develop OncoNet, novel
machine learning algorithm that assesses treatment response from a 1,954 pairs
of sequential FDG PET/CT exams through weak supervision using the standard
uptake values (SUVmax) in associated radiology reports. OncoNet demonstrates an
AUROC of 0.86 and 0.84 on internal and external institution test sets
respectively for determination of change between scans while also showing
strong agreement to clinical scoring systems with a kappa score of 0.8. We also
curated a dataset of 1,954 paired FDG PET/CT exams designed for response
assessment for the broader machine learning in healthcare research community.
Automated assessment of radiographic response from FDG PET/CT with OncoNet
could provide clinicians with a valuable tool to rapidly and consistently
interpret change over time in longitudinal multi-modal imaging exams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Anirudh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eyuboglu_S/0/1/0/all/0/1"&gt;Sabri Eyuboglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dunnmon_J/0/1/0/all/0/1"&gt;Jared Dunnmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soin_A/0/1/0/all/0/1"&gt;Arjun Soin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Davidzon_G/0/1/0/all/0/1"&gt;Guido Davidzon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1"&gt;Akshay Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P Lungren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02147</id>
        <link href="http://arxiv.org/abs/2108.02147"/>
        <updated>2021-08-05T01:56:20.031Z</updated>
        <summary type="html"><![CDATA[Video captioning is an essential technology to understand scenes and describe
events in natural language. To apply it to real-time monitoring, a system needs
not only to describe events accurately but also to produce the captions as soon
as possible. Low-latency captioning is needed to realize such functionality,
but this research area for online video captioning has not been pursued yet.
This paper proposes a novel approach to optimize each caption's output timing
based on a trade-off between latency and caption quality. An audio-visual
Trans-former is trained to generate ground-truth captions using only a small
portion of all video frames, and to mimic outputs of a pre-trained Transformer
to which all the frames are given. A CNN-based timing detector is also trained
to detect a proper output timing, where the captions generated by the two
Trans-formers become sufficiently close to each other. With the jointly trained
Transformer and timing detector, a caption can be generated in the early stages
of an event-triggered video clip, as soon as an event happens or when it can be
forecasted. Experiments with the ActivityNet Captions dataset show that our
approach achieves 94% of the caption quality of the upper bound given by the
pre-trained Transformer using the entire video clips, using only 28% of frames
from the beginning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1"&gt;Chiori Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02059</id>
        <link href="http://arxiv.org/abs/2108.02059"/>
        <updated>2021-08-05T01:56:20.020Z</updated>
        <summary type="html"><![CDATA[For an image with multiple scene texts, different people may be interested in
different text information. Current text-aware image captioning models are not
able to generate distinctive captions according to various information needs.
To explore how to generate personalized text-aware captions, we define a new
challenging task, namely Question-controlled Text-aware Image Captioning
(Qc-TextCap). With questions as control signals, this task requires models to
understand questions, find related scene texts and describe them together with
objects fluently in human language. Based on two existing text-aware captioning
datasets, we automatically construct two datasets, ControlTextCaps and
ControlVizWiz to support the task. We propose a novel Geometry and Question
Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to
fuse region-level object features and region-level scene text features with
considering spatial relationships. Then, we design a Question-guided Encoder to
select the most relevant visual features for each question. Finally, GQAM
generates a personalized text-aware caption with a Multimodal Decoder. Our
model achieves better captioning performance and question answering ability
than carefully designed baselines on both two datasets. With questions as
control signals, our model generates more informative and diverse captions than
the state-of-the-art text-aware captioning model. Our code and datasets are
publicly available at https://github.com/HAWLYQ/Qc-TextCap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02032</id>
        <link href="http://arxiv.org/abs/2108.02032"/>
        <updated>2021-08-05T01:56:20.002Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1"&gt;Cosmin Octavian Pene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1"&gt;Amirmasoud Ghiassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1"&gt;Taraneh Younesian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y.Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRI to PET Cross-Modality Translation using Globally and Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease. (arXiv:2108.02160v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02160</id>
        <link href="http://arxiv.org/abs/2108.02160"/>
        <updated>2021-08-05T01:56:19.995Z</updated>
        <summary type="html"><![CDATA[Medical imaging datasets are inherently high dimensional with large
variability and low sample sizes that limit the effectiveness of deep learning
algorithms. Recently, generative adversarial networks (GANs) with the ability
to synthesize realist images have shown great potential as an alternative to
standard data augmentation techniques. Our work focuses on cross-modality
synthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans
from structural Magnetic Resonance~(MR) images using generative models to
facilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we
propose a novel end-to-end, globally and locally aware image-to-image
translation GAN (GLA-GAN) with a multi-path architecture that enforces both
global structural integrity and fidelity to local details. We further
supplement the standard adversarial loss with voxel-level intensity,
multi-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based
loss components that reduce reconstruction error, enforce structural
consistency at different scales and perceive variation in regional sensitivity
to AD respectively. Experimental results demonstrate that our GLA-GAN not only
generates synthesized FDG-PET scans with enhanced image quality but also
superior clinical utility in improving AD diagnosis compared to
state-of-the-art models. Finally, we attempt to interpret some of the internal
units of the GAN that are closely related to this specific cross-modality
generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sikka_A/0/1/0/all/0/1"&gt;Apoorva Sikka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Skand/0/1/0/all/0/1"&gt;Skand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Virk_J/0/1/0/all/0/1"&gt;Jitender Singh Virk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1"&gt;Deepti R. Bathula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Knowledge Distillation for Efficient Pose Estimation. (arXiv:2108.02092v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02092</id>
        <link href="http://arxiv.org/abs/2108.02092"/>
        <updated>2021-08-05T01:56:19.988Z</updated>
        <summary type="html"><![CDATA[Existing state-of-the-art human pose estimation methods require heavy
computational resources for accurate predictions. One promising technique to
obtain an accurate yet lightweight pose estimator is knowledge distillation,
which distills the pose knowledge from a powerful teacher model to a
less-parameterized student model. However, existing pose distillation works
rely on a heavy pre-trained estimator to perform knowledge transfer and require
a complex two-stage learning procedure. In this work, we investigate a novel
Online Knowledge Distillation framework by distilling Human Pose structure
knowledge in a one-stage manner to guarantee the distillation efficiency,
termed OKDHP. Specifically, OKDHP trains a single multi-branch network and
acquires the predicted heatmaps from each, which are then assembled by a
Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in
reverse. Instead of simply averaging the heatmaps, FAU which consists of
multiple parallel transformations with different receptive fields, leverages
the multi-scale information, thus obtains target heatmaps with higher-quality.
Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to
minimize the discrepancy between the target heatmaps and the predicted ones,
which enables the student network to learn the implicit keypoint relationship.
Besides, an unbalanced OKDHP scheme is introduced to customize the student
networks with different compression rates. The effectiveness of our approach is
demonstrated by extensive experiments on two common benchmark datasets, MPII
and COCO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jingwen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Ying Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhigeng Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02093</id>
        <link href="http://arxiv.org/abs/2108.02093"/>
        <updated>2021-08-05T01:56:19.968Z</updated>
        <summary type="html"><![CDATA[We unveil a long-standing problem in the prevailing co-saliency detection
systems: there is indeed inconsistency between training and testing.
Constructing a high-quality co-saliency detection dataset involves
time-consuming and labor-intensive pixel-level labeling, which has forced most
recent works to rely instead on semantic segmentation or saliency detection
datasets for training. However, the lack of proper co-saliency and the absence
of multiple foreground objects in these datasets can lead to spurious
variations and inherent biases learned by models. To tackle this, we introduce
the idea of counterfactual training through context adjustment, and propose a
"cost-free" group-cut-paste (GCP) procedure to leverage images from
off-the-shelf saliency detection datasets and synthesize new samples. Following
GCP, we collect a novel dataset called Context Adjustment Training. The two
variants of our dataset, i.e., CAT and CAT+, consist of 16,750 and 33,500
images, respectively. All images are automatically annotated with high-quality
masks. As a side-product, object categories, as well as edge information, are
also provided to facilitate other related works. Extensive experiments with
state-of-the-art models are conducted to demonstrate the superiority of our
dataset. We hope that the scale, diversity, and quality of CAT/CAT+ can benefit
researchers in this area and beyond. The dataset and benchmark toolkit will be
accessible through our project page.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingdong Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1"&gt;Prakhar Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online unsupervised Learning for domain shift in COVID-19 CT scan datasets. (arXiv:2108.02002v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02002</id>
        <link href="http://arxiv.org/abs/2108.02002"/>
        <updated>2021-08-05T01:56:19.956Z</updated>
        <summary type="html"><![CDATA[Neural networks often require large amounts of expert annotated data to
train. When changes are made in the process of medical imaging, trained
networks may not perform as well, and obtaining large amounts of expert
annotations for each change in the imaging process can be time consuming and
expensive. Online unsupervised learning is a method that has been proposed to
deal with situations where there is a domain shift in incoming data, and a lack
of annotations. The aim of this study is to see whether online unsupervised
learning can help COVID-19 CT scan classification models adjust to slight
domain shifts, when there are no annotations available for the new data. A
total of six experiments are performed using three test datasets with differing
amounts of domain shift. These experiments compare the performance of the
online unsupervised learning strategy to a baseline, as well as comparing how
the strategy performs on different domain shifts. Code for online unsupervised
learning can be found at this link:
https://github.com/Mewtwo/online-unsupervised-learning]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ewen_N/0/1/0/all/0/1"&gt;Nicolas Ewen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01998</id>
        <link href="http://arxiv.org/abs/2108.01998"/>
        <updated>2021-08-05T01:56:19.938Z</updated>
        <summary type="html"><![CDATA[Energy disaggregation, also known as non-intrusive load monitoring (NILM),
challenges the problem of separating the whole-home electricity usage into
appliance-specific individual consumptions, which is a typical application of
data analysis. {NILM aims to help households understand how the energy is used
and consequently tell them how to effectively manage the energy, thus allowing
energy efficiency which is considered as one of the twin pillars of sustainable
energy policy (i.e., energy efficiency and renewable energy).} Although NILM is
unidentifiable, it is widely believed that the NILM problem can be addressed by
data science. Most of the existing approaches address the energy disaggregation
problem by conventional techniques such as sparse coding, non-negative matrix
factorization, and hidden Markov model. Recent advances reveal that deep neural
networks (DNNs) can get favorable performance for NILM since DNNs can
inherently learn the discriminative signatures of the different appliances. In
this paper, we propose a novel method named adversarial energy disaggregation
(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,
which is new for the energy disaggregation task. Our method trains a generator
and multiple discriminators via an adversarial fashion. The proposed method not
only learns shard representations for different appliances, but captures the
specific multimode structures of each appliance. Extensive experiments on
real-world datasets verify that our method can achieve new state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Compatible Embeddings. (arXiv:2108.01958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01958</id>
        <link href="http://arxiv.org/abs/2108.01958"/>
        <updated>2021-08-05T01:56:19.928Z</updated>
        <summary type="html"><![CDATA[Achieving backward compatibility when rolling out new models can highly
reduce costs or even bypass feature re-encoding of existing gallery images for
in-production visual retrieval systems. Previous related works usually leverage
losses used in knowledge distillation which can cause performance degradations
or not guarantee compatibility. To address these issues, we propose a general
framework called Learning Compatible Embeddings (LCE) which is applicable for
both cross model compatibility and compatible training in
direct/forward/backward manners. Our compatibility is achieved by aligning
class centers between models directly or via a transformation, and restricting
more compact intra-class distributions for the new model. Experiments are
conducted in extensive scenarios such as changes of training dataset, loss
functions, network architectures as well as feature dimensions, and demonstrate
that LCE efficiently enables model compatibility with marginal sacrifices of
accuracies. The code will be available at https://github.com/IrvingMeng/LCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qiang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chixiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaoqiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signature Verification using Geometrical Features and Artificial Neural Network Classifier. (arXiv:2108.02029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02029</id>
        <link href="http://arxiv.org/abs/2108.02029"/>
        <updated>2021-08-05T01:56:19.918Z</updated>
        <summary type="html"><![CDATA[Signature verification has been one of the major researched areas in the
field of computer vision. Many financial and legal organizations use signature
verification as access control and authentication. Signature images are not
rich in texture; however, they have much vital geometrical information. Through
this work, we have proposed a signature verification methodology that is simple
yet effective. The technique presented in this paper harnesses the geometrical
features of a signature image like center, isolated points, connected
components, etc., and with the power of Artificial Neural Network (ANN)
classifier, classifies the signature image based on their geometrical features.
Publicly available dataset MCYT, BHSig260 (contains the image of two regional
languages Bengali and Hindi) has been used in this paper to test the
effectiveness of the proposed method. We have received a lower Equal Error Rate
(EER) on MCYT 100 dataset and higher accuracy on the BHSig260 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anamika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Krishna Pratap Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02050</id>
        <link href="http://arxiv.org/abs/2108.02050"/>
        <updated>2021-08-05T01:56:19.912Z</updated>
        <summary type="html"><![CDATA[Most current image captioning systems focus on describing general image
content, and lack background knowledge to deeply understand the image, such as
exact named entities or concrete events. In this work, we focus on the
entity-aware news image captioning task which aims to generate informative
captions by leveraging the associated news articles to provide background
knowledge about the target image. However, due to the length of news articles,
previous works only employ news articles at the coarse article or sentence
level, which are not fine-grained enough to refine relevant events and choose
named entities accurately. To overcome these limitations, we propose an
Information Concentrated Entity-aware news image CAPtioning (ICECAP) model,
which progressively concentrates on relevant textual information within the
corresponding news article from the sentence level to the word level. Our model
first creates coarse concentration on relevant sentences using a cross-modality
retrieval model and then generates captions by further concentrating on
relevant words within the sentences. Extensive experiments on both BreakingNews
and GoodNews datasets demonstrate the effectiveness of our proposed method,
which outperforms other state-of-the-arts. The code of ICECAP is publicly
available at https://github.com/HAWLYQ/ICECAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-In-The-Loop Document Layout Analysis. (arXiv:2108.02095v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02095</id>
        <link href="http://arxiv.org/abs/2108.02095"/>
        <updated>2021-08-05T01:56:19.905Z</updated>
        <summary type="html"><![CDATA[Document layout analysis (DLA) aims to divide a document image into different
types of regions. DLA plays an important role in the document content
understanding and information extraction systems. Exploring a method that can
use less data for effective training contributes to the development of DLA. We
consider a Human-in-the-loop (HITL) collaborative intelligence in the DLA. Our
approach was inspired by the fact that the HITL push the model to learn from
the unknown problems by adding a small amount of data based on knowledge. The
HITL select key samples by using confidence. However, using confidence to find
key samples is not suitable for DLA tasks. We propose the Key Samples Selection
(KSS) method to find key samples in high-level tasks (semantic segmentation)
more accurately through agent collaboration, effectively reducing costs. Once
selected, these key samples are passed to human beings for active labeling,
then the model will be updated with the labeled samples. Hence, we revisited
the learning system from reinforcement learning and designed a sample-based
agent update strategy, which effectively improves the agent's ability to accept
new samples. It achieves significant improvement results in two benchmarks
(DSSE-200 (from 77.1% to 86.3%) and CS-150 (from 88.0% to 95.6%)) by using 10%
of labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xingjiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianlong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Liang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning. (arXiv:2108.01959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01959</id>
        <link href="http://arxiv.org/abs/2108.01959"/>
        <updated>2021-08-05T01:56:19.885Z</updated>
        <summary type="html"><![CDATA[Skeleton-based human action recognition has attracted increasing attention in
recent years. However, most of the existing works focus on supervised learning
which requiring a large number of annotated action sequences that are often
expensive to collect. We investigate unsupervised representation learning for
skeleton action recognition, and design a novel skeleton cloud colorization
technique that is capable of learning skeleton representations from unlabeled
skeleton sequence data. Specifically, we represent a skeleton action sequence
as a 3D skeleton cloud and colorize each point in the cloud according to its
temporal and spatial orders in the original (unannotated) skeleton sequence.
Leveraging the colorized skeleton point cloud, we design an auto-encoder
framework that can learn spatial-temporal features from the artificial color
labels of skeleton joints effectively. We evaluate our skeleton cloud
colorization approach with action classifiers trained under different
configurations, including unsupervised, semi-supervised and fully-supervised
settings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the
proposed method outperforms existing unsupervised and semi-supervised 3D action
recognition methods by large margins, and it achieves competitive performance
in supervised 3D action recognition as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Siyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1"&gt;Meng Hwa Er&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1"&gt;Alex C. Kot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01997</id>
        <link href="http://arxiv.org/abs/2108.01997"/>
        <updated>2021-08-05T01:56:19.860Z</updated>
        <summary type="html"><![CDATA[Early detection of the coronavirus disease 2019 (COVID-19) helps to treat
patients timely and increase the cure rate, thus further suppressing the spread
of the disease. In this study, we propose a novel deep learning based detection
and similar case recommendation network to help control the epidemic. Our
proposed network contains two stages: the first one is a lung region
segmentation step and is used to exclude irrelevant factors, and the second is
a detection and recommendation stage. Under this framework, in the second
stage, we develop a dual-children network (DuCN) based on a pre-trained
ResNet-18 to simultaneously realize the disease diagnosis and similar case
recommendation. Besides, we employ triplet loss and intrapulmonary distance
maps to assist the detection, which helps incorporate tiny differences between
two images and is conducive to improving the diagnostic accuracy. For each
confirmed COVID-19 case, we give similar cases to provide radiologists with
diagnosis and treatment references. We conduct experiments on a large publicly
available dataset (CC-CCII) and compare the proposed model with
state-of-the-art COVID-19 detection methods. The results show that our proposed
model achieves a promising clinical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Senhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1"&gt;Dandan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Depth and Ego-Motion from Video by Alternative Training and Geometric Constraints from 3D to 2D. (arXiv:2108.01980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01980</id>
        <link href="http://arxiv.org/abs/2108.01980"/>
        <updated>2021-08-05T01:56:19.853Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning of depth and ego-motion from unlabeled monocular
video has acquired promising results and drawn extensive attention. Most
existing methods jointly train the depth and pose networks by photometric
consistency of adjacent frames based on the principle of structure-from-motion
(SFM). However, the coupling relationship of the depth and pose networks
seriously influences the learning performance, and the re-projection relations
is sensitive to scale ambiguity, especially for pose learning. In this paper,
we aim to improve the depth-pose learning performance without the auxiliary
tasks and address the above issues by alternative training each task and
incorporating the epipolar geometric constraints into the Iterative Closest
Point (ICP) based point clouds match process. Distinct from jointly training
the depth and pose networks, our key idea is to better utilize the mutual
dependency of these two tasks by alternatively training each network with
respective losses while fixing the other. We also design a log-scale 3D
structural consistency loss to put more emphasis on the smaller depth values
during training. To makes the optimization easier, we further incorporate the
epipolar geometry into the ICP based learning process for pose learning.
Extensive experiments on various benchmarks datasets indicate the superiority
of our algorithm over the state-of-the-art self-supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiaojiao Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection. (arXiv:2108.01971v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01971</id>
        <link href="http://arxiv.org/abs/2108.01971"/>
        <updated>2021-08-05T01:56:19.846Z</updated>
        <summary type="html"><![CDATA[The popularity and promotion of depth maps have brought new vigor and
vitality into salient object detection (SOD), and a mass of RGB-D SOD
algorithms have been proposed, mainly concentrating on how to better integrate
cross-modality features from RGB image and depth map. For the cross-modality
interaction in feature encoder, existing methods either indiscriminately treat
RGB and depth modalities, or only habitually utilize depth cues as auxiliary
information of the RGB branch. Different from them, we reconsider the status of
two modalities and propose a novel Cross-modality Discrepant Interaction
Network (CDINet) for RGB-D SOD, which differentially models the dependence of
two modalities according to the feature representations of different layers. To
this end, two components are designed to implement the effective cross-modality
interaction: 1) the RGB-induced Detail Enhancement (RDE) module leverages RGB
modality to enhance the details of the depth features in low-level encoder
stage. 2) the Depth-induced Semantic Enhancement (DSE) module transfers the
object positioning and internal consistency of depth features to the RGB branch
in high-level encoder stage. Furthermore, we also design a Dense Decoding
Reconstruction (DDR) structure, which constructs a semantic block by combining
multi-level encoder features to upgrade the skip connection in the feature
decoding. Extensive experiments on five benchmark datasets demonstrate that our
network outperforms $15$ state-of-the-art methods both quantitatively and
qualitatively. Our code is publicly available at:
https://rmcong.github.io/proj_CDINet.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1"&gt;Runmin Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qinwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Feng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1"&gt;Sam Kwong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensing Anomalies like Humans: A Hominine Framework to Detect Abnormal Events from Unlabeled Videos. (arXiv:2108.01975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01975</id>
        <link href="http://arxiv.org/abs/2108.01975"/>
        <updated>2021-08-05T01:56:19.839Z</updated>
        <summary type="html"><![CDATA[Video anomaly detection (VAD) has constantly been a vital topic in video
analysis. As anomalies are often rare, it is typically addressed under a
semi-supervised setup, which requires a training set with pure normal videos.
To avoid exhausted manual labeling, we are inspired by how humans sense
anomalies and propose a hominine framework that enables both unsupervised and
end-to-end VAD. The framework is based on two key observations: 1) Human
perception is usually local, i.e. focusing on local foreground and its context
when sensing anomalies. Thus, we propose to impose locality-awareness by
localizing foreground with generic knowledge, and a region localization
strategy is designed to exploit local context. 2) Frequently-occurred events
will mould humans' definition of normality, which motivates us to devise a
surrogate training paradigm. It trains a deep neural network (DNN) to learn a
surrogate task with unlabeled videos, and frequently-occurred events will play
a dominant role in "moulding" the DNN. In this way, a training loss gap will
automatically manifest rarely-seen novel events as anomalies. For
implementation, we explore various surrogate tasks as well as both classic and
emerging DNN models. Extensive evaluations on commonly-used VAD benchmarks
justify the framework's applicability to different surrogate tasks or DNN
models, and demonstrate its astonishing effectiveness: It not only outperforms
existing unsupervised solutions by a wide margin (8% to 10% AUROC gain), but
also achieves comparable or even superior performance to state-of-the-art
semi-supervised counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiping Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1"&gt;En Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chengzhang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPB: Feature Pyramid Branch for Person Re-Identification. (arXiv:2108.01901v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01901</id>
        <link href="http://arxiv.org/abs/2108.01901"/>
        <updated>2021-08-05T01:56:19.829Z</updated>
        <summary type="html"><![CDATA[High performance person Re-Identification (Re-ID) requires the model to focus
on both global silhouette and local details of pedestrian. To extract such more
representative features, an effective way is to exploit deep models with
multiple branches. However, most multi-branch based methods implemented by
duplication of part backbone structure normally lead to severe increase of
computational cost. In this paper, we propose a lightweight Feature Pyramid
Branch (FPB) to extract features from different layers of networks and
aggregate them in a bidirectional pyramid structure. Cooperated by attention
modules and our proposed cross orthogonality regularization, FPB significantly
prompts the performance of backbone network by only introducing less than 1.5M
extra parameters. Extensive experimental results on standard benchmark datasets
demonstrate that our proposed FPB based model outperforms state-of-the-art
methods with obvious margin as well as much less model complexity. FPB borrows
the idea of the Feature Pyramid Network (FPN) from prevailing object detection
methods. To our best knowledge, it is the first successful application of
similar structure in person Re-ID tasks, which empirically proves that pyramid
network as affiliated branch could be a potential structure in related feature
embedding models. The source code is publicly available at
https://github.com/anocodetest1/FPB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Suofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zirui Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiofu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1"&gt;Bin Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01808</id>
        <link href="http://arxiv.org/abs/2108.01808"/>
        <updated>2021-08-05T01:56:19.807Z</updated>
        <summary type="html"><![CDATA[There is a warning light for the loss of plant habitats worldwide that
entails concerted efforts to conserve plant biodiversity. Thus, plant species
classification is of crucial importance to address this environmental
challenge. In recent years, there is a considerable increase in the number of
studies related to plant taxonomy. While some researchers try to improve their
recognition performance using novel approaches, others concentrate on
computational optimization for their framework. In addition, a few studies are
diving into feature extraction to gain significantly in terms of accuracy. In
this paper, we propose an effective method for the leaf recognition problem. In
our proposed approach, a leaf goes through some pre-processing to extract its
refined color image, vein image, xy-projection histogram, handcrafted shape,
texture features, and Fourier descriptors. These attributes are then
transformed into a better representation by neural network-based encoders
before a support vector machine (SVM) model is utilized to classify different
leaves. Overall, our approach performs a state-of-the-art result on the Flavia
leaf dataset, achieving the accuracy of 99.58\% on test sets under random
10-fold cross-validation and bypassing the previous methods. We also release
our codes\footnote{Scripts are available at
\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to
the research community in the leaf classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1"&gt;Boi M. Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1"&gt;Dinh V. Cuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1"&gt;Nhung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dang Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curriculum learning for language modeling. (arXiv:2108.02170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02170</id>
        <link href="http://arxiv.org/abs/2108.02170"/>
        <updated>2021-08-05T01:56:19.793Z</updated>
        <summary type="html"><![CDATA[Language Models like ELMo and BERT have provided robust representations of
natural language, which serve as the language understanding component for a
diverse range of downstream tasks.Curriculum learning is a method that employs
a structured training regime instead, which has been leveraged in computer
vision and machine translation to improve model training speed and model
performance. While language models have proven transformational for the natural
language processing community, these models have proven expensive,
energy-intensive, and challenging to train. In this work, we explore the effect
of curriculum learning on language model pretraining using various
linguistically motivated curricula and evaluate transfer performance on the
GLUE Benchmark. Despite a broad variety of training methodologies and
experiments we do not find compelling evidence that curriculum learning methods
improve language model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1"&gt;Daniel Campos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Domain Adaption to Adversarial Attacks. (arXiv:2108.01807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01807</id>
        <link href="http://arxiv.org/abs/2108.01807"/>
        <updated>2021-08-05T01:56:19.783Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks (DNNs) have been proved to have
excellent performance on unsupervised domain adaption (UDA). However, recent
work shows that DNNs perform poorly when being attacked by adversarial samples,
where these attacks are implemented by simply adding small disturbances to the
original images. Although plenty of work has focused on this, as far as we
know, there is no systematic research on the robustness of unsupervised domain
adaption model. Hence, we discuss the robustness of unsupervised domain
adaption against adversarial attacking for the first time. We benchmark various
settings of adversarial attack and defense in domain adaption, and propose a
cross domain attack method based on pseudo label. Most importantly, we analyze
the impact of different datasets, models, attack methods and defense methods.
Directly, our work proves the limited robustness of unsupervised domain
adaptation model, and we hope our work may facilitate the community to pay more
attention to improve the robustness of the model against attacking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic hemisphere segmentation in rodent MRI with lesions. (arXiv:2108.01941v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01941</id>
        <link href="http://arxiv.org/abs/2108.01941"/>
        <updated>2021-08-05T01:56:19.773Z</updated>
        <summary type="html"><![CDATA[We present MedicDeepLabv3+, a convolutional neural network that is the first
completely automatic method to segment brain hemispheres in magnetic resonance
(MR) images of rodents with lesions. MedicDeepLabv3+ improves the
state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial
attention layers and additional skip connections that, as we show in our
experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR
image preprocessing, such as bias-field correction or registration to a
template, produces segmentations in less than a second, and its GPU memory
requirements can be adjusted based on the available resources. Using a large
dataset of 723 MR rat brain images, we evaluated our MedicDeepLabv3+, two
state-of-the-art convolutional neural networks (DeepLabv3+, UNet) and three
approaches that were specifically designed for skull-stripping rodent MR images
(Demon, RATS and RBET). In our experiments, MedicDeepLabv3+ outperformed the
other methods, yielding an average Dice coefficient of 0.952 and 0.944 in the
brain and contralateral hemisphere regions. Additionally, we show that despite
limiting the GPU memory and the training data to only three images, our
MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our
method, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus,
yielded excellent results in multiple scenarios, demonstrating its capability
to reduce human workload in rodent neuroimaging studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1"&gt;Juan Miguel Valverde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shatillo_A/0/1/0/all/0/1"&gt;Artem Shatillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feo_R/0/1/0/all/0/1"&gt;Riccardo de Feo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1"&gt;Jussi Tohka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01938</id>
        <link href="http://arxiv.org/abs/2108.01938"/>
        <updated>2021-08-05T01:56:19.753Z</updated>
        <summary type="html"><![CDATA[Graph neural networks are increasingly becoming the go-to approach in various
fields such as computer vision, computational biology and chemistry, where data
are naturally explained by graphs. However, unlike traditional convolutional
neural networks, deep graph networks do not necessarily yield better
performance than shallow graph networks. This behavior usually stems from the
over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical
methods for solving Partial Differential Equations (PDEs) on manifolds, and as
such, their behavior can be explained by similar analysis. Moreover, as we
demonstrate using an extensive set of experiments, our PDE-motivated networks
can generalize and be effective for various types of problems from different
fields. Our architectures obtain better or on par with the current
state-of-the-art results for problems that are typically approached using
different architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1"&gt;Eldad Haber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Similarity and Alignment Learning on Partial Video Copy Detection. (arXiv:2108.01817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01817</id>
        <link href="http://arxiv.org/abs/2108.01817"/>
        <updated>2021-08-05T01:56:19.744Z</updated>
        <summary type="html"><![CDATA[Existing video copy detection methods generally measure video similarity
based on spatial similarities between key frames, neglecting the latent
similarity in temporal dimension, so that the video similarity is biased
towards spatial information. There are methods modeling unified video
similarity in an end-to-end way, but losing detailed partial alignment
information, which causes the incapability of copy segments localization. To
address the above issues, we propose the Video Similarity and Alignment
Learning (VSAL) approach, which jointly models spatial similarity, temporal
similarity and partial alignment. To mitigate the spatial similarity bias, we
model the temporal similarity as the mask map predicted from frame-level
spatial similarity, where each element indicates the probability of frame pair
lying right on the partial alignments. To further localize partial copies, the
step map is learned from the spatial similarity where the elements indicate
extending directions of the current partial alignments on the spatial-temporal
similarity map. Obtained from the mask map, the start points extend out into
partial optimal alignments following instructions of the step map. With the
similarity and alignment learning strategy, VSAL achieves the state-of-the-art
F1-score on VCDB core dataset. Furthermore, we construct a new benchmark of
partial video copy detection and localization by adding new segment-level
annotations for FIVR-200k dataset, where VSAL also achieves the best
performance, verifying its effectiveness in more challenging situations. Our
project is publicly available at https://pvcd-vsal.github.io/vsal/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhen Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangteng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yiliang Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01899</id>
        <link href="http://arxiv.org/abs/2108.01899"/>
        <updated>2021-08-05T01:56:19.736Z</updated>
        <summary type="html"><![CDATA[Most existing neural architecture search (NAS) algorithms are dedicated to
the downstream tasks, e.g., image classification in computer vision. However,
extensive experiments have shown that, prominent neural architectures, such as
ResNet in computer vision and LSTM in natural language processing, are
generally good at extracting patterns from the input data and perform well on
different downstream tasks. These observations inspire us to ask: Is it
necessary to use the performance of specific downstream tasks to evaluate and
search for good neural architectures? Can we perform NAS effectively and
efficiently while being agnostic to the downstream task? In this work, we
attempt to affirmatively answer the above two questions and improve the
state-of-the-art NAS solution by proposing a novel and generic NAS framework,
termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but
instead adopts \textit{regression} on a set of manually designed synthetic
signal bases for architecture evaluation. Such a self-supervised regression
task can effectively evaluate the intrinsic power of an architecture to capture
and transform the input signal patterns, and allow more sufficient usage of
training samples. We then propose an automatic task search to optimize the
combination of synthetic signals using limited downstream-task-specific labels,
further improving the performance of GenNAS. We also thoroughly evaluate
GenNAS's generality and end-to-end NAS performance on all search spaces, which
outperforms almost all existing works with significant speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Deming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Operator-Splitting Method for the Gaussian Curvature Regularization Model with Applications in Surface Smoothing and Imaging. (arXiv:2108.01914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01914</id>
        <link href="http://arxiv.org/abs/2108.01914"/>
        <updated>2021-08-05T01:56:19.720Z</updated>
        <summary type="html"><![CDATA[Gaussian curvature is an important geometric property of surfaces, which has
been used broadly in mathematical modeling. Due to the full nonlinearity of the
Gaussian curvature, efficient numerical methods for models based on it are
uncommon in literature. In this article, we propose an operator-splitting
method for a general Gaussian curvature model. In our method, we decouple the
full nonlinearity of Gaussian curvature from differential operators by
introducing two matrix- and vector-valued functions. The optimization problem
is then converted into the search for the steady state solution of a time
dependent PDE system. The above PDE system is well-suited to time
discretization by operator splitting, the sub-problems encountered at each
fractional step having either a closed form solution or being solvable by
efficient algorithms. The proposed method is not sensitive to the choice of
parameters, its efficiency and performances being demonstrated via systematic
experiments on surface smoothing and image denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1"&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glowinski_R/0/1/0/all/0/1"&gt;Roland Glowinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01912</id>
        <link href="http://arxiv.org/abs/2108.01912"/>
        <updated>2021-08-05T01:56:19.713Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for video inpainting by adopting an internal
learning strategy. Unlike previous methods that use optical flow for
cross-frame context propagation to inpaint unknown regions, we show that this
can be achieved implicitly by fitting a convolutional neural network to the
known region. Moreover, to handle challenging sequences with ambiguous
backgrounds or long-term occlusion, we design two regularization terms to
preserve high-frequency details and long-term temporal consistency. Extensive
experiments on the DAVIS dataset demonstrate that the proposed method achieves
state-of-the-art inpainting quality quantitatively and qualitatively. We
further extend the proposed method to another challenging task: learning to
remove an object from a video giving a single object mask in only one frame in
a 4K video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1"&gt;Hao Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tengfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A universal detector of CNN-generated images using properties of checkerboard artifacts in the frequency domain. (arXiv:2108.01892v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01892</id>
        <link href="http://arxiv.org/abs/2108.01892"/>
        <updated>2021-08-05T01:56:19.705Z</updated>
        <summary type="html"><![CDATA[We propose a novel universal detector for detecting images generated by using
CNNs. In this paper, properties of checkerboard artifacts in CNN-generated
images are considered, and the spectrum of images is enhanced in accordance
with the properties. Next, a classifier is trained by using the enhanced
spectrums to judge a query image to be a CNN-generated ones or not. In
addition, an ensemble of the proposed detector with emphasized spectrums and a
conventional detector is proposed to improve the performance of these methods.
In an experiment, the proposed ensemble is demonstrated to outperform a
state-of-the-art method under some conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1"&gt;Miki Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1"&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01828</id>
        <link href="http://arxiv.org/abs/2108.01828"/>
        <updated>2021-08-05T01:56:19.685Z</updated>
        <summary type="html"><![CDATA[Neural agents trained in reinforcement learning settings can learn to
communicate among themselves via discrete tokens, accomplishing as a team what
agents would be unable to do alone. However, the current standard of using
one-hot vectors as discrete communication tokens prevents agents from acquiring
more desirable aspects of communication such as zero-shot understanding.
Inspired by word embedding techniques from natural language processing, we
propose neural agent architectures that enables them to communicate via
discrete tokens derived from a learned, continuous space. We show in a decision
theoretic framework that our technique optimizes communication over a wide
range of scenarios, whereas one-hot tokens are only optimal under restrictive
assumptions. In self-play experiments, we validate that our trained agents
learn to cluster tokens in semantically-meaningful ways, allowing them
communicate in noisy environments where other techniques fail. Lastly, we
demonstrate both that agents using our method can effectively respond to novel
human communication and that humans can understand unlabeled emergent agent
communication, outperforming the use of one-hot communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1"&gt;Mycal Tucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Siddharth Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1"&gt;Dana Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1"&gt;Katia Sycara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Michael Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Julie Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Attention with Flow for Person Image Synthesis. (arXiv:2108.01823v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01823</id>
        <link href="http://arxiv.org/abs/2108.01823"/>
        <updated>2021-08-05T01:56:19.678Z</updated>
        <summary type="html"><![CDATA[Pose-guided person image synthesis aims to synthesize person images by
transforming reference images into target poses. In this paper, we observe that
the commonly used spatial transformation blocks have complementary advantages.
We propose a novel model by combining the attention operation with the
flow-based operation. Our model not only takes the advantage of the attention
operation to generate accurate target structures but also uses the flow-based
operation to sample realistic source textures. Both objective and subjective
experiments demonstrate the superiority of our model. Meanwhile, comprehensive
ablation studies verify our hypotheses and show the efficacy of the proposed
modules. Besides, additional experiments on the portrait image editing task
demonstrate the versatility of the proposed combination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yurui Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yubo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Thomas H. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01682</id>
        <link href="http://arxiv.org/abs/2108.01682"/>
        <updated>2021-08-05T01:56:19.671Z</updated>
        <summary type="html"><![CDATA[Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. (arXiv:2108.02035v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02035</id>
        <link href="http://arxiv.org/abs/2108.02035"/>
        <updated>2021-08-05T01:56:19.664Z</updated>
        <summary type="html"><![CDATA[Tuning pre-trained language models (PLMs) with task-specific prompts has been
a promising approach for text classification. Particularly, previous studies
suggest that prompt-tuning has remarkable superiority in the low-data scenario
over the generic fine-tuning methods with extra classifiers. The core idea of
prompt-tuning is to insert text pieces, i.e., template, to the input and
transform a classification problem into a masked language modeling problem,
where a crucial step is to construct a projection, i.e., verbalizer, between a
label space and a label word space. A verbalizer is usually handcrafted or
searched by gradient descent, which may lack coverage and bring considerable
bias and high variances to the results. In this work, we focus on incorporating
external knowledge into the verbalizer, forming a knowledgeable prompt-tuning
(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the
label word space of the verbalizer using external knowledge bases (KBs) and
refine the expanded label word space with the PLM itself before predicting with
the expanded label word space. Extensive experiments on zero and few-shot text
classification tasks demonstrate the effectiveness of knowledgeable
prompt-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huadong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Retinal Vessel Segmentation with Adversarial Learning and Transfer Normalization. (arXiv:2108.01821v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01821</id>
        <link href="http://arxiv.org/abs/2108.01821"/>
        <updated>2021-08-05T01:56:19.649Z</updated>
        <summary type="html"><![CDATA[Retinal vessel segmentation plays a key role in computer-aided screening,
diagnosis, and treatment of various cardiovascular and ophthalmic diseases.
Recently, deep learning-based retinal vessel segmentation algorithms have
achieved remarkable performance. However, due to the domain shift problem, the
performance of these algorithms often degrades when they are applied to new
data that is different from the training data. Manually labeling new data for
each test domain is often a time-consuming and laborious task. In this work, we
explore unsupervised domain adaptation in retinal vessel segmentation by using
entropy-based adversarial learning and transfer normalization layer to train a
segmentation network, which generalizes well across domains and requires no
annotation of the target domain. Specifically, first, an entropy-based
adversarial learning strategy is developed to reduce the distribution
discrepancy between the source and target domains while also achieving the
objective of entropy minimization on the target domain. In addition, a new
transfer normalization layer is proposed to further boost the transferability
of the deep network. It normalizes the features of each domain separately to
compensate for the domain distribution gap. Besides, it also adaptively selects
those feature channels that are more transferable between domains, thus further
enhancing the generalization performance of the network. We conducted extensive
experiments on three regular fundus image datasets and an ultra-widefield
fundus image dataset, and the results show that our approach yields significant
performance gains compared to other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lie Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaimin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Q/0/1/0/all/0/1"&gt;Qingyi Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:19.642Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01769</id>
        <link href="http://arxiv.org/abs/2108.01769"/>
        <updated>2021-08-05T01:56:19.634Z</updated>
        <summary type="html"><![CDATA[Previous work has shown that neural architectures are able to perform optical
music recognition (OMR) on monophonic and homophonic music with high accuracy.
However, piano and orchestral scores frequently exhibit polyphonic passages,
which add a second dimension to the task. Monophonic and homophonic music can
be described as homorhythmic, or having a single musical rhythm. Polyphonic
music, on the other hand, can be seen as having multiple rhythmic sequences, or
voices, concurrently. We first introduce a workflow for creating large-scale
polyphonic datasets suitable for end-to-end recognition from sheet music
publicly available on the MuseScore forum. We then propose two novel
formulations for end-to-end polyphonic OMR -- one treating the problem as a
type of multi-task binary classification, and the other treating it as
multi-sequence detection. Building upon the encoder-decoder architecture and an
image encoder proposed in past work on end-to-end OMR, we propose two novel
decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two
formulations. Finally, we compare the empirical performance of these end-to-end
approaches to polyphonic OMR and observe a new state-of-the-art performance
with our multi-sequence detection decoder, RNNDecoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1"&gt;Sachinda Edirisooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09332</id>
        <link href="http://arxiv.org/abs/2107.09332"/>
        <updated>2021-08-05T01:56:19.624Z</updated>
        <summary type="html"><![CDATA[Sentence-level relation extraction mainly aims to classify the relation
between two entities in a sentence. The sentence-level relation extraction
corpus often contains data that are difficult for the model to infer or noise
data. In this paper, we propose a curriculum learning-based relation extraction
model that splits data by difficulty and utilizes them for learning. In the
experiments with the representative sentence-level relation extraction
datasets, TACRED and Re-TACRED, the proposed method obtained an F1-score of
75.0% and 91.4% respectively, which are the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongsik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Harksoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10582</id>
        <link href="http://arxiv.org/abs/2012.10582"/>
        <updated>2021-08-05T01:56:19.617Z</updated>
        <summary type="html"><![CDATA[Previous neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yining Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1"&gt;Daniel Ciao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15715</id>
        <link href="http://arxiv.org/abs/2012.15715"/>
        <updated>2021-08-05T01:56:19.610Z</updated>
        <summary type="html"><![CDATA[Recent research on cross-lingual word embeddings has been dominated by
unsupervised mapping approaches that align monolingual embeddings. Such methods
critically rely on those embeddings having a similar structure, but it was
recently shown that the separate training in different languages causes
departures from this assumption. In this paper, we propose an alternative
approach that does not have this limitation, while requiring a weak seed
dictionary (e.g., a list of identical words) as the only form of supervision.
Rather than aligning two fixed embedding spaces, our method works by fixing the
target language embeddings, and learning a new set of embeddings for the source
language that are aligned with them. To that end, we use an extension of
skip-gram that leverages translated context words as anchor points, and
incorporates self-learning and iterative restarts to reduce the dependency on
the initial dictionary. Our approach outperforms conventional mapping methods
on bilingual lexicon induction, and obtains competitive results in the
downstream XNLI task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1"&gt;Aitor Ormazabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1"&gt;Aitor Soroa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01806</id>
        <link href="http://arxiv.org/abs/2108.01806"/>
        <updated>2021-08-05T01:56:19.602Z</updated>
        <summary type="html"><![CDATA[Furnishing and rendering an indoor scene is a common but tedious task for
interior design: an artist needs to observe the space, create a conceptual
design, build a 3D model, and perform rendering. In this paper, we introduce a
new problem of domain-specific image synthesis using generative modeling,
namely neural scene decoration. Given a photograph of an empty indoor space, we
aim to synthesize a new image of the same space that is fully furnished and
decorated. Neural scene decoration can be applied in practice to efficiently
generate conceptual but realistic interior designs, bypassing the traditional
multi-step and time-consuming pipeline. Our attempt to neural scene decoration
in this paper is a generative adversarial neural network that takes the input
photograph and directly produce the image of the desired furnishing and
decorations. Our network contains a novel image generator that transforms an
initial point-based object layout into a realistic photograph. We demonstrate
the performance of our proposed method by showing that it outperforms the
baselines built upon previous works on image translations both qualitatively
and quantitatively. Our user study further validates the plausibility and
aesthetics in the generated designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1"&gt;Hong-Wing Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1"&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1"&gt;Sai-Kit Yeung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:19.592Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-05T01:56:19.584Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReadOnce Transformers: Reusable Representations of Text for Transformers. (arXiv:2010.12854v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12854</id>
        <link href="http://arxiv.org/abs/2010.12854"/>
        <updated>2021-08-05T01:56:19.538Z</updated>
        <summary type="html"><![CDATA[We present ReadOnce Transformers, an approach to convert a transformer-based
model into one that can build an information-capturing, task-independent, and
compressed representation of text. The resulting representation is reusable
across different examples and tasks, thereby requiring a document shared across
many examples or tasks to only be \emph{read once}. This leads to faster
training and evaluation of models. Additionally, we extend standard
text-to-text transformer models to Representation+Text-to-text models, and
evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and
long-document summarization. Our one-time computed representation results in a
2x-5x speedup compared to standard text-to-text models, while the compression
also allows existing language models to handle longer documents without the
need for designing new pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shih-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06028</id>
        <link href="http://arxiv.org/abs/2007.06028"/>
        <updated>2021-08-05T01:56:19.521Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised speech pre-training method called TERA, which
stands for Transformer Encoder Representations from Alteration. Recent
approaches often learn by using a single auxiliary task like contrastive
prediction, autoregressive prediction, or masked reconstruction. Unlike
previous methods, we use alteration along three orthogonal axes to pre-train
Transformer Encoders on a large amount of unlabeled speech. The model learns
through the reconstruction of acoustic frames from their altered counterpart,
where we use a stochastic policy to alter along various dimensions: time,
frequency, and magnitude. TERA can be used for speech representations
extraction or fine-tuning with downstream models. We evaluate TERA on several
downstream tasks, including phoneme classification, keyword spotting, speaker
recognition, and speech recognition. We present a large-scale comparison of
various self-supervised models. TERA achieves strong performance in the
comparison by improving upon surface features and outperforming previous
models. In our experiments, we study the effect of applying different
alteration techniques, pre-training on more data, and pre-training on various
features. We analyze different model sizes and find that smaller models are
strong representation learners than larger models, while larger models are more
effective for downstream fine-tuning than smaller models. Furthermore, we show
the proposed method is transferable to downstream datasets not used in
pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Round Parsing-based Multiword Rules for Scientific OpenIE. (arXiv:2108.02074v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02074</id>
        <link href="http://arxiv.org/abs/2108.02074"/>
        <updated>2021-08-05T01:56:19.502Z</updated>
        <summary type="html"><![CDATA[Information extraction (IE) in scientific literature has facilitated many
down-stream tasks. OpenIE, which does not require any relation schema but
identifies a relational phrase to describe the relationship between a subject
and an object, is being a trending topic of IE in sciences. The subjects,
objects, and relations are often multiword expressions, which brings challenges
for methods to identify the boundaries of the expressions given very limited or
even no training data. In this work, we present a set of rules for extracting
structured information based on dependency parsing that can be applied to any
scientific dataset requiring no expert's annotation. Results on novel datasets
show the effectiveness of the proposed method. We discuss negative results as
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuebler_J/0/1/0/all/0/1"&gt;Joseph Kuebler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lingbo Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05150</id>
        <link href="http://arxiv.org/abs/2010.05150"/>
        <updated>2021-08-05T01:56:19.494Z</updated>
        <summary type="html"><![CDATA[While safe reinforcement learning (RL) holds great promise for many practical
applications like robotics or autonomous cars, current approaches require
specifying constraints in mathematical form. Such specifications demand domain
expertise, limiting the adoption of safe RL. In this paper, we propose learning
to interpret natural language constraints for safe RL. To this end, we first
introduce HazardWorld, a new multi-task benchmark that requires an agent to
optimize reward while not violating constraints specified in free-form text. We
then develop an agent with a modular architecture that can interpret and adhere
to such textual constraints while learning new tasks. Our model consists of (1)
a constraint interpreter that encodes textual constraints into spatial and
temporal representations of forbidden states, and (2) a policy network that
uses these representations to produce a policy achieving minimal constraint
violations during training. Across different domains in HazardWorld, we show
that our method achieves higher rewards (up to11x) and fewer constraint
violations (by 1.8x) compared to existing approaches. However, in terms of
absolute performance, HazardWorld still poses significant challenges for agents
to learn efficiently, motivating the need for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Michael Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1"&gt;Yinlam Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of multilingual BERT. (arXiv:2106.14625v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14625</id>
        <link href="http://arxiv.org/abs/2106.14625"/>
        <updated>2021-08-05T01:56:19.485Z</updated>
        <summary type="html"><![CDATA[This paper explains our participation in task 1 of the CASE 2021 shared task.
This task is about multilingual event extraction from news. We focused on
sub-task 4, event information extraction. This sub-task has a small training
dataset and we fine-tuned a multilingual BERT to solve this sub-task. We
studied the instability problem on the dataset and tried to mitigate it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouscarrat_L/0/1/0/all/0/1"&gt;L&amp;#xe9;o Bouscarrat&lt;/a&gt; (LIS, TALEP, QARMA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bonnefoy_A/0/1/0/all/0/1"&gt;Antoine Bonnefoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capponi_C/0/1/0/all/0/1"&gt;C&amp;#xe9;cile Capponi&lt;/a&gt; (LIS, QARMA), &lt;a href="http://arxiv.org/find/cs/1/au:+Ramisch_C/0/1/0/all/0/1"&gt;Carlos Ramisch&lt;/a&gt; (LIS, TALEP)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Pose Estimation of Illustrated Characters. (arXiv:2108.01819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01819</id>
        <link href="http://arxiv.org/abs/2108.01819"/>
        <updated>2021-08-05T01:56:19.461Z</updated>
        <summary type="html"><![CDATA[Human pose information is a critical component in many downstream image
processing tasks, such as activity recognition and motion tracking. Likewise, a
pose estimator for the illustrated character domain would provide a valuable
prior for assistive content creation tasks, such as reference pose retrieval
and automatic character animation. But while modern data-driven techniques have
substantially improved pose estimation performance on natural images, little
work has been done for illustrations. In our work, we bridge this domain gap by
efficiently transfer-learning from both domain-specific and task-specific
source models. Additionally, we upgrade and expand an existing illustrated pose
estimation dataset, and introduce two new datasets for classification and
segmentation subtasks. We then apply the resultant state-of-the-art character
pose estimator to solve the novel task of pose-guided illustration retrieval.
All data, models, and code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1"&gt;Matthias Zwicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01866</id>
        <link href="http://arxiv.org/abs/2108.01866"/>
        <updated>2021-08-05T01:56:19.453Z</updated>
        <summary type="html"><![CDATA[We present a novel pyramidal output representation to ensure parsimony with
our "specialize and fuse" process for semantic segmentation. A pyramidal
"output" representation consists of coarse-to-fine levels, where each level is
"specialize" in a different class distribution (e.g., more stuff than things
classes at coarser levels). Two types of pyramidal outputs (i.e., unity and
semantic pyramid) are "fused" into the final semantic output, where the unity
pyramid indicates unity-cells (i.e., all pixels in such cell share the same
semantic label). The process ensures parsimony by predicting a relatively small
number of labels for unity-cells (e.g., a large cell of grass) to build the
final semantic output. In addition to the "output" representation, we design a
coarse-to-fine contextual module to aggregate the "features" representation
from different levels. We validate the effectiveness of each key module in our
method through comprehensive ablation studies. Finally, our approach achieves
state-of-the-art performance on three widely-used semantic segmentation
datasets -- ADE20K, COCO-Stuff, and Pascal-Context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1"&gt;Chi-Wei Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hwann-Tzong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Foreground Learning for Weakly Supervised Localization and Detection. (arXiv:2108.01785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01785</id>
        <link href="http://arxiv.org/abs/2108.01785"/>
        <updated>2021-08-05T01:56:19.446Z</updated>
        <summary type="html"><![CDATA[Modern deep learning models require large amounts of accurately annotated
data, which is often difficult to satisfy. Hence, weakly supervised tasks,
including weakly supervised object localization~(WSOL) and detection~(WSOD),
have recently received attention in the computer vision community. In this
paper, we motivate and propose the weakly supervised foreground learning (WSFL)
task by showing that both WSOL and WSOD can be greatly improved if groundtruth
foreground masks are available. More importantly, we propose a complete WSFL
pipeline with low computational cost, which generates pseudo boxes, learns
foreground masks, and does not need any localization annotations. With the help
of foreground masks predicted by our WSFL model, we achieve 72.97% correct
localization accuracy on CUB for WSOL, and 55.7% mean average precision on
VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL
model also shows excellent transfer ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen-Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer with Progressive Sampling. (arXiv:2108.01684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01684</id>
        <link href="http://arxiv.org/abs/2108.01684"/>
        <updated>2021-08-05T01:56:19.439Z</updated>
        <summary type="html"><![CDATA[Transformers with powerful global relation modeling abilities have been
introduced to fundamental computer vision tasks recently. As a typical example,
the Vision Transformer (ViT) directly applies a pure transformer architecture
on image classification, by simply splitting images into tokens with a fixed
length, and employing transformers to learn relations between these tokens.
However, such naive tokenization could destruct object structures, assign grids
to uninterested regions such as background, and introduce interference signals.
To mitigate the above issues, in this paper, we propose an iterative and
progressive sampling strategy to locate discriminative regions. At each
iteration, embeddings of the current sampling step are fed into a transformer
encoder layer, and a group of sampling offsets is predicted to update the
sampling locations for the next step. The progressive sampling is
differentiable. When combined with the Vision Transformer, the obtained PS-ViT
network can adaptively learn where to look. The proposed PS-ViT is both
effective and efficient. When trained from scratch on ImageNet, PS-ViT performs
3.8% higher than the vanilla ViT in terms of top-1 accuracy with about
$4\times$ fewer parameters and $10\times$ fewer FLOPs. Code is available at
https://github.com/yuexy/PS-ViT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiaoyu Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shuyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Meng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01775</id>
        <link href="http://arxiv.org/abs/2108.01775"/>
        <updated>2021-08-05T01:56:19.430Z</updated>
        <summary type="html"><![CDATA[This paper presents solo-learn, a library of self-supervised methods for
visual representation learning. Implemented in Python, using Pytorch and
Pytorch lightning, the library fits both research and industry needs by
featuring distributed training pipelines with mixed-precision, faster data
loading via Nvidia DALI, online linear evaluation for better prototyping, and
many additional training tricks. Our goal is to provide an easy-to-use library
comprising a large amount of Self-supervised Learning (SSL) methods, that can
be easily extended and fine-tuned by the community. solo-learn opens up avenues
for exploiting large-budget SSL solutions on inexpensive smaller
infrastructures and seeks to democratize SSL by making it accessible to all.
The source code is available at https://github.com/vturrisi/solo-learn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1"&gt;Victor G. Turrisi da Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1"&gt;Moin Nabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Armour: Generalizable Compact Self-Attention for Vision Transformers. (arXiv:2108.01778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01778</id>
        <link href="http://arxiv.org/abs/2108.01778"/>
        <updated>2021-08-05T01:56:19.411Z</updated>
        <summary type="html"><![CDATA[Attention-based transformer networks have demonstrated promising potential as
their applications extend from natural language processing to vision. However,
despite the recent improvements, such as sub-quadratic attention approximation
and various training enhancements, the compact vision transformers to date
using the regular attention still fall short in comparison with its convnet
counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and}
\textit{throughput}. This paper introduces a compact self-attention mechanism
that is fundamental and highly generalizable. The proposed method reduces
redundancy and improves efficiency on top of the existing attention
optimizations. We show its drop-in applicability for both the regular attention
mechanism and some most recent variants in vision transformers. As a result, we
produced smaller and faster models with the same or better accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingchuan Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acyclic and Cyclic Reversing Computations in Petri Nets. (arXiv:2108.02167v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02167</id>
        <link href="http://arxiv.org/abs/2108.02167"/>
        <updated>2021-08-05T01:56:19.404Z</updated>
        <summary type="html"><![CDATA[Reversible computations constitute an unconventional form of computing where
any sequence of performed operations can be undone by executing in reverse
order at any point during a computation. It has been attracting increasing
attention as it provides opportunities for low-power computation, being at the
same time essential or eligible in various applications. In recent work, we
have proposed a structural way of translating Reversing Petri Nets (RPNs) - a
type of Petri nets that embeds reversible computation, to bounded Coloured
Petri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry
data values. Three reversing semantics are possible in RPNs: backtracking
(reversing of the lately executed action), causal reversing (action can be
reversed only when all its effects have been undone) and out of causal
reversing (any previously performed action can be reversed). In this paper, we
extend the RPN to CPN translation with formal proofs of correctness. Moreover,
the possibility of introduction of cycles to RPNs is discussed. We analyze
which type of cycles could be allowed in RPNs to ensure consistency with the
current semantics. It emerged that the most interesting case related to cycles
in RPNs occurs in causal semantics, where various interpretations of dependency
result in different net's behaviour during reversing. Three definitions of
dependence are presented and discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barylska_K/0/1/0/all/0/1"&gt;Kamila Barylska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogolinska_A/0/1/0/all/0/1"&gt;Anna Gogoli&amp;#x144;ska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Biologically Plausible Parser. (arXiv:2108.02189v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02189</id>
        <link href="http://arxiv.org/abs/2108.02189"/>
        <updated>2021-08-05T01:56:19.396Z</updated>
        <summary type="html"><![CDATA[We describe a parser of English effectuated by biologically plausible neurons
and synapses, and implemented through the Assembly Calculus, a recently
proposed computational framework for cognitive function. We demonstrate that
this device is capable of correctly parsing reasonably nontrivial sentences.
While our experiments entail rather simple sentences in English, our results
suggest that the parser can be extended beyond what we have implemented, to
several directions encompassing much of language. For example, we present a
simple Russian version of the parser, and discuss how to handle recursion,
embedding, and polysemy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitropolsky_D/0/1/0/all/0/1"&gt;Daniel Mitropolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1"&gt;Michael J. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1"&gt;Christos H. Papadimitriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01248</id>
        <link href="http://arxiv.org/abs/1810.01248"/>
        <updated>2021-08-05T01:56:19.261Z</updated>
        <summary type="html"><![CDATA[Deep learning researches on the transformation problems for image and text
have raised great attention. However, present methods for music feature
transfer using neural networks are far from practical application. In this
paper, we initiate a novel system for transferring the texture of music, and
release it as an open source project. Its core algorithm is composed of a
converter which represents sounds as texture spectra, a corresponding
reconstructor and a feed-forward transfer network. We evaluate this system from
multiple perspectives, and experimental results reveal that it achieves
convincing results in both sound effects and computational performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xutan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Faqiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yidan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02147</id>
        <link href="http://arxiv.org/abs/2108.02147"/>
        <updated>2021-08-05T01:56:19.253Z</updated>
        <summary type="html"><![CDATA[Video captioning is an essential technology to understand scenes and describe
events in natural language. To apply it to real-time monitoring, a system needs
not only to describe events accurately but also to produce the captions as soon
as possible. Low-latency captioning is needed to realize such functionality,
but this research area for online video captioning has not been pursued yet.
This paper proposes a novel approach to optimize each caption's output timing
based on a trade-off between latency and caption quality. An audio-visual
Trans-former is trained to generate ground-truth captions using only a small
portion of all video frames, and to mimic outputs of a pre-trained Transformer
to which all the frames are given. A CNN-based timing detector is also trained
to detect a proper output timing, where the captions generated by the two
Trans-formers become sufficiently close to each other. With the jointly trained
Transformer and timing detector, a caption can be generated in the early stages
of an event-triggered video clip, as soon as an event happens or when it can be
forecasted. Experiments with the ActivityNet Captions dataset show that our
approach achieves 94% of the caption quality of the upper bound given by the
pre-trained Transformer using the entire video clips, using only 28% of frames
from the beginning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1"&gt;Chiori Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\times$ Compression and 2.7$\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02191</id>
        <link href="http://arxiv.org/abs/2108.02191"/>
        <updated>2021-08-05T01:56:19.246Z</updated>
        <summary type="html"><![CDATA[Deep learning for recommendation data is the one of the most pervasive and
challenging AI workload in recent times. State-of-the-art recommendation models
are one of the largest models rivalling the likes of GPT-3 and Switch
Transformer. Challenges in deep learning recommendation models (DLRM) stem from
learning dense embeddings for each of the categorical values. These embedding
tables in industrial scale models can be as large as hundreds of terabytes.
Such large models lead to a plethora of engineering challenges, not to mention
prohibitive communication overheads, and slower training and inference times.
Of these, slower inference time directly impacts user experience. Model
compression for DLRM is gaining traction and the community has recently shown
impressive compression results. In this paper, we present Random Offset Block
Embedding Array (ROBE) as a low memory alternative to embedding tables which
provide orders of magnitude reduction in memory usage while maintaining
accuracy and boosting execution speed. ROBE is a simple fundamental approach in
improving both cache performance and the variance of randomized hashing, which
could be of independent interest in itself. We demonstrate that we can
successfully train DLRM models with same accuracy while using $1000 \times$
less memory. A $1000\times$ compressed model directly results in faster
inference without any engineering. In particular, we show that we can train
DLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of
0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model
of 100GB while achieving about $2.7\times$ (170\%) improvement in inference
throughput.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1"&gt;Aditya Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1"&gt;Li Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08978</id>
        <link href="http://arxiv.org/abs/2009.08978"/>
        <updated>2021-08-05T01:56:19.213Z</updated>
        <summary type="html"><![CDATA[Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models: we introduce it in a multi-objective approach to
traditionally time-unaware recommender systems and confirm its advantage via
the proposed evaluation protocol. Finally, we validate that the Pareto Fronts
obtained with the added objective dominate those produced by state-of-the-art
models that are only optimized for accuracy on three real-world publicly
available datasets. The results show that including our temporal objective can
improve recall@20 by up to 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:19.202Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:19.194Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01879</id>
        <link href="http://arxiv.org/abs/2108.01879"/>
        <updated>2021-08-05T01:56:19.186Z</updated>
        <summary type="html"><![CDATA[This paper introduces Summary Explorer, a new tool to support the manual
inspection of text summarization systems by compiling the outputs of
55~state-of-the-art single document summarization approaches on three benchmark
datasets, and visually exploring them during a qualitative assessment. The
underlying design of the tool considers three well-known summary quality
criteria (coverage, faithfulness, and position bias), encapsulated in a guided
assessment based on tailored visualizations. The tool complements existing
approaches for locally debugging summarization models and improves upon them.
The tool is available at https://tldr.webis.de/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1"&gt;Shahbaz Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousef_T/0/1/0/all/0/1"&gt;Tariq Yousef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1"&gt;Khalid Al-Khatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janicke_S/0/1/0/all/0/1"&gt;Stefan J&amp;#xe4;nicke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12822</id>
        <link href="http://arxiv.org/abs/2104.12822"/>
        <updated>2021-08-05T01:56:19.168Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1"&gt;Martin Milenkoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Music Relistening Behavior Using the ACT-R Framework. (arXiv:2108.02138v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02138</id>
        <link href="http://arxiv.org/abs/2108.02138"/>
        <updated>2021-08-05T01:56:19.159Z</updated>
        <summary type="html"><![CDATA[Providing suitable recommendations is of vital importance to improve the user
satisfaction of music recommender systems. Here, users often listen to the same
track repeatedly and appreciate recommendations of the same song multiple
times. Thus, accounting for users' relistening behavior is critical for music
recommender systems. In this paper, we describe a psychology-informed approach
to model and predict music relistening behavior that is inspired by studies in
music psychology, which relate music preferences to human memory. We adopt a
well-established psychological theory of human cognition that models the
operations of human memory, i.e., Adaptive Control of Thought-Rational (ACT-R).
In contrast to prior work, which uses only the base-level component of ACT-R,
we utilize five components of ACT-R, i.e., base-level, spreading, partial
matching, valuation, and noise, to investigate the effect of five factors on
music relistening behavior: (i) recency and frequency of prior exposure to
tracks, (ii) co-occurrence of tracks, (iii) the similarity between tracks, (iv)
familiarity with tracks, and (v) randomness in behavior. On a dataset of 1.7
million listening events from Last.fm, we evaluate the performance of our
approach by sequentially predicting the next track(s) in user sessions. We find
that recency and frequency of prior exposure to tracks is an effective
predictor of relistening behavior. Besides, considering the co-occurrence of
tracks and familiarity with tracks further improves performance in terms of
R-precision. We hope that our work inspires future research on the merits of
considering cognitive aspects of memory retrieval to model and predict complex
user behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1"&gt;Markus Reiter-Haas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parada_Cabaleiro_E/0/1/0/all/0/1"&gt;Emilia Parada-Cabaleiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motamedi_E/0/1/0/all/0/1"&gt;Elham Motamedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tkalcic_M/0/1/0/all/0/1"&gt;Marko Tkalcic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1"&gt;Elisabeth Lex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.01949</id>
        <link href="http://arxiv.org/abs/2108.01949"/>
        <updated>2021-08-05T01:56:19.146Z</updated>
        <summary type="html"><![CDATA[Media is evolving from traditional linear narratives to personalised
experiences, where control over information (or how it is presented) is given
to individual audience members. Measuring and understanding audience engagement
with this media is important in at least two ways: (1) a post-hoc understanding
of how engaged audiences are with the content will help production teams learn
from experience and improve future productions; (2), this type of media has
potential for real-time measures of engagement to be used to enhance the user
experience by adapting content on-the-fly. Engagement is typically measured by
asking samples of users to self-report, which is time consuming and expensive.
In some domains, however, interaction data have been used to infer engagement.
Fortuitously, the nature of interactive media facilitates a much richer set of
interaction data than traditional media; our research aims to understand if
these data can be used to infer audience engagement. In this paper, we report a
study using data captured from audience interactions with an interactive TV
show to model and predict engagement. We find that temporal metrics, including
overall time spent on the experience and the interval between events, are
predictive of engagement. The results demonstrate that interaction data can be
used to infer users' engagement during and after an experience, and the
proposed techniques are relevant to better understand audience preference and
responses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1"&gt;Jonathan Carlton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andy Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1"&gt;Caroline Jay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1"&gt;John Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v4 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12148</id>
        <link href="http://arxiv.org/abs/2009.12148"/>
        <updated>2021-08-05T01:56:19.133Z</updated>
        <summary type="html"><![CDATA[Hashing plays an important role in information retrieval, due to its low
storage and high speed of processing. Among the techniques available in the
literature, multi-modal hashing, which can encode heterogeneous multi-modal
features into compact hash codes, has received particular attention. Most of
the existing multi-modal hashing methods adopt the fixed weighting factors to
fuse multiple modalities for any query data, which cannot capture the variation
of different queries. Besides, many methods introduce hyper-parameters to
balance many regularization terms that make the optimization harder. Meanwhile,
it is time-consuming and labor-intensive to set proper parameter values. The
limitations may significantly hinder their promotion in real applications. In
this paper, we propose a simple, yet effective method that is inspired by the
Hadamard matrix. The proposed method captures the multi-modal feature
information in an adaptive manner and preserves the discriminative semantic
information in the hash codes. Our framework is flexible and involves a very
few hyper-parameters. Extensive experimental results show the method is
effective and achieves superior performance compared to state-of-the-art
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1"&gt;Zhenqiu Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01887</id>
        <link href="http://arxiv.org/abs/2108.01887"/>
        <updated>2021-08-05T01:56:19.125Z</updated>
        <summary type="html"><![CDATA[Despite the success of multilingual sequence-to-sequence pretraining, most
existing approaches rely on monolingual corpora, and do not make use of the
strong cross-lingual signal contained in parallel data. In this paper, we
present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence
models), which extends the conventional denoising objective used to train these
models by (i) replacing words in the noised sequence according to a
multilingual dictionary, and (ii) predicting the reference translation
according to a parallel corpus instead of recovering the original sequence. Our
experiments on machine translation and cross-lingual natural language inference
show an average improvement of 2.0 BLEU points and 6.7 accuracy points from
integrating parallel data into pretraining, respectively, obtaining results
that are competitive with several popular models at a fraction of their
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1"&gt;Machel Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analytical study of content and contexts of keywords on physics. (arXiv:2108.01915v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01915</id>
        <link href="http://arxiv.org/abs/2108.01915"/>
        <updated>2021-08-05T01:56:19.116Z</updated>
        <summary type="html"><![CDATA[This paper analysed author-assigned and title keywords into constituent words
collected from 769 articles published in the journal Low Temperature Physics
since the year 2006 to 2010. The total number of distinct keywords over the
said time span has been found as 1155, which have been analyzed into 869
numbers of single words having total frequency of occurrence of 2287. The
single words obtained from keywords have been categorized in four broad
classes, viz. eponymous word, form word, acronym and semantic word. A semantic
word bears several contexts and thus may be considered as relevant in several
other subject areas. These probable relevant subject areas have been found with
the aid of two popular online reference tools. The semantic words are further
categorized in twelve classes according to their contexts. Some parameters have
been defined on the basis of associations among the words and formation of
keywords in consequence, i.e. Word Association Density, Word Association
Coefficient and Keyword Formation Density. The values of these parameters have
been observed for different word categories. The statistics of word association
tending keyword formation would be known from this study. The allied subject
domains also become predictable from this study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1"&gt;Bidyarthi Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02059</id>
        <link href="http://arxiv.org/abs/2108.02059"/>
        <updated>2021-08-05T01:56:19.076Z</updated>
        <summary type="html"><![CDATA[For an image with multiple scene texts, different people may be interested in
different text information. Current text-aware image captioning models are not
able to generate distinctive captions according to various information needs.
To explore how to generate personalized text-aware captions, we define a new
challenging task, namely Question-controlled Text-aware Image Captioning
(Qc-TextCap). With questions as control signals, this task requires models to
understand questions, find related scene texts and describe them together with
objects fluently in human language. Based on two existing text-aware captioning
datasets, we automatically construct two datasets, ControlTextCaps and
ControlVizWiz to support the task. We propose a novel Geometry and Question
Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to
fuse region-level object features and region-level scene text features with
considering spatial relationships. Then, we design a Question-guided Encoder to
select the most relevant visual features for each question. Finally, GQAM
generates a personalized text-aware caption with a Multimodal Decoder. Our
model achieves better captioning performance and question answering ability
than carefully designed baselines on both two datasets. With questions as
control signals, our model generates more informative and diverse captions than
the state-of-the-art text-aware captioning model. Our code and datasets are
publicly available at https://github.com/HAWLYQ/Qc-TextCap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-08-05T01:56:19.065Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management. (arXiv:2108.01764v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01764</id>
        <link href="http://arxiv.org/abs/2108.01764"/>
        <updated>2021-08-05T01:56:19.054Z</updated>
        <summary type="html"><![CDATA[Recent advances in Natural Language Processing (NLP), and specifically
automated Question Answering (QA) systems, have demonstrated both impressive
linguistic fluency and a pernicious tendency to reflect social biases. In this
study, we introduce Q-Pain, a dataset for assessing bias in medical QA in the
context of pain management, one of the most challenging forms of clinical
decision-making. Along with the dataset, we propose a new, rigorous framework,
including a sample experimental design, to measure the potential biases present
when making treatment decisions. We demonstrate its use by assessing two
reference Question-Answering systems, GPT-2 and GPT-3, and find statistically
significant differences in treatment between intersectional race-gender
subgroups, thus reaffirming the risks posed by AI in medical settings, and the
need for datasets like ours to ensure safety before medical AI applications are
deployed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loge_C/0/1/0/all/0/1"&gt;C&amp;#xe9;cile Log&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_E/0/1/0/all/0/1"&gt;Emily Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadey_D/0/1/0/all/0/1"&gt;David Yaw Amoah Dadey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation. (arXiv:2108.01812v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01812</id>
        <link href="http://arxiv.org/abs/2108.01812"/>
        <updated>2021-08-05T01:56:19.038Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pretrained language models (LMs) is a popular approach to
automatic speech recognition (ASR) error detection during post-processing.
While error detection systems often take advantage of statistical language
archetypes captured by LMs, at times the pretrained knowledge can hinder error
detection performance. For instance, presence of speech disfluencies might
confuse the post-processing system into tagging disfluent but accurate
transcriptions as ASR errors. Such confusion occurs because both error
detection and disfluency detection tasks attempt to identify tokens at
statistically unlikely positions. This paper proposes a scheme to improve
existing LM-based ASR error detection systems, both in terms of detection
scores and resilience to such distracting auxiliary tasks. Our approach adopts
the popular mixup method in text feature space and can be utilized with any
black-box ASR output. To demonstrate the effectiveness of our method, we
conduct post-processing experiments with both traditional and end-to-end ASR
systems (both for English and Korean languages) with 5 different speech
corpora. We find that our method improves both ASR error detection F 1 scores
and reduces the number of correctly transcribed disfluencies wrongly detected
as ASR errors. Finally, we suggest methods to utilize resulting LMs directly in
semi-supervised ASR training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Dongchan Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_S/0/1/0/all/0/1"&gt;Sangyoun Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Subong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazakova_A/0/1/0/all/0/1"&gt;Alena Kazakova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihwa Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled Text Generation as Continuous Optimization with Multiple Constraints. (arXiv:2108.01850v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01850</id>
        <link href="http://arxiv.org/abs/2108.01850"/>
        <updated>2021-08-05T01:56:19.015Z</updated>
        <summary type="html"><![CDATA[As large-scale language model pretraining pushes the state-of-the-art in text
generation, recent work has turned to controlling attributes of the text such
models generate. While modifying the pretrained models via fine-tuning remains
the popular approach, it incurs a significant computational cost and can be
infeasible due to lack of appropriate data. As an alternative, we propose
MuCoCO -- a flexible and modular algorithm for controllable inference from
pretrained models. We formulate the decoding process as an optimization problem
which allows for multiple attributes we aim to control to be easily
incorporated as differentiable constraints to the optimization. By relaxing
this discrete optimization to a continuous one, we make use of Lagrangian
multipliers and gradient-descent based techniques to generate the desired text.
We evaluate our approach on controllable machine translation and style transfer
with multiple sentence-level attributes and observe significant improvements
over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1"&gt;Eric Malmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1"&gt;Aliaksei Severyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02050</id>
        <link href="http://arxiv.org/abs/2108.02050"/>
        <updated>2021-08-05T01:56:19.003Z</updated>
        <summary type="html"><![CDATA[Most current image captioning systems focus on describing general image
content, and lack background knowledge to deeply understand the image, such as
exact named entities or concrete events. In this work, we focus on the
entity-aware news image captioning task which aims to generate informative
captions by leveraging the associated news articles to provide background
knowledge about the target image. However, due to the length of news articles,
previous works only employ news articles at the coarse article or sentence
level, which are not fine-grained enough to refine relevant events and choose
named entities accurately. To overcome these limitations, we propose an
Information Concentrated Entity-aware news image CAPtioning (ICECAP) model,
which progressively concentrates on relevant textual information within the
corresponding news article from the sentence level to the word level. Our model
first creates coarse concentration on relevant sentences using a cross-modality
retrieval model and then generates captions by further concentrating on
relevant words within the sentences. Extensive experiments on both BreakingNews
and GoodNews datasets demonstrate the effectiveness of our proposed method,
which outperforms other state-of-the-arts. The code of ICECAP is publicly
available at https://github.com/HAWLYQ/ICECAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. (arXiv:2108.01861v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01861</id>
        <link href="http://arxiv.org/abs/2108.01861"/>
        <updated>2021-08-05T01:56:18.987Z</updated>
        <summary type="html"><![CDATA[In this shared task, we seek the participating teams to investigate the
factors influencing the quality of the code-mixed text generation systems. We
synthetically generate code-mixed Hinglish sentences using two distinct
approaches and employ human annotators to rate the generation quality. We
propose two subtasks, quality rating prediction and annotators' disagreement
prediction of the synthetic Hinglish dataset. The proposed subtasks will put
forward the reasoning and explanation of the factors influencing the quality
and human perception of the code-mixed text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01682</id>
        <link href="http://arxiv.org/abs/2108.01682"/>
        <updated>2021-08-05T01:56:18.976Z</updated>
        <summary type="html"><![CDATA[Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Counterfactual Generation for Fair Hate Speech Detection. (arXiv:2108.01721v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01721</id>
        <link href="http://arxiv.org/abs/2108.01721"/>
        <updated>2021-08-05T01:56:18.964Z</updated>
        <summary type="html"><![CDATA[Bias mitigation approaches reduce models' dependence on sensitive features of
data, such as social group tokens (SGTs), resulting in equal predictions across
the sensitive features. In hate speech detection, however, equalizing model
predictions may ignore important differences among targeted social groups, as
hate speech can contain stereotypical language specific to each SGT. Here, to
take the specific language about each SGT into account, we rely on
counterfactual fairness and equalize predictions among counterfactuals,
generated by changing the SGTs. Our method evaluates the similarity in sentence
likelihoods (via pre-trained language models) among counterfactuals, to treat
SGTs equally only within interchangeable contexts. By applying logit pairing to
equalize outcomes on the restricted set of counterfactuals for each instance,
we improve fairness metrics while preserving model performance on hate speech
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1"&gt;Aida Mostafazadeh Davani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omrani_A/0/1/0/all/0/1"&gt;Ali Omrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_B/0/1/0/all/0/1"&gt;Brendan Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atari_M/0/1/0/all/0/1"&gt;Mohammad Atari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Morteza Dehghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextCNN with Attention for Text Classification. (arXiv:2108.01921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01921</id>
        <link href="http://arxiv.org/abs/2108.01921"/>
        <updated>2021-08-05T01:56:18.943Z</updated>
        <summary type="html"><![CDATA[The vast majority of textual content is unstructured, making automated
classification an important task for many applications. The goal of text
classification is to automatically classify text documents into one or more
predefined categories. Recently proposed simple architectures for text
classification such as Convolutional Neural Networks for Sentence
Classification by Kim, Yoon showed promising results. In this paper, we propose
incorporating an attention mechanism into the network to boost its performance,
we also propose WordRank for vocabulary selection to reduce the network
embedding parameters and speed up training with minimum accuracy loss. By
adopting the proposed ideas TextCNN accuracy on 20News increased from 94.79 to
96.88, moreover, the number of parameters for the embedding layer can be
reduced substantially with little accuracy loss by using WordRank. By using
WordRank for vocabulary selection we can reduce the number of parameters by
more than 5x from 7.9M to 1.5M, and the accuracy will only decrease by 1.2%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1"&gt;Ibrahim Alshubaily&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dyn-ASR: Compact, Multilingual Speech Recognition via Spoken Language and Accent Identification. (arXiv:2108.02034v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02034</id>
        <link href="http://arxiv.org/abs/2108.02034"/>
        <updated>2021-08-05T01:56:18.924Z</updated>
        <summary type="html"><![CDATA[Running automatic speech recognition (ASR) on edge devices is non-trivial due
to resource constraints, especially in scenarios that require supporting
multiple languages. We propose a new approach to enable multilingual speech
recognition on edge devices. This approach uses both language identification
and accent identification to select one of multiple monolingual ASR models
on-the-fly, each fine-tuned for a particular accent. Initial results for both
recognition performance and resource usage are promising with our approach
using less than 1/12th of the memory consumed by other solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghangam_S/0/1/0/all/0/1"&gt;Sangeeta Ghangam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitenack_D/0/1/0/all/0/1"&gt;Daniel Whitenack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1"&gt;Joshua Nemecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text Detection. (arXiv:2108.01809v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.01809</id>
        <link href="http://arxiv.org/abs/2108.01809"/>
        <updated>2021-08-05T01:56:18.908Z</updated>
        <summary type="html"><![CDATA[The latest trend in the bottom-up perspective for arbitrary-shape scene text
detection is to reason the links between text segments using Graph
Convolutional Network (GCN). Notwithstanding, the performance of the best
performing bottom-up method is still inferior to that of the best performing
top-down method even with the help of GCN. We argue that this is not mainly
caused by the limited feature capturing ability of the text proposal backbone
or GCN, but by their failure to make a full use of visual-relational features
for suppressing false detection, as well as the sub-optimal route-finding
mechanism used for grouping text segments. In this paper, we revitalize the
classic text detection frameworks by aggregating the visual-relational features
of text with two effective false positive/negative suppression mechanisms.
First, dense overlapping text segments depicting the `characterness' and
`streamline' of text are generated for further relational reasoning and weakly
supervised segment classification. Here, relational graph features are used for
suppressing false positives/negatives. Then, to fuse the relational features
with visual features, a Location-Aware Transfer (LAT) module is designed to
transfer text's relational features into visual compatible features with a Fuse
Decoding (FD) module to enhance the representation of text regions for the
second step suppression. Finally, a novel multiple-text-map-aware
contour-approximation strategy is developed, instead of the widely-used
route-finding process. Experiments conducted on five benchmark datasets, i.e.,
CTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our
method outperforms the state-of-the-art performance when being embedded in a
classic text detection framework, which revitalises the superb strength of the
bottom-up methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengpei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1"&gt;Wenjing Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1"&gt;Tingcheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruomei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan-fang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangjian He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-complexity Scaling Methods for DCT-II Approximations. (arXiv:2108.02119v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02119</id>
        <link href="http://arxiv.org/abs/2108.02119"/>
        <updated>2021-08-05T01:56:18.869Z</updated>
        <summary type="html"><![CDATA[This paper introduces a collection of scaling methods for generating
$2N$-point DCT-II approximations based on $N$-point low-complexity
transformations. Such scaling is based on the Hou recursive matrix
factorization of the exact $2N$-point DCT-II matrix. Encompassing the widely
employed Jridi-Alfalou-Meher scaling method, the proposed techniques are shown
to produce DCT-II approximations that outperform the transforms resulting from
the JAM scaling method according to total error energy and mean squared error.
Orthogonality conditions are derived and an extensive error analysis based on
statistical simulation demonstrates the good performance of the introduced
scaling methods. A hardware implementation is also provided demonstrating the
competitiveness of the proposed methods when compared to the JAM scaling
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Coelho_D/0/1/0/all/0/1"&gt;D. F. G. Coelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1"&gt;R. J. Cintra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Madanayake_A/0/1/0/all/0/1"&gt;A. Madanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1"&gt;S. Perera&lt;/a&gt;</name>
        </author>
    </entry>
</feed>