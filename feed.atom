<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-07-16T02:00:29.293Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[The Causal-Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00793</id>
        <link href="http://arxiv.org/abs/2107.00793"/>
        <updated>2021-07-16T00:48:26.348Z</updated>
        <summary type="html"><![CDATA[One of the central elements of any causal inference is an object called
structural causal model (SCM), which represents a collection of mechanisms and
exogenous sources of random variation of the system under investigation (Pearl,
2000). An important property of many kinds of neural networks is universal
approximability: the ability to approximate any function to arbitrary
precision. Given this property, one may be tempted to surmise that a collection
of neural nets is capable of learning any SCM by training on data generated by
that SCM. In this paper, we show this is not the case by disentangling the
notions of expressivity and learnability. Specifically, we show that the causal
hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits
of what can be learned from data, still holds for neural models. For instance,
an arbitrarily complex and expressive neural net is unable to predict the
effects of interventions given observational data alone. Given this result, we
introduce a special type of SCM called a neural causal model (NCM), and
formalize a new type of inductive bias to encode structural constraints
necessary for performing causal inferences. Building on this new class of
models, we focus on solving two canonical tasks found in the literature known
as causal identification and estimation. Leveraging the neural toolbox, we
develop an algorithm that is both sufficient and necessary to determine whether
a causal effect can be learned from data (i.e., causal identifiability); it
then estimates the effect whenever identifiability holds (causal estimation).
Simulations corroborate the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1"&gt;Kevin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kai-Zhan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1"&gt;Elias Bareinboim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-07-16T00:48:26.334Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05825</id>
        <link href="http://arxiv.org/abs/2106.05825"/>
        <updated>2021-07-16T00:48:26.309Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are employed in an increasing number of
applications, some of which are safety critical. Unfortunately, DNNs are known
to be vulnerable to so-called adversarial attacks that manipulate inputs to
cause incorrect results that can be beneficial to an attacker or damaging to
the victim. Multiple defenses have been proposed to increase the robustness of
DNNs. In general, these defenses have high overhead, some require
attack-specific re-training of the model or careful tuning to adapt to
different attacks.

This paper presents HASI, a hardware-accelerated defense that uses a process
we call stochastic inference to detect adversarial inputs. We show that by
carefully injecting noise into the model at inference time, we can
differentiate adversarial inputs from benign ones. HASI uses the output
distribution characteristics of noisy inference compared to a non-noisy
reference to detect adversarial inputs. We show an adversarial detection rate
of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches, with a much lower
overhead. We demonstrate two software/hardware-accelerated co-designs, which
reduces the performance impact of stochastic inference to 1.58X-2X relative to
the unprotected baseline, compared to 15X-20X overhead for a software-only GPU
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concept drift detection and adaptation for federated and continual learning. (arXiv:2105.13309v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13309</id>
        <link href="http://arxiv.org/abs/2105.13309"/>
        <updated>2021-07-16T00:48:26.297Z</updated>
        <summary type="html"><![CDATA[Smart devices, such as smartphones, wearables, robots, and others, can
collect vast amounts of data from their environment. This data is suitable for
training machine learning models, which can significantly improve their
behavior, and therefore, the user experience. Federated learning is a young and
popular framework that allows multiple distributed devices to train deep
learning models collaboratively while preserving data privacy. Nevertheless,
this approach may not be optimal for scenarios where data distribution is
non-identical among the participants or changes over time, causing what is
known as concept drift. Little research has yet been done in this field, but
this kind of situation is quite frequent in real life and poses new challenges
to both continual and federated learning. Therefore, in this work, we present a
new method, called Concept-Drift-Aware Federated Averaging (CDA-FedAvg). Our
proposal is an extension of the most popular federated algorithm, Federated
Averaging (FedAvg), enhancing it for continual adaptation under concept drift.
We empirically demonstrate the weaknesses of regular FedAvg and prove that
CDA-FedAvg outperforms it in this type of scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casado_F/0/1/0/all/0/1"&gt;Fernando E. Casado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lema_D/0/1/0/all/0/1"&gt;Dylan Lema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Criado_M/0/1/0/all/0/1"&gt;Marcos F. Criado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iglesias_R/0/1/0/all/0/1"&gt;Roberto Iglesias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regueiro_C/0/1/0/all/0/1"&gt;Carlos V. Regueiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barro_S/0/1/0/all/0/1"&gt;Sen&amp;#xe9;n Barro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08177</id>
        <link href="http://arxiv.org/abs/2006.08177"/>
        <updated>2021-07-16T00:48:26.226Z</updated>
        <summary type="html"><![CDATA[The dissimilarity mixture autoencoder (DMAE) is a neural network model for
feature-based clustering that incorporates a flexible dissimilarity function
and can be integrated into any kind of deep learning architecture. It
internally represents a dissimilarity mixture model (DMM) that extends
classical methods like K-Means, Gaussian mixture models, or Bregman clustering
to any convex and differentiable dissimilarity function through the
reinterpretation of probabilities as neural network representations. DMAE can
be integrated with deep learning architectures into end-to-end models, allowing
the simultaneous estimation of the clustering and neural network's parameters.
Experimental evaluation was performed on image and text clustering benchmark
datasets showing that DMAE is competitive in terms of unsupervised
classification accuracy and normalized mutual information. The source code with
the implementation of DMAE is publicly available at:
https://github.com/juselara1/dmae]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1"&gt;Juan S. Lara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02789</id>
        <link href="http://arxiv.org/abs/2102.02789"/>
        <updated>2021-07-16T00:48:26.220Z</updated>
        <summary type="html"><![CDATA[Machine learning approached through supervised learning requires expensive
annotation of data. This motivates weakly supervised learning, where data are
annotated with incomplete yet discriminative information. In this paper, we
focus on partial labelling, an instance of weak supervision where, from a given
input, we are given a set of potential targets. We review a disambiguation
principle to recover full supervision from weak supervision, and propose an
empirical disambiguation algorithm. We prove exponential convergence rates of
our algorithm under classical learnability assumptions, and we illustrate the
usefulness of our method on practical examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TreeBERT: A Tree-Based Pre-Trained Model for Programming Language. (arXiv:2105.12485v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12485</id>
        <link href="http://arxiv.org/abs/2105.12485"/>
        <updated>2021-07-16T00:48:26.207Z</updated>
        <summary type="html"><![CDATA[Source code can be parsed into the abstract syntax tree (AST) based on
defined syntax rules. However, in pre-training, little work has considered the
incorporation of tree structure into the learning process. In this paper, we
present TreeBERT, a tree-based pre-trained model for improving programming
language-oriented generation tasks. To utilize tree structure, TreeBERT
represents the AST corresponding to the code as a set of composition paths and
introduces node position embedding. The model is trained by tree masked
language modeling (TMLM) and node order prediction (NOP) with a hybrid
objective. TMLM uses a novel masking strategy designed according to the tree's
characteristics to help the model understand the AST and infer the missing
semantics of the AST. With NOP, TreeBERT extracts the syntactical structure by
learning the order constraints of nodes in AST. We pre-trained TreeBERT on
datasets covering multiple programming languages. On code summarization and
code documentation tasks, TreeBERT outperforms other pre-trained models and
state-of-the-art models designed for these tasks. Furthermore, TreeBERT
performs well when transferred to the pre-trained unseen programming language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xue Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhuoran Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1"&gt;Chen Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving Factors using Machine Learning Techniques. (arXiv:2101.12684v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12684</id>
        <link href="http://arxiv.org/abs/2101.12684"/>
        <updated>2021-07-16T00:48:26.183Z</updated>
        <summary type="html"><![CDATA[Sovereign credit ratings summarize the creditworthiness of countries. These
ratings have a large influence on the economy and the yields at which
governments can issue new debt. This paper investigates the use of a Multilayer
Perceptron (MLP), Classification and Regression Trees (CART), Support Vector
Machines (SVM), Na\"ive Bayes (NB), and an Ordered Logit (OL) model for the
prediction of sovereign credit ratings. We show that MLP is best suited for
predicting sovereign credit ratings, with a random cross-validated accuracy of
68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation
of the determining factors shows that there is some heterogeneity in the
important variables across the models. However, the two models with the highest
out-of-sample predictive accuracy, MLP and CART, show a lot of similarities in
the influential variables, with regulatory quality, and GDP per capita as
common important variables. Consistent with economic theory, a higher
regulatory quality and/or GDP per capita are associated with a higher credit
rating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Overes_B/0/1/0/all/0/1"&gt;Bart H.L. Overes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wel_M/0/1/0/all/0/1"&gt;Michel van der Wel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-16T00:48:26.178Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bit More Bayesian: Domain-Invariant Learning with Uncertainty. (arXiv:2105.04030v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04030</id>
        <link href="http://arxiv.org/abs/2105.04030"/>
        <updated>2021-07-16T00:48:26.165Z</updated>
        <summary type="html"><![CDATA[Domain generalization is challenging due to the domain shift and the
uncertainty caused by the inaccessibility of target domain data. In this paper,
we address both challenges with a probabilistic framework based on variational
Bayesian inference, by incorporating uncertainty into neural network weights.
We couple domain invariance in a probabilistic formula with the variational
Bayesian inference. This enables us to explore domain-invariant learning in a
principled way. Specifically, we derive domain-invariant representations and
classifiers, which are jointly established in a two-layer Bayesian neural
network. We empirically demonstrate the effectiveness of our proposal on four
widely used cross-domain visual recognition benchmarks. Ablation studies
validate the synergistic benefits of our Bayesian treatment when jointly
learning domain-invariant representations and classifiers for domain
generalization. Further, our method consistently delivers state-of-the-art mean
accuracy on all benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zehao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jiayi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCMC-driven importance samplers. (arXiv:2105.02579v3 [stat.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02579</id>
        <link href="http://arxiv.org/abs/2105.02579"/>
        <updated>2021-07-16T00:48:26.151Z</updated>
        <summary type="html"><![CDATA[Monte Carlo methods are the standard procedure for estimating complicated
integrals of multidimensional Bayesian posterior distributions. In this work,
we focus on LAIS, a class of adaptive importance samplers where Markov chain
Monte Carlo (MCMC) algorithms are employed to drive an underlying multiple
importance sampling (IS) scheme. Its power lies in the simplicity of the
layered framework: the upper layer locates proposal densities by means of MCMC
algorithms; while the lower layer handles the multiple IS scheme, in order to
compute the final estimators. The modular nature of LAIS allows for different
possible choices in the upper and lower layers, that will have different
performance and computational costs. In this work, we propose different
enhancements in order to increase the efficiency and reduce the computational
cost, of both upper and lower layers. The different variants are essential if
we aim to address computational challenges arising in real-world applications,
such as highly concentrated posterior distributions (due to large amounts of
data, etc.). Hamiltonian-driven importance samplers are presented and tested.
Furthermore, we introduce different strategies for designing cheaper schemes,
for instance, recycling samples generated in the upper layer and using them in
the final estimators in the lower layer. Numerical experiments show the
benefits of the proposed schemes as compared to the vanilla version of LAIS and
other benchmark methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Llorente_F/0/1/0/all/0/1"&gt;F. Llorente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Curbelo_E/0/1/0/all/0/1"&gt;E. Curbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Martino_L/0/1/0/all/0/1"&gt;L. Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elvira_V/0/1/0/all/0/1"&gt;V. Elvira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Delgado_D/0/1/0/all/0/1"&gt;D. Delgado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00652</id>
        <link href="http://arxiv.org/abs/2107.00652"/>
        <updated>2021-07-16T00:48:26.119Z</updated>
        <summary type="html"><![CDATA[We present CSWin Transformer, an efficient and effective Transformer-based
backbone for general-purpose vision tasks. A challenging issue in Transformer
design is that global self-attention is very expensive to compute whereas local
self-attention often limits the field of interactions of each token. To address
this issue, we develop the Cross-Shaped Window self-attention mechanism for
computing self-attention in the horizontal and vertical stripes in parallel
that form a cross-shaped window, with each stripe obtained by splitting the
input feature into stripes of equal width. We provide a detailed mathematical
analysis of the effect of the stripe width and vary the stripe width for
different layers of the Transformer network which achieves strong modeling
capability while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which handles the local positional
information better than existing encoding schemes. LePE naturally supports
arbitrary input resolutions, and is thus especially effective and friendly for
downstream tasks. Incorporated with these designs and a hierarchical structure,
CSWin Transformer demonstrates competitive performance on common vision tasks.
Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra
training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection
task, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing
previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and
+2.0 respectively under the similar FLOPs setting. By further pretraining on
the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K
and state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The
code and models will be available at
https://github.com/microsoft/CSWin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jianmin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissimilarity Mixture Autoencoder for Deep Clustering. (arXiv:2006.08177v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08177</id>
        <link href="http://arxiv.org/abs/2006.08177"/>
        <updated>2021-07-16T00:48:26.114Z</updated>
        <summary type="html"><![CDATA[The dissimilarity mixture autoencoder (DMAE) is a neural network model for
feature-based clustering that incorporates a flexible dissimilarity function
and can be integrated into any kind of deep learning architecture. It
internally represents a dissimilarity mixture model (DMM) that extends
classical methods like K-Means, Gaussian mixture models, or Bregman clustering
to any convex and differentiable dissimilarity function through the
reinterpretation of probabilities as neural network representations. DMAE can
be integrated with deep learning architectures into end-to-end models, allowing
the simultaneous estimation of the clustering and neural network's parameters.
Experimental evaluation was performed on image and text clustering benchmark
datasets showing that DMAE is competitive in terms of unsupervised
classification accuracy and normalized mutual information. The source code with
the implementation of DMAE is publicly available at:
https://github.com/juselara1/dmae]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lara_J/0/1/0/all/0/1"&gt;Juan S. Lara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1"&gt;Fabio A. Gonz&amp;#xe1;lez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Dimension-Free Understanding of Adaptive Linear Control. (arXiv:2103.10620v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10620</id>
        <link href="http://arxiv.org/abs/2103.10620"/>
        <updated>2021-07-16T00:48:26.106Z</updated>
        <summary type="html"><![CDATA[We study the problem of adaptive control of the linear quadratic regulator
for systems in very high, or even infinite dimension. We demonstrate that while
sublinear regret requires finite dimensional inputs, the ambient state
dimension of the system need not be bounded in order to perform online control.
We provide the first regret bounds for LQR which hold for infinite dimensional
systems, replacing dependence on ambient dimension with more natural notions of
problem complexity. Our guarantees arise from a novel perturbation bound for
certainty equivalence which scales with the prediction error in estimating the
system parameters, without requiring consistent parameter recovery in more
stringent measures like the operator norm. When specialized to finite
dimensional settings, our bounds recover near optimal dimension and time
horizon dependence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Juan C. Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter Bartlett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoders and Ensembles for Task-Free Continual Learning. (arXiv:2105.13327v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13327</id>
        <link href="http://arxiv.org/abs/2105.13327"/>
        <updated>2021-07-16T00:48:26.099Z</updated>
        <summary type="html"><![CDATA[We present an architecture that is effective for continual learning in an
especially demanding setting, where task boundaries do not exist or are
unknown. Our architecture comprises an encoder, pre-trained on a separate
dataset, and an ensemble of simple one-layer classifiers. Two main innovations
are required to make this combination work. First, the provision of suitably
generic pre-trained encoders has been made possible thanks to recent progress
in self-supervised training methods. Second, pairing each classifier in the
ensemble with a key, where the key-space is identical to the latent space of
the encoder, allows them to be used collectively, yet selectively, via
k-nearest neighbour lookup. We show that models trained with the
encoders-and-ensembles architecture are state-of-the-art for the task-free
setting on standard image classification continual learning benchmarks, and
improve on prior state-of-the-art by a large margin in the most challenging
cases. We also show that the architecture learns well in a fully incremental
setting, where one class is learned at a time, and we demonstrate its
effectiveness in this setting with up to 100 classes. Finally, we show that the
architecture works in a task-free continual learning context where the data
distribution changes gradually, and existing approaches requiring knowledge of
task boundaries cannot be applied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1"&gt;Murray Shanahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1"&gt;Christos Kaplanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1"&gt;Jovana Mitrovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00774</id>
        <link href="http://arxiv.org/abs/2107.00774"/>
        <updated>2021-07-16T00:48:26.079Z</updated>
        <summary type="html"><![CDATA[Recently, due to an increasing interest for transparency in artificial
intelligence, several methods of explainable machine learning have been
developed with the simultaneous goal of accuracy and interpretability by
humans. In this paper, we study a recent framework of explainable clustering
first suggested by Dasgupta et al.~\cite{dasgupta2020explainable}.
Specifically, we focus on the $k$-means and $k$-medians problems and provide
nearly tight upper and lower bounds.

First, we provide an $O(\log k \log \log k)$-approximation algorithm for
explainable $k$-medians, improving on the best known algorithm of
$O(k)$~\cite{dasgupta2020explainable} and nearly matching the known
$\Omega(\log k)$ lower bound~\cite{dasgupta2020explainable}. In addition, in
low-dimensional spaces $d \ll \log k$, we show that our algorithm also provides
an $O(d \log^2 d)$-approximate solution for explainable $k$-medians. This
improves over the best known bound of $O(d \log k)$ for low
dimensions~\cite{laber2021explainable}, and is a constant for constant
dimensional spaces. To complement this, we show a nearly matching $\Omega(d)$
lower bound. Next, we study the $k$-means problem in this context and provide
an $O(k \log k)$-approximation algorithm for explainable $k$-means, improving
over the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \log k)$ bound of
\cite{laber2021explainable}. To complement this we provide an almost tight
$\Omega(k)$ lower bound, improving over the $\Omega(\log k)$ lower bound of
Dasgupta et al. Given an approximate solution to the classic $k$-means and
$k$-medians, our algorithm for $k$-medians runs in time $O(kd \log^2 k )$ and
our algorithm for $k$-means runs in time $ O(k^2 d)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1"&gt;Hossein Esfandiari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast rates in structured prediction. (arXiv:2102.00760v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00760</id>
        <link href="http://arxiv.org/abs/2102.00760"/>
        <updated>2021-07-16T00:48:26.072Z</updated>
        <summary type="html"><![CDATA[Discrete supervised learning problems such as classification are often
tackled by introducing a continuous surrogate problem akin to regression.
Bounding the original error, between estimate and solution, by the surrogate
error endows discrete problems with convergence rates already shown for
continuous instances. Yet, current approaches do not leverage the fact that
discrete problems are essentially predicting a discrete output when continuous
problems are predicting a continuous value. In this paper, we tackle this issue
for general structured prediction problems, opening the way to "super fast"
rates, that is, convergence rates for the excess risk faster than $n^{-1}$,
where $n$ is the number of observations, with even exponential rates with the
strongest assumptions. We first illustrate it for predictors based on nearest
neighbors, generalizing rates known for binary classification to any discrete
problem within the framework of structured prediction. We then consider kernel
ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast
rates, depending on a parameter characterizing the hardness of the problem,
thus allowing, under smoothness assumptions, to bypass the curse of
dimensionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Functional Model for Structure Learning and Parameter Estimation in Continuous Time Bayesian Network: An Application in Identifying Patterns of Multiple Chronic Conditions. (arXiv:2007.15847v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.15847</id>
        <link href="http://arxiv.org/abs/2007.15847"/>
        <updated>2021-07-16T00:48:26.063Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are powerful statistical models to study the probabilistic
relationships among set random variables with major applications in disease
modeling and prediction. Here, we propose a continuous time Bayesian network
with conditional dependencies, represented as Poisson regression, to model the
impact of exogenous variables on the conditional dependencies of the network.
We also propose an adaptive regularization method with an intuitive early
stopping feature based on density based clustering for efficient learning of
the structure and parameters of the proposed network. Using a dataset of
patients with multiple chronic conditions extracted from electronic health
records of the Department of Veterans Affairs we compare the performance of the
proposed approach with some of the existing methods in the literature for both
short-term (one-year ahead) and long-term (multi-year ahead) predictions. The
proposed approach provides a sparse intuitive representation of the complex
functional relationships between multiple chronic conditions. It also provides
the capability of analyzing multiple disease trajectories over time given any
combination of prior conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faruqui_S/0/1/0/all/0/1"&gt;Syed Hasib Akhter Faruqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaeddini_A/0/1/0/all/0/1"&gt;Adel Alaeddini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaramillo_C/0/1/0/all/0/1"&gt;Carlos A. Jaramillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Space Partitions for Path Planning. (arXiv:2106.10544v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10544</id>
        <link href="http://arxiv.org/abs/2106.10544"/>
        <updated>2021-07-16T00:48:26.048Z</updated>
        <summary type="html"><![CDATA[Path planning, the problem of efficiently discovering high-reward
trajectories, often requires optimizing a high-dimensional and multimodal
reward function. Popular approaches like CEM and CMA-ES greedily focus on
promising regions of the search space and may get trapped in local maxima. DOO
and VOOT balance exploration and exploitation, but use space partitioning
strategies independent of the reward function to be optimized. Recently, LaMCTS
empirically learns to partition the search space in a reward-sensitive manner
for black-box optimization. In this paper, we develop a novel formal regret
analysis for when and why such an adaptive region partitioning scheme works. We
also propose a new path planning method PlaLaM which improves the function
value estimation within each sub-region, and uses a latent representation of
the search space. Empirically, PlaLaM outperforms existing path planning
methods in 2D navigation tasks, especially in the presence of
difficult-to-escape local optima, and shows benefits when plugged into
model-based RL with planning components such as PETS. These gains transfer to
highly multimodal real-world tasks, where we outperform strong baselines in
compiler phase ordering by up to 245% and in molecular design by up to 0.4 on
properties on a 0-1 scale. Code is available at
https://github.com/yangkevin2/plalam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kevin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1"&gt;Chris Cummins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1"&gt;Benoit Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02644</id>
        <link href="http://arxiv.org/abs/2103.02644"/>
        <updated>2021-07-16T00:48:26.042Z</updated>
        <summary type="html"><![CDATA[Recent progress in audio source separation lead by deep learning has enabled
many neural network models to provide robust solutions to this fundamental
estimation problem. In this study, we provide a family of efficient neural
network architectures for general purpose audio source separation while
focusing on multiple computational aspects that hinder the application of
neural networks in real-world scenarios. The backbone structure of this
convolutional network is the SUccessive DOwnsampling and Resampling of
Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is
performed through simple one-dimensional convolutions. This mechanism enables
our models to obtain high fidelity signal separation in a wide variety of
settings where variable number of sources are present and with limited
computational resources (e.g. floating point operations, memory footprint,
number of parameters and latency). Our experiments show that SuDoRM-RF models
perform comparably and even surpass several state-of-the-art benchmarks with
significantly higher computational resource requirements. The causal variation
of SuDoRM-RF is able to obtain competitive performance in real-time speech
separation of around 10dB scale-invariant signal-to-distortion ratio
improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a
laptop device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xilin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn Variational Semantic Memory. (arXiv:2010.10341v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10341</id>
        <link href="http://arxiv.org/abs/2010.10341"/>
        <updated>2021-07-16T00:48:26.030Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce variational semantic memory into meta-learning to
acquire long-term knowledge for few-shot learning. The variational semantic
memory accrues and stores semantic information for the probabilistic inference
of class prototypes in a hierarchical Bayesian framework. The semantic memory
is grown from scratch and gradually consolidated by absorbing information from
tasks it experiences. By doing so, it is able to accumulate long-term, general
knowledge that enables it to learn new concepts of objects. We formulate memory
recall as the variational inference of a latent memory variable from addressed
contents, which offers a principled way to adapt the knowledge to individual
tasks. Our variational semantic memory, as a new long-term memory module,
confers principled recall and update mechanisms that enable semantic
information to be efficiently accrued and adapted for few-shot learning.
Experiments demonstrate that the probabilistic modelling of prototypes achieves
a more informative representation of object classes compared to deterministic
vectors. The consistent new state-of-the-art performance on four benchmarks
shows the benefit of variational semantic memory in boosting few-shot
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yingjun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Huan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1"&gt;Qiang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GMAC: A Distributional Perspective on Actor-Critic Framework. (arXiv:2105.11366v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11366</id>
        <link href="http://arxiv.org/abs/2105.11366"/>
        <updated>2021-07-16T00:48:26.025Z</updated>
        <summary type="html"><![CDATA[In this paper, we devise a distributional framework on actor-critic as a
solution to distributional instability, action type restriction, and conflation
between samples and statistics. We propose a new method that minimizes the
Cram\'er distance with the multi-step Bellman target distribution generated
from a novel Sample-Replacement algorithm denoted SR($\lambda$), which learns
the correct value distribution under multiple Bellman operations.
Parameterizing a value distribution with Gaussian Mixture Model further
improves the efficiency and the performance of the method, which we name GMAC.
We empirically show that GMAC captures the correct representation of value
distributions and improves the performance of a conventional actor-critic
method with low computational cost, in both discrete and continuous action
spaces using Arcade Learning Environment (ALE) and PyBullet environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1"&gt;Daniel Wontae Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Younghoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chan Y. Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Algorithms for Socially Fair Clustering. (arXiv:2103.02512v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02512</id>
        <link href="http://arxiv.org/abs/2103.02512"/>
        <updated>2021-07-16T00:48:26.007Z</updated>
        <summary type="html"><![CDATA[We present an $(e^{O(p)} \frac{\log \ell}{\log\log\ell})$-approximation
algorithm for socially fair clustering with the $\ell_p$-objective. In this
problem, we are given a set of points in a metric space. Each point belongs to
one (or several) of $\ell$ groups. The goal is to find a $k$-medians,
$k$-means, or, more generally, $\ell_p$-clustering that is simultaneously good
for all of the groups. More precisely, we need to find a set of $k$ centers $C$
so as to minimize the maximum over all groups $j$ of $\sum_{u \text{ in group
}j} d(u,C)^p$. The socially fair clustering problem was independently proposed
by Ghadiri, Samadi, and Vempala [2021] and Abbasi, Bhaskara, and
Venkatasubramanian [2021]. Our algorithm improves and generalizes their
$O(\ell)$-approximation algorithms for the problem. The natural LP relaxation
for the problem has an integrality gap of $\Omega(\ell)$. In order to obtain
our result, we introduce a strengthened LP relaxation and show that it has an
integrality gap of $\Theta(\frac{\log \ell}{\log\log\ell})$ for a fixed $p$.
Additionally, we present a bicriteria approximation algorithm, which
generalizes the bicriteria approximation of Abbasi et al. [2021].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarychev_Y/0/1/0/all/0/1"&gt;Yury Makarychev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification. (arXiv:2107.07511v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07511</id>
        <link href="http://arxiv.org/abs/2107.07511"/>
        <updated>2021-07-16T00:48:26.000Z</updated>
        <summary type="html"><![CDATA[Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios N. Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Neural Network Interpretability. (arXiv:2012.14261v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14261</id>
        <link href="http://arxiv.org/abs/2012.14261"/>
        <updated>2021-07-16T00:48:25.993Z</updated>
        <summary type="html"><![CDATA[Along with the great success of deep neural networks, there is also growing
concern about their black-box nature. The interpretability issue affects
people's trust on deep learning systems. It is also related to many ethical
problems, e.g., algorithmic discrimination. Moreover, interpretability is a
desired property for deep networks to become powerful tools in other research
fields, e.g., drug discovery and genomics. In this survey, we conduct a
comprehensive review of the neural network interpretability research. We first
clarify the definition of interpretability as it has been used in many
different contexts. Then we elaborate on the importance of interpretability and
propose a novel taxonomy organized along three dimensions: type of engagement
(passive vs. active interpretation approaches), the type of explanation, and
the focus (from local to global interpretability). This taxonomy provides a
meaningful 3D view of distribution of papers from the relevant literature as
two of the dimensions are not simply categorical but allow ordinal
subcategories. Finally, we summarize the existing interpretability evaluation
methods and suggest possible research directions inspired by our new taxonomy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tino_P/0/1/0/all/0/1"&gt;Peter Ti&amp;#x148;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ale&amp;#x161; Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Investigation of Traffic Density Changes inside Wuhan during the COVID-19 Epidemic with GF-2 Time-Series Images. (arXiv:2006.16098v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16098</id>
        <link href="http://arxiv.org/abs/2006.16098"/>
        <updated>2021-07-16T00:48:25.974Z</updated>
        <summary type="html"><![CDATA[In order to mitigate the spread of COVID-19, Wuhan was the first city to
implement strict lockdown policy in 2020. Even though numerous researches have
discussed the travel restriction between cities and provinces, few studies
focus on the effect of transportation control inside the city due to the lack
of the measurement and available data in Wuhan. Since the public transports
have been shut down in the beginning of city lockdown, the change of traffic
density is a good indicator to reflect the intracity population flow.
Therefore, in this paper, we collected time-series high-resolution remote
sensing images with the resolution of 1m acquired before, during and after
Wuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and
counted for the statistics of traffic density to reflect the changes of human
transmissions in the whole period of Wuhan lockdown. Open Street Map was used
to obtain observation road surfaces, and a vehicle detection method combing
morphology filter and deep learning was utilized to extract vehicles with the
accuracy of 62.56%. According to the experimental results, the traffic density
of Wuhan dropped with the percentage higher than 80%, and even higher than 90%
on main roads during city lockdown; after lockdown lift, the traffic density
recovered to the normal rate. Traffic density distributions also show the
obvious reduction and increase throughout the whole study area. The significant
reduction and recovery of traffic density indicates that the lockdown policy in
Wuhan show effectiveness in controlling human transmission inside the city, and
the city returned to normal after lockdown lift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yinong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Haonan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jingwen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1"&gt;Lixiang Ru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongruixuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangpei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptable Agent Populations via a Generative Model of Policies. (arXiv:2107.07506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07506</id>
        <link href="http://arxiv.org/abs/2107.07506"/>
        <updated>2021-07-16T00:48:25.958Z</updated>
        <summary type="html"><![CDATA[In the natural world, life has found innumerable ways to survive and often
thrive. Between and even within species, each individual is in some manner
unique, and this diversity lends adaptability and robustness to life. In this
work, we aim to learn a space of diverse and high-reward policies on any given
environment. To this end, we introduce a generative model of policies, which
maps a low-dimensional latent space to an agent policy space. Our method
enables learning an entire population of agent policies, without requiring the
use of separate policy parameters. Just as real world populations can adapt and
evolve via natural selection, our method is able to adapt to changes in our
environment solely by selecting for policies in latent space. We test our
generative model's capabilities in a variety of environments, including an
open-ended grid-world and a two-player soccer environment. Code,
visualizations, and additional experiments can be found at
https://kennyderek.github.io/adap/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derek_K/0/1/0/all/0/1"&gt;Kenneth Derek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Partitioning of High-order Models with a Novel Convex Tensor Cone Relaxation. (arXiv:1911.02161v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02161</id>
        <link href="http://arxiv.org/abs/1911.02161"/>
        <updated>2021-07-16T00:48:25.941Z</updated>
        <summary type="html"><![CDATA[In this paper we propose an algorithm for exact partitioning of high-order
models. We define a general class of $m$-degree Homogeneous Polynomial Models,
which subsumes several examples motivated from prior literature. Exact
partitioning can be formulated as a tensor optimization problem. We relax this
high-order combinatorial problem to a convex conic form problem. To this end,
we carefully define the Carath\'eodory symmetric tensor cone, and show its
convexity, and the convexity of its dual cone. This allows us to construct a
primal-dual certificate to show that the solution of the convex relaxation is
correct (equal to the unobserved true group assignment) and to analyze the
statistical upper bound of exact partitioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1"&gt;Chuyang Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Only Train Once: A One-Shot Neural Network Training And Pruning Framework. (arXiv:2107.07467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07467</id>
        <link href="http://arxiv.org/abs/2107.07467"/>
        <updated>2021-07-16T00:48:25.932Z</updated>
        <summary type="html"><![CDATA[Structured pruning is a commonly used technique in deploying deep neural
networks (DNNs) onto resource-constrained devices. However, the existing
pruning methods are usually heuristic, task-specified, and require an extra
fine-tuning procedure. To overcome these limitations, we propose a framework
that compresses DNNs into slimmer architectures with competitive performances
and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two
keys: (i) we partition the parameters of DNNs into zero-invariant groups,
enabling us to prune zero groups without affecting the output; and (ii) to
promote zero groups, we then formulate a structured-sparsity optimization
problem and propose a novel optimization algorithm, Half-Space Stochastic
Projected Gradient (HSPG), to solve it, which outperforms the standard proximal
methods on group sparsity exploration and maintains comparable convergence. To
demonstrate the effectiveness of OTO, we train and compress full models
simultaneously from scratch without fine-tuning for inference speedup and
parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10,
ResNet50 for CIFAR10/ImageNet and Bert for SQuAD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_B/0/1/0/all/0/1"&gt;Bo Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1"&gt;Tianyu Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1"&gt;Biyi Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1"&gt;Luming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yixin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Sheng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1"&gt;Xiao Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12368</id>
        <link href="http://arxiv.org/abs/2012.12368"/>
        <updated>2021-07-16T00:48:25.925Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense against such attacks.
We develop a theoretical framework for adversarial training with FW
optimization (FW-AT) that reveals a geometric connection between the loss
landscape and the distortion of $\ell_\infty$ FW attacks (the attack's $\ell_2$
norm). Specifically, we show that high distortion of FW attacks is equivalent
to low variation along the attack path. It is then experimentally demonstrated
on various deep neural network architectures that $\ell_\infty$ attacks against
robust models achieve near maximal $\ell_2$ distortion. This mathematical
transparency differentiates FW from the more popular Projected Gradient Descent
(PGD) optimization. To demonstrate the utility of our theoretical framework we
develop FW-Adapt, a novel adversarial training algorithm which uses simple
distortion measure to adaptively change number of attack steps during training.
FW-Adapt provides strong robustness at lower training times in comparison to
PGD-AT for a variety of white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1"&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1"&gt;Jay Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00394</id>
        <link href="http://arxiv.org/abs/2008.00394"/>
        <updated>2021-07-16T00:48:25.918Z</updated>
        <summary type="html"><![CDATA[In view of the difficulty in reconstructing object details in point cloud
completion, we propose a shape prior learning method for object completion. The
shape priors include geometric information in both complete and the partial
point clouds. We design a feature alignment strategy to learn the shape prior
from complete points, and a coarse to fine strategy to incorporate partial
prior in the fine stage. To learn the complete objects prior, we first train a
point cloud auto-encoder to extract the latent embeddings from complete points.
Then we learn a mapping to transfer the point features from partial points to
that of the complete points by optimizing feature alignment losses. The feature
alignment losses consist of a L2 distance and an adversarial loss obtained by
Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2
distance optimizes the partial features towards the complete ones in the
feature space, and MMD-GAN decreases the statistical distance of two point
features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art
performances on the point cloud completion task. Our code is available at
https://github.com/xiaogangw/point-cloud-completion-shape-prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wireless Image Retrieval at the Edge. (arXiv:2007.10915v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10915</id>
        <link href="http://arxiv.org/abs/2007.10915"/>
        <updated>2021-07-16T00:48:25.911Z</updated>
        <summary type="html"><![CDATA[We study the image retrieval problem at the wireless edge, where an edge
device captures an image, which is then used to retrieve similar images from an
edge server. These can be images of the same person or a vehicle taken from
other cameras at different times and locations. Our goal is to maximize the
accuracy of the retrieval task under power and bandwidth constraints over the
wireless link. Due to the stringent delay constraint of the underlying
application, sending the whole image at a sufficient quality is not possible.
We propose two alternative schemes based on digital and analog communications,
respectively. In the digital approach, we first propose a deep neural network
(DNN) aided retrieval-oriented image compression scheme, whose output bit
sequence is transmitted over the channel using conventional channel codes. In
the analog joint source and channel coding (JSCC) approach, the feature vectors
are directly mapped into channel symbols. We evaluate both schemes on image
based re-identification (re-ID) tasks under different channel conditions,
including both static and fading channels. We show that the JSCC scheme
significantly increases the end-to-end accuracy, speeds up the encoding
process, and provides graceful degradation with channel conditions. The
proposed architecture is evaluated through extensive simulations on different
datasets and channel conditions, as well as through ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1"&gt;Mikolaj Jankowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1"&gt;Krystian Mikolajczyk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Transferability in Wearable Sensor Systems. (arXiv:2003.07982v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07982</id>
        <link href="http://arxiv.org/abs/2003.07982"/>
        <updated>2021-07-16T00:48:25.897Z</updated>
        <summary type="html"><![CDATA[Machine learning is used for inference and decision making in wearable sensor
systems. However, recent studies have found that machine learning algorithms
are easily fooled by the addition of adversarial perturbations to their inputs.
What is more interesting is that adversarial examples generated for one machine
learning system is also effective against other systems. This property of
adversarial examples is called transferability. In this work, we take the first
stride in studying adversarial transferability in wearable sensor systems from
the following perspectives: 1) transferability between machine learning
systems, 2) transferability across subjects, 3) transferability across sensor
body locations, and 4) transferability across datasets. We found strong
untargeted transferability in most cases. Targeted attacks were less successful
with success scores from $0\%$ to $80\%$. The transferability of adversarial
examples depends on many factors such as the inclusion of data from all
subjects, sensor body position, number of samples in the dataset, type of
learning algorithm, and the distribution of source and target system dataset.
The transferability of adversarial examples decreases sharply when the data
distribution of the source and target system becomes more distinct. We also
provide guidelines for the community for designing robust sensor systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1"&gt;Ramesh Kumar Sah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1"&gt;Hassan Ghasemzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Online Gaussian Process Regression Without the Sample Complexity Bottleneck. (arXiv:2004.11094v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11094</id>
        <link href="http://arxiv.org/abs/2004.11094"/>
        <updated>2021-07-16T00:48:25.890Z</updated>
        <summary type="html"><![CDATA[Gaussian processes provide a framework for nonlinear nonparametric Bayesian
inference widely applicable across science and engineering. Unfortunately,
their computational burden scales cubically with the training sample size,
which in the case that samples arrive in perpetuity, approaches infinity. This
issue necessitates approximations for use with streaming data, which to date
mostly lack convergence guarantees. Thus, we develop the first online Gaussian
process approximation that preserves convergence to the population posterior,
i.e., asymptotic posterior consistency, while ameliorating its intractable
complexity growth with the sample size. We propose an online compression scheme
that, following each a posteriori update, fixes an error neighborhood with
respect to the Hellinger metric centered at the current posterior, and greedily
tosses out past kernel dictionary elements until its boundary is hit. We call
the resulting method Parsimonious Online Gaussian Processes (POG). For
diminishing error radius, exact asymptotic consistency is preserved (Theorem
1(i)) at the cost of unbounded memory in the limit. On the other hand, for
constant error radius, POG converges to a neighborhood of the population
posterior (Theorem 1(ii))but with finite memory at-worst determined by the
metric entropy of the feature space (Theorem 2). Experimental results are
presented on several nonlinear regression problems which illuminates the merits
of this approach as compared with alternatives that fix the subspace dimension
defining the history of past points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pradhan_H/0/1/0/all/0/1"&gt;Hrusikesha Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1"&gt;Ketan Rajawat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Goodness-of-fit Test on the Number of Biclusters in a Relational Data Matrix. (arXiv:2102.11658v3 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11658</id>
        <link href="http://arxiv.org/abs/2102.11658"/>
        <updated>2021-07-16T00:48:25.882Z</updated>
        <summary type="html"><![CDATA[Biclustering is a method for detecting homogeneous submatrices in a given
observed matrix, and it is an effective tool for relational data analysis.
Although there are many studies that estimate the underlying bicluster
structure of a matrix, few have enabled us to determine the appropriate number
of biclusters in an observed matrix. Recently, a statistical test on the number
of biclusters has been proposed for a regular-grid bicluster structure, where
we assume that the latent bicluster structure can be represented by row-column
clustering. However, when the latent bicluster structure does not satisfy such
regular-grid assumption, the previous test requires a larger number of
biclusters than necessary (i.e., a finer bicluster structure than necessary)
for the null hypothesis to be accepted, which is not desirable in terms of
interpreting the accepted bicluster structure. In this study, we propose a new
statistical test on the number of biclusters that does not require the
regular-grid assumption and derive the asymptotic behavior of the proposed test
statistic in both null and alternative cases. We illustrate the effectiveness
of the proposed method by applying it to both synthetic and practical
relational data matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APo-VAE: Text Generation in Hyperbolic Space. (arXiv:2005.00054v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00054</id>
        <link href="http://arxiv.org/abs/2005.00054"/>
        <updated>2021-07-16T00:48:25.876Z</updated>
        <summary type="html"><![CDATA[Natural language often exhibits inherent hierarchical structure ingrained
with complex syntax and semantics. However, most state-of-the-art deep
generative models learn embeddings only in Euclidean vector space, without
accounting for this structural property of language. In this paper, we
investigate text generation in a hyperbolic latent space to learn continuous
hierarchical representations. An Adversarial Poincare Variational Autoencoder
(APo-VAE) is presented, where both the prior and variational posterior of
latent variables are defined over a Poincare ball via wrapped normal
distributions. By adopting the primal-dual formulation of KL divergence, an
adversarial learning procedure is introduced to empower robust model training.
Extensive experiments in language modeling and dialog-response generation tasks
demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs
in Euclidean latent space, thanks to its superb capabilities in capturing
latent language hierarchies in hyperbolic space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shuyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Training of Deep Neural Networks for Emerging Memory Technology. (arXiv:2009.03887v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03887</id>
        <link href="http://arxiv.org/abs/2009.03887"/>
        <updated>2021-07-16T00:48:25.859Z</updated>
        <summary type="html"><![CDATA[The recent success of neural networks for solving difficult decision tasks
has incentivized incorporating smart decision making "at the edge." However,
this work has traditionally focused on neural network inference, rather than
training, due to memory and compute limitations, especially in emerging
non-volatile memory systems, where writes are energetically costly and reduce
lifespan. Yet, the ability to train at the edge is becoming increasingly
important as it enables real-time adaptability to device drift and
environmental variation, user customization, and federated learning across
devices. In this work, we address two key challenges for training on edge
devices with non-volatile memory: low write density and low auxiliary memory.
We present a low-rank training scheme that addresses these challenges while
maintaining computational efficiency. We then demonstrate the technique on a
representative convolutional neural network across several adaptation problems,
where it out-performs standard SGD both in accuracy and in number of weight
writes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gural_A/0/1/0/all/0/1"&gt;Albert Gural&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeau_P/0/1/0/all/0/1"&gt;Phillip Nadeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tikekar_M/0/1/0/all/0/1"&gt;Mehul Tikekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1"&gt;Boris Murmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview of Machine Learning-aided Optical Performance Monitoring Techniques. (arXiv:2107.07338v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07338</id>
        <link href="http://arxiv.org/abs/2107.07338"/>
        <updated>2021-07-16T00:48:25.825Z</updated>
        <summary type="html"><![CDATA[Future communication systems are faced with increased demand for high
capacity, dynamic bandwidth, reliability and heterogeneous traffic. To meet
these requirements, networks have become more complex and thus require new
design methods and monitoring techniques, as they evolve towards becoming
autonomous. Machine learning has come to the forefront in recent years as a
promising technology to aid in this evolution. Optical fiber communications can
already provide the high capacity required for most applications, however,
there is a need for increased scalability and adaptability to changing user
demands and link conditions. Accurate performance monitoring is an integral
part of this transformation. In this paper we review optical performance
monitoring techniques where machine learning algorithms have been applied.
Moreover, since alot of OPM depends on knowledge of the signal type, we also
review work for modulation format recognition and bitrate identification. We
additionally briefly introduce a neuromorphic approach to OPM as an emerging
technique that has only recently been applied to this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tizikara_D/0/1/0/all/0/1"&gt;Dativa K. Tizikara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serugunda_J/0/1/0/all/0/1"&gt;Jonathan Serugunda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katumba_A/0/1/0/all/0/1"&gt;Andrew Katumba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis. (arXiv:2107.07373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07373</id>
        <link href="http://arxiv.org/abs/2107.07373"/>
        <updated>2021-07-16T00:48:25.816Z</updated>
        <summary type="html"><![CDATA[We convert the DeepMind Mathematics Dataset into a reinforcement learning
environment by interpreting it as a program synthesis problem. Each action
taken in the environment adds an operator or an input into a discrete compute
graph. Graphs which compute correct answers yield positive reward, enabling the
optimization of a policy to construct compute graphs conditioned on problem
statements. Baseline models are trained using Double DQN on various subsets of
problem types, demonstrating the capability to learn to correctly construct
graphs despite the challenges of combinatorial explosion and noisy rewards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palermo_J/0/1/0/all/0/1"&gt;Joseph Palermo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Johnny Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Alok Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic analysis of solar cell optical performance using Gaussian processes. (arXiv:2107.07342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07342</id>
        <link href="http://arxiv.org/abs/2107.07342"/>
        <updated>2021-07-16T00:48:25.802Z</updated>
        <summary type="html"><![CDATA[This work investigates application of different machine learning based
prediction methodologies to estimate the performance of silicon based textured
cells. Concept of confidence bound regions is introduced and advantages of this
concept are discussed in detail. Results show that reflection profiles and
depth dependent optical generation profiles can be accurately estimated using
Gaussian processes with exact knowledge of uncertainty in the prediction
values.It is also shown that cell design parameters can be estimated for a
desired performance metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1"&gt;Rahul Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1"&gt;Manel Mart&amp;#xed;nez-Ram&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busani_T/0/1/0/all/0/1"&gt;Tito Busani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Do Not Need a Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack. (arXiv:2107.07346v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07346</id>
        <link href="http://arxiv.org/abs/2107.07346"/>
        <updated>2021-07-16T00:48:25.796Z</updated>
        <summary type="html"><![CDATA[We argue that immature data pipelines are preventing a large portion of
industry practitioners from leveraging the latest research on recommender
systems. We propose our template data stack for machine learning at "reasonable
scale", and show how many challenges are solved by embracing a serverless
paradigm. Leveraging our experience, we detail how modern open source can
provide a pipeline processing terabytes of data with limited infrastructure
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Newton-LESS: Sparsification without Trade-offs for the Sketched Newton Update. (arXiv:2107.07480v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.07480</id>
        <link href="http://arxiv.org/abs/2107.07480"/>
        <updated>2021-07-16T00:48:25.778Z</updated>
        <summary type="html"><![CDATA[In second-order optimization, a potential bottleneck can be computing the
Hessian matrix of the optimized function at every iteration. Randomized
sketching has emerged as a powerful technique for constructing estimates of the
Hessian which can be used to perform approximate Newton steps. This involves
multiplication by a random sketching matrix, which introduces a trade-off
between the computational cost of sketching and the convergence rate of the
optimization algorithm. A theoretically desirable but practically much too
expensive choice is to use a dense Gaussian sketching matrix, which produces
unbiased estimates of the exact Newton step and which offers strong
problem-independent convergence guarantees. We show that the Gaussian sketching
matrix can be drastically sparsified, significantly reducing the computational
cost of sketching, without substantially affecting its convergence properties.
This approach, called Newton-LESS, is based on a recently introduced sketching
technique: LEverage Score Sparsified (LESS) embeddings. We prove that
Newton-LESS enjoys nearly the same problem-independent local convergence rate
as Gaussian embeddings, not just up to constant factors but even down to lower
order terms, for a large class of optimization tasks. In particular, this leads
to a new state-of-the-art convergence result for an iterative least squares
solver. Finally, we extend LESS embeddings to include uniformly sparsified
random sign matrices which can be implemented efficiently and which perform
well in numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lacotte_J/0/1/0/all/0/1"&gt;Jonathan Lacotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks. (arXiv:2107.07455v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07455</id>
        <link href="http://arxiv.org/abs/2107.07455"/>
        <updated>2021-07-16T00:48:25.772Z</updated>
        <summary type="html"><![CDATA[There has been significant research done on developing methods for improving
robustness to distributional shift and uncertainty estimation. In contrast,
only limited work has examined developing standard datasets and benchmarks for
assessing these approaches. Additionally, most work on uncertainty estimation
and robustness has developed new techniques based on small-scale regression or
image classification tasks. However, many tasks of practical interest have
different modalities, such as tabular data, audio, text, or sensor data, which
offer significant challenges involving regression and discrete or continuous
structured prediction. Thus, given the current state of the field, a
standardized large-scale dataset of tasks across a range of modalities affected
by distributional shifts is necessary. This will enable researchers to
meaningfully evaluate the plethora of recently developed uncertainty
quantification methods, as well as assessment criteria and state-of-the-art
baselines. In this work, we propose the \emph{Shifts Dataset} for evaluation of
uncertainty estimates and robustness to distributional shift. The dataset,
which has been collected from industrial sources and services, is composed of
three tasks, with each corresponding to a particular data modality: tabular
weather prediction, machine translation, and self-driving car (SDC) vehicle
motion prediction. All of these data modalities and tasks are affected by real,
`in-the-wild' distributional shifts and pose interesting challenges with
respect to uncertainty estimation. In this work we provide a description of the
dataset and baseline results for all tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1"&gt;Andrey Malinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chesnokov_G/0/1/0/all/0/1"&gt;German Chesnokov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1"&gt;Mark J. F. Gales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noskov_A/0/1/0/all/0/1"&gt;Alexey Noskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ploskonosov_A/0/1/0/all/0/1"&gt;Andrey Ploskonosov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorenkova_L/0/1/0/all/0/1"&gt;Liudmila Prokhorenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1"&gt;Ivan Provilkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vatsal Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vyas Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmatova_M/0/1/0/all/0/1"&gt;Mariya Shmatova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tigas_P/0/1/0/all/0/1"&gt;Panos Tigas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yangel_B/0/1/0/all/0/1"&gt;Boris Yangel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Scoring Rule Design. (arXiv:2107.07420v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.07420</id>
        <link href="http://arxiv.org/abs/2107.07420"/>
        <updated>2021-07-16T00:48:25.765Z</updated>
        <summary type="html"><![CDATA[This paper introduces an optimization problem for proper scoring rule design.
Consider a principal who wants to collect an agent's prediction about an
unknown state. The agent can either report his prior prediction or access a
costly signal and report the posterior prediction. Given a collection of
possible distributions containing the agent's posterior prediction
distribution, the principal's objective is to design a bounded scoring rule to
maximize the agent's worst-case payoff increment between reporting his
posterior prediction and reporting his prior prediction.

We study two settings of such optimization for proper scoring rules: static
and asymptotic settings. In the static setting, where the agent can access one
signal, we propose an efficient algorithm to compute an optimal scoring rule
when the collection of distributions is finite. The agent can adaptively and
indefinitely refine his prediction in the asymptotic setting. We first consider
a sequence of collections of posterior distributions with vanishing covariance,
which emulates general estimators with large samples, and show the optimality
of the quadratic scoring rule. Then, when the agent's posterior distribution is
a Beta-Bernoulli process, we find that the log scoring rule is optimal. We also
prove the optimality of the log scoring rule over a smaller set of functions
for categorical distributions with Dirichlet priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiling Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fang-Yi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Untrained DNN for Channel Estimation of RIS-Assisted Multi-User OFDM System with Hardware Impairments. (arXiv:2107.07423v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07423</id>
        <link href="http://arxiv.org/abs/2107.07423"/>
        <updated>2021-07-16T00:48:25.725Z</updated>
        <summary type="html"><![CDATA[Reconfigurable intelligent surface (RIS) is an emerging technology for
improving performance in fifth-generation (5G) and beyond networks. Practically
channel estimation of RIS-assisted systems is challenging due to the passive
nature of the RIS. The purpose of this paper is to introduce a deep
learning-based, low complexity channel estimator for the RIS-assisted
multi-user single-input-multiple-output (SIMO) orthogonal frequency division
multiplexing (OFDM) system with hardware impairments. We propose an untrained
deep neural network (DNN) based on the deep image prior (DIP) network to
denoise the effective channel of the system obtained from the conventional
pilot-based least-square (LS) estimation and acquire a more accurate
estimation. We have shown that our proposed method has high performance in
terms of accuracy and low complexity compared to conventional methods. Further,
we have shown that the proposed estimator is robust to interference caused by
the hardware impairments at the transceiver and RIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ginige_N/0/1/0/all/0/1"&gt;Nipuni Ginige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manosha_K/0/1/0/all/0/1"&gt;K. B. Shashika Manosha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1"&gt;Nandana Rajatheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Latva_aho_M/0/1/0/all/0/1"&gt;Matti Latva-aho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ApproxNet: Content and Contention-Aware Video Analytics System for Embedded Clients. (arXiv:1909.02068v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.02068</id>
        <link href="http://arxiv.org/abs/1909.02068"/>
        <updated>2021-07-16T00:48:25.719Z</updated>
        <summary type="html"><![CDATA[Videos take a lot of time to transport over the network, hence running
analytics on the live video on embedded or mobile devices has become an
important system driver. Considering that such devices, e.g., surveillance
cameras or AR/VR gadgets, are resource constrained, creating lightweight deep
neural networks (DNNs) for embedded devices is crucial. None of the current
approximation techniques for object classification DNNs can adapt to changing
runtime conditions, e.g., changes in resource availability on the device, the
content characteristics, or requirements from the user. In this paper, we
introduce ApproxNet, a video object classification system for embedded or
mobile clients. It enables novel dynamic approximation techniques to achieve
desired inference latency and accuracy trade-off under changing runtime
conditions. It achieves this by enabling two approximation knobs within a
single DNN model, rather than creating and maintaining an ensemble of models
(e.g., MCDNN [MobiSys-16]. We show that ApproxNet can adapt seamlessly at
runtime to these changes, provides low and stable latency for the image and
video frame classification problems, and show the improvement in accuracy and
latency over ResNet [CVPR-16], MCDNN [MobiSys-16], MobileNets [Google-17],
NestDNN [MobiCom-18], and MSDNet [ICLR-18].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ran Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Rakesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengcheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peter Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meghanath_G/0/1/0/all/0/1"&gt;Ganga Meghanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1"&gt;Somali Chaterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1"&gt;Subrata Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring the Structure of Ordinary Differential Equations. (arXiv:2107.07345v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07345</id>
        <link href="http://arxiv.org/abs/2107.07345"/>
        <updated>2021-07-16T00:48:25.703Z</updated>
        <summary type="html"><![CDATA[Understanding physical phenomena oftentimes means understanding the
underlying dynamical system that governs observational measurements. While
accurate prediction can be achieved with black box systems, they often lack
interpretability and are less amenable for further expert investigation.
Alternatively, the dynamics can be analysed via symbolic regression. In this
paper, we extend the approach by (Udrescu et al., 2020) called AIFeynman to the
dynamic setting to perform symbolic regression on ODE systems based on
observations from the resulting trajectories. We compare this extension to
state-of-the-art approaches for symbolic regression empirically on several
dynamical systems for which the ground truth equations of increasing complexity
are available. Although the proposed approach performs best on this benchmark,
we observed difficulties of all the compared symbolic regression approaches on
more complex systems, such as Cart-Pole.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weilbach_J/0/1/0/all/0/1"&gt;Juliane Weilbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerwinn_S/0/1/0/all/0/1"&gt;Sebastian Gerwinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weilbach_C/0/1/0/all/0/1"&gt;Christian Weilbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Melih Kandemir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07436</id>
        <link href="http://arxiv.org/abs/2107.07436"/>
        <updated>2021-07-16T00:48:25.693Z</updated>
        <summary type="html"><![CDATA[Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1"&gt;Neil Jethani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1"&gt;Mukund Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems. (arXiv:2107.07483v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.07483</id>
        <link href="http://arxiv.org/abs/2107.07483"/>
        <updated>2021-07-16T00:48:25.687Z</updated>
        <summary type="html"><![CDATA[In this study, we present a novel clinical decision support system and
discuss its interpretability-related properties. It combines a decision set of
rules with a machine learning scheme to offer global and local
interpretability. More specifically, machine learning is used to predict the
likelihood of each of those rules to be correct for a particular patient, which
may also contribute to better predictive performances. Moreover, the
reliability analysis of individual predictions is also addressed, contributing
to further personalized interpretability. The combination of these several
elements may be crucial to obtain the clinical stakeholders' trust, leading to
a better assessment of patients' conditions and improvement of the physicians'
decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Valente_F/0/1/0/all/0/1"&gt;Francisco Valente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paredes_S/0/1/0/all/0/1"&gt;Sim&amp;#xe3;o Paredes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Jorge Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07431</id>
        <link href="http://arxiv.org/abs/2107.07431"/>
        <updated>2021-07-16T00:48:25.677Z</updated>
        <summary type="html"><![CDATA[The increasing demand for commodities is leading to changes in land use
worldwide. In the tropics, deforestation, which causes high carbon emissions
and threatens biodiversity, is often linked to agricultural expansion. While
the need for deforestation-free global supply chains is widely recognized,
making progress in practice remains a challenge. Here, we propose an automated
approach that aims to support conservation and sustainable land use planning
decisions by mapping tropical landscapes at large scale and high spatial
resolution following the High Carbon Stock (HCS) approach. A deep learning
approach is developed that estimates canopy height for each 10 m Sentinel-2
pixel by learning from sparse GEDI LIDAR reference data, achieving an overall
RMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are
predictive for classifying HCS forests and degraded areas with an overall
accuracy of 86 % and produce a first high carbon stock map for Indonesia,
Malaysia, and the Philippines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1"&gt;Nico Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan Dirk Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copula-Based Normalizing Flows. (arXiv:2107.07352v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07352</id>
        <link href="http://arxiv.org/abs/2107.07352"/>
        <updated>2021-07-16T00:48:25.658Z</updated>
        <summary type="html"><![CDATA[Normalizing flows, which learn a distribution by transforming the data to
samples from a Gaussian base distribution, have proven powerful density
approximations. But their expressive power is limited by this choice of the
base distribution. We, therefore, propose to generalize the base distribution
to a more elaborate copula distribution to capture the properties of the target
distribution more accurately. In a first empirical analysis, we demonstrate
that this replacement can dramatically improve the vanilla normalizing flows in
terms of flexibility, stability, and effectivity for heavy-tailed data. Our
results suggest that the improvements are related to an increased local
Lipschitz-stability of the learned flow.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1"&gt;Mike Laszkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1"&gt;Johannes Lederer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1"&gt;Asja Fischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Fixed Version of Quadratic Program in Gradient Episodic Memory. (arXiv:2107.07384v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07384</id>
        <link href="http://arxiv.org/abs/2107.07384"/>
        <updated>2021-07-16T00:48:25.640Z</updated>
        <summary type="html"><![CDATA[Gradient Episodic Memory is indeed a novel method for continual learning,
which solves new problems quickly without forgetting previously acquired
knowledge. However, in the process of studying the paper, we found there were
some problems in the proof of the dual problem of Quadratic Program, so here we
give our fixed version for this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiying Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explore and Control with Adversarial Surprise. (arXiv:2107.07394v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07394</id>
        <link href="http://arxiv.org/abs/2107.07394"/>
        <updated>2021-07-16T00:48:25.631Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) provides a framework for learning goal-directed
policies given user-specified rewards. However, since designing rewards often
requires substantial engineering effort, we are interested in the problem of
learning without rewards, where agents must discover useful behaviors in the
absence of task-specific incentives. Intrinsic motivation is a family of
unsupervised RL techniques which develop general objectives for an RL agent to
optimize that lead to better exploration or the discovery of skills. In this
paper, we propose a new unsupervised RL technique based on an adversarial game
which pits two policies against each other to compete over the amount of
surprise an RL agent experiences. The policies each take turns controlling the
agent. The Explore policy maximizes entropy, putting the agent into surprising
or unfamiliar situations. Then, the Control policy takes over and seeks to
recover from those situations by minimizing entropy. The game harnesses the
power of multi-agent competition to drive the agent to seek out increasingly
surprising parts of the environment while learning to gain mastery over them.
We show empirically that our method leads to the emergence of complex skills by
exhibiting clear phase transitions. Furthermore, we show both theoretically
(via a latent state space coverage argument) and empirically that our method
has the potential to be applied to the exploration of stochastic,
partially-observed environments. We show that Adversarial Surprise learns more
complex behaviors, and explores more effectively than competitive baselines,
outperforming intrinsic motivation methods based on active inference,
novelty-seeking (Random Network Distillation (RND)), and multi-agent
unsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fickinger_A/0/1/0/all/0/1"&gt;Arnaud Fickinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1"&gt;Natasha Jaques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1"&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Michael Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1"&gt;Nicholas Rhinehart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berseth_G/0/1/0/all/0/1"&gt;Glen Berseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1"&gt;Stuart Russell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration. (arXiv:2107.07410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07410</id>
        <link href="http://arxiv.org/abs/2107.07410"/>
        <updated>2021-07-16T00:48:25.625Z</updated>
        <summary type="html"><![CDATA[Model-based Reinforcement Learning (RL) is a popular learning paradigm due to
its potential sample efficiency compared to model-free RL. However, existing
empirical model-based RL approaches lack the ability to explore. This work
studies a computationally and statistically efficient model-based algorithm for
both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes
(MDPs). For both models, our algorithm guarantees polynomial sample complexity
and only uses access to a planning oracle. Experimentally, we first demonstrate
the flexibility and efficacy of our algorithm on a set of exploration
challenging control tasks where existing empirical model-based RL approaches
completely fail. We then show that our approach retains excellent performance
even in common dense reward control benchmarks that do not require heavy
exploration. Finally, we demonstrate that our method can also perform
reward-free exploration efficiently. Our code can be found at
https://github.com/yudasong/PCMLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yuda Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proceedings of the Sixteenth Workshop on Logical Frameworks and Meta-Languages: Theory and Practice. (arXiv:2107.07376v1 [cs.LO])]]></title>
        <id>http://arxiv.org/abs/2107.07376</id>
        <link href="http://arxiv.org/abs/2107.07376"/>
        <updated>2021-07-16T00:48:25.611Z</updated>
        <summary type="html"><![CDATA[Logical frameworks and meta-languages form a common substrate for
representing, implementing and reasoning about a wide variety of deductive
systems of interest in logic and computer science. Their design, implementation
and their use in reasoning tasks, ranging from the correctness of software to
the properties of formal systems, have been the focus of considerable research
over the last two decades. This workshop brings together designers,
implementors and practitioners to discuss various aspects impinging on the
structure and utility of logical frameworks, including the treatment of
variable binding, inductive and co-inductive reasoning techniques and the
expressiveness and lucidity of the reasoning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_E/0/1/0/all/0/1"&gt;Elaine Pimentel&lt;/a&gt; (UFRN), &lt;a href="http://arxiv.org/find/cs/1/au:+Tassi_E/0/1/0/all/0/1"&gt;Enrico Tassi&lt;/a&gt; (Inria)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07261</id>
        <link href="http://arxiv.org/abs/2107.07261"/>
        <updated>2021-07-16T00:48:25.604Z</updated>
        <summary type="html"><![CDATA[Models pre-trained with a language modeling objective possess ample world
knowledge and language skills, but are known to struggle in tasks that require
reasoning. In this work, we propose to leverage semi-structured tables, and
automatically generate at scale question-paragraph pairs, where answering the
question requires reasoning over multiple facts in the paragraph. We add a
pre-training step over this synthetic data, which includes examples that
require 16 different reasoning skills such as number comparison, conjunction,
and fact composition. To improve data efficiency, we propose sampling
strategies that focus training on reasoning skills the model is currently
lacking. We evaluate our approach on three reading comprehension datasets that
are focused on reasoning, and show that our model, PReasM, substantially
outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling
examples based on current model errors leads to faster training and higher
overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1"&gt;Ori Yoran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1"&gt;Alon Talmor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging wisdom of the crowds to improve consensus among radiologists by real time, blinded collaborations on a digital swarm platform. (arXiv:2107.07341v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07341</id>
        <link href="http://arxiv.org/abs/2107.07341"/>
        <updated>2021-07-16T00:48:25.589Z</updated>
        <summary type="html"><![CDATA[Radiologists today play a key role in making diagnostic decisions and
labeling images for training A.I. algorithms. Low inter-reader reliability
(IRR) can be seen between experts when interpreting challenging cases. While
teams-based decisions are known to outperform individual decisions,
inter-personal biases often creep up in group interactions which limit
non-dominant participants from expressing true opinions. To overcome the dual
problems of low consensus and inter-personal bias, we explored a solution
modeled on biological swarms of bees. Two separate cohorts; three radiologists
and five radiology residents collaborated on a digital swarm platform in real
time and in a blinded fashion, grading meniscal lesions on knee MR exams. These
consensus votes were benchmarked against clinical (arthroscopy) and
radiological (senior-most radiologist) observations. The IRR of the consensus
votes was compared to the IRR of the majority and most confident votes of the
two cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm
votes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm
votes over majority vote, was observed. The 5-resident swarm had an even higher
improvement of 32% in IRR over majority vote. Swarm consensus votes also
improved specificity by up to 50%. The swarm consensus votes outperformed
individual and majority vote decisions in both the radiologists and resident
cohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating
positive effect of increased swarm size. The attending and resident swarms also
outperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a
digital swarm platform improved agreement and allows participants to express
judgement free intent, resulting in superior clinical performance and robust
A.I. training labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rutwik Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1"&gt;Bruno Astuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1"&gt;Tyler Gleason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1"&gt;Will Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1"&gt;Justin Banaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1"&gt;Kevin Sweetwood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1"&gt;Allen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1"&gt;Rina Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1"&gt;Kevin McGill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1"&gt;Thomas Link&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1"&gt;Jason Crane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1"&gt;Valentina Pedoia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Sharmila Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A multi-schematic classifier-independent oversampling approach for imbalanced datasets. (arXiv:2107.07349v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07349</id>
        <link href="http://arxiv.org/abs/2107.07349"/>
        <updated>2021-07-16T00:48:25.573Z</updated>
        <summary type="html"><![CDATA[Over 85 oversampling algorithms, mostly extensions of the SMOTE algorithm,
have been built over the past two decades, to solve the problem of imbalanced
datasets. However, it has been evident from previous studies that different
oversampling algorithms have different degrees of efficiency with different
classifiers. With numerous algorithms available, it is difficult to decide on
an oversampling algorithm for a chosen classifier. Here, we overcome this
problem with a multi-schematic and classifier-independent oversampling
approach: ProWRAS(Proximity Weighted Random Affine Shadowsampling). ProWRAS
integrates the Localized Random Affine Shadowsampling (LoRAS)algorithm and the
Proximity Weighted Synthetic oversampling (ProWSyn) algorithm. By controlling
the variance of the synthetic samples, as well as a proximity-weighted
clustering system of the minority classdata, the ProWRAS algorithm improves
performance, compared to algorithms that generate synthetic samples through
modelling high dimensional convex spaces of the minority class. ProWRAS has
four oversampling schemes, each of which has its unique way to model the
variance of the generated data. Most importantly, the performance of ProWRAS
with proper choice of oversampling schemes, is independent of the classifier
used. We have benchmarked our newly developed ProWRAS algorithm against five
sate-of-the-art oversampling models and four different classifiers on 20
publicly available datasets. ProWRAS outperforms other oversampling algorithms
in a statistically significant way, in terms of both F1-score and Kappa-score.
Moreover, we have introduced a novel measure for classifier independence
I-score, and showed quantitatively that ProWRAS performs better, independent of
the classifier used. In practice, ProWRAS customizes synthetic sample
generation according to a classifier of choice and thereby reduces benchmarking
efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bej_S/0/1/0/all/0/1"&gt;Saptarshi Bej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schultz_K/0/1/0/all/0/1"&gt;Kristian Schultz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1"&gt;Prashant Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfien_M/0/1/0/all/0/1"&gt;Markus Wolfien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolkenhauer_O/0/1/0/all/0/1"&gt;Olaf Wolkenhauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Ant Swarm-Based Data Clustering. (arXiv:2107.07382v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.07382</id>
        <link href="http://arxiv.org/abs/2107.07382"/>
        <updated>2021-07-16T00:48:25.558Z</updated>
        <summary type="html"><![CDATA[Biologically inspired computing techniques are very effective and useful in
many areas of research including data clustering. Ant clustering algorithm is a
nature-inspired clustering technique which is extensively studied for over two
decades. In this study, we extend the ant clustering algorithm (ACA) to a
hybrid ant clustering algorithm (hACA). Specifically, we include a genetic
algorithm in standard ACA to extend the hybrid algorithm for better
performance. We also introduced novel pick up and drop off rules to speed up
the clustering performance. We study the performance of the hACA algorithm and
compare with standard ACA as a benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azam_M/0/1/0/all/0/1"&gt;Md Ali Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hossen_A/0/1/0/all/0/1"&gt;Abir Hossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Hafizur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting. (arXiv:2107.07240v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07240</id>
        <link href="http://arxiv.org/abs/2107.07240"/>
        <updated>2021-07-16T00:48:25.541Z</updated>
        <summary type="html"><![CDATA[We study the realistic potential of conducting backdoor attack against deep
neural networks (DNNs) during deployment stage. Specifically, our goal is to
design a deployment-stage backdoor attack algorithm that is both threatening
and realistically implementable. To this end, we propose Subnet Replacement
Attack (SRA), which is capable of embedding backdoor into DNNs by directly
modifying a limited number of model parameters. Considering the realistic
practicability, we abandon the strong white-box assumption widely adopted in
existing studies, instead, our algorithm works in a gray-box setting, where
architecture information of the victim model is available but the adversaries
do not have any knowledge of parameter values. The key philosophy underlying
our approach is -- given any neural network instance (regardless of its
specific parameter values) of a certain architecture, we can always embed a
backdoor into that model instance, by replacing a very narrow subnet of a
benign model (without backdoor) with a malicious backdoor subnet, which is
designed to be sensitive (fire large activation value) to a particular backdoor
trigger pattern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiangyu Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jifeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chulin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07110</id>
        <link href="http://arxiv.org/abs/2107.07110"/>
        <updated>2021-07-16T00:48:25.503Z</updated>
        <summary type="html"><![CDATA[We present a generic method for recurrently using the same parameters for
many different convolution layers to build a deep network. Specifically, for a
network, we create a recurrent parameter generator (RPG), from which the
parameters of each convolution layer are generated. Though using recurrent
models to build a deep convolutional neural network (CNN) is not entirely new,
our method achieves significant performance gain compared to the existing
works. We demonstrate how to build a one-layer neural network to achieve
similar performance compared to other traditional CNN models on various
applications and datasets. Such a method allows us to build an arbitrarily
complex neural network with any amount of parameters. For example, we build a
ResNet34 with model parameters reduced by more than $400$ times, which still
achieves $41.6\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG
can be applied at different scales, such as layers, blocks, or even
sub-networks. Specifically, we use the RPG to build a ResNet18 network with the
number of weights equivalent to one convolutional layer of a conventional
ResNet and show this model can achieve $67.2\%$ ImageNet top-1 accuracy. The
proposed method can be viewed as an inverse approach to model compression.
Rather than removing the unused parameters from a large model, it aims to
squeeze more information into a small number of parameters. Extensive
experiment results are provided to demonstrate the power of the proposed
recurrent parameter generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1"&gt;Brian Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Input Dependent Sparse Gaussian Processes. (arXiv:2107.07281v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07281</id>
        <link href="http://arxiv.org/abs/2107.07281"/>
        <updated>2021-07-16T00:48:25.487Z</updated>
        <summary type="html"><![CDATA[Gaussian Processes (GPs) are Bayesian models that provide uncertainty
estimates associated to the predictions made. They are also very flexible due
to their non-parametric nature. Nevertheless, GPs suffer from poor scalability
as the number of training instances N increases. More precisely, they have a
cubic cost with respect to $N$. To overcome this problem, sparse GP
approximations are often used, where a set of $M \ll N$ inducing points is
introduced during training. The location of the inducing points is learned by
considering them as parameters of an approximate posterior distribution $q$.
Sparse GPs, combined with variational inference for inferring $q$, reduce the
training cost of GPs to $\mathcal{O}(M^3)$. Critically, the inducing points
determine the flexibility of the model and they are often located in regions of
the input space where the latent function changes. A limitation is, however,
that for some learning tasks a large number of inducing points may be required
to obtain a good prediction performance. To address this limitation, we propose
here to amortize the computation of the inducing points locations, as well as
the parameters of the variational posterior approximation q. For this, we use a
neural network that receives the observed data as an input and outputs the
inducing points locations and the parameters of $q$. We evaluate our method in
several experiments, showing that it performs similar or better than other
state-of-the-art sparse variational GP approaches. However, with our method the
number of inducing points is reduced drastically due to their dependency on the
input data. This makes our method scale to larger datasets and have faster
training and prediction times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafrasteh_B/0/1/0/all/0/1"&gt;Bahram Jafrasteh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villacampa_Calvo_C/0/1/0/all/0/1"&gt;Carlos Villacampa-Calvo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1"&gt;Daniel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Bayesian Learning with Metropolis-Adjusted Hamiltonian Monte Carlo. (arXiv:2107.07211v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07211</id>
        <link href="http://arxiv.org/abs/2107.07211"/>
        <updated>2021-07-16T00:48:25.480Z</updated>
        <summary type="html"><![CDATA[Federated learning performed by a decentralized networks of agents is
becoming increasingly important with the prevalence of embedded software on
autonomous devices. Bayesian approaches to learning benefit from offering more
information as to the uncertainty of a random quantity, and Langevin and
Hamiltonian methods are effective at realizing sampling from an uncertain
distribution with large parameter dimensions. Such methods have only recently
appeared in the decentralized setting, and either exclusively use stochastic
gradient Langevin and Hamiltonian Monte Carlo approaches that require a
diminishing stepsize to asymptotically sample from the posterior and are known
in practice to characterize uncertainty less faithfully than constant step-size
methods with a Metropolis adjustment, or assume strong convexity properties of
the potential function. We present the first approach to incorporating constant
stepsize Metropolis-adjusted HMC in the decentralized sampling framework, show
theoretical guarantees for consensus and probability distance to the posterior
stationary distribution, and demonstrate their effectiveness numerically on
standard real world problems, including decentralized learning of neural
networks which is known to be highly non-convex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kungurtsev_V/0/1/0/all/0/1"&gt;Vyacheslav Kungurtsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1"&gt;Adam Cobb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalaian_B/0/1/0/all/0/1"&gt;Brian Jalaian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07314</id>
        <link href="http://arxiv.org/abs/2107.07314"/>
        <updated>2021-07-16T00:48:25.471Z</updated>
        <summary type="html"><![CDATA[Automating report generation for medical imaging promises to reduce workload
and assist diagnosis in clinical practice. Recent work has shown that deep
learning models can successfully caption natural images. However, learning from
medical data is challenging due to the diversity and uncertainty inherent in
the reports written by different radiologists with discrepant expertise and
experience. To tackle these challenges, we propose variational topic inference
for automatic report generation. Specifically, we introduce a set of topics as
latent variables to guide sentence generation by aligning image and language
modalities in a latent space. The topics are inferred in a conditional
variational inference framework, with each topic governing the generation of a
sentence in the report. Further, we adopt a visual attention module that
enables the model to attend to different locations in the image and generate
more informative descriptions. We conduct extensive experiments on two
benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results
demonstrate that our proposed variational topic inference method can generate
novel reports rather than mere copies of reports used in training, while still
achieving comparable performance to state-of-the-art methods in terms of
standard language generation criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1"&gt;Ivona Najdenkoska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1"&gt;Marcel Worring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07075</id>
        <link href="http://arxiv.org/abs/2107.07075"/>
        <updated>2021-07-16T00:48:25.464Z</updated>
        <summary type="html"><![CDATA[The recent success of deep learning has partially been driven by training
increasingly overparametrized networks on ever larger datasets. It is therefore
natural to ask: how much of the data is superfluous, which examples are
important for generalization, and how do we find them? In this work, we make
the striking observation that, on standard vision benchmarks, the initial loss
gradient norm of individual training examples, averaged over several weight
initializations, can be used to identify a smaller set of training data that is
important for generalization. Furthermore, after only a few epochs of training,
the information in gradient norms is reflected in the normed error--L2 distance
between the predicted probabilities and one hot labels--which can be used to
prune a significant fraction of the dataset without sacrificing test accuracy.
Based on this, we propose data pruning methods which use only local information
early in training, and connect them to recent work that prunes data by
discarding examples that are rarely forgotten over the course of training. Our
methods also shed light on how the underlying data distribution shapes the
training dynamics: they rank examples based on their importance for
generalization, detect noisy examples and identify subspaces of the model's
data representation that are relatively stable over training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1"&gt;Mansheej Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1"&gt;Gintare Karolina Dziugaite&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parsimony-Enhanced Sparse Bayesian Learning for Robust Discovery of Partial Differential Equations. (arXiv:2107.07040v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07040</id>
        <link href="http://arxiv.org/abs/2107.07040"/>
        <updated>2021-07-16T00:48:25.449Z</updated>
        <summary type="html"><![CDATA[Robust physics discovery is of great interest for many scientific and
engineering fields. Inspired by the principle that a representative model is
the one simplest possible, a new model selection criteria considering both
model's Parsimony and Sparsity is proposed. A Parsimony Enhanced Sparse
Bayesian Learning (PeSBL) method is developed for discovering the governing
Partial Differential Equations (PDEs) of nonlinear dynamical systems. Compared
with the conventional Sparse Bayesian Learning (SBL) method, the PeSBL method
promotes parsimony of the learned model in addition to its sparsity. In this
method, the parsimony of model terms is evaluated using their locations in the
prescribed candidate library, for the first time, considering the increased
complexity with the power of polynomials and the order of spatial derivatives.
Subsequently, the model parameters are updated through Bayesian inference with
the raw data. This procedure aims to reduce the error associated with the
possible loss of information in data preprocessing and numerical
differentiation prior to sparse regression. Results of numerical case studies
indicate that the governing PDEs of many canonical dynamical systems can be
correctly identified using the proposed PeSBL method from highly noisy data (up
to 50% in the current study). Next, the proposed methodology is extended for
stochastic PDE learning where all parameters and modeling error are considered
as random variables. Hierarchical Bayesian Inference (HBI) is integrated with
the proposed framework for stochastic PDE learning from a population of
observations. Finally, the proposed PeSBL is demonstrated for system response
prediction with uncertainties and anomaly diagnosis. Codes of all demonstrated
examples in this study are available on the website: https://github.com/ymlasu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search. (arXiv:2107.06997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06997</id>
        <link href="http://arxiv.org/abs/2107.06997"/>
        <updated>2021-07-16T00:48:25.429Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) has been successfully applied to a wide range of
application domains, including safety-critical ones. Several DL testing
approaches have been recently proposed in the literature but none of them aims
to assess how different interpretable features of the generated inputs affect
the system's behaviour. In this paper, we resort to Illumination Search to find
the highest-performing test cases (i.e., misbehaving and closest to
misbehaving), spread across the cells of a map representing the feature space
of the system. We introduce a methodology that guides the users of our approach
in the tasks of identifying and quantifying the dimensions of the feature space
for a given domain. We developed DeepHyperion, a search-based tool for DL
systems that illuminates, i.e., explores at large, the feature space, by
providing developers with an interpretable feature map where automatically
generated inputs are placed along with information about the exposed
behaviours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zohdinasab_T/0/1/0/all/0/1"&gt;Tahereh Zohdinasab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riccio_V/0/1/0/all/0/1"&gt;Vincenzo Riccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gambi_A/0/1/0/all/0/1"&gt;Alessio Gambi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonella_P/0/1/0/all/0/1"&gt;Paolo Tonella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07271</id>
        <link href="http://arxiv.org/abs/2107.07271"/>
        <updated>2021-07-16T00:48:25.382Z</updated>
        <summary type="html"><![CDATA[Domain shift is a problem commonly encountered when developing automated
histopathology pipelines. The performance of machine learning models such as
convolutional neural networks within automated histopathology pipelines is
often diminished when applying them to novel data domains due to factors
arising from differing staining and scanning protocols. The Dual-Channel
Auto-Encoder (DCAE) model was previously shown to produce feature
representations that are less sensitive to appearance variation introduced by
different digital slide scanners. In this work, the Multi-Channel Auto-Encoder
(MCAE) model is presented as an extension to DCAE which learns from more than
two domains of data. Additionally, a synthetic dataset is generated using
CycleGANs that contains aligned tissue images that have had their appearance
synthetically modified. Experimental results show that the MCAE model produces
feature representations that are less sensitive to inter-domain variations than
the comparative StaNoSA method when tested on the novel synthetic data.
Additionally, the MCAE and StaNoSA models are tested on a novel tissue
classification task. The results of this experiment show the MCAE model out
performs the StaNoSA model by 5 percentage-points in the f1-score. These
results show that the MCAE model is able to generalise better to novel data and
tasks than existing approaches by actively learning normalised feature
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1"&gt;Andrew Moyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1"&gt;Richard Gault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1"&gt;Ji Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1"&gt;Danny Crookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration. (arXiv:2011.06572v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06572</id>
        <link href="http://arxiv.org/abs/2011.06572"/>
        <updated>2021-07-16T00:48:25.365Z</updated>
        <summary type="html"><![CDATA[We show that standard extragradient methods (i.e. mirror prox and dual
extrapolation) recover optimal accelerated rates for first-order minimization
of smooth convex functions. To obtain this result we provide a fine-grained
characterization of the convergence rates of extragradient methods for solving
monotone variational inequalities in terms of a natural condition we call
relative Lipschitzness. We further generalize this framework to handle local
and randomized notions of relative Lipschitzness and thereby recover rates for
box-constrained $\ell_\infty$ regression based on area convexity and complexity
bounds achieved by accelerated (randomized) coordinate descent for smooth
convex function minimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_M/0/1/0/all/0/1"&gt;Michael B. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sidford_A/0/1/0/all/0/1"&gt;Aaron Sidford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping Learning Algorithms on Data, a useful step for optimizing performances and their comparison. (arXiv:2107.06981v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06981</id>
        <link href="http://arxiv.org/abs/2107.06981"/>
        <updated>2021-07-16T00:48:25.343Z</updated>
        <summary type="html"><![CDATA[In the paper, we propose a novel methodology to map learning algorithms on
data (performance map) in order to gain more insights in the distribution of
their performances across their parameter space. This methodology provides
useful information when selecting a learner's best configuration for the data
at hand, and it also enhances the comparison of learners across learning
contexts. In order to explain the proposed methodology, the study introduces
the notions of learning context, performance map, and high performance
function. It then applies these concepts to a variety of learning contexts to
show how their use can provide more insights in a learner's behavior, and can
enhance the comparison of learners across learning contexts. The study is
completed by an extensive experimental study describing how the proposed
methodology can be applied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neri_F/0/1/0/all/0/1"&gt;Filippo Neri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:25.336Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assign Hysteresis Parameter For Ericsson BTS Power Saving Algorithm Using Unsupervised Learning. (arXiv:2107.07412v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07412</id>
        <link href="http://arxiv.org/abs/2107.07412"/>
        <updated>2021-07-16T00:48:25.316Z</updated>
        <summary type="html"><![CDATA[Gaza Strip suffers from a chronic electricity deficit that affects all
industries including the telecommunication field, so there is a need to
optimize and reduce power consumption of the telecommunication equipment. In
this paper we propose a new model that helps GSM radio frequency engineers to
choose the optimal value of hysteresis parameter for Ericsson BTS power saving
algorithm which aims to switch OFF unused frequency channels, our model is
based on unsupervised machine learning clustering K-means algorithm. By using
our model with BTS power saving algorithm we reduce number of active TRX by
20.9%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sahmoud_T/0/1/0/all/0/1"&gt;Thaer Sahmoud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashor_W/0/1/0/all/0/1"&gt;Wesam Ashor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition. (arXiv:2107.03107v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03107</id>
        <link href="http://arxiv.org/abs/2107.03107"/>
        <updated>2021-07-16T00:48:25.308Z</updated>
        <summary type="html"><![CDATA[As various databases of facial expressions have been made accessible over the
last few decades, the Facial Expression Recognition (FER) task has gotten a lot
of interest. The multiple sources of the available databases raised several
challenges for facial recognition task. These challenges are usually addressed
by Convolution Neural Network (CNN) architectures. Different from CNN models, a
Transformer model based on attention mechanism has been presented recently to
address vision tasks. One of the major issue with Transformers is the need of a
large data for training, while most FER databases are limited compared to other
vision applications. Therefore, we propose in this paper to learn a vision
Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The
proposed method is evaluated on different publicly available FER databases
including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model
outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive
results on JAFFE and RAF-DB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aouayeb_M/0/1/0/all/0/1"&gt;Mouath Aouayeb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soladie_C/0/1/0/all/0/1"&gt;Catherine Soladie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kpalma_K/0/1/0/all/0/1"&gt;Kidiyo Kpalma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seguier_R/0/1/0/all/0/1"&gt;Renaud Seguier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing GANs: A Likelihood Ratio Approach. (arXiv:2002.00865v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00865</id>
        <link href="http://arxiv.org/abs/2002.00865"/>
        <updated>2021-07-16T00:48:25.301Z</updated>
        <summary type="html"><![CDATA[We are interested in the design of generative networks. The training of these
mathematical structures is mostly performed with the help of adversarial
(min-max) optimization problems. We propose a simple methodology for
constructing such problems assuring, at the same time, consistency of the
corresponding solution. We give characteristic examples developed by our
method, some of which can be recognized from other applications, and some are
introduced here for the first time. We present a new metric, the likelihood
ratio, that can be employed online to examine the convergence and stability
during the training of different Generative Adversarial Networks (GANs).
Finally, we compare various possibilities by applying them to well-known
datasets using neural networks of different configurations and sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1"&gt;Kalliopi Basioti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustakides_G/0/1/0/all/0/1"&gt;George V. Moustakides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators. (arXiv:2107.07260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07260</id>
        <link href="http://arxiv.org/abs/2107.07260"/>
        <updated>2021-07-16T00:48:25.283Z</updated>
        <summary type="html"><![CDATA[We propose a generative adversarial network with multiple discriminators,
where each discriminator is specialized to distinguish the subset of a real
dataset. This approach facilitates learning a generator coinciding with the
underlying data distribution and thus mitigates the chronic mode collapse
problem. From the inspiration of multiple choice learning, we guide each
discriminator to have expertise in the subset of the entire data and allow the
generator to find reasonable correspondences between the latent and real data
spaces automatically without supervision for training examples and the number
of discriminators. Despite the use of multiple discriminators, the backbone
networks are shared across the discriminators and the increase of training cost
is minimized. We demonstrate the effectiveness of our algorithm in the standard
datasets using multiple evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMNet: Latent Feature-wise Mapping Network for Cleaning up Noisy Micro-Doppler Spectrogram. (arXiv:2107.07312v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07312</id>
        <link href="http://arxiv.org/abs/2107.07312"/>
        <updated>2021-07-16T00:48:25.277Z</updated>
        <summary type="html"><![CDATA[Micro-Doppler signatures contain considerable information about target
dynamics. However, the radar sensing systems are easily affected by noisy
surroundings, resulting in uninterpretable motion patterns on the micro-Doppler
spectrogram. Meanwhile, radar returns often suffer from multipath, clutter and
interference. These issues lead to difficulty in, for example motion feature
extraction, activity classification using micro Doppler signatures ($\mu$-DS),
etc. In this paper, we propose a latent feature-wise mapping strategy, called
Feature Mapping Network (FMNet), to transform measured spectrograms so that
they more closely resemble the output from a simulation under the same
conditions. Based on measured spectrogram and the matched simulated data, our
framework contains three parts: an Encoder which is used to extract latent
representations/features, a Decoder outputs reconstructed spectrogram according
to the latent features, and a Discriminator minimizes the distance of latent
features of measured and simulated data. We demonstrate the FMNet with six
activities data and two experimental scenarios, and final results show strong
enhanced patterns and can keep actual motion information to the greatest
extent. On the other hand, we also propose a novel idea which trains a
classifier with only simulated data and predicts new measured samples after
cleaning them up with the FMNet. From final classification results, we can see
significant improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vishwakarma_S/0/1/0/all/0/1"&gt;Shelly Vishwakarma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1"&gt;Fangzhan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon Julier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chetty_K/0/1/0/all/0/1"&gt;Kevin Chetty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07331</id>
        <link href="http://arxiv.org/abs/2107.07331"/>
        <updated>2021-07-16T00:48:25.265Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) based on IMU sensors is an essential domain
in ubiquitous computing. Because of the improving trend to deploy artificial
intelligence into IoT devices or smartphones, more researchers design the HAR
models for embedded devices. We propose a plug-and-play HAR modeling pipeline
with multi-level distillation to build deep convolutional HAR models with
native support of embedded devices. SMLDist consists of stage distillation,
memory distillation, and logits distillation, which covers all the information
flow of the deep models. Stage distillation constrains the learning direction
of the intermediate features. Memory distillation teaches the student models
how to explain and store the inner relationship between high-dimensional
features based on Hopfield networks. Logits distillation constructs distilled
logits by a smoothed conditional rule to keep the probable distribution and
improve the correctness of the soft target. We compare the performance of
accuracy, F1 macro score, and energy cost on the embedded platform of various
state-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The
produced model has well balance with robustness, efficiency, and accuracy.
SMLDist can also compress the models with minor performance loss in an equal
compression rate than other state-of-the-art knowledge distillation methods on
seven public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Runze Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haiyong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xuechun Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhiqing Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yida Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07305</id>
        <link href="http://arxiv.org/abs/2107.07305"/>
        <updated>2021-07-16T00:48:25.258Z</updated>
        <summary type="html"><![CDATA[Activation sparsity improves compute efficiency and resource utilization in
sparsity-aware neural network accelerators. As the predominant operation in
DNNs is multiply-accumulate (MAC) of activations with weights to compute inner
products, skipping operations where (at least) one of the two operands is zero
can make inference more efficient in terms of latency and power. Spatial
sparsification of activations is a popular topic in DNN literature and several
methods have already been established to bias a DNN for it. On the other hand,
temporal sparsity is an inherent feature of bio-inspired spiking neural
networks (SNNs), which neuromorphic processing exploits for hardware
efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic
much less explored in DNN literature, but in perfect resonance with the trend
in DNN, to shift from static signal processing to more streaming signal
processing. Towards this goal, in this paper we introduce a new DNN layer
(called Delta Activation Layer), whose sole purpose is to promote temporal
sparsity of activations during training. A Delta Activation Layer casts
temporal sparsity into spatial activation sparsity to be exploited when
performing sparse tensor multiplications in hardware. By employing delta
inference and ``the usual'' spatial sparsification heuristics during training,
the resulting model learns to exploit not only spatial but also temporal
activation sparsity (for a given input data distribution). One may use the
Delta Activation Layer either during vanilla training or during a refinement
phase. We have implemented Delta Activation Layer as an extension of the
standard Tensoflow-Keras library, and applied it to train deep neural networks
on the Human Action Recognition (UCF101) dataset. We report an almost 3x
improvement of activation sparsity, with recoverable loss of model accuracy
after longer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1"&gt;Amirreza Yousefzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1"&gt;Manolis Sifalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tournesol: A quest for a large, secure and trustworthy database of reliable human judgments. (arXiv:2107.07334v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07334</id>
        <link href="http://arxiv.org/abs/2107.07334"/>
        <updated>2021-07-16T00:48:25.252Z</updated>
        <summary type="html"><![CDATA[Today's large-scale algorithms have become immensely influential, as they
recommend and moderate the content that billions of humans are exposed to on a
daily basis. They are the de-facto regulators of our societies' information
diet, from shaping opinions on public health to organizing groups for social
movements. This creates serious concerns, but also great opportunities to
promote quality information. Addressing the concerns and seizing the
opportunities is a challenging, enormous and fabulous endeavor, as intuitively
appealing ideas often come with unwanted {\it side effects}, and as it requires
us to think about what we deeply prefer.

Understanding how today's large-scale algorithms are built is critical to
determine what interventions will be most effective. Given that these
algorithms rely heavily on {\it machine learning}, we make the following key
observation: \emph{any algorithm trained on uncontrolled data must not be
trusted}. Indeed, a malicious entity could take control over the data, poison
it with dangerously manipulative fabricated inputs, and thereby make the
trained algorithm extremely unsafe. We thus argue that the first step towards
safe and ethical large-scale algorithms must be the collection of a large,
secure and trustworthy dataset of reliable human judgments.

To achieve this, we introduce \emph{Tournesol}, an open source platform
available at \url{https://tournesol.app}. Tournesol aims to collect a large
database of human judgments on what algorithms ought to widely recommend (and
what they ought to stop widely recommending). We outline the structure of the
Tournesol database, the key features of the Tournesol platform and the main
hurdles that must be overcome to make it a successful project. Most
importantly, we argue that, if successful, Tournesol may then serve as the
essential foundation for any safe and ethical large-scale algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea;-Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faucon_L/0/1/0/all/0/1"&gt;Louis Faucon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jungo_A/0/1/0/all/0/1"&gt;Aidan Jungo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volodin_S/0/1/0/all/0/1"&gt;Sergei Volodin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papuc_D/0/1/0/all/0/1"&gt;Dalia Papuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liossatos_O/0/1/0/all/0/1"&gt;Orfeas Liossatos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crulis_B/0/1/0/all/0/1"&gt;Ben Crulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighanimine_M/0/1/0/all/0/1"&gt;Mariame Tighanimine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantin_I/0/1/0/all/0/1"&gt;Isabela Constantin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kucherenko_A/0/1/0/all/0/1"&gt;Anastasiia Kucherenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Alexandre Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grimberg_F/0/1/0/all/0/1"&gt;Felix Grimberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nitu_V/0/1/0/all/0/1"&gt;Vlad Nitu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vossen_C/0/1/0/all/0/1"&gt;Chris Vossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1"&gt;El-Mahdi El-Mhamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework for bandit multiple testing. (arXiv:2107.07322v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07322</id>
        <link href="http://arxiv.org/abs/2107.07322"/>
        <updated>2021-07-16T00:48:25.245Z</updated>
        <summary type="html"><![CDATA[In bandit multiple hypothesis testing, each arm corresponds to a different
null hypothesis that we wish to test, and the goal is to design adaptive
algorithms that correctly identify large set of interesting arms (true
discoveries), while only mistakenly identifying a few uninteresting ones (false
discoveries). One common metric in non-bandit multiple testing is the false
discovery rate (FDR). We propose a unified, modular framework for bandit FDR
control that emphasizes the decoupling of exploration and summarization of
evidence. We utilize the powerful martingale-based concept of ``e-processes''
to ensure FDR control for arbitrary composite nulls, exploration rules and
stopping times in generic problem settings. In particular, valid FDR control
holds even if the reward distributions of the arms could be dependent, multiple
arms may be queried simultaneously, and multiple (cooperating or competing)
agents may be querying arms, covering combinatorial semi-bandit type settings
as well. Prior work has considered in great detail the setting where each arm's
reward distribution is independent and sub-Gaussian, and a single arm is
queried at each step. Our framework recovers matching sample complexity
guarantees in this special case, and performs comparably or better in practice.
For other settings, sample complexities will depend on the finer details of the
problem (composite nulls being tested, exploration algorithm, data dependence
structure, stopping rule) and we do not explore these; our contribution is to
show that the FDR guarantee is clean and entirely agnostic to these details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruodu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramdas_A/0/1/0/all/0/1"&gt;Aaditya Ramdas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutation is all you need. (arXiv:2107.07343v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07343</id>
        <link href="http://arxiv.org/abs/2107.07343"/>
        <updated>2021-07-16T00:48:25.203Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) promises to make deep learning accessible to
non-experts by automating architecture engineering of deep neural networks.
BANANAS is one state-of-the-art NAS method that is embedded within the Bayesian
optimization framework. Recent experimental findings have demonstrated the
strong performance of BANANAS on the NAS-Bench-101 benchmark being determined
by its path encoding and not its choice of surrogate model. We present
experimental results suggesting that the performance of BANANAS on the
NAS-Bench-301 benchmark is determined by its acquisition function optimizer,
which minimally mutates the incumbent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1"&gt;Lennart Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1"&gt;Florian Pfisterer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1"&gt;Martin Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07344</id>
        <link href="http://arxiv.org/abs/2107.07344"/>
        <updated>2021-07-16T00:48:25.194Z</updated>
        <summary type="html"><![CDATA[The increasing population of elderly people is associated with the need to
meet their increasing requirements and to provide solutions that can improve
their quality of life in a smart home. In addition to fear and anxiety towards
interfacing with systems; cognitive disabilities, weakened memory, disorganized
behavior and even physical limitations are some of the problems that elderly
people tend to face with increasing age. The essence of providing
technology-based solutions to address these needs of elderly people and to
create smart and assisted living spaces for the elderly; lies in developing
systems that can adapt by addressing their diversity and can augment their
performances in the context of their day to day goals. Therefore, this work
proposes a framework for development of a Personalized Intelligent Assistant to
help elderly people perform Activities of Daily Living (ADLs) in a smart and
connected Internet of Things (IoT) based environment. This Personalized
Intelligent Assistant can analyze different tasks performed by the user and
recommend activities by considering their daily routine, current affective
state and the underlining user experience. To uphold the efficacy of this
proposed framework, it has been tested on a couple of datasets for modelling an
average user and a specific user respectively. The results presented show that
the model achieves a performance accuracy of 73.12% when modelling a specific
user, which is considerably higher than its performance while modelling an
average user, this upholds the relevance for development and implementation of
this proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized ReLU Activation for Uncertainty Estimation of Deep Neural Networks. (arXiv:2107.07197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07197</id>
        <link href="http://arxiv.org/abs/2107.07197"/>
        <updated>2021-07-16T00:48:25.172Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have successfully learned useful data
representations in various tasks, however, assessing the reliability of these
representations remains a challenge. Deep Ensemble is widely considered the
state-of-the-art method for uncertainty estimation, but it is very expensive to
train and test. MC-Dropout is another alternative method, which is less
expensive but lacks the diversity of predictions. To get more diverse
predictions in less time, we introduce Randomized ReLU Activation (RRA)
framework. Under the framework, we propose two strategies, MC-DropReLU and
MC-RReLU, to estimate uncertainty. Instead of randomly dropping some neurons of
the network as in MC-Dropout, the RRA framework adds randomness to the
activation function module, making the outputs diverse. As far as we know, this
is the first attempt to add randomness to the activation function module to
generate predictive uncertainty. We analyze and compare the output diversity of
MC-Dropout and our method from the variance perspective and obtain the
relationship between the hyperparameters and output diversity in the two
methods. Moreover, our method is simple to implement and does not need to
modify the existing model. We experimentally validate the RRA framework on
three widely used datasets, CIFAR10, CIFAR100, and TinyImageNet. The
experiments demonstrate that our method has competitive performance but is more
favorable in training time and memory requirements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yufeng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07148</id>
        <link href="http://arxiv.org/abs/2107.07148"/>
        <updated>2021-07-16T00:48:25.154Z</updated>
        <summary type="html"><![CDATA[The attractiveness of a property is one of the most interesting, yet
challenging, categories to model. Image characteristics are used to describe
certain attributes, and to examine the influence of visual factors on the price
or timeframe of the listing. In this paper, we propose a set of techniques for
the extraction of visual features for efficient numerical inclusion in
modern-day predictive algorithms. We discuss techniques such as Shannon's
entropy, calculating the center of gravity, employing image segmentation, and
using Convolutional Neural Networks. After comparing these techniques as
applied to a set of property-related images (indoor, outdoor, and satellite),
we conclude the following: (i) the entropy is the most efficient single-digit
visual measure for housing price prediction; (ii) image segmentation is the
most important visual feature for the prediction of housing lifespan; and (iii)
deep image features can be used to quantify interior characteristics and
contribute to captivation modeling. The set of 40 image features selected here
carries a significant amount of predictive power and outperforms some of the
strongest metadata predictors. Without any need to replace a human expert in a
real-estate appraisal process, we conclude that the techniques presented in
this paper can efficiently describe visible characteristics, thus introducing
perceived attractiveness as a quantitative measure into the predictive modeling
of housing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1"&gt;Zona Kostic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1"&gt;Aleksandar Jevremovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07106</id>
        <link href="http://arxiv.org/abs/2107.07106"/>
        <updated>2021-07-16T00:48:25.137Z</updated>
        <summary type="html"><![CDATA[We propose a method to easily modify existing offline Recommender Systems to
run online using Transfer Learning. Online Learning for Recommender Systems has
two main advantages: quality and scale. Like many Machine Learning algorithms
in production if not regularly retrained will suffer from Concept Drift. A
policy that is updated frequently online can adapt to drift faster than a batch
system. This is especially true for user-interaction systems like recommenders
where the underlying distribution can shift drastically to follow user
behaviour. As a platform grows rapidly like Grubhub, the cost of running batch
training jobs becomes material. A shift from stateless batch learning offline
to stateful incremental learning online can recover, for example, at Grubhub,
up to a 45x cost savings and a +20% metrics increase. There are a few
challenges to overcome with the transition to online stateful learning, namely
convergence, non-stationary embeddings and off-policy evaluation, which we
explore from our experiences running this system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1"&gt;Alex Egg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeFed: A Principled Decentralized and Privacy-Preserving Federated Learning Algorithm. (arXiv:2107.07171v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07171</id>
        <link href="http://arxiv.org/abs/2107.07171"/>
        <updated>2021-07-16T00:48:25.121Z</updated>
        <summary type="html"><![CDATA[Federated learning enables a large number of clients to participate in
learning a shared model while maintaining the training data stored in each
client, which protects data privacy and security. Till now, federated learning
frameworks are built in a centralized way, in which a central client is needed
for collecting and distributing information from every other client. This not
only leads to high communication pressure at the central client, but also
renders the central client highly vulnerable to failure and attack. Here we
propose a principled decentralized federated learning algorithm (DeFed), which
removes the central client in the classical Federated Averaging (FedAvg)
setting and only relies information transmission between clients and their
local neighbors. The proposed DeFed algorithm is proven to reach the global
minimum with a convergence rate of $O(1/T)$ when the loss function is smooth
and strongly convex, where $T$ is the number of iterations in gradient descent.
Finally, the proposed algorithm has been applied to a number of toy examples to
demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Ye Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Ruijuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Maolin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_F/0/1/0/all/0/1"&gt;Feng Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinlei Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning. (arXiv:2107.07184v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07184</id>
        <link href="http://arxiv.org/abs/2107.07184"/>
        <updated>2021-07-16T00:48:25.115Z</updated>
        <summary type="html"><![CDATA[Exploration in reinforcement learning is a challenging problem: in the worst
case, the agent must search for reward states that could be hidden anywhere in
the state space. Can we define a more tractable class of RL problems, where the
agent is provided with examples of successful outcomes? In this problem
setting, the reward function can be obtained automatically by training a
classifier to categorize states as successful or not. If trained properly, such
a classifier can not only afford a reward function, but actually provide a
well-shaped objective landscape that both promotes progress toward good states
and provides a calibrated exploration bonus. In this work, we we show that an
uncertainty aware classifier can solve challenging reinforcement learning
problems by both encouraging exploration and provided directed guidance towards
positive outcomes. We propose a novel mechanism for obtaining these calibrated,
uncertainty-aware classifiers based on an amortized technique for computing the
normalized maximum likelihood (NML) distribution, also showing how these
techniques can be made computationally tractable by leveraging tools from
meta-learning. We show that the resulting algorithm has a number of intriguing
connections to both count-based exploration methods and prior algorithms for
learning reward functions, while also providing more effective guidance towards
the goal. We demonstrate that our algorithm solves a number of challenging
navigation and robotic manipulation tasks which prove difficult or impossible
for prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kevin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1"&gt;Ashwin Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pong_V/0/1/0/all/0/1"&gt;Vitchyr Pong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aurick Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Justin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backprop-Free Reinforcement Learning with Active Neural Generative Coding. (arXiv:2107.07046v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07046</id>
        <link href="http://arxiv.org/abs/2107.07046"/>
        <updated>2021-07-16T00:48:25.095Z</updated>
        <summary type="html"><![CDATA[In humans, perceptual awareness facilitates the fast recognition and
extraction of information from sensory input. This awareness largely depends on
how the human agent interacts with the environment. In this work, we propose
active neural generative coding, a computational framework for learning
action-driven generative models without backpropagation of errors (backprop) in
dynamic environments. Specifically, we develop an intelligent agent that
operates even with sparse rewards, drawing inspiration from the cognitive
theory of planning as inference. We demonstrate on several control problems, in
the online learning setting, that our proposed modeling framework performs
competitively with deep Q-learning models. The robust performance of our agent
offers promising evidence that a backprop-free approach for neural inference
and learning can drive goal-directed behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1"&gt;Alexander Ororbia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1"&gt;Ankur Mali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Deep Learning Workflow to Predict Multiphase Flow Behavior during Geological CO2 Sequestration Injection and Post-Injection Periods. (arXiv:2107.07274v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07274</id>
        <link href="http://arxiv.org/abs/2107.07274"/>
        <updated>2021-07-16T00:48:25.087Z</updated>
        <summary type="html"><![CDATA[This paper contributes to the development and evaluation of a deep learning
workflow that accurately and efficiently predicts the temporal-spatial
evolution of pressure and CO2 plumes during injection and post-injection
periods of geologic CO2 sequestration (GCS) operations. Based on a Fourier
Neuron Operator, the deep learning workflow takes input variables or features
including rock properties, well operational controls and time steps, and
predicts the state variables of pressure and CO2 saturation. To further improve
the predictive fidelity, separate deep learning models are trained for CO2
injection and post-injection periods due the difference in primary driving
force of fluid flow and transport during these two phases. We also explore
different combinations of features to predict the state variables. We use a
realistic example of CO2 injection and storage in a 3D heterogeneous saline
aquifer, and apply the deep learning workflow that is trained from
physics-based simulation data and emulate the physics process. Through this
numerical experiment, we demonstrate that using two separate deep learning
models to distinguish post-injection from injection period generates the most
accurate prediction of pressure, and a single deep learning model of the whole
GCS process including the cumulative injection volume of CO2 as a deep learning
feature, leads to the most accurate prediction of CO2 saturation. For the
post-injection period, it is key to use cumulative CO2 injection volume to
inform the deep learning models about the total carbon storage when predicting
either pressure or saturation. The deep learning workflow not only provides
high predictive fidelity across temporal and spatial scales, but also offers a
speedup of 250 times compared to full physics reservoir simulation, and thus
will be a significant predictive tool for engineers to manage the long term
process of GCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bailian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harp_D/0/1/0/all/0/1"&gt;Dylan Robert Harp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawar_R/0/1/0/all/0/1"&gt;Rajesh J. Pawar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04727</id>
        <link href="http://arxiv.org/abs/2105.04727"/>
        <updated>2021-07-16T00:48:25.080Z</updated>
        <summary type="html"><![CDATA[We propose FEDENHANCE, an unsupervised federated learning (FL) approach for
speech enhancement and separation with non-IID distributed data across multiple
clients. We simulate a real-world scenario where each client only has access to
a few noisy recordings from a limited and disjoint number of speakers (hence
non-IID). Each client trains their model in isolation using mixture invariant
training while periodically providing updates to a central server. Our
experiments show that our approach achieves competitive enhancement performance
compared to IID training on a single device and that we can further facilitate
the convergence speed and the overall performance using transfer learning on
the server-side. Moreover, we show that we can effectively combine updates from
clients trained locally with supervised and unsupervised losses. We also
release a new dataset LibriFSD50K and its creation recipe in order to
facilitate FL research for source separation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1"&gt;Jonah Casebeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-16T00:48:25.059Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the expressivity of bi-Lipschitz normalizing flows. (arXiv:2107.07232v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07232</id>
        <link href="http://arxiv.org/abs/2107.07232"/>
        <updated>2021-07-16T00:48:25.051Z</updated>
        <summary type="html"><![CDATA[An invertible function is bi-Lipschitz if both the function and its inverse
have bounded Lipschitz constants. Nowadays, most Normalizing Flows are
bi-Lipschitz by design or by training to limit numerical errors (among other
things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing
Flows and identify several target distributions that are difficult to
approximate using such models. Then, we characterize the expressivity of
bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total
Variation distance between these particularly unfavorable distributions and
their best possible approximation. Finally, we discuss potential remedies which
include using more complex latent distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verine_A/0/1/0/all/0/1"&gt;Alexandre Verine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negrevergne_B/0/1/0/all/0/1"&gt;Benjamin Negrevergne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1"&gt;Fabrice Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chevaleyre_Y/0/1/0/all/0/1"&gt;Yann Chevaleyre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Environment Inference for Invariant Learning. (arXiv:2010.07249v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07249</id>
        <link href="http://arxiv.org/abs/2010.07249"/>
        <updated>2021-07-16T00:48:25.044Z</updated>
        <summary type="html"><![CDATA[Learning models that gracefully handle distribution shifts is central to
research on domain generalization, robust optimization, and fairness. A
promising formulation is domain-invariant learning, which identifies the key
issue of learning which features are domain-specific versus domain-invariant.
An important assumption in this area is that the training examples are
partitioned into "domains" or "environments". Our focus is on the more common
setting where such partitions are not provided. We propose EIIL, a general
framework for domain-invariant learning that incorporates Environment Inference
to directly infer partitions that are maximally informative for downstream
Invariant Learning. We show that EIIL outperforms invariant learning methods on
the CMNIST benchmark without using environment labels, and significantly
outperforms ERM on worst-group performance in the Waterbirds and CivilComments
datasets. Finally, we establish connections between EIIL and algorithmic
fairness, which enables EIIL to improve accuracy and calibration in a fair
prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1"&gt;Elliot Creager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rn-Henrik Jacobsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JKOnet: Proximal Optimal Transport Modeling of Population Dynamics. (arXiv:2106.06345v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06345</id>
        <link href="http://arxiv.org/abs/2106.06345"/>
        <updated>2021-07-16T00:48:25.020Z</updated>
        <summary type="html"><![CDATA[Consider a heterogeneous population of points evolving with time. While the
population evolves, both in size and nature, we can observe it periodically,
through snapshots taken at different timestamps. Each of these snapshots is
formed by sampling points from the population at that time, and then creating
features to recover point clouds. While these snapshots describe the
population's evolution on aggregate, they do not provide directly insights on
individual trajectories. This scenario is encountered in several applications,
notably single-cell genomics experiments, tracking of particles, or when
studying crowd motion. In this paper, we propose to model that dynamic as
resulting from the celebrated Jordan-Kinderlehrer-Otto (JKO) proximal scheme.
The JKO scheme posits that the configuration taken by a population at time $t$
is one that trades off a decrease w.r.t. an energy (the model we seek to learn)
penalized by an optimal transport distance w.r.t. the previous configuration.
To that end, we propose JKOnet, a neural architecture that combines an energy
model on measures, with (small) optimal displacements solved with input convex
neural networks (ICNN). We demonstrate the applicability of our model to
explain and predict population dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Papaxanthos_L/0/1/0/all/0/1"&gt;Laetitia Meng-Papaxanthos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Processing on the Permutahedron: Tight Spectral Frames for Ranked Data Analysis. (arXiv:2103.04150v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04150</id>
        <link href="http://arxiv.org/abs/2103.04150"/>
        <updated>2021-07-16T00:48:25.014Z</updated>
        <summary type="html"><![CDATA[Ranked data sets, where m judges/voters specify a preference ranking of n
objects/candidates, are increasingly prevalent in contexts such as political
elections, computer vision, recommender systems, and bioinformatics. The vote
counts for each ranking can be viewed as an n! data vector lying on the
permutahedron, which is a Cayley graph of the symmetric group with vertices
labeled by permutations and an edge when two permutations differ by an adjacent
transposition. Leveraging combinatorial representation theory and recent
progress in signal processing on graphs, we investigate a novel, scalable
transform method to interpret and exploit structure in ranked data. We
represent data on the permutahedron using an overcomplete dictionary of atoms,
each of which captures both smoothness information about the data (typically
the focus of spectral graph decomposition methods in graph signal processing)
and structural information about the data (typically the focus of symmetry
decomposition methods from representation theory). These atoms have a more
naturally interpretable structure than any known basis for signals on the
permutahedron, and they form a Parseval frame, ensuring beneficial numerical
properties such as energy preservation. We develop specialized algorithms and
open software that take advantage of the symmetry and structure of the
permutahedron to improve the scalability of the proposed method, making it more
applicable to the high-dimensional ranked data found in applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+DeJong_J/0/1/0/all/0/1"&gt;Jennifer DeJong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Halverson_T/0/1/0/all/0/1"&gt;Tom Halverson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shuman_D/0/1/0/all/0/1"&gt;David I Shuman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework for non-negative matrix and tensor factorisations with a smoothed Wasserstein loss. (arXiv:2104.01708v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01708</id>
        <link href="http://arxiv.org/abs/2104.01708"/>
        <updated>2021-07-16T00:48:24.998Z</updated>
        <summary type="html"><![CDATA[Non-negative matrix and tensor factorisations are a classical tool for
finding low-dimensional representations of high-dimensional datasets. In
applications such as imaging, datasets can be regarded as distributions
supported on a space with metric structure. In such a setting, a loss function
based on the Wasserstein distance of optimal transportation theory is a natural
choice since it incorporates the underlying geometry of the data. We introduce
a general mathematical framework for computing non-negative factorisations of
both matrices and tensors with respect to an optimal transport loss. We derive
an efficient computational method for its solution using a convex dual
formulation, and demonstrate the applicability of this approach with several
numerical illustrations with both matrix and tensor-valued data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Stephen Y. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data vs classifiers, who wins?. (arXiv:2107.07451v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07451</id>
        <link href="http://arxiv.org/abs/2107.07451"/>
        <updated>2021-07-16T00:48:24.981Z</updated>
        <summary type="html"><![CDATA[The classification experiments covered by machine learning (ML) are composed
by two important parts: the data and the algorithm. As they are a fundamental
part of the problem, both must be considered when evaluating a model's
performance against a benchmark. The best classifiers need robust benchmarks to
be properly evaluated. For this, gold standard benchmarks such as OpenML-CC18
are used. However, data complexity is commonly not considered along with the
model during a performance evaluation. Recent studies employ Item Response
Theory (IRT) as a new approach to evaluating datasets and algorithms, capable
of evaluating both simultaneously. This work presents a new evaluation
methodology based on IRT and Glicko-2, jointly with the decodIRT tool developed
to guide the estimation of IRT in ML. It explores the IRT as a tool to evaluate
the OpenML-CC18 benchmark for its algorithmic evaluation capability and checks
if there is a subset of datasets more efficient than the original benchmark.
Several classifiers, from classics to ensemble, are also evaluated using the
IRT models. The Glicko-2 rating system was applied together with IRT to
summarize the innate ability and classifiers performance. It was noted that not
all OpenML-CC18 datasets are really useful for evaluating algorithms, where
only 10% were rated as being really difficult. Furthermore, it was verified the
existence of a more efficient subset containing only 50% of the original size.
While Randon Forest was singled out as the algorithm with the best innate
ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_L/0/1/0/all/0/1"&gt;Lucas F. F. Cardoso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_V/0/1/0/all/0/1"&gt;Vitor C. A. Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frances_R/0/1/0/all/0/1"&gt;Regiane S. Kawasaki Franc&amp;#xea;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prudencio_R/0/1/0/all/0/1"&gt;Ricardo B. C. Prud&amp;#xea;ncio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alves_R/0/1/0/all/0/1"&gt;Ronnie C. O. Alves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.05383</id>
        <link href="http://arxiv.org/abs/2003.05383"/>
        <updated>2021-07-16T00:48:24.971Z</updated>
        <summary type="html"><![CDATA[We study the XAI (explainable AI) on the face recognition task, particularly
the face verification here. Face verification is a crucial task in recent days
and it has been deployed to plenty of applications, such as access control,
surveillance, and automatic personal log-on for mobile devices. With the
increasing amount of data, deep convolutional neural networks can achieve very
high accuracy for the face verification task. Beyond exceptional performances,
deep face verification models need more interpretability so that we can trust
the results they generate. In this paper, we propose a novel similarity metric,
called explainable cosine ($xCos$), that comes with a learnable module that can
be plugged into most of the verification models to provide meaningful
explanations. With the help of $xCos$, we can see which parts of the two input
faces are similar, where the model pays its attention to, and how the local
similarities are weighted to form the output $xCos$ score. We demonstrate the
effectiveness of our proposed method on LFW and various competitive benchmarks,
resulting in not only providing novel and desiring model interpretability for
face verification but also ensuring the accuracy as plugging into existing face
recognition models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-An Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Siang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Ya-Liang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[USCO-Solver: Solving Undetermined Stochastic Combinatorial Optimization Problems. (arXiv:2107.07508v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07508</id>
        <link href="http://arxiv.org/abs/2107.07508"/>
        <updated>2021-07-16T00:48:24.957Z</updated>
        <summary type="html"><![CDATA[Real-world decision-making systems are often subject to uncertainties that
have to be resolved through observational data. Therefore, we are frequently
confronted with combinatorial optimization problems of which the objective
function is unknown and thus has to be debunked using empirical evidence. In
contrast to the common practice that relies on a learning-and-optimization
strategy, we consider the regression between combinatorial spaces, aiming to
infer high-quality optimization solutions from samples of input-solution pairs
-- without the need to learn the objective function. Our main deliverable is a
universal solver that is able to handle abstract undetermined stochastic
combinatorial optimization problems. For learning foundations, we present
learning-error analysis under the PAC-Bayesian framework using a new
margin-based analysis. In empirical studies, we demonstrate our design using
proof-of-concept experiments, and compare it with other methods that are
potentially applicable. Overall, we obtain highly encouraging experimental
results for several classic combinatorial problems on both synthetic and
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tong_G/0/1/0/all/0/1"&gt;Guangmo Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07402</id>
        <link href="http://arxiv.org/abs/2107.07402"/>
        <updated>2021-07-16T00:48:24.904Z</updated>
        <summary type="html"><![CDATA[We present a CLSRIL-23, a self supervised learning based audio pre-trained
model which learns cross lingual speech representations from raw audio across
23 Indic languages. It is built on top of wav2vec 2.0 which is solved by
training a contrastive task over masked latent speech representations and
jointly learns the quantization of latents shared across all languages. We
compare the language wise loss during pretraining to compare effects of
monolingual and multilingual pretraining. Performance on some downstream
fine-tuning tasks for speech recognition is also compared and our experiments
show that multilingual pretraining outperforms monolingual training, in terms
of learning speech representations which encodes phonetic similarity of
languages and also in terms of performance on down stream tasks. A decrease of
5% is observed in WER and 9.5% in CER when a multilingual pretrained model is
used for finetuning in Hindi. All the code models are also open sourced.
CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio
data to facilitate research in speech recognition for Indic languages. We hope
that new state of the art systems will be created using the self supervised
approach, especially for low resources Indic languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anirudh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1"&gt;Harveen Singh Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1"&gt;Priyanshi Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1"&gt;Neeraj Chimmwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1"&gt;Ankur Dhuriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1"&gt;Rishabh Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1"&gt;Vivek Raghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical graph neural nets can capture long-range interactions. (arXiv:2107.07432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07432</id>
        <link href="http://arxiv.org/abs/2107.07432"/>
        <updated>2021-07-16T00:48:24.898Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) based on message passing between neighboring
nodes are known to be insufficient for capturing long-range interactions in
graphs. In this project we study hierarchical message passing models that
leverage a multi-resolution representation of a given graph. This facilitates
learning of features that span large receptive fields without loss of local
information, an aspect not studied in preceding work on hierarchical GNNs. We
introduce Hierarchical Graph Net (HGNet), which for any two connected nodes
guarantees existence of message-passing paths of at most logarithmic length
w.r.t. the input graph size. Yet, under mild assumptions, its internal
hierarchy maintains asymptotic size equivalent to that of the input graph. We
observe that our HGNet outperforms conventional stacking of GCN layers
particularly in molecular property prediction benchmarks. Finally, we propose
two benchmarking tasks designed to elucidate capability of GNNs to leverage
long-range interactions in graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1"&gt;Ladislav Ramp&amp;#xe1;&amp;#x161;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1"&gt;Guy Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning-Based Analysis of Free-Text Keystroke Dynamics. (arXiv:2107.07409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07409</id>
        <link href="http://arxiv.org/abs/2107.07409"/>
        <updated>2021-07-16T00:48:24.892Z</updated>
        <summary type="html"><![CDATA[The development of active and passive biometric authentication and
identification technology plays an increasingly important role in
cybersecurity. Keystroke dynamics can be used to analyze the way that a user
types based on various keyboard input. Previous work has shown that user
authentication and classification can be achieved based on keystroke dynamics.
In this research, we consider the problem of user classification based on
keystroke dynamics features collected from free-text. We implement and analyze
a novel a deep learning model that combines a convolutional neural network
(CNN) and a gated recurrent unit (GRU). We optimize the resulting model and
consider several relevant related problems. Our model is competitive with the
best results obtained in previous comparable research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Han-Chih Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SilGAN: Generating driving maneuvers for scenario-based software-in-the-loop testing. (arXiv:2107.07364v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.07364</id>
        <link href="http://arxiv.org/abs/2107.07364"/>
        <updated>2021-07-16T00:48:24.885Z</updated>
        <summary type="html"><![CDATA[Automotive software testing continues to rely largely upon expensive field
tests to ensure quality because alternatives like simulation-based testing are
relatively immature. As a step towards lowering reliance on field tests, we
present SilGAN, a deep generative model that eases specification, stimulus
generation, and automation of automotive software-in-the-loop testing. The
model is trained using data recorded from vehicles in the field. Upon training,
the model uses a concise specification for a driving scenario to generate
realistic vehicle state transitions that can occur during such a scenario. Such
authentic emulation of internal vehicle behavior can be used for rapid,
systematic and inexpensive testing of vehicle control software. In addition, by
presenting a targeted method for searching through the information learned by
the model, we show how a test objective like code coverage can be automated.
The data driven end-to-end testing pipeline that we present vastly expands the
scope and credibility of automotive simulation-based testing. This reduces time
to market while helping maintain required standards of quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathy_D/0/1/0/all/0/1"&gt;Dhasarathy Parthasarathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_A/0/1/0/all/0/1"&gt;Anton Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expert Graphs: Synthesizing New Expertise via Collaboration. (arXiv:2107.07054v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07054</id>
        <link href="http://arxiv.org/abs/2107.07054"/>
        <updated>2021-07-16T00:48:24.879Z</updated>
        <summary type="html"><![CDATA[Consider multiple experts with overlapping expertise working on a
classification problem under uncertain input. What constitutes a consistent set
of opinions? How can we predict the opinions of experts on missing sub-domains?
In this paper, we define a framework of to analyze this problem, termed "expert
graphs." In an expert graph, vertices represent classes and edges represent
binary opinions on the topics of their vertices. We derive necessary conditions
for expert graph validity and use them to create "synthetic experts" which
describe opinions consistent with the observed opinions of other experts. We
show this framework to be equivalent to the well-studied linear ordering
polytope. We show our conditions are not sufficient for describing all expert
graphs on cliques, but are sufficient for cycles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_B/0/1/0/all/0/1"&gt;Bijan Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Siddharth Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruck_J/0/1/0/all/0/1"&gt;Jehoshua Bruck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mid-flight Forecasting for CPA Lines in Online Advertising. (arXiv:2107.07494v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07494</id>
        <link href="http://arxiv.org/abs/2107.07494"/>
        <updated>2021-07-16T00:48:24.872Z</updated>
        <summary type="html"><![CDATA[For Verizon MediaDemand Side Platform(DSP), forecasting of ad campaign
performance not only feeds key information to the optimization server to allow
the system to operate on a high-performance mode, but also produces actionable
insights to the advertisers. In this paper, the forecasting problem for CPA
lines in the middle of the flight is investigated by taking the bidding
mechanism into account. The proposed methodology generates relationships
between various key performance metrics and optimization signals. It can also
be used to estimate the sensitivity of ad campaign performance metrics to the
adjustments of optimization signal, which is important to the design of a
campaign management system. The relationship between advertiser spends and
effective Cost Per Action(eCPA) is also characterized, which serves as a
guidance for mid-flight line adjustment to the advertisers. Several practical
issues in implementation, such as downsampling of the dataset, are also
discussed in the paper. At last, the forecasting results are validated against
actual deliveries and demonstrates promising accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_L/0/1/0/all/0/1"&gt;Lihua Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karlsson_N/0/1/0/all/0/1"&gt;Niklas Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Flores_A/0/1/0/all/0/1"&gt;Aaron Flores&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Approximations for Thompson Sampling. (arXiv:2105.09232v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09232</id>
        <link href="http://arxiv.org/abs/2105.09232"/>
        <updated>2021-07-16T00:48:24.856Z</updated>
        <summary type="html"><![CDATA[We study the behavior of Thompson sampling from the perspective of weak
convergence. In the regime where the gaps between arm means scale as
$1/\sqrt{n}$ with the time horizon $n$, we show that the dynamics of Thompson
sampling evolve according to discrete versions of SDEs and random ODEs. As $n
\to \infty$, we show that the dynamics converge weakly to solutions of the
corresponding SDEs and random ODEs. (Recently, Wager and Xu (arXiv:2101.09855)
independently proposed this regime and developed similar SDE and random ODE
approximations for Thompson sampling in the multi-armed bandit setting.) Our
weak convergence theory, which covers both multi-armed and linear bandit
settings, is developed from first principles using the Continuous Mapping
Theorem and can be directly adapted to analyze other sampling-based bandit
algorithms, for example, algorithms using the bootstrap for exploration. We
also establish an invariance principle for multi-armed bandits with gaps
scaling as $1/\sqrt{n}$ -- for Thompson sampling and related algorithms
involving posterior approximation or the bootstrap, the weak diffusion limits
are in general the same regardless of the specifics of the reward distributions
or the choice of prior. In particular, as suggested by the classical
Bernstein-von Mises normal approximation for posterior distributions, the weak
diffusion limits generally coincide with the limit for normally-distributed
rewards and priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lin Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glynn_P/0/1/0/all/0/1"&gt;Peter W. Glynn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection. (arXiv:2106.12382v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12382</id>
        <link href="http://arxiv.org/abs/2106.12382"/>
        <updated>2021-07-16T00:48:24.848Z</updated>
        <summary type="html"><![CDATA[An innovations sequence of a time series is a sequence of independent and
identically distributed random variables with which the original time series
has a causal representation. The innovation at a time is statistically
independent of the history of the time series. As such, it represents the new
information contained at present but not in the past. Because of its simple
probability structure, an innovations sequence is the most efficient signature
of the original. Unlike the principle or independent component analysis
representations, an innovations sequence preserves not only the complete
statistical properties but also the temporal order of the original time series.
An long-standing open problem is to find a computationally tractable way to
extract an innovations sequence of non-Gaussian processes. This paper presents
a deep learning approach, referred to as Innovations Autoencoder (IAE), that
extracts innovations sequences using a causal convolutional neural network. An
application of IAE to the one-class anomalous sequence detection problem with
unknown anomaly and anomaly-free models is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06992</id>
        <link href="http://arxiv.org/abs/2107.06992"/>
        <updated>2021-07-16T00:48:24.841Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a relatively new technique that specializes in problems
where we have little amounts of data. The goal of these methods is to classify
categories that have not been seen before with just a handful of samples.
Recent approaches, such as metric learning, adopt the meta-learning strategy in
which we have episodic tasks conformed by support (training) data and query
(test) data. Metric learning methods have demonstrated that simple models can
achieve good performance by learning a similarity function to compare the
support and the query data. However, the feature space learned by a given
metric learning approach may not exploit the information given by a specific
few-shot task. In this work, we explore the use of dimension reduction
techniques as a way to find task-significant features helping to make better
predictions. We measure the performance of the reduced features by assigning a
score based on the intra-class and inter-class distance, and selecting a
feature reduction method in which instances of different classes are far away
and instances of the same class are close. This module helps to improve the
accuracy performance by allowing the similarity function, given by the metric
learning method, to have more discriminative features for the classification.
Our method outperforms the metric learning baselines in the miniImageNet
dataset by around 2% in accuracy performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1"&gt;Mauricio Mendez-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1"&gt;Ivan Garcia Jorge Gonzalez-Zapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1"&gt;Andres Mendez-Vazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-label Chaining with Imprecise Probabilities. (arXiv:2107.07443v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07443</id>
        <link href="http://arxiv.org/abs/2107.07443"/>
        <updated>2021-07-16T00:48:24.835Z</updated>
        <summary type="html"><![CDATA[We present two different strategies to extend the classical multi-label
chaining approach to handle imprecise probability estimates. These estimates
use convex sets of distributions (or credal sets) in order to describe our
uncertainty rather than a precise one. The main reasons one could have for
using such estimations are (1) to make cautious predictions (or no decision at
all) when a high uncertainty is detected in the chaining and (2) to make better
precise predictions by avoiding biases caused in early decisions in the
chaining. Through the use of the naive credal classifier, we propose efficient
procedures with theoretical justifications to solve both strategies. Our
experimental results on missing labels, which investigate how reliable these
predictions are in both approaches, indicate that our approaches produce
relevant cautiousness on those hard-to-predict instances where the precise
models fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alarcon_Y/0/1/0/all/0/1"&gt;Yonatan Carlos Carranza Alarc&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Destercke_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Destercke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests. (arXiv:2107.06929v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06929</id>
        <link href="http://arxiv.org/abs/2107.06929"/>
        <updated>2021-07-16T00:48:24.821Z</updated>
        <summary type="html"><![CDATA[While previous distribution shift detection approaches can identify if a
shift has occurred, these approaches cannot localize which specific features
have caused a distribution shift -- a critical step in diagnosing or fixing any
underlying issue. For example, in military sensor networks, users will want to
detect when one or more of the sensors has been compromised, and critically,
they will want to know which specific sensors might be compromised. Thus, we
first define a formalization of this problem as multiple conditional
distribution hypothesis tests and propose both non-parametric and parametric
statistical tests. For both efficiency and flexibility, we then propose to use
a test statistic based on the density model score function (i.e. gradient with
respect to the input) -- which can easily compute test statistics for all
dimensions in a single forward and backward pass. Any density model could be
used for computing the necessary statistics including deep density models such
as normalizing flows or autoregressive models. We additionally develop methods
for identifying when and where a shift occurs in multivariate time-series data
and show results for multiple scenarios using realistic attack models on both
simulated and real world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1"&gt;Sean Kulinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1"&gt;David I. Inouye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07438</id>
        <link href="http://arxiv.org/abs/2107.07438"/>
        <updated>2021-07-16T00:48:24.807Z</updated>
        <summary type="html"><![CDATA[Online advertising is ubiquitous in web business. Image displaying is
considered as one of the most commonly used formats to interact with customers.
Contextual multi-armed bandit has shown success in the application of
advertising to solve the exploration-exploitation dilemma existed in the
recommendation procedure. Inspired by the visual-aware advertising, in this
paper, we propose a contextual bandit algorithm, where the convolutional neural
network (CNN) is utilized to learn the reward function along with an upper
confidence bound (UCB) for exploration. We also prove a near-optimal regret
bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is over-parameterized
and establish strong connections with convolutional neural tangent kernel
(CNTK). Finally, we evaluate the empirical performance of the proposed
algorithm and show that it outperforms other state-of-the-art UCB-based bandit
algorithms on real-world image data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yikun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Concept-based Explainable Reasoning. (arXiv:2107.07493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07493</id>
        <link href="http://arxiv.org/abs/2107.07493"/>
        <updated>2021-07-16T00:48:24.799Z</updated>
        <summary type="html"><![CDATA[Recent research on graph neural network (GNN) models successfully applied
GNNs to classical graph algorithms and combinatorial optimisation problems.
This has numerous benefits, such as allowing applications of algorithms when
preconditions are not satisfied, or reusing learned models when sufficient
training data is not available or can't be generated. Unfortunately, a key
hindrance of these approaches is their lack of explainability, since GNNs are
black-box models that cannot be interpreted directly. In this work, we address
this limitation by applying existing work on concept-based explanations to GNN
models. We introduce concept-bottleneck GNNs, which rely on a modification to
the GNN readout mechanism. Using three case studies we demonstrate that: (i)
our proposed model is capable of accurately learning concepts and extracting
propositional formulas based on the learned concepts for each target class;
(ii) our concept-based GNN models achieve comparative performance with
state-of-the-art models; (iii) we can derive global graph concepts, without
explicitly providing any supervision on graph-level concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Dobrik Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auditing for Diversity using Representative Examples. (arXiv:2107.07393v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.07393</id>
        <link href="http://arxiv.org/abs/2107.07393"/>
        <updated>2021-07-16T00:48:24.792Z</updated>
        <summary type="html"><![CDATA[Assessing the diversity of a dataset of information associated with people is
crucial before using such data for downstream applications. For a given
dataset, this often involves computing the imbalance or disparity in the
empirical marginal distribution of a protected attribute (e.g. gender, dialect,
etc.). However, real-world datasets, such as images from Google Search or
collections of Twitter posts, often do not have protected attributes labeled.
Consequently, to derive disparity measures for such datasets, the elements need
to hand-labeled or crowd-annotated, which are expensive processes.

We propose a cost-effective approach to approximate the disparity of a given
unlabeled dataset, with respect to a protected attribute, using a control set
of labeled representative examples. Our proposed algorithm uses the pairwise
similarity between elements in the dataset and elements in the control set to
effectively bootstrap an approximation to the disparity of the dataset.
Importantly, we show that using a control set whose size is much smaller than
the size of the dataset is sufficient to achieve a small approximation error.
Further, based on our theoretical framework, we also provide an algorithm to
construct adaptive control sets that achieve smaller approximation errors than
randomly chosen control sets. Simulations on two image datasets and one Twitter
dataset demonstrate the efficacy of our approach (using random and adaptive
control sets) in auditing the diversity of a wide variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1"&gt;Vijay Keswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celis_L/0/1/0/all/0/1"&gt;L. Elisa Celis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lockout: Sparse Regularization of Neural Networks. (arXiv:2107.07160v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07160</id>
        <link href="http://arxiv.org/abs/2107.07160"/>
        <updated>2021-07-16T00:48:24.785Z</updated>
        <summary type="html"><![CDATA[Many regression and classification procedures fit a parameterized function
$f(x;w)$ of predictor variables $x$ to data $\{x_{i},y_{i}\}_1^N$ based on some
loss criterion $L(y,f)$. Often, regularization is applied to improve accuracy
by placing a constraint $P(w)\leq t$ on the values of the parameters $w$.
Although efficient methods exist for finding solutions to these constrained
optimization problems for all values of $t\geq0$ in the special case when $f$
is a linear function, none are available when $f$ is non-linear (e.g. Neural
Networks). Here we present a fast algorithm that provides all such solutions
for any differentiable function $f$ and loss $L$, and any constraint $P$ that
is an increasing monotone function of the absolute value of each parameter.
Applications involving sparsity inducing regularization of arbitrary Neural
Networks are discussed. Empirical results indicate that these sparse solutions
are usually superior to their dense counterparts in both accuracy and
interpretability. This improvement in accuracy can often make Neural Networks
competitive with, and sometimes superior to, state-of-the-art methods in the
analysis of tabular data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valdes_G/0/1/0/all/0/1"&gt;Gilmer Valdes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelo_W/0/1/0/all/0/1"&gt;Wilmer Arbelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Interian_Y/0/1/0/all/0/1"&gt;Yannet Interian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1"&gt;Jerome H. Friedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07445</id>
        <link href="http://arxiv.org/abs/2107.07445"/>
        <updated>2021-07-16T00:48:24.767Z</updated>
        <summary type="html"><![CDATA[Transformer-based pre-trained language models like BERT and its variants have
recently achieved promising performance in various natural language processing
(NLP) tasks. However, the conventional paradigm constructs the backbone by
purely stacking the manually designed global self-attention layers, introducing
inductive bias and thus leading to sub-optimal. In this work, we propose an
Operation-Priority Neural Architecture Search (OP-NAS) algorithm to
automatically search for promising hybrid backbone architectures. Our
well-designed search space (i) contains primitive math operations in the
intra-layer level to explore novel attention structures, and (ii) leverages
convolution blocks to be the supplementary for attention structure in the
inter-layer level to better learn local dependency. We optimize both the search
algorithm and evaluation of candidate models to boost the efficiency of our
proposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution
strategy to facilitate model search via balancing exploration and exploitation.
Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for
fast model evaluation. Extensive experiments show that the searched
architecture (named AutoBERT-Zero) significantly outperforms BERT and its
variants of different model capacities in various downstream tasks, proving the
architecture's transfer and generalization abilities. Remarkably,
AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and
BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE
test set. Code and pre-trained models will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiahui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1"&gt;Han shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip L.H. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable AI: current status and future directions. (arXiv:2107.07045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07045</id>
        <link href="http://arxiv.org/abs/2107.07045"/>
        <updated>2021-07-16T00:48:24.755Z</updated>
        <summary type="html"><![CDATA[Explainable Artificial Intelligence (XAI) is an emerging area of research in
the field of Artificial Intelligence (AI). XAI can explain how AI obtained a
particular solution (e.g., classification or object detection) and can also
answer other "wh" questions. This explainability is not possible in traditional
AI. Explainability is essential for critical applications, such as defense,
health care, law and order, and autonomous driving vehicles, etc, where the
know-how is required for trust and transparency. A number of XAI techniques so
far have been purposed for such applications. This paper provides an overview
of these techniques from a multimedia (i.e., text, image, audio, and video)
point of view. The advantages and shortcomings of these techniques have been
discussed, and pointers to some future directions have also been provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gohel_P/0/1/0/all/0/1"&gt;Prashant Gohel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Priyanka Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanty_M/0/1/0/all/0/1"&gt;Manoranjan Mohanty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-variable neural-network quantum states and the quantum rotor model. (arXiv:2107.07105v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.07105</id>
        <link href="http://arxiv.org/abs/2107.07105"/>
        <updated>2021-07-16T00:48:24.740Z</updated>
        <summary type="html"><![CDATA[We initiate the study of neural-network quantum state algorithms for
analyzing continuous-variable lattice quantum systems in first quantization. A
simple family of continuous-variable trial wavefunctons is introduced which
naturally generalizes the restricted Boltzmann machine (RBM) wavefunction
introduced for analyzing quantum spin systems. By virtue of its simplicity, the
same variational Monte Carlo training algorithms that have been developed for
ground state determination and time evolution of spin systems have natural
analogues in the continuum. We offer a proof of principle demonstration in the
context of ground state determination of a stoquastic quantum rotor
Hamiltonian. Results are compared against those obtained from partial
differential equation (PDE) based scalable eigensolvers. This study serves as a
benchmark against which future investigation of continuous-variable neural
quantum states can be compared, and points to the need to consider deep network
architectures and more sophisticated training algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Stokes_J/0/1/0/all/0/1"&gt;James Stokes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+De_S/0/1/0/all/0/1"&gt;Saibal De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Veerapaneni_S/0/1/0/all/0/1"&gt;Shravan Veerapaneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1"&gt;Giuseppe Carleo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07170</id>
        <link href="http://arxiv.org/abs/2107.07170"/>
        <updated>2021-07-16T00:48:24.711Z</updated>
        <summary type="html"><![CDATA[Few-shot NLP research is highly active, yet conducted in disjoint research
threads with evaluation suites that lack challenging-yet-realistic testing
setups and fail to employ careful experimental design. Consequently, the
community does not know which techniques perform best or even if they
outperform simple baselines. We formulate desiderata for an ideal few-shot NLP
benchmark and present FLEX, the first benchmark, public leaderboard, and
framework that provides unified, comprehensive measurement for few-shot NLP
techniques. FLEX incorporates and introduces new best practices for few-shot
evaluation, including measurement of four transfer settings, textual labels for
zero-shot evaluation, and a principled approach to benchmark design that
optimizes statistical accuracy while keeping evaluation costs accessible to
researchers without large compute resources. In addition, we present UniFew, a
simple yet strong prompt-based model for few-shot learning which unifies the
pretraining and finetuning prompt formats, eschewing complex machinery of
recent prompt-based approaches in adapting downstream task formats to language
model pretraining objectives. We demonstrate that despite simplicity UniFew
achieves results competitive with both popular meta-learning and prompt-based
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1"&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What underlies rapid learning and systematic generalization in humans. (arXiv:2107.06994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06994</id>
        <link href="http://arxiv.org/abs/2107.06994"/>
        <updated>2021-07-16T00:48:24.704Z</updated>
        <summary type="html"><![CDATA[Despite the groundbreaking successes of neural networks, contemporary models
require extensive training with massive datasets and exhibit poor out-of-sample
generalization. One proposed solution is to build systematicity and
domain-specific constraints into the model, echoing the tenets of classical,
symbolic cognitive architectures. In this paper, we consider the limitations of
this approach by examining human adults' ability to learn an abstract reasoning
task from a brief instructional tutorial and explanatory feedback for incorrect
responses, demonstrating that human learning dynamics and ability to generalize
outside the range of the training examples differ drastically from those of a
representative neural network model, and that the model is brittle to changes
in features not anticipated by its authors. We present further evidence from
human data that the ability to consistently solve the puzzles was associated
with education, particularly basic mathematics education, and with the ability
to provide a reliably identifiable, valid description of the strategy used. We
propose that rapid learning and systematic generalization in humans may depend
on a gradual, experience-dependent process of learning-to-learn using
instructions and explanations to guide the construction of explicit abstract
rules that support generalizable inferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_A/0/1/0/all/0/1"&gt;Andrew Joohun Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1"&gt;James L. McClelland&lt;/a&gt; (Stanford University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Spectrum Sensing and Access in Cognitive Radios via Approximate POMDPs. (arXiv:2107.07049v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07049</id>
        <link href="http://arxiv.org/abs/2107.07049"/>
        <updated>2021-07-16T00:48:24.684Z</updated>
        <summary type="html"><![CDATA[A novel LEarning-based Spectrum Sensing and Access (LESSA) framework is
proposed, wherein a cognitive radio (CR) learns a time-frequency correlation
model underlying spectrum occupancy of licensed users (LUs) in a radio
ecosystem; concurrently, it devises an approximately optimal spectrum sensing
and access policy under sensing constraints. A Baum-Welch algorithm is proposed
to learn a parametric Markov transition model of LU spectrum occupancy based on
noisy spectrum measurements. Spectrum sensing and access are cast as a
Partially-Observable Markov Decision Process, approximately optimized via
randomized point-based value iteration. Fragmentation, Hamming-distance state
filters and Monte-Carlo methods are proposed to alleviate the inherent
computational complexity, and a weighted reward metric to regulate the
trade-off between CR throughput and LU interference. Numerical evaluations
demonstrate that LESSA performs within 5 percent of a genie-aided upper bound
with foreknowledge of LU spectrum occupancy, and outperforms state-of-the-art
algorithms across the entire trade-off region: 71 percent over
correlation-based clustering, 26 percent over Neyman-Pearson detection, 6
percent over the Viterbi algorithm, and 9 percent over an adaptive Deep
Q-Network. LESSA is then extended to a distributed Multi-Agent setting
(MA-LESSA), by proposing novel neighbor discovery and channel access rank
allocation. MA-LESSA improves CR throughput by 43 percent over cooperative
TD-SARSA, 84 percent over cooperative greedy distributed learning, and 3x over
non-cooperative learning via g-statistics and ACKs. Finally, MA-LESSA is
implemented on the DARPA SC2 platform, manifesting superior performance over
competitors in a real-world TDWR-UNII WLAN emulation; its implementation
feasibility is further validated on a testbed of ESP32 radios, exhibiting 96
percent success probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Keshavamurthy_B/0/1/0/all/0/1"&gt;Bharath Keshavamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Michelusi_N/0/1/0/all/0/1"&gt;Nicolo Michelusi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hida-Mat\'ern Kernel. (arXiv:2107.07098v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07098</id>
        <link href="http://arxiv.org/abs/2107.07098"/>
        <updated>2021-07-16T00:48:24.677Z</updated>
        <summary type="html"><![CDATA[We present the class of Hida-Mat\'ern kernels, which is the canonical family
of covariance functions over the entire space of stationary Gauss-Markov
Processes. It extends upon Mat\'ern kernels, by allowing for flexible
construction of priors over processes with oscillatory components. Any
stationary kernel, including the widely used squared-exponential and spectral
mixture kernels, are either directly within this class or are appropriate
asymptotic limits, demonstrating the generality of this class. Taking advantage
of its Markovian nature we show how to represent such processes as state space
models using only the kernel and its derivatives. In turn this allows us to
perform Gaussian Process inference more efficiently and side step the usual
computational burdens. We also show how exploiting special properties of the
state space representation enables improved numerical stability in addition to
further reductions of computational complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dowling_M/0/1/0/all/0/1"&gt;Matthew Dowling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokol_P/0/1/0/all/0/1"&gt;Piotr Sok&amp;#xf3;&amp;#x142;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Park_I/0/1/0/all/0/1"&gt;Il Memming Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating Memorization in Sample Selection for Learning with Noisy Labels. (arXiv:2107.07041v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07041</id>
        <link href="http://arxiv.org/abs/2107.07041"/>
        <updated>2021-07-16T00:48:24.619Z</updated>
        <summary type="html"><![CDATA[Because deep learning is vulnerable to noisy labels, sample selection
techniques, which train networks with only clean labeled data, have attracted a
great attention. However, if the labels are dominantly corrupted by few
classes, these noisy samples are called dominant-noisy-labeled samples, the
network also learns dominant-noisy-labeled samples rapidly via content-aware
optimization. In this study, we propose a compelling criteria to penalize
dominant-noisy-labeled samples intensively through class-wise penalty labels.
By averaging prediction confidences for the each observed label, we obtain
suitable penalty labels that have high values if the labels are largely
corrupted by some classes. Experiments were performed using benchmarks
(CIFAR-10, CIFAR-100, Tiny-ImageNet) and real-world datasets (ANIMAL-10N,
Clothing1M) to evaluate the proposed criteria in various scenarios with
different noise rates. Using the proposed sample selection, the learning
process of the network becomes significantly robust to noisy labels compared to
existing methods in several noise types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyeongbo Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Junggi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_Y/0/1/0/all/0/1"&gt;Youngchul Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Young-Rae Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong-Eun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Woo-Jin Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NVCell: Standard Cell Layout in Advanced Technology Nodes with Reinforcement Learning. (arXiv:2107.07044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07044</id>
        <link href="http://arxiv.org/abs/2107.07044"/>
        <updated>2021-07-16T00:48:24.611Z</updated>
        <summary type="html"><![CDATA[High quality standard cell layout automation in advanced technology nodes is
still challenging in the industry today because of complex design rules. In
this paper we introduce an automatic standard cell layout generator called
NVCell that can generate layouts with equal or smaller area for over 90% of
single row cells in an industry standard cell library on an advanced technology
node. NVCell leverages reinforcement learning (RL) to fix design rule
violations during routing and to generate efficient placements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haoxing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fojtik_M/0/1/0/all/0/1"&gt;Matthew Fojtik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1"&gt;Brucek Khailany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Inequality Constraints from $e$-separation Relations in Directed Acyclic Graphs with Hidden Variables. (arXiv:2107.07087v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07087</id>
        <link href="http://arxiv.org/abs/2107.07087"/>
        <updated>2021-07-16T00:48:24.603Z</updated>
        <summary type="html"><![CDATA[Directed acyclic graphs (DAGs) with hidden variables are often used to
characterize causal relations between variables in a system. When some
variables are unobserved, DAGs imply a notoriously complicated set of
constraints on the distribution of observed variables. In this work, we present
entropic inequality constraints that are implied by $e$-separation relations in
hidden variable DAGs with discrete observed variables. The constraints can
intuitively be understood to follow from the fact that the capacity of
variables along a causal pathway to convey information is restricted by their
entropy; e.g. at the extreme case, a variable with entropy $0$ can convey no
information. We show how these constraints can be used to learn about the true
causal model from an observed data distribution. In addition, we propose a
measure of causal influence called the minimal mediary entropy, and demonstrate
that it can augment traditional measures such as the average causal effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Finkelstein_N/0/1/0/all/0/1"&gt;Noam Finkelstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zjawin_B/0/1/0/all/0/1"&gt;Beata Zjawin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wolfe_E/0/1/0/all/0/1"&gt;Elie Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shpitser_I/0/1/0/all/0/1"&gt;Ilya Shpitser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spekkens_R/0/1/0/all/0/1"&gt;Robert W. Spekkens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-16T00:48:24.585Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07127</id>
        <link href="http://arxiv.org/abs/2107.07127"/>
        <updated>2021-07-16T00:48:24.579Z</updated>
        <summary type="html"><![CDATA[Video streaming services strive to support high-quality videos at higher
resolutions and frame rates to improve the quality of experience (QoE).
However, high-quality videos consume considerable amounts of energy on mobile
devices. This paper proposes NeuSaver, which reduces the power consumption of
mobile devices when streaming videos by applying an adaptive frame rate to each
video chunk without compromising user experience. NeuSaver generates an optimal
policy that determines the appropriate frame rate for each video chunk using
reinforcement learning (RL). The RL model automatically learns the policy that
maximizes the QoE goals based on previous observations. NeuSaver also uses an
asynchronous advantage actor-critic algorithm to reinforce the RL model quickly
and robustly. Streaming servers that support NeuSaver preprocesses videos into
segments with various frame rates, which is similar to the process of creating
videos with multiple bit rates in dynamic adaptive streaming over HTTP.
NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in
various experiments and a user study through four video categories along with
the state-of-the-art model. Our experiments showed that NeuSaver effectively
reduces the power consumption of mobile devices when streaming video by an
average of 16.14% and up to 23.12% while achieving high QoE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Myungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Laihyuk Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis. (arXiv:2107.07116v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.07116</id>
        <link href="http://arxiv.org/abs/2107.07116"/>
        <updated>2021-07-16T00:48:24.572Z</updated>
        <summary type="html"><![CDATA[CNF-based SAT and MaxSAT solvers are central to logic synthesis and
verification systems. The increasing popularity of these constraint problems in
electronic design automation encourages studies on different SAT problems and
their properties for further computational efficiency. There has been both
theoretical and practical success of modern Conflict-driven clause learning SAT
solvers, which allows solving very large industrial instances in a relatively
short amount of time. Recently, machine learning approaches provide a new
dimension to solving this challenging problem. Neural symbolic models could
serve as generic solvers that can be specialized for specific domains based on
data without any changes to the structure of the model. In this work, we
propose a one-shot model derived from the Transformer architecture to solve the
MaxSAT problem, which is the optimization version of SAT where the goal is to
satisfy the maximum number of clauses. Our model has a scale-free structure
which could process varying size of instances. We use meta-path and
self-attention mechanism to capture interactions among homogeneous nodes. We
adopt cross-attention mechanisms on the bipartite graph to capture interactions
among heterogeneous nodes. We further apply an iterative algorithm to our model
to satisfy additional clauses, enabling a solution approaching that of an
exact-SAT problem. The attention mechanisms leverage the parallelism for
speedup. Our evaluation indicates improved speedup compared to heuristic
approaches and improved completion rate compared to machine learning
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Feng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chonghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bashar_M/0/1/0/all/0/1"&gt;Mohammad Khairul Bashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_N/0/1/0/all/0/1"&gt;Nikhil Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1"&gt;Vijaykrishnan Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Bayesian Neural Networks with Functional Probabilistic Layers. (arXiv:2107.07014v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07014</id>
        <link href="http://arxiv.org/abs/2107.07014"/>
        <updated>2021-07-16T00:48:24.565Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks provide a direct and natural way to extend standard
deep neural networks to support probabilistic deep learning through the use of
probabilistic layers that, traditionally, encode weight (and bias) uncertainty.
In particular, hybrid Bayesian neural networks utilize standard deterministic
layers together with few probabilistic layers judicially positioned in the
networks for uncertainty estimation. A major aspect and benefit of Bayesian
inference is that priors, in principle, provide the means to encode prior
knowledge for use in inference and prediction. However, it is difficult to
specify priors on weights since the weights have no intuitive interpretation.
Further, the relationships of priors on weights to the functions computed by
networks are difficult to characterize. In contrast, functions are intuitive to
interpret and are direct since they map inputs to outputs. Therefore, it is
natural to specify priors on functions to encode prior knowledge, and to use
them in inference and prediction based on functions. To support this, we
propose hybrid Bayesian neural networks with functional probabilistic layers
that encode function (and activation) uncertainty. We discuss their foundations
in functional Bayesian inference, functional variational inference, sparse
Gaussian processes, and sparse variational Gaussian processes. We further
perform few proof-of-concept experiments using GPflus, a new library that
provides Gaussian process layers and supports their use with deterministic
Keras layers to form hybrid neural network and Gaussian process models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks. (arXiv:2107.07039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07039</id>
        <link href="http://arxiv.org/abs/2107.07039"/>
        <updated>2021-07-16T00:48:24.552Z</updated>
        <summary type="html"><![CDATA[The frequency and impact of floods are expected to increase due to climate
change. It is crucial to predict streamflow, consequently flooding, in order to
prepare and mitigate its consequences in terms of property damage and
fatalities. This paper presents a Graph Convolutional GRUs based model to
predict the next 36 hours of streamflow for a sensor location using the
upstream river network. As shown in experiment results, the model presented in
this study provides better performance than the persistence baseline and a
Convolutional Bidirectional GRU network for the selected study area in
short-term streamflow prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sit_M/0/1/0/all/0/1"&gt;Muhammed Sit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiray_B/0/1/0/all/0/1"&gt;Bekir Demiray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_I/0/1/0/all/0/1"&gt;Ibrahim Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overview and Experimental Study of Learning-based Optimization Algorithms for Vehicle Routing Problem. (arXiv:2107.07076v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07076</id>
        <link href="http://arxiv.org/abs/2107.07076"/>
        <updated>2021-07-16T00:48:24.544Z</updated>
        <summary type="html"><![CDATA[Vehicle routing problem (VRP) is a typical discrete combinatorial
optimization problem, and many models and algorithms have been proposed to
solve VRP and variants. Although existing approaches has contributed a lot to
the development of this field, these approaches either are limited in problem
size or need manual intervening in choosing parameters. To tackle these
difficulties, many studies consider learning-based optimization algorithms to
solve VRP. This paper reviews recent advances in this field and divides
relevant approaches into end-to-end approaches and step-by-step approaches. We
design three part experiments to justly evaluate performance of four
representative learning-based optimization algorithms and conclude that
combining heuristic search can effectively improve learning ability and sampled
efficiency of LBO models. Finally we point out that research trend of LBO
algorithms is to solve large-scale and multiple constraints problems from real
world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guohua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yongming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Mingfeng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1"&gt;Witold Pedrycz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principal component analysis for Gaussian process posteriors. (arXiv:2107.07115v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07115</id>
        <link href="http://arxiv.org/abs/2107.07115"/>
        <updated>2021-07-16T00:48:24.525Z</updated>
        <summary type="html"><![CDATA[This paper proposes an extension of principal component analysis for Gaussian
process posteriors denoted by GP-PCA. Since GP-PCA estimates a low-dimensional
space of GP posteriors, it can be used for meta-learning, which is a framework
for improving the precision of a new task by estimating a structure of a set of
tasks. The issue is how to define a structure of a set of GPs with an
infinite-dimensional parameter, such as coordinate system and a divergence. In
this study, we reduce the infiniteness of GP to the finite-dimensional case
under the information geometrical framework by considering a space of GP
posteriors that has the same prior. In addition, we propose an approximation
method of GP-PCA based on variational inference and demonstrate the
effectiveness of GP-PCA as meta-learning through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ishibashi_H/0/1/0/all/0/1"&gt;Hideaki Ishibashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Akaho_S/0/1/0/all/0/1"&gt;Shotaro Akaho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeightScale: Interpreting Weight Change in Neural Networks. (arXiv:2107.07005v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07005</id>
        <link href="http://arxiv.org/abs/2107.07005"/>
        <updated>2021-07-16T00:48:24.513Z</updated>
        <summary type="html"><![CDATA[Interpreting the learning dynamics of neural networks can provide useful
insights into how networks learn and the development of better training and
design approaches. We present an approach to interpret learning in neural
networks by measuring relative weight change on a per layer basis and
dynamically aggregating emerging trends through combination of dimensionality
reduction and clustering which allows us to scale to very deep networks. We use
this approach to investigate learning in the context of vision tasks across a
variety of state-of-the-art networks and provide insights into the learning
behavior of these networks, including how task complexity affects layer-wise
learning in deeper layers of networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1"&gt;Ayush Manish Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tendle_A/0/1/0/all/0/1"&gt;Atharva Tendle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sikka_H/0/1/0/all/0/1"&gt;Harshvardhan Sikka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sahib Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07042</id>
        <link href="http://arxiv.org/abs/2107.07042"/>
        <updated>2021-07-16T00:48:24.472Z</updated>
        <summary type="html"><![CDATA[Function is defined as the ensemble of tasks that enable the product to
complete the designed purpose. Functional tools, such as functional modeling,
offer decision guidance in the early phase of product design, where explicit
design decisions are yet to be made. Function-based design data is often sparse
and grounded in individual interpretation. As such, function-based design tools
can benefit from automatic function classification to increase data fidelity
and provide function representation models that enable function-based
intelligent design agents. Function-based design data is commonly stored in
manually generated design repositories. These design repositories are a
collection of expert knowledge and interpretations of function in product
design bounded by function-flow and component taxonomies. In this work, we
represent a structured taxonomy-based design repository as assembly-flow
graphs, then leverage a graph neural network (GNN) model to perform automatic
function classification. We support automated function classification by
learning from repository data to establish the ground truth of component
function assignment. Experimental results show that our GNN model achieves a
micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and
0.783 for tier 3 (specific) functions. Given the imbalance of data features,
the results are encouraging. Our efforts in this paper can be a starting point
for more sophisticated applications in knowledge-based CAD systems and
Design-for-X consideration in function-based design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1"&gt;Vincenzo Ferrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1"&gt;Kaveh Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1"&gt;Daniele Grandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1"&gt;Bryony DuPont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Benchmark Lottery. (arXiv:2107.07002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07002</id>
        <link href="http://arxiv.org/abs/2107.07002"/>
        <updated>2021-07-16T00:48:24.464Z</updated>
        <summary type="html"><![CDATA[The world of empirical machine learning (ML) strongly relies on benchmarks in
order to determine the relative effectiveness of different algorithms and
methods. This paper proposes the notion of "a benchmark lottery" that describes
the overall fragility of the ML benchmarking process. The benchmark lottery
postulates that many factors, other than fundamental algorithmic superiority,
may lead to a method being perceived as superior. On multiple benchmark setups
that are prevalent in the ML community, we show that the relative performance
of algorithms may be altered significantly simply by choosing different
benchmark tasks, highlighting the fragility of the current paradigms and
potential fallacious interpretation derived from benchmarking ML methods. Given
that every benchmark makes a statement about what it perceives to be important,
we argue that this might lead to biased progress in the community. We discuss
the implications of the observed phenomena and provide recommendations on
mitigating them using multiple machine learning domains and communities as use
cases, including natural language processing, computer vision, information
retrieval, recommender systems, and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1"&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhe Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_F/0/1/0/all/0/1"&gt;Fernando Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Hierarchical Structures for Few-Shot Musical Instrument Recognition. (arXiv:2107.07029v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.07029</id>
        <link href="http://arxiv.org/abs/2107.07029"/>
        <updated>2021-07-16T00:48:24.445Z</updated>
        <summary type="html"><![CDATA[Deep learning work on musical instrument recognition has generally focused on
instrument classes for which we have abundant data. In this work, we exploit
hierarchical relationships between instruments in a few-shot learning setup to
enable classification of a wider set of musical instruments, given a few
examples at inference. We apply a hierarchical loss function to the training of
prototypical networks, combined with a method to aggregate prototypes
hierarchically, mirroring the structure of a predefined musical instrument
hierarchy. These extensions require no changes to the network architecture and
new levels can be easily added or removed. Compared to a non-hierarchical
few-shot baseline, our method leads to a significant increase in classification
accuracy and significant decrease mistake severity on instrument classes unseen
in training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1"&gt;Hugo Flores Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aguilar_A/0/1/0/all/0/1"&gt;Aldo Aguilar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manilow_E/0/1/0/all/0/1"&gt;Ethan Manilow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pardo_B/0/1/0/all/0/1"&gt;Bryan Pardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Teaching Size. (arXiv:2107.07038v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07038</id>
        <link href="http://arxiv.org/abs/2107.07038"/>
        <updated>2021-07-16T00:48:24.440Z</updated>
        <summary type="html"><![CDATA[Recent research in machine teaching has explored the instruction of any
concept expressed in a universal language. In this compositional context, new
experimental results have shown that there exist data teaching sets
surprisingly shorter than the concept description itself. However, there exists
a bound for those remarkable experimental findings through teaching size and
concept complexity that we further explore here. As concepts are rarely taught
in isolation we investigate the best configuration of concepts to teach a given
set of concepts, where those that have been acquired first can be reused for
the description of new ones. This new notion of conditional teaching size
uncovers new insights, such as the interposition phenomenon: certain prior
knowledge generates simpler compatible concepts that increase the teaching size
of the concept that we want to teach. This does not happen for conditional
Kolmogorov complexity. Furthermore, we provide an algorithm that constructs
optimal curricula based on interposition avoidance. This paper presents a
series of theoretical results, including their proofs, and some directions for
future work. New research possibilities in curriculum teaching in compositional
scenarios are now wide open to exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Piqueras_M/0/1/0/all/0/1"&gt;Manuel Garcia-Piqueras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Hern&amp;#xe1;ndez-Orallo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Rank Temporal Attention-Augmented Bilinear Network for financial time-series forecasting. (arXiv:2107.06995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06995</id>
        <link href="http://arxiv.org/abs/2107.06995"/>
        <updated>2021-07-16T00:48:24.433Z</updated>
        <summary type="html"><![CDATA[Financial market analysis, especially the prediction of movements of stock
prices, is a challenging problem. The nature of financial time-series data,
being non-stationary and nonlinear, is the main cause of these challenges. Deep
learning models have led to significant performance improvements in many
problems coming from different domains, including prediction problems of
financial time-series data. Although the prediction performance is the main
goal of such models, dealing with ultra high-frequency data sets restrictions
in terms of the number of model parameters and its inference speed. The
Temporal Attention-Augmented Bilinear network was recently proposed as an
efficient and high-performing model for Limit Order Book time-series
forecasting. In this paper, we propose a low-rank tensor approximation of the
model to further reduce the number of trainable parameters and increase its
speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabani_M/0/1/0/all/0/1"&gt;Mostafa Shabani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free-Text Keystroke Dynamics for User Authentication. (arXiv:2107.07009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07009</id>
        <link href="http://arxiv.org/abs/2107.07009"/>
        <updated>2021-07-16T00:48:24.419Z</updated>
        <summary type="html"><![CDATA[In this research, we consider the problem of verifying user identity based on
keystroke dynamics obtained from free-text. We employ a novel feature
engineering method that generates image-like transition matrices. For this
image-like feature, a convolution neural network (CNN) with cutout achieves the
best results. A hybrid model consisting of a CNN and a recurrent neural network
(RNN) is also shown to outperform previous research in this field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Han-Chih Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1"&gt;Mark Stamp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06990</id>
        <link href="http://arxiv.org/abs/2107.06990"/>
        <updated>2021-07-16T00:48:24.413Z</updated>
        <summary type="html"><![CDATA[Automated writing evaluation systems can improve students' writing insofar as
students attend to the feedback provided and revise their essay drafts in ways
aligned with such feedback. Existing research on revision of argumentative
writing in such systems, however, has focused on the types of revisions
students make (e.g., surface vs. content) rather than the extent to which
revisions actually respond to the feedback provided and improve the essay. We
introduce an annotation scheme to capture the nature of sentence-level
revisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-
and 6th-grade students' argumentative essays. We show that reliable manual
annotation can be achieved and that revision annotations correlate with a
holistic assessment of essay improvement in line with the feedback provided.
Furthermore, we explore the feasibility of automatically classifying revisions
according to our scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1"&gt;Tazin Afrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elaine Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1"&gt;Diane Litman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1"&gt;Lindsay C. Matsumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1"&gt;Richard Correnti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Field Guide to Federated Optimization. (arXiv:2107.06917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06917</id>
        <link href="http://arxiv.org/abs/2107.06917"/>
        <updated>2021-07-16T00:48:24.404Z</updated>
        <summary type="html"><![CDATA[Federated learning and analytics are a distributed approach for
collaboratively learning models (or statistics) from decentralized data,
motivated by and designed for privacy protection. The distributed learning
process can be formulated as solving federated optimization problems, which
emphasize communication efficiency, data heterogeneity, compatibility with
privacy and system requirements, and other constraints that are not primary
considerations in other problem settings. This paper provides recommendations
and guidelines on formulating, designing, evaluating and analyzing federated
optimization algorithms through concrete examples and practical implementation,
with a focus on conducting effective simulations to infer real-world
performance. The goal of this work is not to survey the current literature, but
to inspire researchers and practitioners to design federated learning
algorithms that can be used in various practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1"&gt;H. Brendan McMahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1"&gt;Blaise Aguera y Arcas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1"&gt;Maruan Al-Shedivat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1"&gt;Galen Andrew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1"&gt;Katharine Daly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Data_D/0/1/0/all/0/1"&gt;Deepesh Data&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1"&gt;Suhas Diggavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eichner_H/0/1/0/all/0/1"&gt;Hubert Eichner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadhikar_A/0/1/0/all/0/1"&gt;Advait Gadhikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girgis_A/0/1/0/all/0/1"&gt;Antonious M. Girgis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanzely_F/0/1/0/all/0/1"&gt;Filip Hanzely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hard_A/0/1/0/all/0/1"&gt;Andrew Hard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horvath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1"&gt;Zhouyuan Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingerman_A/0/1/0/all/0/1"&gt;Alex Ingerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1"&gt;Tara Javidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1"&gt;Peter Kairouz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1"&gt;Jakub Konecny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1"&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank J. Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richtarik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1"&gt;Virginia Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1"&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Weikang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shanshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Felix X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chunxiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chen Zhu&lt;/a&gt;, et al. (1 additional author not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder. (arXiv:2107.07064v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.07064</id>
        <link href="http://arxiv.org/abs/2107.07064"/>
        <updated>2021-07-16T00:48:24.396Z</updated>
        <summary type="html"><![CDATA[Brain-computer interface (BCI) is one of the tools which enables the
communication between humans and devices by reflecting intention and status of
humans. With the development of artificial intelligence, the interest in
communication between humans and drones using electroencephalogram (EEG) is
increased. Especially, in the case of controlling drone swarms such as
direction or formation, there are many advantages compared with controlling a
drone unit. Imagined speech is one of the endogenous BCI paradigms, which can
identify intentions of users. When conducting imagined speech, the users
imagine the pronunciation as if actually speaking. In contrast, overt speech is
a task in which the users directly pronounce the words. When controlling drone
swarms using imagined speech, complex commands can be delivered more
intuitively, but decoding performance is lower than that of other endogenous
BCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of
overt speech for imagined speech-based EEG signals classification. To the best
of our knowledge, this study is the first attempt to use EEG features of overt
speech to decode imagined speech-based EEG signals with an autoencoder. A total
of eight subjects participated in the experiment. When classifying four words,
the average accuracy of the DAL was 48.41%. In addition, when comparing the
performance between w/o and w/ EEG features of overt speech, there was a
performance improvement of 7.42% when including EEG features of overt speech.
Hence, we demonstrated that EEG features of overt speech could improve the
decoding performance of imagined speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dae-Hyeok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sung-Jin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06960</id>
        <link href="http://arxiv.org/abs/2107.06960"/>
        <updated>2021-07-16T00:48:24.390Z</updated>
        <summary type="html"><![CDATA[A rising research challenge is running costly machine learning (ML) networks
locally on resource-constrained edge devices. ML networks with large
convolutional layers can easily exceed available memory, increasing latency due
to excessive swapping. Previous memory reduction techniques such as pruning and
quantization reduce model accuracy and often require retraining. Alternatively,
distributed methods partition the convolutions into equivalent smaller
sub-computations, but the implementations introduce communication costs and
require a network of devices. However, a distributed partitioning approach can
also be used to run in a reduced memory footprint on a single device by
subdividing the network into smaller operations.

This report extends prior work on distributed partitioning using tiling and
fusing of convolutional layers into a memory-aware execution on a single
device. Our approach extends prior fusing strategies to allow for two groups of
convolutional layers that are fused and tiled independently. This approach
reduces overhead via data reuse, and reduces the memory footprint further. We
also propose a memory usage predictor coupled with a search algorithm to
provide fusing and tiling configurations for an arbitrary set of convolutional
layers. When applied to the YOLOv2 object detection network, results show that
our approach can run in less than half the memory, and with a speedup of up to
2.78 under severe memory constraints. Additionally, our algorithm will return a
configuration with a latency that is within 6% of the best latency measured in
a manual search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farley_J/0/1/0/all/0/1"&gt;Jackson Farley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerstlauer_A/0/1/0/all/0/1"&gt;Andreas Gerstlauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GGT: Graph-Guided Testing for Adversarial Sample Detection of Deep Neural Network. (arXiv:2107.07043v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07043</id>
        <link href="http://arxiv.org/abs/2107.07043"/>
        <updated>2021-07-16T00:48:24.382Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples,
the detection of which is crucial for the wide application of these DNN models.
Recently, a number of deep testing methods in software engineering were
proposed to find the vulnerability of DNN systems, and one of them, i.e., Model
Mutation Testing (MMT), was used to successfully detect various adversarial
samples generated by different kinds of adversarial attacks. However, the
mutated models in MMT are always huge in number (e.g., over 100 models) and
lack diversity (e.g., can be easily circumvented by high-confidence adversarial
samples), which makes it less efficient in real applications and less effective
in detecting high-confidence adversarial samples. In this study, we propose
Graph-Guided Testing (GGT) for adversarial sample detection to overcome these
aforementioned challenges. GGT generates pruned models with the guide of graph
characteristics, each of them has only about 5% parameters of the mutated model
in MMT, and graph guided models have higher diversity. The experiments on
CIFAR10 and SVHN validate that GGT performs much better than MMT with respect
to both effectiveness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zuohui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1"&gt;Jingyang Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xin Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1"&gt;Qi Xuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoniu Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed generative neural network: an application to troposphere temperature prediction. (arXiv:2107.06991v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06991</id>
        <link href="http://arxiv.org/abs/2107.06991"/>
        <updated>2021-07-16T00:48:24.377Z</updated>
        <summary type="html"><![CDATA[The troposphere is one of the atmospheric layers where most weather phenomena
occur. Temperature variations in the troposphere, especially at 500 hPa, a
typical level of the middle troposphere, are significant indicators of future
weather changes. Numerical weather prediction is effective for temperature
prediction, but its computational complexity hinders a timely response. This
paper proposes a novel temperature prediction approach in framework
ofphysics-informed deep learning. The new model, called PGnet, builds upon a
generative neural network with a mask matrix. The mask is designed to
distinguish the low-quality predicted regions generated by the first physical
stage. The generative neural network takes the mask as prior for the
second-stage refined predictions. A mask-loss and a jump pattern strategy are
developed to train the generative neural network without accumulating errors
during making time-series predictions. Experiments on ERA5 demonstrate that
PGnet can generate more refined temperature predictions than the
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zheng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the impossibility of non-trivial accuracy under fairness constraints. (arXiv:2107.06944v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06944</id>
        <link href="http://arxiv.org/abs/2107.06944"/>
        <updated>2021-07-16T00:48:24.361Z</updated>
        <summary type="html"><![CDATA[One of the main concerns about fairness in machine learning (ML) is that, in
order to achieve it, one may have to renounce to some accuracy. Having this
trade-off in mind, Hardt et al. have proposed the notion of equal opportunities
(EO), designed so as to be compatible with accuracy. In fact, it can be shown
that if the source of input data is deterministic, the two notions go well
along with each other. In the probabilistic case, however, things change.

As we show, there are probabilistic data sources for which EO can only be
achieved at the total detriment of accuracy, i.e. among the models that achieve
EO, those whose prediction does not depend on the input have the highest
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinzon_C/0/1/0/all/0/1"&gt;Carlos Pinz&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valencia_F/0/1/0/all/0/1"&gt;Frank Valencia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06955</id>
        <link href="http://arxiv.org/abs/2107.06955"/>
        <updated>2021-07-16T00:48:24.349Z</updated>
        <summary type="html"><![CDATA[We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1"&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic Graph Neural Networks. (arXiv:2107.06996v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06996</id>
        <link href="http://arxiv.org/abs/2107.06996"/>
        <updated>2021-07-16T00:48:24.332Z</updated>
        <summary type="html"><![CDATA[While many existing graph neural networks (GNNs) have been proven to perform
$\ell_2$-based graph smoothing that enforces smoothness globally, in this work
we aim to further enhance the local smoothness adaptivity of GNNs via
$\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs
(Elastic GNNs) based on $\ell_1$ and $\ell_2$-based graph smoothing. In
particular, we propose a novel and general message passing scheme into GNNs.
This message passing algorithm is not only friendly to back-propagation
training but also achieves the desired smoothing properties with a theoretical
convergence guarantee. Experiments on semi-supervised learning tasks
demonstrate that the proposed Elastic GNNs obtain better adaptivity on
benchmark datasets and are significantly robust to graph adversarial attacks.
The implementation of Elastic GNNs is available at
\url{https://github.com/lxiaorui/ElasticGNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaorui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Wei Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantifying the Carbon Emissions of Differentially Private Machine Learning. (arXiv:2107.06946v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.06946</id>
        <link href="http://arxiv.org/abs/2107.06946"/>
        <updated>2021-07-16T00:48:24.316Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning techniques utilizing large-scale datasets
have achieved remarkable performance. Differential privacy, by means of adding
noise, provides strong privacy guarantees for such learning algorithms. The
cost of differential privacy is often a reduced model accuracy and a lowered
convergence speed. This paper investigates the impact of differential privacy
on learning algorithms in terms of their carbon footprint due to either longer
run-times or failed experiments. Through extensive experiments, further
guidance is provided on choosing the noise levels which can strike a balance
between desired privacy levels and reduced carbon emissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1"&gt;Rakshit Naidu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diddee_H/0/1/0/all/0/1"&gt;Harshita Diddee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulay_A/0/1/0/all/0/1"&gt;Ajinkya Mulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1"&gt;Aleti Vardhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1"&gt;Krithika Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzam_A/0/1/0/all/0/1"&gt;Ahmed Zamzam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06993</id>
        <link href="http://arxiv.org/abs/2107.06993"/>
        <updated>2021-07-16T00:48:24.295Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel confidence conditioned knowledge distillation (CCKD)
scheme for transferring the knowledge from a teacher model to a student model
is proposed. Existing state-of-the-art methods employ fixed loss functions for
this purpose and ignore the different levels of information that need to be
transferred for different samples. In addition to that, these methods are also
inefficient in terms of data usage. CCKD addresses these issues by leveraging
the confidence assigned by the teacher model to the correct class to devise
sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T
formulation). Further, CCKD improves the data efficiency by employing
self-regulation to stop those samples from participating in the distillation
process on which the student model learns faster. Empirical evaluations on
several benchmark datasets show that CCKD methods achieve at least as much
generalization performance levels as other state-of-the-art methods while being
data efficient in the process. Student models trained through CCKD methods do
not retain most of the misclassifications commited by the teacher model on the
training set. Distillation through CCKD methods improves the resilience of the
student models against adversarial attacks compared to the conventional KD
method. Experiments show at least 3% increase in performance against
adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least
6% increase for the CIFAR10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Sourav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Perceptual Image Quality Assessment for Compression. (arXiv:2103.01114v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01114</id>
        <link href="http://arxiv.org/abs/2103.01114"/>
        <updated>2021-07-16T00:48:24.289Z</updated>
        <summary type="html"><![CDATA[Lossy Image compression is necessary for efficient storage and transfer of
data. Typically the trade-off between bit-rate and quality determines the
optimal compression level. This makes the image quality metric an integral part
of any imaging system. While the existing full-reference metrics such as PSNR
and SSIM may be less sensitive to perceptual quality, the recently introduced
learning methods may fail to generalize to unseen data. In this paper we
propose the largest image compression quality dataset to date with human
perceptual preferences, enabling the use of deep learning, and we develop a
full reference perceptual quality assessment metric for lossy image compression
that outperforms the existing state-of-the-art methods. We show that the
proposed model can effectively learn from thousands of examples available in
the new dataset, and consequently it generalizes better to other unseen
datasets of human perceptual preference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mier_J/0/1/0/all/0/1"&gt;Juan Carlos Mier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1"&gt;Eddie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1"&gt;Hossein Talebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1"&gt;Peyman Milanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group. (arXiv:2107.06898v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2107.06898</id>
        <link href="http://arxiv.org/abs/2107.06898"/>
        <updated>2021-07-16T00:48:24.270Z</updated>
        <summary type="html"><![CDATA[We investigate the analogy between the renormalization group (RG) and deep
neural networks, wherein subsequent layers of neurons are analogous to
successive steps along the RG. In particular, we quantify the flow of
information by explicitly computing the relative entropy or Kullback-Leibler
divergence in both the one- and two-dimensional Ising models under decimation
RG, as well as in a feedforward neural network as a function of depth. We
observe qualitatively identical behavior characterized by the monotonic
increase to a parameter-dependent asymptotic value. On the quantum field theory
side, the monotonic increase confirms the connection between the relative
entropy and the c-theorem. For the neural networks, the asymptotic behavior may
have implications for various information maximization methods in machine
learning, as well as for disentangling compactness and generalizability.
Furthermore, while both the two-dimensional Ising model and the random neural
networks we consider exhibit non-trivial critical points, the relative entropy
appears insensitive to the phase structure of either system. In this sense,
more refined probes are required in order to fully elucidate the flow of
information in these models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erdmenger_J/0/1/0/all/0/1"&gt;Johanna Erdmenger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Grosvenor_K/0/1/0/all/0/1"&gt;Kevin T. Grosvenor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Jefferson_R/0/1/0/all/0/1"&gt;Ro Jefferson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance of Bayesian linear regression in a model with mismatch. (arXiv:2107.06936v1 [math.PR])]]></title>
        <id>http://arxiv.org/abs/2107.06936</id>
        <link href="http://arxiv.org/abs/2107.06936"/>
        <updated>2021-07-16T00:48:24.262Z</updated>
        <summary type="html"><![CDATA[For a model of high-dimensional linear regression with random design, we
analyze the performance of an estimator given by the mean of a log-concave
Bayesian posterior distribution with gaussian prior. The model is mismatched in
the following sense: like the model assumed by the statistician, the
labels-generating process is linear in the input data, but both the classifier
ground-truth prior and gaussian noise variance are unknown to her. This
inference model can be rephrased as a version of the Gardner model in spin
glasses and, using the cavity method, we provide fixed point equations for
various overlap order parameters, yielding in particular an expression for the
mean-square reconstruction error on the classifier (under an assumption of
uniqueness of solutions). As a direct corollary we obtain an expression for the
free energy. Similar models have already been studied by Shcherbina and Tirozzi
and by Talagrand, but our arguments are more straightforward and some
assumptions are relaxed. An interesting consequence of our analysis is that in
the random design setting of ridge regression, the performance of the posterior
mean is independent of the noise variance (or "temperature") assumed by the
statistician, and matches the one of the usual (zero temperature) ridge
estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Barbier_J/0/1/0/all/0/1"&gt;Jean Barbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Kuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Panchenko_D/0/1/0/all/0/1"&gt;Dmitry Panchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Saenz_M/0/1/0/all/0/1"&gt;Manuel S&amp;#xe1;enz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos. (arXiv:2104.11452v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11452</id>
        <link href="http://arxiv.org/abs/2104.11452"/>
        <updated>2021-07-16T00:48:24.247Z</updated>
        <summary type="html"><![CDATA[Markerless motion capture and understanding of professional non-daily human
movements is an important yet unsolved task, which suffers from complex motion
patterns and severe self-occlusion, especially for the monocular setting. In
this paper, we propose SportsCap -- the first approach for simultaneously
capturing 3D human motions and understanding fine-grained actions from
monocular challenging sports video input. Our approach utilizes the semantic
and temporally structured sub-motion prior in the embedding space for motion
capture and understanding in a data-driven multi-task manner. To enable robust
capture under complex motion patterns, we propose an effective motion embedding
module to recover both the implicit motion embedding and explicit 3D motion
details via a corresponding mapping function as well as a sub-motion
classifier. Based on such hybrid motion information, we introduce a
multi-stream spatial-temporal Graph Convolutional Network(ST-GCN) to predict
the fine-grained semantic action attributes, and adopt a semantic attribute
mapping block to assemble various correlated action attributes into a
high-level action label for the overall detailed understanding of the whole
sequence, so as to enable various applications like action assessment or motion
scoring. Comprehensive experiments on both public and our proposed datasets
show that with a challenging monocular sports video input, our novel approach
not only significantly improves the accuracy of 3D human motion capture, but
also recovers accurate fine-grained semantic action attributes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yuexin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines. (arXiv:2107.06925v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06925</id>
        <link href="http://arxiv.org/abs/2107.06925"/>
        <updated>2021-07-16T00:48:24.229Z</updated>
        <summary type="html"><![CDATA[Training large deep learning models at scale is very challenging. This paper
proposes Chimera, a novel pipeline parallelism scheme which combines
bidirectional pipelines for efficiently training large-scale models. Chimera is
a synchronous approach and therefore no loss of accuracy, which is more
convergence-friendly than asynchronous approaches. Compared with the latest
synchronous pipeline approach, Chimera reduces the number of bubbles by up to
50%; benefiting from the sophisticated scheduling of bidirectional pipelines,
Chimera has a more balanced activation memory consumption. Evaluations are
conducted on Transformer based language models. For a GPT-2 model with 1.3
billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,
Chimera improves the training throughput by 1.16x-2.34x over the
state-of-the-art synchronous and asynchronous pipeline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shigang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1"&gt;Torsten Hoefler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Failures in Out-of-Distribution Detection with Deep Generative Models. (arXiv:2107.06908v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06908</id>
        <link href="http://arxiv.org/abs/2107.06908"/>
        <updated>2021-07-16T00:48:24.223Z</updated>
        <summary type="html"><![CDATA[Deep generative models (DGMs) seem a natural fit for detecting
out-of-distribution (OOD) inputs, but such models have been shown to assign
higher probabilities or densities to OOD images than images from the training
distribution. In this work, we explain why this behavior should be attributed
to model misestimation. We first prove that no method can guarantee performance
beyond random chance without assumptions on which out-distributions are
relevant. We then interrogate the typical set hypothesis, the claim that
relevant out-distributions can lie in high likelihood regions of the data
distribution, and that OOD detection should be defined based on the data
distribution's typical set. We highlight the consequences implied by assuming
support overlap between in- and out-distributions, as well as the arbitrariness
of the typical set for OOD detection. Our results suggest that estimation error
is a more plausible explanation than the misalignment between likelihood-based
OOD detection and out-distributions of interest, and we illustrate how even
minimal estimation error can lead to OOD detection failures, yielding
implications for future work in deep generative modeling and OOD detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lily H. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_M/0/1/0/all/0/1"&gt;Mark Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06943</id>
        <link href="http://arxiv.org/abs/2107.06943"/>
        <updated>2021-07-16T00:48:24.213Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1"&gt;Szymon P&amp;#x142;otka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1"&gt;Tomasz W&amp;#x142;odarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1"&gt;Adam Klasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Lipa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-Uncertainty Assisted Spatial and Temporal Regularization Weighting for Learning-based Registration. (arXiv:2107.02433v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02433</id>
        <link href="http://arxiv.org/abs/2107.02433"/>
        <updated>2021-07-16T00:48:24.188Z</updated>
        <summary type="html"><![CDATA[In order to tackle the difficulty associated with the ill-posed nature of the
image registration problem, researchers use regularization to constrain the
solution space. For most learning-based registration approaches, the
regularization usually has a fixed weight and only constrains the spatial
transformation. Such convention has two limitations: (1) The regularization
strength of a specific image pair should be associated with the content of the
images, thus the ``one value fits all'' scheme is not ideal; (2) Only spatially
regularizing the transformation (but overlooking the temporal consistency of
different estimations) may not be the best strategy to cope with the
ill-posedness. In this study, we propose a mean-teacher based registration
framework. This framework incorporates an additional \textit{temporal
regularization} term by encouraging the teacher model's temporal ensemble
prediction to be consistent with that of the student model. At each training
step, it also automatically adjusts the weights of the \textit{spatial
regularization} and the \textit{temporal regularization} by taking account of
the transformation uncertainty and appearance uncertainty derived from the
perturbed teacher model. We perform experiments on multi- and uni-modal
registration tasks, and the results show that our strategy outperforms the
traditional and learning-based benchmark methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Donghuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1"&gt;Jayender Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1"&gt;William Wells III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1"&gt;Sarah Frisken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1"&gt;Raymond Kai-yu Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-07-16T00:48:24.167Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00652</id>
        <link href="http://arxiv.org/abs/2107.00652"/>
        <updated>2021-07-16T00:48:24.161Z</updated>
        <summary type="html"><![CDATA[We present CSWin Transformer, an efficient and effective Transformer-based
backbone for general-purpose vision tasks. A challenging issue in Transformer
design is that global self-attention is very expensive to compute whereas local
self-attention often limits the field of interactions of each token. To address
this issue, we develop the Cross-Shaped Window self-attention mechanism for
computing self-attention in the horizontal and vertical stripes in parallel
that form a cross-shaped window, with each stripe obtained by splitting the
input feature into stripes of equal width. We provide a detailed mathematical
analysis of the effect of the stripe width and vary the stripe width for
different layers of the Transformer network which achieves strong modeling
capability while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which handles the local positional
information better than existing encoding schemes. LePE naturally supports
arbitrary input resolutions, and is thus especially effective and friendly for
downstream tasks. Incorporated with these designs and a hierarchical structure,
CSWin Transformer demonstrates competitive performance on common vision tasks.
Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra
training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection
task, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing
previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and
+2.0 respectively under the similar FLOPs setting. By further pretraining on
the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K
and state-of-the-art segmentation performance on ADE20K with 55.7 mIoU. The
code and models will be available at
https://github.com/microsoft/CSWin-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jianmin Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1"&gt;Baining Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expectation-Maximization Regularized Deep Learning for Weakly Supervised Tumor Segmentation for Glioblastoma. (arXiv:2101.08757v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08757</id>
        <link href="http://arxiv.org/abs/2101.08757"/>
        <updated>2021-07-16T00:48:24.154Z</updated>
        <summary type="html"><![CDATA[We present an Expectation-Maximization (EM) Regularized Deep Learning
(EMReDL) model for weakly supervised tumor segmentation. The proposed framework
is tailored to glioblastoma, a type of malignant tumor characterized by its
diffuse infiltration into the surrounding brain tissue, which poses significant
challenge to treatment target and tumor burden estimation using conventional
structural MRI. Although physiological MRI provides more specific information
regarding tumor infiltration, the relatively low resolution hinders a precise
full annotation. This has motivated us to develop a weakly supervised deep
learning solution that exploits the partial labelled tumor regions.

EMReDL contains two components: a physiological prior prediction model and
EM-regularized segmentation model. The physiological prior prediction model
exploits the physiological MRI by training a classifier to generate a
physiological prior map. This map is passed to the segmentation model for
regularization using the EM algorithm. We evaluated the model on a glioblastoma
dataset with the pre-operative multiparametric and recurrence MRI available.
EMReDL showed to effectively segment the infiltrated tumor from the partially
labelled region of potential infiltration. The segmented core tumor and
infiltrated tumor demonstrated high consistency with the tumor burden labelled
by experts. The performance comparisons showed that EMReDL achieved higher
accuracy than published state-of-the-art models. On MR spectroscopy, the
segmented region displayed more aggressive features than other partial labelled
region. The proposed model can be generalized to other segmentation tasks that
rely on partial labels, with the CNN architecture flexible in the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wenjian Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1"&gt;Stephen J. Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08573</id>
        <link href="http://arxiv.org/abs/2103.08573"/>
        <updated>2021-07-16T00:48:24.116Z</updated>
        <summary type="html"><![CDATA[The use of local detectors and descriptors in typical computer vision
pipelines work well until variations in viewpoint and appearance change become
extreme. Past research in this area has typically focused on one of two
approaches to this challenge: the use of projections into spaces more suitable
for feature matching under extreme viewpoint changes, and attempting to learn
features that are inherently more robust to viewpoint change. In this paper, we
present a novel framework that combines learning of invariant descriptors
through data augmentation and orthographic viewpoint projection. We propose
rotation-robust local descriptors, learnt through training data augmentation
based on rotation homographies, and a correspondence ensemble technique that
combines vanilla feature correspondences with those obtained through
rotation-robust features. Using a range of benchmark datasets as well as
contributing a new bespoke dataset for this research domain, we evaluate the
effectiveness of the proposed approach on key tasks including pose estimation
and visual place recognition. Our system outperforms a range of baseline and
state-of-the-art techniques, including enabling higher levels of place
recognition precision across opposing place viewpoints and achieves
practically-useful performance levels even under extreme viewpoint changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1"&gt;Udit Singh Parihar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1"&gt;Aniket Gujarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1"&gt;Kinal Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1"&gt;Satyajit Tourani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1"&gt;K. Madhava Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Multi-View SLAM. (arXiv:2101.06562v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06562</id>
        <link href="http://arxiv.org/abs/2101.06562"/>
        <updated>2021-07-16T00:48:24.104Z</updated>
        <summary type="html"><![CDATA[Existing multi-camera SLAM systems assume synchronized shutters for all
cameras, which is often not the case in practice. In this work, we propose a
generalized multi-camera SLAM formulation which accounts for asynchronous
sensor observations. Our framework integrates a continuous-time motion model to
relate information across asynchronous multi-frames during tracking, local
mapping, and loop closing. For evaluation, we collected AMV-Bench, a
challenging new SLAM dataset covering 482 km of driving recorded using our
asynchronous multi-camera robotic platform. AMV-Bench is over an order of
magnitude larger than previous multi-view HD outdoor SLAM datasets, and covers
diverse and challenging motions and environments. Our experiments emphasize the
necessity of asynchronous sensor modeling, and show that the use of multiple
cameras is critical towards robust and accurate SLAM in challenging outdoor
scenes. For additional information, please see the project website at:
https://www.cs.toronto.edu/~ajyang/amv-slam]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barsan_I/0/1/0/all/0/1"&gt;Ioan Andrei B&amp;#xe2;rsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shenlong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generator Versus Segmentor: Pseudo-healthy Synthesis. (arXiv:2009.05722v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05722</id>
        <link href="http://arxiv.org/abs/2009.05722"/>
        <updated>2021-07-16T00:48:24.080Z</updated>
        <summary type="html"><![CDATA[This paper investigates the problem of pseudo-healthy synthesis that is
defined as synthesizing a subject-specific pathology-free image from a
pathological one. Recent approaches based on Generative Adversarial Network
(GAN) have been developed for this task. However, these methods will inevitably
fall into the trade-off between preserving the subject-specific identity and
generating healthy-like appearances. To overcome this challenge, we propose a
novel adversarial training regime, Generator versus Segmentor (GVS), to
alleviate this trade-off by a divide-and-conquer strategy. We further consider
the deteriorating generalization performance of the segmentor throughout the
training and develop a pixel-wise weighted loss by muting the well-transformed
pixels to promote it. Moreover, we propose a new metric to measure how healthy
the synthetic images look. The qualitative and quantitative experiments on the
public dataset BraTS demonstrate that the proposed method outperforms the
existing methods. Besides, we also certify the effectiveness of our method on
datasets LiTS. Our implementation and pre-trained networks are publicly
available at https://github.com/Au3C2/Generator-Versus-Segmentor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yunlong_Z/0/1/0/all/0/1"&gt;Zhang Yunlong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenxin_L/0/1/0/all/0/1"&gt;Li Chenxin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_L/0/1/0/all/0/1"&gt;Lin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liyan_S/0/1/0/all/0/1"&gt;Sun Liyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yihong_Z/0/1/0/all/0/1"&gt;Zhuang Yihong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1"&gt;Huang Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xinghao_D/0/1/0/all/0/1"&gt;Ding Xinghao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaoqing_L/0/1/0/all/0/1"&gt;Liu Xiaoqing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yizhou_Y/0/1/0/all/0/1"&gt;Yu Yizhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging. (arXiv:2008.02766v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02766</id>
        <link href="http://arxiv.org/abs/2008.02766"/>
        <updated>2021-07-16T00:48:24.064Z</updated>
        <summary type="html"><![CDATA[Saliency maps have become a widely used method to make deep learning models
more interpretable by providing post-hoc explanations of classifiers through
identification of the most pertinent areas of the input medical image. They are
increasingly being used in medical imaging to provide clinically plausible
explanations for the decisions the neural network makes. However, the utility
and robustness of these visualization maps has not yet been rigorously examined
in the context of medical imaging. We posit that trustworthiness in this
context requires 1) localization utility, 2) sensitivity to model weight
randomization, 3) repeatability, and 4) reproducibility. Using the localization
information available in two large public radiology datasets, we quantify the
performance of eight commonly used saliency map approaches for the above
criteria using area under the precision-recall curves (AUPRC) and structural
similarity index (SSIM), comparing their performance to various baseline
measures. Using our framework to quantify the trustworthiness of saliency maps,
we show that all eight saliency map techniques fail at least one of the
criteria and are, in most cases, less trustworthy when compared to the
baselines. We suggest that their usage in the high-risk domain of medical
imaging warrants additional scrutiny and recommend that detection or
segmentation models be used if localization is the desired output of the
network. Additionally, to promote reproducibility of our findings, we provide
the code we used for all tests performed in this work at this link:
https://github.com/QTIM-Lab/Assessing-Saliency-Maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arun_N/0/1/0/all/0/1"&gt;Nishanth Arun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaw_N/0/1/0/all/0/1"&gt;Nathan Gaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Praveer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Ken Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Mehak Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bryan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoebel_K/0/1/0/all/0/1"&gt;Katharina Hoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sharut Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jay Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidwani_M/0/1/0/all/0/1"&gt;Mishka Gidwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adebayo_J/0/1/0/all/0/1"&gt;Julius Adebayo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Matthew D. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1"&gt;Jayashree Kalpathy-Cramer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleFusion: A Generative Model for Disentangling Spatial Segments. (arXiv:2107.07437v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07437</id>
        <link href="http://arxiv.org/abs/2107.07437"/>
        <updated>2021-07-16T00:48:24.050Z</updated>
        <summary type="html"><![CDATA[We present StyleFusion, a new mapping architecture for StyleGAN, which takes
as input a number of latent codes and fuses them into a single style code.
Inserting the resulting style code into a pre-trained StyleGAN generator
results in a single harmonized image in which each semantic region is
controlled by one of the input latent codes. Effectively, StyleFusion yields a
disentangled representation of the image, providing fine-grained control over
each region of the generated image. Moreover, to help facilitate global control
over the generated image, a special input latent code is incorporated into the
fused representation. StyleFusion operates in a hierarchical manner, where each
level is tasked with learning to disentangle a pair of image regions (e.g., the
car body and wheels). The resulting learned disentanglement allows one to
modify both local, fine-grained semantics (e.g., facial features) as well as
more global features (e.g., pose and background), providing improved
flexibility in the synthesis process. As a natural extension, StyleFusion
enables one to perform semantically-aware cross-image mixing of regions that
are not necessarily aligned. Finally, we demonstrate how StyleFusion can be
paired with existing editing techniques to more faithfully constrain the edit
to the user's region of interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kafri_O/0/1/0/all/0/1"&gt;Omer Kafri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1"&gt;Or Patashnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1"&gt;Yuval Alaluf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[xCos: An Explainable Cosine Metric for Face Verification Task. (arXiv:2003.05383v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.05383</id>
        <link href="http://arxiv.org/abs/2003.05383"/>
        <updated>2021-07-16T00:48:24.013Z</updated>
        <summary type="html"><![CDATA[We study the XAI (explainable AI) on the face recognition task, particularly
the face verification here. Face verification is a crucial task in recent days
and it has been deployed to plenty of applications, such as access control,
surveillance, and automatic personal log-on for mobile devices. With the
increasing amount of data, deep convolutional neural networks can achieve very
high accuracy for the face verification task. Beyond exceptional performances,
deep face verification models need more interpretability so that we can trust
the results they generate. In this paper, we propose a novel similarity metric,
called explainable cosine ($xCos$), that comes with a learnable module that can
be plugged into most of the verification models to provide meaningful
explanations. With the help of $xCos$, we can see which parts of the two input
faces are similar, where the model pays its attention to, and how the local
similarities are weighted to form the output $xCos$ score. We demonstrate the
effectiveness of our proposed method on LFW and various competitive benchmarks,
resulting in not only providing novel and desiring model interpretability for
face verification but also ensuring the accuracy as plugging into existing face
recognition models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Sheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe-Yu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-An Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu-Siang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Ya-Liang Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots. (arXiv:2107.07243v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07243</id>
        <link href="http://arxiv.org/abs/2107.07243"/>
        <updated>2021-07-16T00:48:23.997Z</updated>
        <summary type="html"><![CDATA[We present VILENS (Visual Inertial Lidar Legged Navigation System), an
odometry system for legged robots based on factor graphs. The key novelty is
the tight fusion of four different sensor modalities to achieve reliable
operation when the individual sensors would otherwise produce degenerate
estimation. To minimize leg odometry drift, we extend the robot's state with a
linear velocity bias term which is estimated online. This bias is only
observable because of the tight fusion of this preintegrated velocity factor
with vision, lidar, and IMU factors. Extensive experimental validation on the
ANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km
traveled. The experiments involved dynamic locomotion over loose rocks, slopes,
and mud; these included perceptual challenges, such as dark and dusty
underground caverns or open, feature-deprived areas, as well as mobility
challenges such as slipping and terrain deformation. We show an average
improvement of 62% translational and 51% rotational errors compared to a
state-of-the-art loosely coupled approach. To demonstrate its robustness,
VILENS was also integrated with a perceptive controller and a local path
planner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1"&gt;David Wisth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1"&gt;Marco Camurri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1"&gt;Maurice Fallon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Completion by Learning Shape Priors. (arXiv:2008.00394v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00394</id>
        <link href="http://arxiv.org/abs/2008.00394"/>
        <updated>2021-07-16T00:48:23.969Z</updated>
        <summary type="html"><![CDATA[In view of the difficulty in reconstructing object details in point cloud
completion, we propose a shape prior learning method for object completion. The
shape priors include geometric information in both complete and the partial
point clouds. We design a feature alignment strategy to learn the shape prior
from complete points, and a coarse to fine strategy to incorporate partial
prior in the fine stage. To learn the complete objects prior, we first train a
point cloud auto-encoder to extract the latent embeddings from complete points.
Then we learn a mapping to transfer the point features from partial points to
that of the complete points by optimizing feature alignment losses. The feature
alignment losses consist of a L2 distance and an adversarial loss obtained by
Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2
distance optimizes the partial features towards the complete ones in the
feature space, and MMD-GAN decreases the statistical distance of two point
features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art
performances on the point cloud completion task. Our code is available at
https://github.com/xiaogangw/point-cloud-completion-shape-prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gim Hee Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:23.960Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutually improved endoscopic image synthesis and landmark detection in unpaired image-to-image translation. (arXiv:2107.06941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06941</id>
        <link href="http://arxiv.org/abs/2107.06941"/>
        <updated>2021-07-16T00:48:23.952Z</updated>
        <summary type="html"><![CDATA[The CycleGAN framework allows for unsupervised image-to-image translation of
unpaired data. In a scenario of surgical training on a physical surgical
simulator, this method can be used to transform endoscopic images of phantoms
into images which more closely resemble the intra-operative appearance of the
same surgical target structure. This can be viewed as a novel augmented reality
approach, which we coined Hyperrealism in previous work. In this use case, it
is of paramount importance to display objects like needles, sutures or
instruments consistent in both domains while altering the style to a more
tissue-like appearance. Segmentation of these objects would allow for a direct
transfer, however, contouring of these, partly tiny and thin foreground objects
is cumbersome and perhaps inaccurate. Instead, we propose to use landmark
detection on the points when sutures pass into the tissue. This objective is
directly incorporated into a CycleGAN framework by treating the performance of
pre-trained detector models as an additional optimization goal. We show that a
task defined on these sparse landmark labels improves consistency of synthesis
by the generator network in both domains. Comparing a baseline CycleGAN
architecture to our proposed extension (DetCycleGAN), mean precision (PPV)
improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by
+0.4743. Furthermore, it could be shown that by dataset fusion, generated
intra-operative images can be leveraged as additional training data for the
detection network itself. The data is released within the scope of the AdaptOR
MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at
https://github.com/Cardio-AI/detcyclegan_pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1"&gt;Lalith Sharan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_G/0/1/0/all/0/1"&gt;Gabriele Romano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1"&gt;Sven Koehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelm_H/0/1/0/all/0/1"&gt;Halvar Kelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karck_M/0/1/0/all/0/1"&gt;Matthias Karck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simone_R/0/1/0/all/0/1"&gt;Raffaele De Simone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1"&gt;Sandy Engelhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Frank-Wolfe Adversarial Training. (arXiv:2012.12368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12368</id>
        <link href="http://arxiv.org/abs/2012.12368"/>
        <updated>2021-07-16T00:48:23.934Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are easily fooled by small perturbations known as
adversarial attacks. Adversarial Training (AT) is a technique that
approximately solves a robust optimization problem to minimize the worst-case
loss and is widely regarded as the most effective defense against such attacks.
We develop a theoretical framework for adversarial training with FW
optimization (FW-AT) that reveals a geometric connection between the loss
landscape and the distortion of $\ell_\infty$ FW attacks (the attack's $\ell_2$
norm). Specifically, we show that high distortion of FW attacks is equivalent
to low variation along the attack path. It is then experimentally demonstrated
on various deep neural network architectures that $\ell_\infty$ attacks against
robust models achieve near maximal $\ell_2$ distortion. This mathematical
transparency differentiates FW from the more popular Projected Gradient Descent
(PGD) optimization. To demonstrate the utility of our theoretical framework we
develop FW-Adapt, a novel adversarial training algorithm which uses simple
distortion measure to adaptively change number of attack steps during training.
FW-Adapt provides strong robustness at lower training times in comparison to
PGD-AT for a variety of white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsiligkaridis_T/0/1/0/all/0/1"&gt;Theodoros Tsiligkaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1"&gt;Jay Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending best course of treatment based on similarities of prognostic markers\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07500</id>
        <link href="http://arxiv.org/abs/2107.07500"/>
        <updated>2021-07-16T00:48:23.918Z</updated>
        <summary type="html"><![CDATA[With the advancement in the technology sector spanning over every field, a
huge influx of information is inevitable. Among all the opportunities that the
advancements in the technology have brought, one of them is to propose
efficient solutions for data retrieval. This means that from an enormous pile
of data, the retrieval methods should allow the users to fetch the relevant and
recent data over time. In the field of entertainment and e-commerce,
recommender systems have been functioning to provide the aforementioned.
Employing the same systems in the medical domain could definitely prove to be
useful in variety of ways. Following this context, the goal of this paper is to
propose collaborative filtering based recommender system in the healthcare
sector to recommend remedies based on the symptoms experienced by the patients.
Furthermore, a new dataset is developed consisting of remedies concerning
various diseases to address the limited availability of the data. The proposed
recommender system accepts the prognostic markers of a patient as the input and
generates the best remedy course. With several experimental trials, the
proposed model achieved promising results in recommending the possible remedy
for given prognostic markers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1"&gt;Sudhanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Level generation and style enhancement -- deep learning for game development overview. (arXiv:2107.07397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07397</id>
        <link href="http://arxiv.org/abs/2107.07397"/>
        <updated>2021-07-16T00:48:23.875Z</updated>
        <summary type="html"><![CDATA[We present practical approaches of using deep learning to create and enhance
level maps and textures for video games -- desktop, mobile, and web. We aim to
present new possibilities for game developers and level artists. The task of
designing levels and filling them with details is challenging. It is both
time-consuming and takes effort to make levels rich, complex, and with a
feeling of being natural. Fortunately, recent progress in deep learning
provides new tools to accompany level designers and visual artists. Moreover,
they offer a way to generate infinite worlds for game replayability and adjust
educational games to players' needs. We present seven approaches to create
level maps, each using statistical methods, machine learning, or deep learning.
In particular, we include:

- Generative Adversarial Networks for creating new images from existing
examples (e.g. ProGAN).

- Super-resolution techniques for upscaling images while preserving crisp
detail (e.g. ESRGAN).

- Neural style transfer for changing visual themes.

- Image translation - turning semantic maps into images (e.g. GauGAN).

- Semantic segmentation for turning images into semantic masks (e.g. U-Net).

- Unsupervised semantic segmentation for extracting semantic features (e.g.
Tile2Vec).

- Texture synthesis - creating large patterns based on a smaller sample (e.g.
InGAN).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Migdal_P/0/1/0/all/0/1"&gt;Piotr Migda&amp;#x142;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olechno_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Olechno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podgorski_B/0/1/0/all/0/1"&gt;B&amp;#x142;a&amp;#x17c;ej Podg&amp;#xf3;rski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Channel Auto-Encoders and a Novel Dataset for Learning Domain Invariant Representations of Histopathology Images. (arXiv:2107.07271v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07271</id>
        <link href="http://arxiv.org/abs/2107.07271"/>
        <updated>2021-07-16T00:48:23.866Z</updated>
        <summary type="html"><![CDATA[Domain shift is a problem commonly encountered when developing automated
histopathology pipelines. The performance of machine learning models such as
convolutional neural networks within automated histopathology pipelines is
often diminished when applying them to novel data domains due to factors
arising from differing staining and scanning protocols. The Dual-Channel
Auto-Encoder (DCAE) model was previously shown to produce feature
representations that are less sensitive to appearance variation introduced by
different digital slide scanners. In this work, the Multi-Channel Auto-Encoder
(MCAE) model is presented as an extension to DCAE which learns from more than
two domains of data. Additionally, a synthetic dataset is generated using
CycleGANs that contains aligned tissue images that have had their appearance
synthetically modified. Experimental results show that the MCAE model produces
feature representations that are less sensitive to inter-domain variations than
the comparative StaNoSA method when tested on the novel synthetic data.
Additionally, the MCAE and StaNoSA models are tested on a novel tissue
classification task. The results of this experiment show the MCAE model out
performs the StaNoSA model by 5 percentage-points in the f1-score. These
results show that the MCAE model is able to generalise better to novel data and
tasks than existing approaches by actively learning normalised feature
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Moyes_A/0/1/0/all/0/1"&gt;Andrew Moyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gault_R/0/1/0/all/0/1"&gt;Richard Gault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ming_J/0/1/0/all/0/1"&gt;Ji Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crookes_D/0/1/0/all/0/1"&gt;Danny Crookes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jing Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MeNToS: Tracklets Association with a Space-Time Memory Network. (arXiv:2107.07067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07067</id>
        <link href="http://arxiv.org/abs/2107.07067"/>
        <updated>2021-07-16T00:48:23.860Z</updated>
        <summary type="html"><![CDATA[We propose a method for multi-object tracking and segmentation (MOTS) that
does not require fine-tuning or per benchmark hyperparameter selection. The
proposed method addresses particularly the data association problem. Indeed,
the recently introduced HOTA metric, that has a better alignment with the human
visual assessment by evenly balancing detections and associations quality, has
shown that improvements are still needed for data association. After creating
tracklets using instance segmentation and optical flow, the proposed method
relies on a space-time memory network (STM) developed for one-shot video object
segmentation to improve the association of tracklets with temporal gaps. To the
best of our knowledge, our method, named MeNToS, is the first to use the STM
network to track object masks for MOTS. We took the 4th place in the RobMOTS
challenge. The project page is https://mehdimiah.com/mentos.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miah_M/0/1/0/all/0/1"&gt;Mehdi Miah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1"&gt;Guillaume-Alexandre Bilodeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1"&gt;Nicolas Saunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-image Full-body Human Relighting. (arXiv:2107.07259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07259</id>
        <link href="http://arxiv.org/abs/2107.07259"/>
        <updated>2021-07-16T00:48:23.853Z</updated>
        <summary type="html"><![CDATA[We present a single-image data-driven method to automatically relight images
with full-body humans in them. Our framework is based on a realistic scene
decomposition leveraging precomputed radiance transfer (PRT) and spherical
harmonics (SH) lighting. In contrast to previous work, we lift the assumptions
on Lambertian materials and explicitly model diffuse and specular reflectance
in our data. Moreover, we introduce an additional light-dependent residual term
that accounts for errors in the PRT-based image reconstruction. We propose a
new deep learning architecture, tailored to the decomposition performed in PRT,
that is trained using a combination of L1, logarithmic, and rendering losses.
Our model outperforms the state of the art for full-body human relighting both
with synthetic images and photographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lagunas_M/0/1/0/all/0/1"&gt;Manuel Lagunas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jimei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1"&gt;Ruben Villegas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1"&gt;Zhixin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1"&gt;Belen Masia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1"&gt;Diego Gutierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Topic Inference for Chest X-Ray Report Generation. (arXiv:2107.07314v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07314</id>
        <link href="http://arxiv.org/abs/2107.07314"/>
        <updated>2021-07-16T00:48:23.836Z</updated>
        <summary type="html"><![CDATA[Automating report generation for medical imaging promises to reduce workload
and assist diagnosis in clinical practice. Recent work has shown that deep
learning models can successfully caption natural images. However, learning from
medical data is challenging due to the diversity and uncertainty inherent in
the reports written by different radiologists with discrepant expertise and
experience. To tackle these challenges, we propose variational topic inference
for automatic report generation. Specifically, we introduce a set of topics as
latent variables to guide sentence generation by aligning image and language
modalities in a latent space. The topics are inferred in a conditional
variational inference framework, with each topic governing the generation of a
sentence in the report. Further, we adopt a visual attention module that
enables the model to attend to different locations in the image and generate
more informative descriptions. We conduct extensive experiments on two
benchmarks, namely Indiana U. Chest X-rays and MIMIC-CXR. The results
demonstrate that our proposed variational topic inference method can generate
novel reports rather than mere copies of reports used in training, while still
achieving comparable performance to state-of-the-art methods in terms of
standard language generation criteria.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1"&gt;Ivona Najdenkoska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1"&gt;Marcel Worring&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What and When to Look?: Temporal Span Proposal Network for Video Visual Relation Detection. (arXiv:2107.07154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07154</id>
        <link href="http://arxiv.org/abs/2107.07154"/>
        <updated>2021-07-16T00:48:23.829Z</updated>
        <summary type="html"><![CDATA[Identifying relations between objects is central to understanding the scene.
While several works have been proposed for relation modeling in the image
domain, there have been many constraints in the video domain due to challenging
dynamics of spatio-temporal interactions (e.g., Between which objects are there
an interaction? When do relations occur and end?). To date, two representative
methods have been proposed to tackle Video Visual Relation Detection (VidVRD):
segment-based and window-based. We first point out the limitations these two
methods have and propose Temporal Span Proposal Network (TSPN), a novel method
with two advantages in terms of efficiency and effectiveness. 1) TSPN tells
what to look: it sparsifies relation search space by scoring relationness
(i.e., confidence score for the existence of a relation between pair of
objects) of object pair. 2) TSPN tells when to look: it leverages the full
video context to simultaneously predict the temporal span and categories of the
entire relations. TSPN demonstrates its effectiveness by achieving new
state-of-the-art by a significant margin on two VidVRD benchmarks
(ImageNet-VidVDR and VidOR) while also showing lower time complexity than
existing methods - in particular, twice as efficient as a popular segment-based
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Sangmin Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1"&gt;Junhyug Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kangil Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighbor-view Enhanced Model for Vision and Language Navigation. (arXiv:2107.07201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07201</id>
        <link href="http://arxiv.org/abs/2107.07201"/>
        <updated>2021-07-16T00:48:23.822Z</updated>
        <summary type="html"><![CDATA[Vision and Language Navigation (VLN) requires an agent to navigate to a
target location by following natural language instructions. Most of existing
works represent a navigation candidate by the feature of the corresponding
single view where the candidate lies in. However, an instruction may mention
landmarks out of the single view as references, which might lead to failures of
textual-visual matching of existing methods. In this work, we propose a
multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate
visual contexts from neighbor views for better textual-visual matching.
Specifically, our NvEM utilizes a subject module and a reference module to
collect contexts from neighbor views. The subject module fuses neighbor views
at a global level, and the reference module fuses neighbor objects at a local
level. Subjects and references are adaptively determined via attention
mechanisms. Our model also includes an action module to utilize the strong
orientation guidance (e.g., ``turn left'') in instructions. Each module
predicts navigation action separately and their weighted sum is used for
predicting the final action. Extensive experimental results demonstrate the
effectiveness of the proposed method on the R2R and R4R benchmarks against
several state-of-the-art navigators, and NvEM even beats some pre-training
ones. Our code is available at https://github.com/MarSaKi/NvEM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1"&gt;Dong An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1"&gt;Yuankai Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amodal segmentation just like doing a jigsaw. (arXiv:2107.07464v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07464</id>
        <link href="http://arxiv.org/abs/2107.07464"/>
        <updated>2021-07-16T00:48:23.816Z</updated>
        <summary type="html"><![CDATA[Amodal segmentation is a new direction of instance segmentation while
considering the segmentation of the visible and occluded parts of the instance.
The existing state-of-the-art method uses multi-task branches to predict the
amodal part and the visible part separately and subtract the visible part from
the amodal part to obtain the occluded part. However, the amodal part contains
visible information. Therefore, the separated prediction method will generate
duplicate information. Different from this method, we propose a method of
amodal segmentation based on the idea of the jigsaw. The method uses multi-task
branches to predict the two naturally decoupled parts of visible and occluded,
which is like getting two matching jigsaw pieces. Then put the two jigsaw
pieces together to get the amodal part. This makes each branch focus on the
modeling of the object. And we believe that there are certain rules in the
occlusion relationship in the real world. This is a kind of occlusion context
information. This jigsaw method can better model the occlusion relationship and
use the occlusion context information, which is important for amodal
segmentation. Experiments on two widely used amodally annotated datasets prove
that our method exceeds existing state-of-the-art methods. The source code of
this work will be made public soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xunli Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianqin Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence Conditioned Knowledge Distillation. (arXiv:2107.06993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06993</id>
        <link href="http://arxiv.org/abs/2107.06993"/>
        <updated>2021-07-16T00:48:23.808Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel confidence conditioned knowledge distillation (CCKD)
scheme for transferring the knowledge from a teacher model to a student model
is proposed. Existing state-of-the-art methods employ fixed loss functions for
this purpose and ignore the different levels of information that need to be
transferred for different samples. In addition to that, these methods are also
inefficient in terms of data usage. CCKD addresses these issues by leveraging
the confidence assigned by the teacher model to the correct class to devise
sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T
formulation). Further, CCKD improves the data efficiency by employing
self-regulation to stop those samples from participating in the distillation
process on which the student model learns faster. Empirical evaluations on
several benchmark datasets show that CCKD methods achieve at least as much
generalization performance levels as other state-of-the-art methods while being
data efficient in the process. Student models trained through CCKD methods do
not retain most of the misclassifications commited by the teacher model on the
training set. Distillation through CCKD methods improves the resilience of the
student models against adversarial attacks compared to the conventional KD
method. Experiments show at least 3% increase in performance against
adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least
6% increase for the CIFAR10 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Sourav Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attacks on Multi-task Visual Perception for Autonomous Driving. (arXiv:2107.07449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07449</id>
        <link href="http://arxiv.org/abs/2107.07449"/>
        <updated>2021-07-16T00:48:23.791Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have accomplished impressive success in various
applications, including autonomous driving perception tasks, in recent years.
On the other hand, current deep neural networks are easily fooled by
adversarial attacks. This vulnerability raises significant concerns,
particularly in safety-critical applications. As a result, research into
attacking and defending DNNs has gained much coverage. In this work, detailed
adversarial attacks are applied on a diverse multi-task visual perception deep
network across distance estimation, semantic segmentation, motion detection,
and object detection. The experiments consider both white and black box attacks
for targeted and un-targeted cases, while attacking a task and inspecting the
effect on all the others, in addition to inspecting the effect of applying a
simple defense method. We conclude this paper by comparing and discussing the
experimental results, proposing insights and future work. The visualizations of
the attacks are available at https://youtu.be/R3JUV41aiPY.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sobh_I/0/1/0/all/0/1"&gt;Ibrahim Sobh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamed_A/0/1/0/all/0/1"&gt;Ahmed Hamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Conditional Adaptation for Recognizing Unseen Classes in Unseen Domains. (arXiv:2107.07497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07497</id>
        <link href="http://arxiv.org/abs/2107.07497"/>
        <updated>2021-07-16T00:48:23.785Z</updated>
        <summary type="html"><![CDATA[Recent progress towards designing models that can generalize to unseen
domains (i.e domain generalization) or unseen classes (i.e zero-shot learning)
has embarked interest towards building models that can tackle both domain-shift
and semantic shift simultaneously (i.e zero-shot domain generalization). For
models to generalize to unseen classes in unseen domains, it is crucial to
learn feature representation that preserves class-level (domain-invariant) as
well as domain-specific information. Motivated from the success of generative
zero-shot approaches, we propose a feature generative framework integrated with
a COntext COnditional Adaptive (COCOA) Batch-Normalization to seamlessly
integrate class-level semantic and domain-specific information. The generated
visual features better capture the underlying data distribution enabling us to
generalize to unseen classes and domains at test-time. We thoroughly evaluate
and analyse our approach on established large-scale benchmark - DomainNet and
demonstrate promising performance over baselines and state-of-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mangla_P/0/1/0/all/0/1"&gt;Puneet Mangla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandhok_S/0/1/0/all/0/1"&gt;Shivam Chandhok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahad Shahbaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A modular U-Net for automated segmentation of X-ray tomography images in composite materials. (arXiv:2107.07468v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.07468</id>
        <link href="http://arxiv.org/abs/2107.07468"/>
        <updated>2021-07-16T00:48:23.779Z</updated>
        <summary type="html"><![CDATA[X-ray Computed Tomography (XCT) techniques have evolved to a point that
high-resolution data can be acquired so fast that classic segmentation methods
are prohibitively cumbersome, demanding automated data pipelines capable of
dealing with non-trivial 3D images. Deep learning has demonstrated success in
many image processing tasks, including material science applications, showing a
promising alternative for a humanfree segmentation pipeline. In this paper a
modular interpretation of UNet (Modular U-Net) is proposed and trained to
segment 3D tomography images of a three-phased glass fiber-reinforced Polyamide
66. We compare 2D and 3D versions of our model, finding that the former is
slightly better than the latter. We observe that human-comparable results can
be achievied even with only 10 annotated layers and using a shallow U-Net
yields better results than a deeper one. As a consequence, Neural Network (NN)
show indeed a promising venue to automate XCT data processing pipelines needing
no human, adhoc intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bertoldo_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o P C Bertoldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Decenciere_E/0/1/0/all/0/1"&gt;Etienne Decenci&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ryckelynck_D/0/1/0/all/0/1"&gt;David Ryckelynck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Proudhon_H/0/1/0/all/0/1"&gt;Henry Proudhon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition. (arXiv:2107.07333v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07333</id>
        <link href="http://arxiv.org/abs/2107.07333"/>
        <updated>2021-07-16T00:48:23.766Z</updated>
        <summary type="html"><![CDATA[Identifying potential threats concealed within the baggage is of prime
concern for the security staff. Many researchers have developed frameworks that
can detect baggage threats from X-ray scans. However, to the best of our
knowledge, all of these frameworks require extensive training on large-scale
and well-annotated datasets, which are hard to procure in the real world. This
paper presents a novel unsupervised anomaly instance segmentation framework
that recognizes baggage threats, in X-ray scans, as anomalies without requiring
any ground truth labels. Furthermore, thanks to its stylization capacity, the
framework is trained only once, and at the inference stage, it detects and
extracts contraband items regardless of their scanner specifications. Our
one-staged approach initially learns to reconstruct normal baggage content via
an encoder-decoder network utilizing a proposed stylization loss function. The
model subsequently identifies the abnormal regions by analyzing the disparities
within the original and the reconstructed scans. The anomalous regions are then
clustered and post-processed to fit a bounding box for their localization. In
addition, an optional classifier can also be appended with the proposed
framework to recognize the categories of these extracted anomalies. A thorough
evaluation of the proposed system on four public baggage X-ray datasets,
without any re-training, demonstrates that it achieves competitive performance
as compared to the conventional fully supervised methods (i.e., the mean
average precision score of 0.7941 on SIXray, 0.8591 on GDXray, 0.7483 on
OPIXray, and 0.5439 on COMPASS-XP dataset) while outperforming state-of-the-art
semi-supervised and unsupervised baggage threat detection frameworks by 67.37%,
32.32%, 47.19%, and 45.81% in terms of F1 score across SIXray, GDXray, OPIXray,
and COMPASS-XP datasets, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1"&gt;Taimur Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1"&gt;Samet Akcay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1"&gt;Naoufel Werghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.07436</id>
        <link href="http://arxiv.org/abs/2107.07436"/>
        <updated>2021-07-16T00:48:23.748Z</updated>
        <summary type="html"><![CDATA[Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jethani_N/0/1/0/all/0/1"&gt;Neil Jethani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sudarshan_M/0/1/0/all/0/1"&gt;Mukund Sudarshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Covert_I/0/1/0/all/0/1"&gt;Ian Covert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Su-In Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ranganath_R/0/1/0/all/0/1"&gt;Rajesh Ranganath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Food Instance Segmentation using Synthetic Data. (arXiv:2107.07191v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07191</id>
        <link href="http://arxiv.org/abs/2107.07191"/>
        <updated>2021-07-16T00:48:23.735Z</updated>
        <summary type="html"><![CDATA[In the process of intelligently segmenting foods in images using deep neural
networks for diet management, data collection and labeling for network training
are very important but labor-intensive tasks. In order to solve the
difficulties of data collection and annotations, this paper proposes a food
segmentation method applicable to real-world through synthetic data. To perform
food segmentation on healthcare robot systems, such as meal assistance robot
arm, we generate synthetic data using the open-source 3D graphics software
Blender placing multiple objects on meal plate and train Mask R-CNN for
instance segmentation. Also, we build a data collection system and verify our
segmentation model on real-world food data. As a result, on our real-world
dataset, the model trained only synthetic data is available to segment food
instances that are not trained with 52.2% mask AP@all, and improve performance
by +6.4%p after fine-tuning comparing to the model trained from scratch. In
addition, we also confirm the possibility and performance improvement on the
public dataset for fair analysis. Our code and pre-trained weights are
avaliable online at: https://github.com/gist-ailab/Food-Instance-Segmentation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;D. Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;J. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;J. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;K. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training for temporal sparsity in deep neural networks, application in video processing. (arXiv:2107.07305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07305</id>
        <link href="http://arxiv.org/abs/2107.07305"/>
        <updated>2021-07-16T00:48:23.725Z</updated>
        <summary type="html"><![CDATA[Activation sparsity improves compute efficiency and resource utilization in
sparsity-aware neural network accelerators. As the predominant operation in
DNNs is multiply-accumulate (MAC) of activations with weights to compute inner
products, skipping operations where (at least) one of the two operands is zero
can make inference more efficient in terms of latency and power. Spatial
sparsification of activations is a popular topic in DNN literature and several
methods have already been established to bias a DNN for it. On the other hand,
temporal sparsity is an inherent feature of bio-inspired spiking neural
networks (SNNs), which neuromorphic processing exploits for hardware
efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic
much less explored in DNN literature, but in perfect resonance with the trend
in DNN, to shift from static signal processing to more streaming signal
processing. Towards this goal, in this paper we introduce a new DNN layer
(called Delta Activation Layer), whose sole purpose is to promote temporal
sparsity of activations during training. A Delta Activation Layer casts
temporal sparsity into spatial activation sparsity to be exploited when
performing sparse tensor multiplications in hardware. By employing delta
inference and ``the usual'' spatial sparsification heuristics during training,
the resulting model learns to exploit not only spatial but also temporal
activation sparsity (for a given input data distribution). One may use the
Delta Activation Layer either during vanilla training or during a refinement
phase. We have implemented Delta Activation Layer as an extension of the
standard Tensoflow-Keras library, and applied it to train deep neural networks
on the Human Action Recognition (UCF101) dataset. We report an almost 3x
improvement of activation sparsity, with recoverable loss of model accuracy
after longer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_A/0/1/0/all/0/1"&gt;Amirreza Yousefzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sifalakis_M/0/1/0/all/0/1"&gt;Manolis Sifalakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN. (arXiv:2107.07224v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07224</id>
        <link href="http://arxiv.org/abs/2107.07224"/>
        <updated>2021-07-16T00:48:23.715Z</updated>
        <summary type="html"><![CDATA[Generative adversarial models (GANs) continue to produce advances in terms of
the visual quality of still images, as well as the learning of temporal
correlations. However, few works manage to combine these two interesting
capabilities for the synthesis of video content: Most methods require an
extensive training dataset in order to learn temporal correlations, while being
rather limited in the resolution and visual quality of their output frames. In
this paper, we present a novel approach to the video synthesis problem that
helps to greatly improve visual quality and drastically reduce the amount of
training data and resources necessary for generating video content. Our
formulation separates the spatial domain, in which individual frames are
synthesized, from the temporal domain, in which motion is generated. For the
spatial domain we make use of a pre-trained StyleGAN network, the latent space
of which allows control over the appearance of the objects it was trained for.
The expressive power of this model allows us to embed our training videos in
the StyleGAN latent space. Our temporal architecture is then trained not on
sequences of RGB frames, but on sequences of StyleGAN latent codes. The
advantageous properties of the StyleGAN space simplify the discovery of
temporal correlations. We demonstrate that it suffices to train our temporal
architecture on only 10 minutes of footage of 1 subject for about 6 hours.
After training, our model can not only generate new portrait videos for the
training subject, but also for any random subject which can be embedded in the
StyleGAN space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1"&gt;Gereon Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ayush Tewari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1"&gt;Mohamed Elgharib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Bandit: Provable Algorithm for Visual-aware Advertising. (arXiv:2107.07438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07438</id>
        <link href="http://arxiv.org/abs/2107.07438"/>
        <updated>2021-07-16T00:48:23.709Z</updated>
        <summary type="html"><![CDATA[Online advertising is ubiquitous in web business. Image displaying is
considered as one of the most commonly used formats to interact with customers.
Contextual multi-armed bandit has shown success in the application of
advertising to solve the exploration-exploitation dilemma existed in the
recommendation procedure. Inspired by the visual-aware advertising, in this
paper, we propose a contextual bandit algorithm, where the convolutional neural
network (CNN) is utilized to learn the reward function along with an upper
confidence bound (UCB) for exploration. We also prove a near-optimal regret
bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is over-parameterized
and establish strong connections with convolutional neural tangent kernel
(CNTK). Finally, we evaluate the empirical performance of the proposed
algorithm and show that it outperforms other state-of-the-art UCB-based bandit
algorithms on real-world image data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1"&gt;Yikun Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynaDog+T: A Parametric Animal Model for Synthetic Canine Image Generation. (arXiv:2107.07330v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07330</id>
        <link href="http://arxiv.org/abs/2107.07330"/>
        <updated>2021-07-16T00:48:23.691Z</updated>
        <summary type="html"><![CDATA[Synthetic data is becoming increasingly common for training computer vision
models for a variety of tasks. Notably, such data has been applied in tasks
related to humans such as 3D pose estimation where data is either difficult to
create or obtain in realistic settings. Comparatively, there has been less work
into synthetic animal data and it's uses for training models. Consequently, we
introduce a parametric canine model, DynaDog+T, for generating synthetic canine
images and data which we use for a common computer vision task, binary
segmentation, which would otherwise be difficult due to the lack of available
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deane_J/0/1/0/all/0/1"&gt;Jake Deane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearney_S/0/1/0/all/0/1"&gt;Sinead Kearney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwang In Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cosker_D/0/1/0/all/0/1"&gt;Darren Cosker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. (arXiv:2107.07431v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07431</id>
        <link href="http://arxiv.org/abs/2107.07431"/>
        <updated>2021-07-16T00:48:23.675Z</updated>
        <summary type="html"><![CDATA[The increasing demand for commodities is leading to changes in land use
worldwide. In the tropics, deforestation, which causes high carbon emissions
and threatens biodiversity, is often linked to agricultural expansion. While
the need for deforestation-free global supply chains is widely recognized,
making progress in practice remains a challenge. Here, we propose an automated
approach that aims to support conservation and sustainable land use planning
decisions by mapping tropical landscapes at large scale and high spatial
resolution following the High Carbon Stock (HCS) approach. A deep learning
approach is developed that estimates canopy height for each 10 m Sentinel-2
pixel by learning from sparse GEDI LIDAR reference data, achieving an overall
RMSE of 6.3 m. We show that these wall-to-wall maps of canopy top height are
predictive for classifying HCS forests and degraded areas with an overall
accuracy of 86 % and produce a first high carbon stock map for Indonesia,
Malaysia, and the Philippines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1"&gt;Nico Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;Jan Dirk Wegner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framework for A Personalized Intelligent Assistant to Elderly People for Activities of Daily Living. (arXiv:2107.07344v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.07344</id>
        <link href="http://arxiv.org/abs/2107.07344"/>
        <updated>2021-07-16T00:48:23.649Z</updated>
        <summary type="html"><![CDATA[The increasing population of elderly people is associated with the need to
meet their increasing requirements and to provide solutions that can improve
their quality of life in a smart home. In addition to fear and anxiety towards
interfacing with systems; cognitive disabilities, weakened memory, disorganized
behavior and even physical limitations are some of the problems that elderly
people tend to face with increasing age. The essence of providing
technology-based solutions to address these needs of elderly people and to
create smart and assisted living spaces for the elderly; lies in developing
systems that can adapt by addressing their diversity and can augment their
performances in the context of their day to day goals. Therefore, this work
proposes a framework for development of a Personalized Intelligent Assistant to
help elderly people perform Activities of Daily Living (ADLs) in a smart and
connected Internet of Things (IoT) based environment. This Personalized
Intelligent Assistant can analyze different tasks performed by the user and
recommend activities by considering their daily routine, current affective
state and the underlining user experience. To uphold the efficacy of this
proposed framework, it has been tested on a couple of datasets for modelling an
average user and a specific user respectively. The results presented show that
the model achieves a performance accuracy of 73.12% when modelling a specific
user, which is considerably higher than its performance while modelling an
average user, this upholds the relevance for development and implementation of
this proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Automatic Natural Image Matting. (arXiv:2107.07235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07235</id>
        <link href="http://arxiv.org/abs/2107.07235"/>
        <updated>2021-07-16T00:48:23.587Z</updated>
        <summary type="html"><![CDATA[Automatic image matting (AIM) refers to estimating the soft foreground from
an arbitrary natural image without any auxiliary input like trimap, which is
useful for image editing. Prior methods try to learn semantic features to aid
the matting process while being limited to images with salient opaque
foregrounds such as humans and animals. In this paper, we investigate the
difficulties when extending them to natural images with salient
transparent/meticulous foregrounds or non-salient foregrounds. To address the
problem, a novel end-to-end matting network is proposed, which can predict a
generalized trimap for any image of the above types as a unified semantic
representation. Simultaneously, the learned semantic features guide the matting
network to focus on the transition areas via an attention mechanism. We also
construct a test set AIM-500 that contains 500 diverse natural images covering
all types along with manually labeled alpha mattes, making it feasible to
benchmark the generalization ability of AIM models. Results of the experiments
demonstrate that our network trained on available composite matting datasets
outperforms existing methods both objectively and subjectively. The source code
and dataset are available at https://github.com/JizhiziLi/AIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Image Cropping. (arXiv:2107.07153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07153</id>
        <link href="http://arxiv.org/abs/2107.07153"/>
        <updated>2021-07-16T00:48:23.563Z</updated>
        <summary type="html"><![CDATA[Automatic image cropping techniques are commonly used to enhance the
aesthetic quality of an image; they do it by detecting the most beautiful or
the most salient parts of the image and removing the unwanted content to have a
smaller image that is more visually pleasing. In this thesis, I introduce an
additional dimension to the problem of cropping, semantics. I argue that image
cropping can also enhance the image's relevancy for a given entity by using the
semantic information contained in the image. I call this problem, Semantic
Image Cropping. To support my argument, I provide a new dataset containing 100
images with at least two different entities per image and four ground truth
croppings collected using Amazon Mechanical Turk. I use this dataset to show
that state-of-the-art cropping algorithms that only take into account
aesthetics do not perform well in the problem of semantic image cropping.
Additionally, I provide a new deep learning system that takes not just
aesthetics but also semantics into account to generate image croppings, and I
evaluate its performance using my new semantic cropping dataset, showing that
using the semantic information of an image can help to produce better
croppings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Corcoll_O/0/1/0/all/0/1"&gt;Oriol Corcoll&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Lambertian Priors into Surface Normals Measurement. (arXiv:2107.07192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07192</id>
        <link href="http://arxiv.org/abs/2107.07192"/>
        <updated>2021-07-16T00:48:23.554Z</updated>
        <summary type="html"><![CDATA[The goal of photometric stereo is to measure the precise surface normal of a
3D object from observations with various shading cues. However, non-Lambertian
surfaces influence the measurement accuracy due to irregular shading cues.
Despite deep neural networks have been employed to simulate the performance of
non-Lambertian surfaces, the error in specularities, shadows, and crinkle
regions is hard to be reduced. In order to address this challenge, we here
propose a photometric stereo network that incorporates Lambertian priors to
better measure the surface normal. In this paper, we use the initial normal
under the Lambertian assumption as the prior information to refine the normal
measurement, instead of solely applying the observed shading cues to deriving
the surface normal. Our method utilizes the Lambertian information to
reparameterize the network weights and the powerful fitting ability of deep
neural networks to correct these errors caused by general reflectance
properties. Our explorations include: the Lambertian priors (1) reduce the
learning hypothesis space, making our method learn the mapping in the same
surface normal space and improving the accuracy of learning, and (2) provides
the differential features learning, improving the surfaces reconstruction of
details. Extensive experiments verify the effectiveness of the proposed
Lambertian prior photometric stereo network in accurate surface normal
measurement, on the challenging benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1"&gt;Yakun Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_M/0/1/0/all/0/1"&gt;Muwei Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Shaoxiang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Interaction Graphs of Partially Observed Pedestrians for Trajectory Prediction. (arXiv:2107.07056v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.07056</id>
        <link href="http://arxiv.org/abs/2107.07056"/>
        <updated>2021-07-16T00:48:23.530Z</updated>
        <summary type="html"><![CDATA[Multi-pedestrian trajectory prediction is an indispensable safety element of
autonomous systems that interact with crowds in unstructured environments. Many
recent efforts have developed trajectory prediction algorithms with focus on
understanding social norms behind pedestrian motions. Yet we observe these
works usually hold two assumptions that prevent them from being smoothly
applied to robot applications: positions of all pedestrians are consistently
tracked; the target agent pays attention to all pedestrians in the scene. The
first assumption leads to biased interaction modeling with incomplete
pedestrian data, and the second assumption introduces unnecessary disturbances
and leads to the freezing robot problem. Thus, we propose Gumbel Social
Transformer, in which an Edge Gumbel Selector samples a sparse interaction
graph of partially observed pedestrians at each time step. A Node Transformer
Encoder and a Masked LSTM encode the pedestrian features with the sampled
sparse graphs to predict trajectories. We demonstrate that our model overcomes
the potential problems caused by the assumptions, and our approach outperforms
the related works in benchmark evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kazuki Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1"&gt;Katherine Driggs-Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Parameter Generators. (arXiv:2107.07110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07110</id>
        <link href="http://arxiv.org/abs/2107.07110"/>
        <updated>2021-07-16T00:48:23.523Z</updated>
        <summary type="html"><![CDATA[We present a generic method for recurrently using the same parameters for
many different convolution layers to build a deep network. Specifically, for a
network, we create a recurrent parameter generator (RPG), from which the
parameters of each convolution layer are generated. Though using recurrent
models to build a deep convolutional neural network (CNN) is not entirely new,
our method achieves significant performance gain compared to the existing
works. We demonstrate how to build a one-layer neural network to achieve
similar performance compared to other traditional CNN models on various
applications and datasets. Such a method allows us to build an arbitrarily
complex neural network with any amount of parameters. For example, we build a
ResNet34 with model parameters reduced by more than $400$ times, which still
achieves $41.6\%$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG
can be applied at different scales, such as layers, blocks, or even
sub-networks. Specifically, we use the RPG to build a ResNet18 network with the
number of weights equivalent to one convolutional layer of a conventional
ResNet and show this model can achieve $67.2\%$ ImageNet top-1 accuracy. The
proposed method can be viewed as an inverse approach to model compression.
Rather than removing the unused parameters from a large model, it aims to
squeeze more information into a small number of parameters. Extensive
experiment results are provided to demonstrate the power of the proposed
recurrent parameter generator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Stella X. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_B/0/1/0/all/0/1"&gt;Brian Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1"&gt;Yann LeCun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lidar Light Scattering Augmentation (LISA): Physics-based Simulation of Adverse Weather Conditions for 3D Object Detection. (arXiv:2107.07004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07004</id>
        <link href="http://arxiv.org/abs/2107.07004"/>
        <updated>2021-07-16T00:48:23.517Z</updated>
        <summary type="html"><![CDATA[Lidar-based object detectors are critical parts of the 3D perception pipeline
in autonomous navigation systems such as self-driving cars. However, they are
known to be sensitive to adverse weather conditions such as rain, snow and fog
due to reduced signal-to-noise ratio (SNR) and signal-to-background ratio
(SBR). As a result, lidar-based object detectors trained on data captured in
normal weather tend to perform poorly in such scenarios. However, collecting
and labelling sufficient training data in a diverse range of adverse weather
conditions is laborious and prohibitively expensive. To address this issue, we
propose a physics-based approach to simulate lidar point clouds of scenes in
adverse weather conditions. These augmented datasets can then be used to train
lidar-based detectors to improve their all-weather reliability. Specifically,
we introduce a hybrid Monte-Carlo based approach that treats (i) the effects of
large particles by placing them randomly and comparing their back reflected
power against the target, and (ii) attenuation effects on average through
calculation of scattering efficiencies from the Mie theory and particle size
distributions. Retraining networks with this augmented data improves mean
average precision evaluated on real world rainy scenes and we observe greater
improvement in performance with our model relative to existing models from the
literature. Furthermore, we evaluate recent state-of-the-art detectors on the
simulated weather conditions and present an in-depth analysis of their
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kilic_V/0/1/0/all/0/1"&gt;Velat Kilic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_D/0/1/0/all/0/1"&gt;Deepti Hegde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sindagi_V/0/1/0/all/0/1"&gt;Vishwanath Sindagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Brinton Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_M/0/1/0/all/0/1"&gt;Mark A. Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applying the Case Difference Heuristic to Learn Adaptations from Deep Network Features. (arXiv:2107.07095v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.07095</id>
        <link href="http://arxiv.org/abs/2107.07095"/>
        <updated>2021-07-16T00:48:23.509Z</updated>
        <summary type="html"><![CDATA[The case difference heuristic (CDH) approach is a knowledge-light method for
learning case adaptation knowledge from the case base of a case-based reasoning
system. Given a pair of cases, the CDH approach attributes the difference in
their solutions to the difference in the problems they solve, and generates
adaptation rules to adjust solutions accordingly when a retrieved case and new
query have similar problem differences. As an alternative to learning
adaptation rules, several researchers have applied neural networks to learn to
predict solution differences from problem differences. Previous work on such
approaches has assumed that the feature set describing problems is predefined.
This paper investigates a two-phase process combining deep learning for feature
extraction and neural network based adaptation learning from extracted
features. Its performance is demonstrated in a regression task on an image
data: predicting age given the image of a face. Results show that the combined
process can successfully learn adaptation knowledge applicable to nonsymbolic
differences in cases. The CBR system achieves slightly lower performance
overall than a baseline deep network regressor, but better performance than the
baseline on novel queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaomeng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Ziwei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leake_D/0/1/0/all/0/1"&gt;David Leake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xizi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1"&gt;David Crandall&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COAST: COntrollable Arbitrary-Sampling NeTwork for Compressive Sensing. (arXiv:2107.07225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07225</id>
        <link href="http://arxiv.org/abs/2107.07225"/>
        <updated>2021-07-16T00:48:23.418Z</updated>
        <summary type="html"><![CDATA[Recent deep network-based compressive sensing (CS) methods have achieved
great success. However, most of them regard different sampling matrices as
different independent tasks and need to train a specific model for each target
sampling matrix. Such practices give rise to inefficiency in computing and
suffer from poor generalization ability. In this paper, we propose a novel
COntrollable Arbitrary-Sampling neTwork, dubbed COAST, to solve CS problems of
arbitrary-sampling matrices (including unseen sampling matrices) with one
single model. Under the optimization-inspired deep unfolding framework, our
COAST exhibits good interpretability. In COAST, a random projection
augmentation (RPA) strategy is proposed to promote the training diversity in
the sampling space to enable arbitrary sampling, and a controllable proximal
mapping module (CPMM) and a plug-and-play deblocking (PnP-D) strategy are
further developed to dynamically modulate the network features and effectively
eliminate the blocking artifacts, respectively. Extensive experiments on widely
used benchmark datasets demonstrate that our proposed COAST is not only able to
handle arbitrary sampling matrices with one single model but also to achieve
state-of-the-art performance with fast speed. The source code is available on
https://github.com/jianzhangcs/COAST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_D/0/1/0/all/0/1"&gt;Di You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jingfen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Siwei Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STAR: Sparse Transformer-based Action Recognition. (arXiv:2107.07089v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07089</id>
        <link href="http://arxiv.org/abs/2107.07089"/>
        <updated>2021-07-16T00:48:23.377Z</updated>
        <summary type="html"><![CDATA[The cognitive system for human action and behavior has evolved into a deep
learning regime, and especially the advent of Graph Convolution Networks has
transformed the field in recent years. However, previous works have mainly
focused on over-parameterized and complex models based on dense graph
convolution networks, resulting in low efficiency in training and inference.
Meanwhile, the Transformer architecture-based model has not yet been well
explored for cognitive application in human action and behavior estimation.
This work proposes a novel skeleton-based human action recognition model with
sparse attention on the spatial dimension and segmented linear attention on the
temporal dimension of data. Our model can also process the variable length of
video clips grouped as a single batch. Experiments show that our model can
achieve comparable performance while utilizing much less trainable parameters
and achieve high speed in training and inference. Experiments show that our
model achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline
models at competitive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Feng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chonghan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yizhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tianyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muralidhar_S/0/1/0/all/0/1"&gt;Shivran Muralidhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1"&gt;Vijaykrishnan Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Component Function in Product Assemblies with Graph Neural Networks. (arXiv:2107.07042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07042</id>
        <link href="http://arxiv.org/abs/2107.07042"/>
        <updated>2021-07-16T00:48:23.370Z</updated>
        <summary type="html"><![CDATA[Function is defined as the ensemble of tasks that enable the product to
complete the designed purpose. Functional tools, such as functional modeling,
offer decision guidance in the early phase of product design, where explicit
design decisions are yet to be made. Function-based design data is often sparse
and grounded in individual interpretation. As such, function-based design tools
can benefit from automatic function classification to increase data fidelity
and provide function representation models that enable function-based
intelligent design agents. Function-based design data is commonly stored in
manually generated design repositories. These design repositories are a
collection of expert knowledge and interpretations of function in product
design bounded by function-flow and component taxonomies. In this work, we
represent a structured taxonomy-based design repository as assembly-flow
graphs, then leverage a graph neural network (GNN) model to perform automatic
function classification. We support automated function classification by
learning from repository data to establish the ground truth of component
function assignment. Experimental results show that our GNN model achieves a
micro-average F${_1}$-score of 0.832 for tier 1 (broad), 0.756 for tier 2, and
0.783 for tier 3 (specific) functions. Given the imbalance of data features,
the results are encouraging. Our efforts in this paper can be a starting point
for more sophisticated applications in knowledge-based CAD systems and
Design-for-X consideration in function-based design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferrero_V/0/1/0/all/0/1"&gt;Vincenzo Ferrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassani_K/0/1/0/all/0/1"&gt;Kaveh Hassani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandi_D/0/1/0/all/0/1"&gt;Daniele Grandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DuPont_B/0/1/0/all/0/1"&gt;Bryony DuPont&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient and Small Convolutional Neural Network for Pest Recognition -- ExquisiteNet. (arXiv:2107.07167v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07167</id>
        <link href="http://arxiv.org/abs/2107.07167"/>
        <updated>2021-07-16T00:48:23.363Z</updated>
        <summary type="html"><![CDATA[Nowadays, due to the rapid population expansion, food shortage has become a
critical issue. In order to stabilizing the food source production, preventing
crops from being attacked by pests is very important. In generally, farmers use
pesticides to kill pests, however, improperly using pesticides will also kill
some insects which is beneficial to crops, such as bees. If the number of bees
is too few, the supplement of food in the world will be in short. Besides,
excessive pesticides will seriously pollute the environment. Accordingly,
farmers need a machine which can automatically recognize the pests. Recently,
deep learning is popular because its effectiveness in the field of image
classification. In this paper, we propose a small and efficient model called
ExquisiteNet to complete the task of recognizing the pests and we expect to
apply our model on mobile devices. ExquisiteNet mainly consists of two blocks.
One is double fusion with squeeze-and-excitation-bottleneck block (DFSEB
block), and the other is max feature expansion block (ME block). ExquisiteNet
only has 0.98M parameters and its computing speed is very fast almost the same
as SqueezeNet. In order to evaluate our model's performance, we test our model
on a benchmark pest dataset called IP102. Compared to many state-of-the-art
models, such as ResNet101, ShuffleNetV2, MobileNetV3-large and EfficientNet
etc., our model achieves higher accuracy, that is, 52.32% on the test set of
IP102 without any data augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shi-Yao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chung-Yen Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Retrieval and Localization in Large Art Collections using Deep Multi-Style Feature Fusion and Iterative Voting. (arXiv:2107.06935v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06935</id>
        <link href="http://arxiv.org/abs/2107.06935"/>
        <updated>2021-07-16T00:48:23.356Z</updated>
        <summary type="html"><![CDATA[The search for specific objects or motifs is essential to art history as both
assist in decoding the meaning of artworks. Digitization has produced large art
collections, but manual methods prove to be insufficient to analyze them. In
the following, we introduce an algorithm that allows users to search for image
regions containing specific motifs or objects and find similar regions in an
extensive dataset, helping art historians to analyze large digitized art
collections. Computer vision has presented efficient methods for visual
instance retrieval across photographs. However, applied to art collections,
they reveal severe deficiencies because of diverse motifs and massive domain
shifts induced by differences in techniques, materials, and styles. In this
paper, we present a multi-style feature fusion approach that successfully
reduces the domain gap and improves retrieval results without labelled data or
curated image collections. Our region-based voting with GPU-accelerated
approximate nearest-neighbour search allows us to find and localize even small
motifs within an extensive dataset in a few seconds. We obtain state-of-the-art
results on the Brueghel dataset and demonstrate its generalization to
inhomogeneous collections with a large number of distractors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ufer_N/0/1/0/all/0/1"&gt;Nikolai Ufer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1"&gt;Sabine Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-07-16T00:48:23.350Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diff-Net: Image Feature Difference based High-Definition Map Change Detection. (arXiv:2107.07030v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07030</id>
        <link href="http://arxiv.org/abs/2107.07030"/>
        <updated>2021-07-16T00:48:23.333Z</updated>
        <summary type="html"><![CDATA[Up-to-date High-Definition (HD) maps are essential for self-driving cars. To
achieve constantly updated HD maps, we present a deep neural network (DNN),
Diff-Net, to detect changes in them. Compared to traditional methods based on
object detectors, the essential design in our work is a parallel feature
difference calculation structure that infers map changes by comparing features
extracted from the camera and rasterized images. To generate these rasterized
images, we project map elements onto images in the camera view, yielding
meaningful map representations that can be consumed by a DNN accordingly. As we
formulate the change detection task as an object detection problem, we leverage
the anchor-based structure that predicts bounding boxes with different change
status categories. Furthermore, rather than relying on single frame input, we
introduce a spatio-temporal fusion module that fuses features from history
frames into the current, thus improving the overall performance. Finally, we
comprehensively validate our method's effectiveness using freshly collected
datasets. Results demonstrate that our Diff-Net achieves better performance
than the baseline methods and is ready to be integrated into a map production
pipeline maintaining an up-to-date HD map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengjie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaoqing Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Ning Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiyu Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Significant Features for Few-Shot Learning using Dimensionality Reduction. (arXiv:2107.06992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06992</id>
        <link href="http://arxiv.org/abs/2107.06992"/>
        <updated>2021-07-16T00:48:23.325Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a relatively new technique that specializes in problems
where we have little amounts of data. The goal of these methods is to classify
categories that have not been seen before with just a handful of samples.
Recent approaches, such as metric learning, adopt the meta-learning strategy in
which we have episodic tasks conformed by support (training) data and query
(test) data. Metric learning methods have demonstrated that simple models can
achieve good performance by learning a similarity function to compare the
support and the query data. However, the feature space learned by a given
metric learning approach may not exploit the information given by a specific
few-shot task. In this work, we explore the use of dimension reduction
techniques as a way to find task-significant features helping to make better
predictions. We measure the performance of the reduced features by assigning a
score based on the intra-class and inter-class distance, and selecting a
feature reduction method in which instances of different classes are far away
and instances of the same class are close. This module helps to improve the
accuracy performance by allowing the similarity function, given by the metric
learning method, to have more discriminative features for the classification.
Our method outperforms the metric learning baselines in the miniImageNet
dataset by around 2% in accuracy performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Ruiz_M/0/1/0/all/0/1"&gt;Mauricio Mendez-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Zapata_I/0/1/0/all/0/1"&gt;Ivan Garcia Jorge Gonzalez-Zapata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_A/0/1/0/all/0/1"&gt;Andres Mendez-Vazquez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06943</id>
        <link href="http://arxiv.org/abs/2107.06943"/>
        <updated>2021-07-16T00:48:23.318Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1"&gt;Szymon P&amp;#x142;otka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1"&gt;Tomasz W&amp;#x142;odarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1"&gt;Adam Klasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Lipa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAD-free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording. (arXiv:2107.07509v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.07509</id>
        <link href="http://arxiv.org/abs/2107.07509"/>
        <updated>2021-07-16T00:48:23.310Z</updated>
        <summary type="html"><![CDATA[In this work, we propose novel decoding algorithms to enable streaming
automatic speech recognition (ASR) on unsegmented long-form recordings without
voice activity detection (VAD), based on monotonic chunkwise attention (MoChA)
with an auxiliary connectionist temporal classification (CTC) objective. We
propose a block-synchronous beam search decoding to take advantage of efficient
batched output-synchronous and low-latency input-synchronous searches. We also
propose a VAD-free inference algorithm that leverages CTC probabilities to
determine a suitable timing to reset the model states to tackle the
vulnerability to long-form data. Experimental evaluations demonstrate that the
block-synchronous decoding achieves comparable accuracy to the
label-synchronous one. Moreover, the VAD-free inference can recognize long-form
speech robustly for up to a few hours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Instruction Generation with Transformers. (arXiv:2107.06964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06964</id>
        <link href="http://arxiv.org/abs/2107.06964"/>
        <updated>2021-07-16T00:48:23.301Z</updated>
        <summary type="html"><![CDATA[Automatic surgical instruction generation is a prerequisite towards
intra-operative context-aware surgical assistance. However, generating
instructions from surgical scenes is challenging, as it requires jointly
understanding the surgical activity of current view and modelling relationships
between visual information and textual description. Inspired by the neural
machine translation and imaging captioning tasks in open domain, we introduce a
transformer-backboned encoder-decoder network with self-critical reinforcement
learning to generate instructions from surgical images. We evaluate the
effectiveness of our method on DAISI dataset, which includes 290 procedures
from various medical disciplines. Our approach outperforms the existing
baseline over all caption evaluation metrics. The results demonstrate the
benefits of the encoder-decoder structure backboned by transformer in handling
multimodal context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinglu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Yinyu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jian Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Jun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Image Features Boost Housing Market Predictions?. (arXiv:2107.07148v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07148</id>
        <link href="http://arxiv.org/abs/2107.07148"/>
        <updated>2021-07-16T00:48:23.284Z</updated>
        <summary type="html"><![CDATA[The attractiveness of a property is one of the most interesting, yet
challenging, categories to model. Image characteristics are used to describe
certain attributes, and to examine the influence of visual factors on the price
or timeframe of the listing. In this paper, we propose a set of techniques for
the extraction of visual features for efficient numerical inclusion in
modern-day predictive algorithms. We discuss techniques such as Shannon's
entropy, calculating the center of gravity, employing image segmentation, and
using Convolutional Neural Networks. After comparing these techniques as
applied to a set of property-related images (indoor, outdoor, and satellite),
we conclude the following: (i) the entropy is the most efficient single-digit
visual measure for housing price prediction; (ii) image segmentation is the
most important visual feature for the prediction of housing lifespan; and (iii)
deep image features can be used to quantify interior characteristics and
contribute to captivation modeling. The set of 40 image features selected here
carries a significant amount of predictive power and outperforms some of the
strongest metadata predictors. Without any need to replace a human expert in a
real-estate appraisal process, we conclude that the techniques presented in
this paper can efficiently describe visible characteristics, thus introducing
perceived attractiveness as a quantitative measure into the predictive modeling
of housing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostic_Z/0/1/0/all/0/1"&gt;Zona Kostic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevremovic_A/0/1/0/all/0/1"&gt;Aleksandar Jevremovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Compact CNNs for Image Classification using Dynamic-coded Filter Fusion. (arXiv:2107.06916v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06916</id>
        <link href="http://arxiv.org/abs/2107.06916"/>
        <updated>2021-07-16T00:48:23.278Z</updated>
        <summary type="html"><![CDATA[The mainstream approach for filter pruning is usually either to force a
hard-coded importance estimation upon a computation-heavy pretrained model to
select "important" filters, or to impose a hyperparameter-sensitive sparse
constraint on the loss objective to regularize the network training. In this
paper, we present a novel filter pruning method, dubbed dynamic-coded filter
fusion (DCFF), to derive compact CNNs in a computation-economical and
regularization-free manner for efficient image classification. Each filter in
our DCFF is firstly given an inter-similarity distribution with a temperature
parameter as a filter proxy, on top of which, a fresh Kullback-Leibler
divergence based dynamic-coded criterion is proposed to evaluate the filter
importance. In contrast to simply keeping high-score filters in other methods,
we propose the concept of filter fusion, i.e., the weighted averages using the
assigned proxies, as our preserved filters. We obtain a one-hot
inter-similarity distribution as the temperature parameter approaches infinity.
Thus, the relative importance of each filter can vary along with the training
of the compact CNN, leading to dynamically changeable fused filters without
both the dependency on the pretrained model and the introduction of sparse
constraints. Extensive experiments on classification benchmarks demonstrate the
superiority of our DCFF over the compared counterparts. For example, our DCFF
derives a compact VGGNet-16 with only 72.77M FLOPs and 1.06M parameters while
reaching top-1 accuracy of 93.47% on CIFAR-10. A compact ResNet-50 is obtained
with 63.8% FLOPs and 58.6% parameter reductions, retaining 75.60% top-1
accuracy on ILSVRC-2012. Our code, narrower models and training logs are
available at https://github.com/lmbxmu/DCFF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00635</id>
        <link href="http://arxiv.org/abs/2107.00635"/>
        <updated>2021-07-16T00:48:23.271Z</updated>
        <summary type="html"><![CDATA[While attention-based encoder-decoder (AED) models have been successfully
extended to the online variants for streaming automatic speech recognition
(ASR), such as monotonic chunkwise attention (MoChA), the models still have a
large label emission latency because of the unconstrained end-to-end training
objective. Previous works tackled this problem by leveraging alignment
information to control the timing to emit tokens during training. In this work,
we propose a simple alignment-free regularization method, StableEmit, to
encourage MoChA to emit tokens earlier. StableEmit discounts the selection
probabilities in hard monotonic attention for token boundary detection by a
constant factor and regularizes them to recover the total attention mass during
training. As a result, the scale of the selection probabilities is increased,
and the values can reach a threshold for token emission earlier, leading to a
reduction of emission latency and deletion errors. Moreover, StableEmit can be
combined with methods that constraint alignments to further improve the
accuracy and latency. Experimental evaluations with LSTM and Conformer encoders
demonstrate that StableEmit significantly reduces the recognition errors and
the emission latency simultaneously. We also show that the use of alignment
information is complementary in both metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1"&gt;Hirofumi Inaguma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-07-16T00:48:23.264Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04727</id>
        <link href="http://arxiv.org/abs/2105.04727"/>
        <updated>2021-07-16T00:48:23.256Z</updated>
        <summary type="html"><![CDATA[We propose FEDENHANCE, an unsupervised federated learning (FL) approach for
speech enhancement and separation with non-IID distributed data across multiple
clients. We simulate a real-world scenario where each client only has access to
a few noisy recordings from a limited and disjoint number of speakers (hence
non-IID). Each client trains their model in isolation using mixture invariant
training while periodically providing updates to a central server. Our
experiments show that our approach achieves competitive enhancement performance
compared to IID training on a single device and that we can further facilitate
the convergence speed and the overall performance using transfer learning on
the server-side. Moreover, we show that we can effectively combine updates from
clients trained locally with supervised and unsupervised losses. We also
release a new dataset LibriFSD50K and its creation recipe in order to
facilitate FL research for source separation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1"&gt;Jonah Casebeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Passive attention in artificial neural networks predicts human visual selectivity. (arXiv:2107.07013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.07013</id>
        <link href="http://arxiv.org/abs/2107.07013"/>
        <updated>2021-07-16T00:48:23.237Z</updated>
        <summary type="html"><![CDATA[Developments in machine learning interpretability techniques over the past
decade have provided new tools to observe the image regions that are most
informative for classification and localization in artificial neural networks
(ANNs). Are the same regions similarly informative to human observers? Using
data from 78 new experiments and 6,610 participants, we show that passive
attention techniques reveal a significant overlap with human visual selectivity
estimates derived from 6 distinct behavioral tasks including visual
discrimination, spatial localization, recognizability, free-viewing,
cued-object search, and saliency search fixations. We find that input
visualizations derived from relatively simple ANN architectures probed using
guided backpropagation methods are the best predictors of a shared component in
the joint variability of the human measures. We validate these correlational
results with causal manipulations using recognition experiments. We show that
images masked with ANN attention maps were easier for humans to classify than
control masks in a speeded recognition experiment. Similarly, we find that
recognition performance in the same ANN models was likewise influenced by
masking input images using human visual selectivity maps. This work contributes
a new approach to evaluating the biological and psychological validity of
leading ANNs as models of human vision: by examining their similarities and
differences in terms of their visual selectivity to the information contained
in images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Langlois_T/0/1/0/all/0/1"&gt;Thomas A. Langlois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;H. Charles Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1"&gt;Erin Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1"&gt;Ishita Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1"&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1"&gt;Nori Jacoby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:23.230Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00368</id>
        <link href="http://arxiv.org/abs/2107.00368"/>
        <updated>2021-07-16T00:48:23.222Z</updated>
        <summary type="html"><![CDATA[Machine Reading Comprehension (MRC) is an active field in natural language
processing with many successful developed models in recent years. Despite their
high in-distribution accuracy, these models suffer from two issues: high
training cost and low out-of-distribution accuracy. Even though some approaches
have been presented to tackle the generalization problem, they have high,
intolerable training costs. In this paper, we investigate the effect of
ensemble learning approach to improve generalization of MRC systems without
retraining a big model. After separately training the base models with
different structures on different datasets, they are ensembled using weighting
and stacking approaches in probabilistic and non-probabilistic settings. Three
configurations are investigated including heterogeneous, homogeneous, and
hybrid on eight datasets and six state-of-the-art models. We identify the
important factors in the effectiveness of ensemble methods. Also, we compare
the robustness of ensemble and fine-tuned models against data distribution
shifts. The experimental results show the effectiveness and robustness of the
ensemble approach in improving the out-of-distribution accuracy of MRC systems,
especially when the base models are similar in accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1"&gt;Razieh Baradaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1"&gt;Hossein Amirkhani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07498</id>
        <link href="http://arxiv.org/abs/2107.07498"/>
        <updated>2021-07-16T00:48:23.183Z</updated>
        <summary type="html"><![CDATA[Pretrained Language Models (PLMs) have achieved tremendous success in natural
language understanding tasks. While different learning schemes -- fine-tuning,
zero-shot and few-shot learning -- have been widely explored and compared for
languages such as English, there is comparatively little work in Chinese to
fairly and comprehensively evaluate and compare these methods. This work first
introduces Chinese Few-shot Learning Evaluation Benchmark (FewCLUE), the first
comprehensive small sample evaluation benchmark in Chinese. It includes nine
tasks, ranging from single-sentence and sentence-pair classification tasks to
machine reading comprehension tasks. Given the high variance of the few-shot
learning performance, we provide multiple training/validation sets to
facilitate a more accurate and stable evaluation of few-shot modeling. An
unlabeled training set with up to 20,000 additional samples per task is
provided, allowing researchers to explore better ways of using unlabeled
samples. Next, we implement a set of state-of-the-art (SOTA) few-shot learning
methods (including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their
performance with fine-tuning and zero-shot learning schemes on the newly
constructed FewCLUE benchmark.Our results show that: 1) all five few-shot
learning methods exhibit better performance than fine-tuning or zero-shot
learning; 2) among the five methods, PET is the best performing few-shot
method; 3) few-shot learning performance is highly dependent on the specific
task. Our benchmark and code are available at
https://github.com/CLUEbenchmark/FewCLUE]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaojing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Chenyang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuanwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Huilin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Guoao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xiang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hai Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potential UAV Landing Sites Detection through Digital Elevation Models Analysis. (arXiv:2107.06921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06921</id>
        <link href="http://arxiv.org/abs/2107.06921"/>
        <updated>2021-07-16T00:48:23.157Z</updated>
        <summary type="html"><![CDATA[In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)
potential landing site detection using terrain information through
identification of flat areas, is presented. The algorithm utilizes digital
elevation models (DEM) that represent the height distribution of an area. Flat
areas which constitute appropriate landing zones for UAVs in normal or
emergency situations result by thresholding the image gradient magnitude of the
digital surface model (DSM). The proposed technique also uses connected
components evaluation on the thresholded gradient image in order to discover
connected regions of sufficient size for landing. Moreover, man-made structures
and vegetation areas are detected and excluded from the potential landing
sites. Quantitative performance evaluation of the proposed landing site
detection algorithm in a number of areas on real world and synthetic datasets,
accompanied by a comparison with a state-of-the-art algorithm, proves its
efficiency and superiority.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kakaletsis_E/0/1/0/all/0/1"&gt;Efstratios Kakaletsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1"&gt;Nikos Nikolaidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triage and diagnosis of COVID-19 from medical social media. (arXiv:2103.11850v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11850</id>
        <link href="http://arxiv.org/abs/2103.11850"/>
        <updated>2021-07-16T00:48:23.138Z</updated>
        <summary type="html"><![CDATA[Objective: This study aims to develop an end-to-end natural language
processing pipeline for triage and diagnosis of COVID-19 from patient-authored
social media posts, in order to provide researchers and other interested
parties with additional information on the symptoms, severity and prevalence of
the disease. Materials and Methods: The text processing pipeline first extracts
COVID-19 symptoms and related concepts such as severity, duration, negations,
and body parts from patients posts using conditional random fields. An
unsupervised rule-based algorithm is then applied to establish relations
between concepts in the next step of the pipeline. The extracted concepts and
relations are subsequently used to construct two different vector
representations of each post. These vectors are applied separately to build
support vector machine learning models to triage patients into three categories
and diagnose them for COVID-19. Results: We report that macro- and
micro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for
the triage and diagnosis of COVID-19, when the models are trained on human
labelled data. Our experimental results indicate that similar performance can
be achieved when the models are trained using predicted labels from concept
extraction and rule-based classifiers, thus yielding end-to-end machine
learning. Also, we highlight important features uncovered by our diagnostic
machine learning models and compare them with the most frequent symptoms
revealed in another COVID-19 dataset. In particular, we found that the most
important features are not always the most frequent ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Abul Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levene_M/0/1/0/all/0/1"&gt;Mark Levene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_D/0/1/0/all/0/1"&gt;David Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fromson_R/0/1/0/all/0/1"&gt;Renate Fromson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koslover_N/0/1/0/all/0/1"&gt;Nicolas Koslover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levene_T/0/1/0/all/0/1"&gt;Tamara Levene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compute and memory efficient universal sound source separation. (arXiv:2103.02644v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02644</id>
        <link href="http://arxiv.org/abs/2103.02644"/>
        <updated>2021-07-16T00:48:23.079Z</updated>
        <summary type="html"><![CDATA[Recent progress in audio source separation lead by deep learning has enabled
many neural network models to provide robust solutions to this fundamental
estimation problem. In this study, we provide a family of efficient neural
network architectures for general purpose audio source separation while
focusing on multiple computational aspects that hinder the application of
neural networks in real-world scenarios. The backbone structure of this
convolutional network is the SUccessive DOwnsampling and Resampling of
Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is
performed through simple one-dimensional convolutions. This mechanism enables
our models to obtain high fidelity signal separation in a wide variety of
settings where variable number of sources are present and with limited
computational resources (e.g. floating point operations, memory footprint,
number of parameters and latency). Our experiments show that SuDoRM-RF models
perform comparably and even surpass several state-of-the-art benchmarks with
significantly higher computational resource requirements. The causal variation
of SuDoRM-RF is able to obtain competitive performance in real-time speech
separation of around 10dB scale-invariant signal-to-distortion ratio
improvement (SI-SDRi) while remaining up to 20 times faster than real-time on a
laptop device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1"&gt;Efthymios Tzinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhepei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xilin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1"&gt;Paris Smaragdis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VT-SSum: A Benchmark Dataset for Video Transcript Segmentation and Summarization. (arXiv:2106.05606v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05606</id>
        <link href="http://arxiv.org/abs/2106.05606"/>
        <updated>2021-07-16T00:48:23.072Z</updated>
        <summary type="html"><![CDATA[Video transcript summarization is a fundamental task for video understanding.
Conventional approaches for transcript summarization are usually built upon the
summarization data for written language such as news articles, while the domain
discrepancy may degrade the model performance on spoken text. In this paper, we
present VT-SSum, a benchmark dataset with spoken language for video transcript
segmentation and summarization, which includes 125K transcript-summary pairs
from 9,616 videos. VT-SSum takes advantage of the videos from VideoLectures.NET
by leveraging the slides content as the weak supervision to generate the
extractive summary for video transcripts. Experiments with a state-of-the-art
deep learning approach show that the model trained with VT-SSum brings a
significant improvement on the AMI spoken text summarization benchmark. VT-SSum
is publicly available at https://github.com/Dod-o/VT-SSum to support the future
research of video transcript segmentation and summarization tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1"&gt;Tengchao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lei Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilijevic_M/0/1/0/all/0/1"&gt;Momcilo Vasilijevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Furu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. (arXiv:2107.07261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07261</id>
        <link href="http://arxiv.org/abs/2107.07261"/>
        <updated>2021-07-16T00:48:22.991Z</updated>
        <summary type="html"><![CDATA[Models pre-trained with a language modeling objective possess ample world
knowledge and language skills, but are known to struggle in tasks that require
reasoning. In this work, we propose to leverage semi-structured tables, and
automatically generate at scale question-paragraph pairs, where answering the
question requires reasoning over multiple facts in the paragraph. We add a
pre-training step over this synthetic data, which includes examples that
require 16 different reasoning skills such as number comparison, conjunction,
and fact composition. To improve data efficiency, we propose sampling
strategies that focus training on reasoning skills the model is currently
lacking. We evaluate our approach on three reading comprehension datasets that
are focused on reasoning, and show that our model, PReasM, substantially
outperforms T5, a popular pre-trained encoder-decoder model. Moreover, sampling
examples based on current model errors leads to faster training and higher
overall performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoran_O/0/1/0/all/0/1"&gt;Ori Yoran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talmor_A/0/1/0/all/0/1"&gt;Alon Talmor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07150</id>
        <link href="http://arxiv.org/abs/2107.07150"/>
        <updated>2021-07-16T00:48:22.906Z</updated>
        <summary type="html"><![CDATA[Making controlled perturbations is essential for various tasks (e.g., data
augmentation), but building task-specific generators can be expensive. We
introduce Tailor, a task-agnostic generation system that perturbs text in a
semantically-controlled way. With unlikelihood training, we design Tailor's
generator to follow a series of control codes derived from semantic roles.
Through modifications of these control codes, Tailor can produce fine-grained
perturbations. We implement a set of operations on control codes that can be
composed into complex perturbation strategies, and demonstrate their
effectiveness in three distinct applications: First, Tailor facilitates the
construction of high-quality contrast sets that are lexically diverse, and less
biased than original task test data. Second, paired with automated labeling
heuristics, Tailor helps improve model generalization through data
augmentation: We obtain an average gain of 1.73 on an NLI challenge set by
perturbing just 5% of training data. Third, without any finetuning overhead,
Tailor's perturbations effectively improve compositionality in fine-grained
style transfer, outperforming fine-tuned baselines on 6 transfers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1"&gt;Alexis Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1"&gt;Matthew E. Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEX: Unifying Evaluation for Few-Shot NLP. (arXiv:2107.07170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07170</id>
        <link href="http://arxiv.org/abs/2107.07170"/>
        <updated>2021-07-16T00:48:22.899Z</updated>
        <summary type="html"><![CDATA[Few-shot NLP research is highly active, yet conducted in disjoint research
threads with evaluation suites that lack challenging-yet-realistic testing
setups and fail to employ careful experimental design. Consequently, the
community does not know which techniques perform best or even if they
outperform simple baselines. We formulate desiderata for an ideal few-shot NLP
benchmark and present FLEX, the first benchmark, public leaderboard, and
framework that provides unified, comprehensive measurement for few-shot NLP
techniques. FLEX incorporates and introduces new best practices for few-shot
evaluation, including measurement of four transfer settings, textual labels for
zero-shot evaluation, and a principled approach to benchmark design that
optimizes statistical accuracy while keeping evaluation costs accessible to
researchers without large compute resources. In addition, we present UniFew, a
simple yet strong prompt-based model for few-shot learning which unifies the
pretraining and finetuning prompt formats, eschewing complex machinery of
recent prompt-based approaches in adapting downstream task formats to language
model pretraining objectives. We demonstrate that despite simplicity UniFew
achieves results competitive with both popular meta-learning and prompt-based
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1"&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1"&gt;Iz Beltagy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining. (arXiv:2107.07113v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07113</id>
        <link href="http://arxiv.org/abs/2107.07113"/>
        <updated>2021-07-16T00:48:22.816Z</updated>
        <summary type="html"><![CDATA[Many real-world applications involve the use of Optical Character Recognition
(OCR) engines to transform handwritten images into transcripts on which
downstream Natural Language Processing (NLP) models are applied. In this
process, OCR engines may introduce errors and inputs to downstream NLP models
become noisy. Despite that pre-trained models achieve state-of-the-art
performance in many NLP benchmarks, we prove that they are not robust to noisy
texts generated by real OCR engines. This greatly limits the application of NLP
models in real-world scenarios. In order to improve model performance on noisy
OCR transcripts, it is natural to train the NLP model on labelled noisy texts.
However, in most cases there are only labelled clean texts. Since there is no
handwritten pictures corresponding to the text, it is impossible to directly
use the recognition model to obtain noisy labelled data. Human resources can be
employed to copy texts and take pictures, but it is extremely expensive
considering the size of data for model training. Consequently, we are
interested in making NLP models intrinsically robust to OCR errors in a low
resource manner. We propose a novel robust training framework which 1) employs
simple but effective methods to directly simulate natural OCR noises from clean
texts and 2) iteratively mines the hard examples from a large number of
simulated samples for optimal performance. 3) To make our model learn
noise-invariant representations, a stability loss is employed. Experiments on
three real-world datasets show that the proposed framework boosts the
robustness of pre-trained models by a large margin. We believe that this work
can greatly promote the application of NLP models in actual scenarios, although
the algorithm we use is simple and straightforward. We make our codes and three
datasets publicly
available\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guowei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1"&gt;Weiping Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features. (arXiv:2107.06963v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06963</id>
        <link href="http://arxiv.org/abs/2107.06963"/>
        <updated>2021-07-16T00:48:22.800Z</updated>
        <summary type="html"><![CDATA[Knowledge-grounded dialogue systems are intended to convey information that
is based on evidence provided in a given source text. We discuss the challenges
of training a generative neural dialogue model for such systems that is
controlled to stay faithful to the evidence. Existing datasets contain a mix of
conversational responses that are faithful to selected evidence as well as more
subjective or chit-chat style responses. We propose different evaluation
measures to disentangle these different styles of responses by quantifying the
informativeness and objectivity. At training time, additional inputs based on
these evaluation measures are given to the dialogue model. At generation time,
these additional inputs act as stylistic controls that encourage the model to
generate responses that are faithful to the provided evidence. We also
investigate the usage of additional controls at decoding time using resampling
techniques. In addition to automatic metrics, we perform a human evaluation
study where raters judge the output of these controlled generation models to be
generally more objective and faithful to the evidence compared to baseline
dialogue systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1"&gt;Hannah Rashkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1"&gt;David Reitter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1"&gt;Gaurav Singh Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Dipanjan Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoBERT-Zero: Evolving BERT Backbone from Scratch. (arXiv:2107.07445v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07445</id>
        <link href="http://arxiv.org/abs/2107.07445"/>
        <updated>2021-07-16T00:48:22.784Z</updated>
        <summary type="html"><![CDATA[Transformer-based pre-trained language models like BERT and its variants have
recently achieved promising performance in various natural language processing
(NLP) tasks. However, the conventional paradigm constructs the backbone by
purely stacking the manually designed global self-attention layers, introducing
inductive bias and thus leading to sub-optimal. In this work, we propose an
Operation-Priority Neural Architecture Search (OP-NAS) algorithm to
automatically search for promising hybrid backbone architectures. Our
well-designed search space (i) contains primitive math operations in the
intra-layer level to explore novel attention structures, and (ii) leverages
convolution blocks to be the supplementary for attention structure in the
inter-layer level to better learn local dependency. We optimize both the search
algorithm and evaluation of candidate models to boost the efficiency of our
proposed OP-NAS. Specifically, we propose Operation-Priority (OP) evolution
strategy to facilitate model search via balancing exploration and exploitation.
Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for
fast model evaluation. Extensive experiments show that the searched
architecture (named AutoBERT-Zero) significantly outperforms BERT and its
variants of different model capacities in various downstream tasks, proving the
architecture's transfer and generalization abilities. Remarkably,
AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and
BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE
test set. Code and pre-trained models will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jiahui Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+shi_H/0/1/0/all/0/1"&gt;Han shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip L.H. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLSRIL-23: Cross Lingual Speech Representations for Indic Languages. (arXiv:2107.07402v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07402</id>
        <link href="http://arxiv.org/abs/2107.07402"/>
        <updated>2021-07-16T00:48:22.760Z</updated>
        <summary type="html"><![CDATA[We present a CLSRIL-23, a self supervised learning based audio pre-trained
model which learns cross lingual speech representations from raw audio across
23 Indic languages. It is built on top of wav2vec 2.0 which is solved by
training a contrastive task over masked latent speech representations and
jointly learns the quantization of latents shared across all languages. We
compare the language wise loss during pretraining to compare effects of
monolingual and multilingual pretraining. Performance on some downstream
fine-tuning tasks for speech recognition is also compared and our experiments
show that multilingual pretraining outperforms monolingual training, in terms
of learning speech representations which encodes phonetic similarity of
languages and also in terms of performance on down stream tasks. A decrease of
5% is observed in WER and 9.5% in CER when a multilingual pretrained model is
used for finetuning in Hindi. All the code models are also open sourced.
CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio
data to facilitate research in speech recognition for Indic languages. We hope
that new state of the art systems will be created using the self supervised
approach, especially for low resources Indic languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Anirudh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1"&gt;Harveen Singh Chadha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1"&gt;Priyanshi Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimmwal_N/0/1/0/all/0/1"&gt;Neeraj Chimmwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1"&gt;Ankur Dhuriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1"&gt;Rishabh Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1"&gt;Vivek Raghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spanish Language Models. (arXiv:2107.07253v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07253</id>
        <link href="http://arxiv.org/abs/2107.07253"/>
        <updated>2021-07-16T00:48:22.751Z</updated>
        <summary type="html"><![CDATA[This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as
well as the corresponding performance evaluations. Both models were pre-trained
using the largest Spanish corpus known to date, with a total of 570GB of clean
and deduplicated text processed for this work, compiled from the web crawlings
performed by the National Library of Spain from 2009 to 2019.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1"&gt;Asier Guti&amp;#xe9;rrez-Fandi&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1"&gt;Jordi Armengol-Estap&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1"&gt;Marc P&amp;#xe0;mies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1"&gt;Joan Llop-Palao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1"&gt;Joaqu&amp;#xed;n Silveira-Ocampo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1"&gt;Casimiro Pio Carrino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1"&gt;Aitor Gonzalez-Agirre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1"&gt;Carme Armentano-Oller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1"&gt;Carlos Rodriguez-Penagos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1"&gt;Marta Villegas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation and Classification of Evidence and Reasoning Revisions in Argumentative Writing. (arXiv:2107.06990v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06990</id>
        <link href="http://arxiv.org/abs/2107.06990"/>
        <updated>2021-07-16T00:48:22.730Z</updated>
        <summary type="html"><![CDATA[Automated writing evaluation systems can improve students' writing insofar as
students attend to the feedback provided and revise their essay drafts in ways
aligned with such feedback. Existing research on revision of argumentative
writing in such systems, however, has focused on the types of revisions
students make (e.g., surface vs. content) rather than the extent to which
revisions actually respond to the feedback provided and improve the essay. We
introduce an annotation scheme to capture the nature of sentence-level
revisions of evidence use and reasoning (the `RER' scheme) and apply it to 5th-
and 6th-grade students' argumentative essays. We show that reliable manual
annotation can be achieved and that revision annotations correlate with a
holistic assessment of essay improvement in line with the feedback provided.
Furthermore, we explore the feasibility of automatically classifying revisions
according to our scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1"&gt;Tazin Afrin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Elaine Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1"&gt;Diane Litman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumura_L/0/1/0/all/0/1"&gt;Lindsay C. Matsumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correnti_R/0/1/0/all/0/1"&gt;Richard Correnti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning based Online Dialogic Instruction Detection with Pre-trained Language Models. (arXiv:2107.07119v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07119</id>
        <link href="http://arxiv.org/abs/2107.07119"/>
        <updated>2021-07-16T00:48:22.722Z</updated>
        <summary type="html"><![CDATA[In this work, we study computational approaches to detect online dialogic
instructions, which are widely used to help students understand learning
materials, and build effective study habits. This task is rather challenging
due to the widely-varying quality and pedagogical styles of dialogic
instructions. To address these challenges, we utilize pre-trained language
models, and propose a multi-task paradigm which enhances the ability to
distinguish instances of different classes by enlarging the margin between
categories via contrastive loss. Furthermore, we design a strategy to fully
exploit the misclassified examples during the training stage. Extensive
experiments on a real-world online educational data set demonstrate that our
approach achieves superior performance compared to representative baselines. To
encourage reproducible results, we make our implementation online available at
\url{https://github.com/AIED2021/multitask-dialogic-instruction}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luckin_R/0/1/0/all/0/1"&gt;Rose Luckin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06959</id>
        <link href="http://arxiv.org/abs/2107.06959"/>
        <updated>2021-07-16T00:48:22.698Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe our end-to-end multilingual speech translation
system submitted to the IWSLT 2021 evaluation campaign on the Multilingual
Speech Translation shared task. Our system is built by leveraging transfer
learning across modalities, tasks and languages. First, we leverage
general-purpose multilingual modules pretrained with large amounts of
unlabelled and labelled data. We further enable knowledge transfer from the
text task to the speech task by training two tasks jointly. Finally, our
multilingual model is finetuned on speech translation task-specific data to
achieve the best translation results. Experimental results show our system
outperforms the reported systems, including both end-to-end and cascaded based
approaches, by a large margin.

In some translation directions, our speech translation results evaluated on
the public Multilingual TEDx test set are even comparable with the ones from a
strong text-to-text translation system, which uses the oracle speech
transcripts as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1"&gt;Holger Schwenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Naman Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wordcraft: a Human-AI Collaborative Editor for Story Writing. (arXiv:2107.07430v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07430</id>
        <link href="http://arxiv.org/abs/2107.07430"/>
        <updated>2021-07-16T00:48:22.690Z</updated>
        <summary type="html"><![CDATA[As neural language models grow in effectiveness, they are increasingly being
applied in real-world settings. However these applications tend to be limited
in the modes of interaction they support. In this extended abstract, we propose
Wordcraft, an AI-assisted editor for story writing in which a writer and a
dialog system collaborate to write a story. Our novel interface uses few-shot
learning and the natural affordances of conversation to support a variety of
interactions. Our editor provides a sandbox for writers to probe the boundaries
of transformer-based language models and paves the way for future
human-in-the-loop training pipelines and novel evaluation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1"&gt;Andy Coenen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Luke Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1"&gt;Emily Reif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1"&gt;Ann Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Security in McAdams Coefficient-Based Speaker Anonymization by Watermarking Method. (arXiv:2107.07223v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.07223</id>
        <link href="http://arxiv.org/abs/2107.07223"/>
        <updated>2021-07-16T00:48:22.664Z</updated>
        <summary type="html"><![CDATA[Speaker anonymization aims to suppress speaker individuality to protect
privacy in speech while preserving the other aspects, such as speech content.
One effective solution for anonymization is to modify the McAdams coefficient.
In this work, we propose a method to improve the security for speaker
anonymization based on the McAdams coefficient by using a speech watermarking
approach. The proposed method consists of two main processes: one for embedding
and one for detection. In embedding process, two different McAdams coefficients
represent binary bits ``0" and ``1". The watermarked speech is then obtained by
frame-by-frame bit inverse switching. Subsequently, the detection process is
carried out by a power spectrum comparison. We conducted objective evaluations
with reference to the VoicePrivacy 2020 Challenge (VP2020) and of the speech
watermarking with reference to the Information Hiding Challenge (IHC) and found
that our method could satisfy the blind detection, inaudibility, and robustness
requirements in watermarking. It also significantly improved the anonymization
performance in comparison to the secondary baseline system in VP2020.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mawalim_C/0/1/0/all/0/1"&gt;Candy Olivia Mawalim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unoki_M/0/1/0/all/0/1"&gt;Masashi Unoki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTLM: Hyper-Text Pre-Training and Prompting of Language Models. (arXiv:2107.06955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06955</id>
        <link href="http://arxiv.org/abs/2107.06955"/>
        <updated>2021-07-16T00:48:22.621Z</updated>
        <summary type="html"><![CDATA[We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1"&gt;Armen Aghajanyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1"&gt;Dmytro Okhonko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1"&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving ESL Sentence Completion Questions via Pre-trained Neural Language Models. (arXiv:2107.07122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.07122</id>
        <link href="http://arxiv.org/abs/2107.07122"/>
        <updated>2021-07-16T00:48:22.592Z</updated>
        <summary type="html"><![CDATA[Sentence completion (SC) questions present a sentence with one or more blanks
that need to be filled in, three to five possible words or phrases as options.
SC questions are widely used for students learning English as a Second Language
(ESL) and building computational approaches to automatically solve such
questions is beneficial to language learners. In this work, we propose a neural
framework to solve SC questions in English examinations by utilizing
pre-trained language models. We conduct extensive experiments on a real-world
K-12 ESL SC question dataset and the results demonstrate the superiority of our
model in terms of prediction accuracy. Furthermore, we run precision-recall
trade-off analysis to discuss the practical issues when deploying it in
real-life scenarios. To encourage reproducible results, we make our code
publicly available at \url{https://github.com/AIED2021/ESL-SentenceCompletion}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiongqiong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jiafu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1"&gt;Qiang Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Wenbiao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zitao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiBench: Multiscale Benchmarks for Multimodal Representation Learning. (arXiv:2107.07502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07502</id>
        <link href="http://arxiv.org/abs/2107.07502"/>
        <updated>2021-07-16T00:48:22.536Z</updated>
        <summary type="html"><![CDATA[Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zetian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jason Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Leslie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peter Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Michelle A. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Transmission Scheme and Coded Content Placement in Cluster-centric UAV-aided Cellular Networks. (arXiv:2101.11787v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11787</id>
        <link href="http://arxiv.org/abs/2101.11787"/>
        <updated>2021-07-16T00:48:22.525Z</updated>
        <summary type="html"><![CDATA[Recently, as a consequence of the COVID-19 pandemic, dependence on
telecommunication for remote working and telemedicine has significantly
increased. In cellular networks, incorporation of Unmanned Aerial Vehicles
(UAVs) can result in enhanced connectivity for outdoor users due to the high
probability of establishing Line of Sight (LoS) links. The UAV's limited
battery life and its signal attenuation in indoor areas, however, make it
inefficient to manage users' requests in indoor environments. Referred to as
the Cluster centric and Coded UAV-aided Femtocaching (CCUF) framework, the
network's coverage in both indoor and outdoor environments increases via a
two-phase clustering for FAPs' formation and UAVs' deployment. First objective
is to increase the content diversity. In this context, we propose a coded
content placement in a cluster-centric cellular network, which is integrated
with the Coordinated Multi-Point (CoMP) to mitigate the inter-cell interference
in edge areas. Then, we compute, experimentally, the number of coded contents
to be stored in each caching node to increase the cache-hit ratio,
Signal-to-Interference-plus-Noise Ratio (SINR), and cache diversity and
decrease the users' access delay and cache redundancy for different content
popularity profiles. Capitalizing on clustering, our second objective is to
assign the best caching node to indoor/outdoor users for managing their
requests. In this regard, we define the movement speed of ground users as the
decision metric of the transmission scheme for serving outdoor users' requests
to avoid frequent handovers between FAPs and increase the battery life of UAVs.
Simulation results illustrate that the proposed CCUF implementation increases
the cache hit-ratio, SINR, and cache diversity and decrease the users' access
delay, cache redundancy and UAVs' energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HajiAkhondi_Meybodi_Z/0/1/0/all/0/1"&gt;Zohreh HajiAkhondi-Meybodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohammadi_A/0/1/0/all/0/1"&gt;Arash Mohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abouei_J/0/1/0/all/0/1"&gt;Jamshid Abouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_M/0/1/0/all/0/1"&gt;Ming Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos N. Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Show to Tell: A Survey on Image Captioning. (arXiv:2107.06912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06912</id>
        <link href="http://arxiv.org/abs/2107.06912"/>
        <updated>2021-07-16T00:48:22.510Z</updated>
        <summary type="html"><![CDATA[Connecting Vision and Language plays an essential role in Generative
Intelligence. For this reason, in the last few years, a large research effort
has been devoted to image captioning, i.e. the task of describing images with
syntactically and semantically meaningful sentences. Starting from 2015 the
task has generally been addressed with pipelines composed of a visual encoding
step and a language model for text generation. During these years, both
components have evolved considerably through the exploitation of object
regions, attributes, and relationships and the introduction of multi-modal
connections, fully-attentive approaches, and BERT-like early-fusion strategies.
However, regardless of the impressive results obtained, research in image
captioning has not reached a conclusive answer yet. This work aims at providing
a comprehensive overview and categorization of image captioning approaches,
from visual encoding and text generation to training strategies, used datasets,
and evaluation metrics. In this respect, we quantitatively compare many
relevant state-of-the-art approaches to identify the most impactful technical
innovations in image captioning architectures and training strategies.
Moreover, many variants of the problem and its open challenges are analyzed and
discussed. The final goal of this work is to serve as a tool for understanding
the existing state-of-the-art and highlighting the future directions for an
area of research where Computer Vision and Natural Language Processing can find
an optimal synergy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1"&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1"&gt;Giuseppe Fiameni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction. (arXiv:2107.06905v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06905</id>
        <link href="http://arxiv.org/abs/2107.06905"/>
        <updated>2021-07-16T00:48:22.501Z</updated>
        <summary type="html"><![CDATA[We propose a transition-based bubble parser to perform coordination structure
identification and dependency-based syntactic analysis simultaneously. Bubble
representations were proposed in the formal linguistics literature decades ago;
they enhance dependency trees by encoding coordination boundaries and internal
relationships within coordination structures explicitly. In this paper, we
introduce a transition system and neural models for parsing these
bubble-enhanced structures. Experimental results on the English Penn Treebank
and the English GENIA corpus show that our parsers beat previous
state-of-the-art approaches on the task of coordination structure prediction,
especially for the subset of sentences with complex coordination structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tianze Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lillian Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning. (arXiv:2107.06907v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06907</id>
        <link href="http://arxiv.org/abs/2107.06907"/>
        <updated>2021-07-16T00:48:22.340Z</updated>
        <summary type="html"><![CDATA[We present our contribution to the IWPT 2021 shared task on parsing into
enhanced Universal Dependencies. Our main system component is a hybrid
tree-graph parser that integrates (a) predictions of spanning trees for the
enhanced graphs with (b) additional graph edges not present in the spanning
trees. We also adopt a finetuning strategy where we first train a
language-generic parser on the concatenation of data from all available
languages, and then, in a second step, finetune on each individual language
separately. Additionally, we develop our own complete set of pre-processing
modules relevant to the shared task, including tokenization, sentence
segmentation, and multiword token expansion, based on pre-trained XLM-R models
and our own pre-training of character-level language models. Our submission
reaches a macro-average ELAS of 89.24 on the test set. It ranks top among all
teams, with a margin of more than 2 absolute ELAS over the next best-performing
submission, and best score on 16 out of 17 languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tianze Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1"&gt;Lillian Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07268</id>
        <link href="http://arxiv.org/abs/2107.07268"/>
        <updated>2021-07-16T00:48:22.242Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jing Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Variational Auto-encoder for Content-based Micro-video Background Music Recommendation. (arXiv:2107.07268v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07268</id>
        <link href="http://arxiv.org/abs/2107.07268"/>
        <updated>2021-07-16T00:48:22.212Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jing Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiayi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending best course of treatment based on similarities of prognostic markers\thanks{All authors contributed equally. (arXiv:2107.07500v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07500</id>
        <link href="http://arxiv.org/abs/2107.07500"/>
        <updated>2021-07-16T00:48:22.196Z</updated>
        <summary type="html"><![CDATA[With the advancement in the technology sector spanning over every field, a
huge influx of information is inevitable. Among all the opportunities that the
advancements in the technology have brought, one of them is to propose
efficient solutions for data retrieval. This means that from an enormous pile
of data, the retrieval methods should allow the users to fetch the relevant and
recent data over time. In the field of entertainment and e-commerce,
recommender systems have been functioning to provide the aforementioned.
Employing the same systems in the medical domain could definitely prove to be
useful in variety of ways. Following this context, the goal of this paper is to
propose collaborative filtering based recommender system in the healthcare
sector to recommend remedies based on the symptoms experienced by the patients.
Furthermore, a new dataset is developed consisting of remedies concerning
various diseases to address the limited availability of the data. The proposed
recommender system accepts the prognostic markers of a patient as the input and
generates the best remedy course. With several experimental trials, the
proposed model achieved promising results in recommending the possible remedy
for given prognostic markers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhanshu/0/1/0/all/0/1"&gt;Sudhanshu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-detecting groups based on textual similarity for group recommendations. (arXiv:2107.07284v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07284</id>
        <link href="http://arxiv.org/abs/2107.07284"/>
        <updated>2021-07-16T00:48:22.160Z</updated>
        <summary type="html"><![CDATA[In general, recommender systems are designed to provide personalized items to
a user. But in few cases, items are recommended for a group, and the challenge
is to aggregate the individual user preferences to infer the recommendation to
a group. It is also important to consider the similarity of characteristics
among the members of a group to generate a better recommendation. Members of an
automatically identified group will have similar characteristics, and reaching
a consensus with a decision-making process is preferable in this case. It
requires users-items and their rating interactions over a utility matrix to
auto-detect the groups in group recommendations. We may not overlook other
intrinsic information to form a group. The textual information also plays a
pivotal role in user clustering. In this paper, we auto-detect the groups based
on the textual similarity of the metadata (review texts). We consider the order
in user preferences in our models. We have conducted extensive experiments over
two real-world datasets to check the efficacy of the proposed models. We have
also conducted a competitive comparison with a baseline model to show the
improvements in the quality of recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1"&gt;Chintoo Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdary_C/0/1/0/all/0/1"&gt;C. Ravindranath Chowdary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next-item Recommendations in Short Sessions. (arXiv:2107.07453v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07453</id>
        <link href="http://arxiv.org/abs/2107.07453"/>
        <updated>2021-07-16T00:48:22.148Z</updated>
        <summary type="html"><![CDATA[The changing preferences of users towards items trigger the emergence of
session-based recommender systems (SBRSs), which aim to model the dynamic
preferences of users for next-item recommendations. However, most of the
existing studies on SBRSs are based on long sessions only for recommendations,
ignoring short sessions, though short sessions, in fact, account for a large
proportion in most of the real-world datasets. As a result, the applicability
of existing SBRSs solutions is greatly reduced. In a short session, quite
limited contextual information is available, making the next-item
recommendation very challenging. To this end, in this paper, inspired by the
success of few-shot learning (FSL) in effectively learning a model with limited
instances, we formulate the next-item recommendation as an FSL problem.
Accordingly, following the basic idea of a representative approach for FSL,
i.e., meta-learning, we devise an effective SBRS called INter-SEssion
collaborative Recommender netTwork (INSERT) for next-item recommendations in
short sessions. With the carefully devised local module and global module,
INSERT is able to learn an optimal preference representation of the current
user in a given short session. In particular, in the global module, a similar
session retrieval network (SSRN) is designed to find out the sessions similar
to the current short session from the historical sessions of both the current
user and other users, respectively. The obtained similar sessions are then
utilized to complement and optimize the preference representation learned from
the current short session by the local module for more accurate next-item
recommendations in this short session. Extensive experiments conducted on two
real-world datasets demonstrate the superiority of our proposed INSERT over the
state-of-the-art SBRSs when making next-item recommendations in short sessions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wenzhuo Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shoujin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengsheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuSaver: Neural Adaptive Power Consumption Optimization for Mobile Video Streaming. (arXiv:2107.07127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.07127</id>
        <link href="http://arxiv.org/abs/2107.07127"/>
        <updated>2021-07-16T00:48:22.137Z</updated>
        <summary type="html"><![CDATA[Video streaming services strive to support high-quality videos at higher
resolutions and frame rates to improve the quality of experience (QoE).
However, high-quality videos consume considerable amounts of energy on mobile
devices. This paper proposes NeuSaver, which reduces the power consumption of
mobile devices when streaming videos by applying an adaptive frame rate to each
video chunk without compromising user experience. NeuSaver generates an optimal
policy that determines the appropriate frame rate for each video chunk using
reinforcement learning (RL). The RL model automatically learns the policy that
maximizes the QoE goals based on previous observations. NeuSaver also uses an
asynchronous advantage actor-critic algorithm to reinforce the RL model quickly
and robustly. Streaming servers that support NeuSaver preprocesses videos into
segments with various frame rates, which is similar to the process of creating
videos with multiple bit rates in dynamic adaptive streaming over HTTP.
NeuSaver utilizes the commonly used H.264 video codec. We evaluated NeuSaver in
various experiments and a user study through four video categories along with
the state-of-the-art model. Our experiments showed that NeuSaver effectively
reduces the power consumption of mobile devices when streaming video by an
average of 16.14% and up to 23.12% while achieving high QoE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Myungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_L/0/1/0/all/0/1"&gt;Laihyuk Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search. (arXiv:2107.07173v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07173</id>
        <link href="http://arxiv.org/abs/2107.07173"/>
        <updated>2021-07-16T00:48:22.123Z</updated>
        <summary type="html"><![CDATA[Sequential recommender systems (SRS) have become a research hotspot due to
its power in modeling user dynamic interests and sequential behavioral
patterns. To maximize model expressive ability, a default choice is to apply a
larger and deeper network architecture, which, however, often brings high
network latency when generating online recommendations. Naturally, we argue
that compressing the heavy recommendation models into middle- or light- weight
neural networks is of great importance for practical production systems. To
realize such a goal, we propose AdaRec, a knowledge distillation (KD) framework
which compresses knowledge of a teacher model into a student model adaptively
according to its recommendation scene by using differentiable Neural
Architecture Search (NAS). Specifically, we introduce a target-oriented
distillation loss to guide the structure search process for finding the student
network architecture, and a cost-sensitive loss as constraints for model size,
which achieves a superior trade-off between recommendation effectiveness and
efficiency. In addition, we leverage Earth Mover's Distance (EMD) to realize
many-to-many layer mapping during knowledge distillation, which enables each
intermediate student layer to learn from other intermediate teacher layers
adaptively. Extensive experiments on real-world recommendation datasets
demonstrate that our model achieves competitive or better accuracy with notable
inference speedup comparing to strong counterparts, while discovering diverse
neural architectures for sequential recommender models under different
recommendation scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1"&gt;Fajie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketching sounds: an exploratory study on sound-shape associations. (arXiv:2107.07360v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.07360</id>
        <link href="http://arxiv.org/abs/2107.07360"/>
        <updated>2021-07-16T00:48:22.090Z</updated>
        <summary type="html"><![CDATA[Sound synthesiser controls typically correspond to technical parameters of
signal processing algorithms rather than intuitive sound descriptors that
relate to human perception of sound. This makes it difficult to realise sound
ideas in a straightforward way. Cross-modal mappings, for example between
gestures and sound, have been suggested as a more intuitive control mechanism.
A large body of research shows consistency in human associations between sounds
and shapes. However, the use of drawings to drive sound synthesis has not been
explored to its full extent. This paper presents an exploratory study that
asked participants to sketch visual imagery of sounds with a monochromatic
digital drawing interface, with the aim to identify different representational
approaches and determine whether timbral sound characteristics can be
communicated reliably through visual sketches. Results imply that the
development of a synthesiser exploiting sound-shape associations is feasible,
but a larger and more focused dataset is needed in followup studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobbers_S/0/1/0/all/0/1"&gt;Sebastian L&amp;#xf6;bbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04170</id>
        <link href="http://arxiv.org/abs/2105.04170"/>
        <updated>2021-07-16T00:48:22.054Z</updated>
        <summary type="html"><![CDATA[Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hande Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guli Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Keping Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning for Recommendations at Grubhub. (arXiv:2107.07106v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.07106</id>
        <link href="http://arxiv.org/abs/2107.07106"/>
        <updated>2021-07-16T00:48:21.994Z</updated>
        <summary type="html"><![CDATA[We propose a method to easily modify existing offline Recommender Systems to
run online using Transfer Learning. Online Learning for Recommender Systems has
two main advantages: quality and scale. Like many Machine Learning algorithms
in production if not regularly retrained will suffer from Concept Drift. A
policy that is updated frequently online can adapt to drift faster than a batch
system. This is especially true for user-interaction systems like recommenders
where the underlying distribution can shift drastically to follow user
behaviour. As a platform grows rapidly like Grubhub, the cost of running batch
training jobs becomes material. A shift from stateless batch learning offline
to stateful incremental learning online can recover, for example, at Grubhub,
up to a 45x cost savings and a +20% metrics increase. There are a few
challenges to overcome with the transition to online stateful learning, namely
convergence, non-stationary embeddings and off-policy evaluation, which we
explore from our experiences running this system in production.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Egg_A/0/1/0/all/0/1"&gt;Alex Egg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02266</id>
        <link href="http://arxiv.org/abs/2107.02266"/>
        <updated>2021-07-15T01:59:05.080Z</updated>
        <summary type="html"><![CDATA[When data is collected in an adaptive manner, even simple methods like
ordinary least squares can exhibit non-normal asymptotic behavior. As an
undesirable consequence, hypothesis tests and confidence intervals based on
asymptotic normality can lead to erroneous results. We propose an online
debiasing estimator to correct these distributional anomalies in least squares
estimation. Our proposed method takes advantage of the covariance structure
present in the dataset and provides sharper estimates in directions for which
more information has accrued. We establish an asymptotic normality property for
our proposed online debiasing estimator under mild conditions on the data
collection process, and provide asymptotically exact confidence intervals. We
additionally prove a minimax lower bound for the adaptive linear regression
problem, thereby providing a baseline by which to compare estimators. There are
various conditions under which our proposed estimator achieves the minimax
lower bound up to logarithmic factors. We demonstrate the usefulness of our
theory via applications to multi-armed bandit, autoregressive time series
estimation, and active learning with exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1"&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deshpande_Y/0/1/0/all/0/1"&gt;Yash Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wainwright_M/0/1/0/all/0/1"&gt;Martin J. Wainwright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13370</id>
        <link href="http://arxiv.org/abs/2009.13370"/>
        <updated>2021-07-15T01:59:04.915Z</updated>
        <summary type="html"><![CDATA[This paper estimates free energy, average mutual information, and minimum
mean square error (MMSE) of a linear model under two assumptions: (1) the
source is generated by a Markov chain, (2) the source is generated via a hidden
Markov model. Our estimates are based on the replica method in statistical
physics. We show that under the posterior mean estimator, the linear model with
Markov sources or hidden Markov sources is decoupled into single-input AWGN
channels with state information available at both encoder and decoder where the
state distribution follows the left Perron-Frobenius eigenvector with unit
Manhattan norm of the stochastic matrix of Markov chains. Numerical results
show that the free energies and MSEs obtained via the replica method closely
approximate to their counterparts achieved by the Metropolis-Hastings algorithm
or some well-known approximate message passing algorithms in the research
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-07-15T01:59:04.856Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08583</id>
        <link href="http://arxiv.org/abs/2105.08583"/>
        <updated>2021-07-15T01:59:04.842Z</updated>
        <summary type="html"><![CDATA[This paper proposes a machine learning method based on the Extra Trees (ET)
algorithm for forecasting Significant Wave Heights in oceanic waters. To derive
multiple features from the CDIP buoys, which make point measurements, we first
nowcast various parameters and then forecast them at 30-min intervals. The
proposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root
Mean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for
one day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for
14-day ahead prediction on the testing dataset. While other state-of-the-art
methods can only forecast up to 120 hours ahead, we extend it further to 14
days. Our proposed setup includes spectral features, hv-block cross-validation,
and stringent QC criteria. The proposed algorithm performs significantly better
than the state-of-the-art methods commonly used for significant wave height
forecasting for one-day ahead prediction. Moreover, the improved performance of
the proposed machine learning method compared to the numerical methods shows
that this performance can be extended to even longer periods allowing for early
prediction of significant wave heights in oceanic waters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving the Kolmogorov PDE by means of deep learning. (arXiv:1806.00421v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.00421</id>
        <link href="http://arxiv.org/abs/1806.00421"/>
        <updated>2021-07-15T01:59:04.827Z</updated>
        <summary type="html"><![CDATA[Stochastic differential equations (SDEs) and the Kolmogorov partial
differential equations (PDEs) associated to them have been widely used in
models from engineering, finance, and the natural sciences. In particular, SDEs
and Kolmogorov PDEs, respectively, are highly employed in models for the
approximative pricing of financial derivatives. Kolmogorov PDEs and SDEs,
respectively, can typically not be solved explicitly and it has been and still
is an active topic of research to design and analyze numerical methods which
are able to approximately solve Kolmogorov PDEs and SDEs, respectively. Nearly
all approximation methods for Kolmogorov PDEs in the literature suffer under
the curse of dimensionality or only provide approximations of the solution of
the PDE at a single fixed space-time point. In this paper we derive and propose
a numerical approximation method which aims to overcome both of the above
mentioned drawbacks and intends to deliver a numerical approximation of the
Kolmogorov PDE on an entire region $[a,b]^d$ without suffering from the curse
of dimensionality. Numerical results on examples including the heat equation,
the Black-Scholes model, the stochastic Lorenz equation, and the Heston model
suggest that the proposed approximation algorithm is quite effective in high
dimensions in terms of both accuracy and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1"&gt;Christian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1"&gt;Sebastian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Grohs_P/0/1/0/all/0/1"&gt;Philipp Grohs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jaafari_N/0/1/0/all/0/1"&gt;Nor Jaafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational graph convolutional networks for predicting blood-brain barrier penetration of drug molecules. (arXiv:2107.06773v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.06773</id>
        <link href="http://arxiv.org/abs/2107.06773"/>
        <updated>2021-07-15T01:59:04.821Z</updated>
        <summary type="html"><![CDATA[The evaluation of the BBB penetrating ability of drug molecules is a critical
step in brain drug development. Computational prediction based on machine
learning has proved to be an efficient way to conduct the evaluation. However,
performance of the established models has been limited by their incapability of
dealing with the interactions between drugs and proteins, which play an
important role in the mechanism behind BBB penetrating behaviors. To address
this issue, we employed the relational graph convolutional network (RGCN) to
handle the drug-protein (denoted by the encoding gene) relations as well as the
features of each individual drug. In addition, drug-drug similarity was also
introduced to connect structurally similar drugs in the graph. The RGCN model
was initially trained without input of any drug features. And the performance
was already promising, demonstrating the significant role of the
drug-protein/drug-drug relations in the prediction of BBB permeability.
Moreover, molecular embeddings from a pre-trained knowledge graph were used as
the drug features to further enhance the predictive ability of the model.
Finally, the best performing RGCN model was built with a large number of
unlabeled drugs integrated into the graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yejin Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization. (arXiv:2107.01253v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01253</id>
        <link href="http://arxiv.org/abs/2107.01253"/>
        <updated>2021-07-15T01:59:04.805Z</updated>
        <summary type="html"><![CDATA[The pipeline optimization problem in machine learning requires simultaneous
optimization of pipeline structures and parameter adaptation of their elements.
Having an elegant way to express these structures can help lessen the
complexity in the management and analysis of their performances together with
the different choices of optimization strategies. With these issues in mind, we
created the AutoMLPipeline (AMLP) toolkit which facilitates the creation and
evaluation of complex machine learning pipeline structures using simple
expressions. We use AMLP to find optimal pipeline signatures, datamine them,
and use these datamined features to speed-up learning and prediction. We
formulated a two-stage pipeline optimization with surrogate modeling in AMLP
which outperforms other AutoML approaches with a 4-hour time budget in less
than 5 minutes of AMLP computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palmes_P/0/1/0/all/0/1"&gt;Paulito P. Palmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kishimoto_A/0/1/0/all/0/1"&gt;Akihiro Kishimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marinescu_R/0/1/0/all/0/1"&gt;Radu Marinescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1"&gt;Parikshit Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daly_E/0/1/0/all/0/1"&gt;Elizabeth Daly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06755</id>
        <link href="http://arxiv.org/abs/2107.06755"/>
        <updated>2021-07-15T01:59:04.797Z</updated>
        <summary type="html"><![CDATA[The research internship at UiT - The Arctic University of Norway was offered
for our team being the winner of the 'Smart Roads - Winter Road Maintenance
2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May
2021 with meetings happening twice each week. In spite of having different
nationalities and educational backgrounds, we both interns tried to collaborate
as a team as much as possible. The most alluring part was working on this
project made us realize the critical conditions faced by the arctic people,
where it was hard to gain such a unique experience from our residence. We
developed and implemented several deep learning models to classify the states
(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the
weather forecast app will predict the state taking the Ta, Tsurf, Height,
Speed, Water, etc. into consideration. The crucial part was to define a safety
metric which is the product of the accident rates based on friction and the
accident rates based on states. We developed a regressor that will predict the
safety metric depending upon the state obtained from the classifier and the
friction obtained from the sensor data. A pathfinding algorithm has been
designed using the sensor data, open street map data, weather data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jahin_M/0/1/0/all/0/1"&gt;Md. Abrar Jahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krutsylo_A/0/1/0/all/0/1"&gt;Andrii Krutsylo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations. (arXiv:2003.08938v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08938</id>
        <link href="http://arxiv.org/abs/2003.08938"/>
        <updated>2021-07-15T01:59:04.765Z</updated>
        <summary type="html"><![CDATA[A deep reinforcement learning (DRL) agent observes its states through
observations, which may contain natural measurement errors or adversarial
noises. Since the observations deviate from the true states, they can mislead
the agent into making suboptimal actions. Several works have shown this
vulnerability via adversarial attacks, but existing approaches on improving the
robustness of DRL under this setting have limited success and lack for
theoretical principles. We show that naively applying existing techniques on
improving robustness for classification tasks, like adversarial training, is
ineffective for many RL tasks. We propose the state-adversarial Markov decision
process (SA-MDP) to study the fundamental properties of this problem, and
develop a theoretically principled policy regularization which can be applied
to a large family of DRL algorithms, including proximal policy optimization
(PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for
both discrete and continuous action control problems. We significantly improve
the robustness of PPO, DDPG and DQN agents under a suite of strong white box
adversarial attacks, including new attacks of our own. Additionally, we find
that a robust policy noticeably improves DRL performance even without an
adversary in a number of environments. Our code is available at
https://github.com/chenhongge/StateAdvDRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hongge Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boning_D/0/1/0/all/0/1"&gt;Duane Boning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11730</id>
        <link href="http://arxiv.org/abs/2105.11730"/>
        <updated>2021-07-15T01:59:04.751Z</updated>
        <summary type="html"><![CDATA[Error-bounded lossy compression is becoming an indispensable technique for
the success of today's scientific projects with vast volumes of data produced
during the simulations or instrument data acquisitions. Not only can it
significantly reduce data size, but it also can control the compression errors
based on user-specified error bounds. Autoencoder (AE) models have been widely
used in image compression, but few AE-based compression approaches support
error-bounding features, which are highly required by scientific applications.
To address this issue, we explore using convolutional autoencoders to improve
error-bounded lossy compression for scientific data, with the following three
key contributions. (1) We provide an in-depth investigation of the
characteristics of various autoencoder models and develop an error-bounded
autoencoder-based framework in terms of the SZ model. (2) We optimize the
compression quality for main stages in our designed AE-based error-bounded
compression framework, fine-tuning the block sizes and latent sizes and also
optimizing the compression efficiency of latent vectors. (3) We evaluate our
proposed solution using five real-world scientific datasets and comparing them
with six other related works. Experiments show that our solution exhibits a
very competitive compression quality from among all the compressors in our
tests. In absolute terms, it can obtain a much better compression quality (100%
~ 800% improvement in compression ratio with the same data distortion) compared
with SZ2.1 and ZFP in cases with a high compression ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_S/0/1/0/all/0/1"&gt;Sheng Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Sian Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dingwen Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zizhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cappello_F/0/1/0/all/0/1"&gt;Franck Cappello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Physics-Informed Deep Learning Paradigm for Car-Following Models. (arXiv:2012.13376v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13376</id>
        <link href="http://arxiv.org/abs/2012.13376"/>
        <updated>2021-07-15T01:59:04.732Z</updated>
        <summary type="html"><![CDATA[Car-following behavior has been extensively studied using physics-based
models, such as the Intelligent Driver Model. These models successfully
interpret traffic phenomena observed in the real-world but may not fully
capture the complex cognitive process of driving. Deep learning models, on the
other hand, have demonstrated their power in capturing observed traffic
phenomena but require a large amount of driving data to train. This paper aims
to develop a family of neural network based car-following models that are
informed by physics-based models, which leverage the advantage of both
physics-based (being data-efficient and interpretable) and deep learning based
(being generalizable) models. We design physics-informed deep learning
car-following (PIDL-CF) architectures encoded with two popular physics-based
models - IDM and OVM, on which acceleration is predicted for four traffic
regimes: acceleration, deceleration, cruising, and emergency braking. Two types
of PIDL-CFM problems are studied, one to predict acceleration only and the
other to jointly predict acceleration and discover model parameters. We also
demonstrate the superior performance of PIDL with the Next Generation
SIMulation (NGSIM) dataset over baselines, especially when the training data is
sparse. The results demonstrate the superior performance of neural networks
informed by physics over those without. The developed PIDL-CF framework holds
the potential for system identification of driving models and for the
development of driving-based controls for automated vehicles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Z/0/1/0/all/0/1"&gt;Zhaobin Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1"&gt;Xuan Di&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1"&gt;Rongye Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06785</id>
        <link href="http://arxiv.org/abs/2107.06785"/>
        <updated>2021-07-15T01:59:04.696Z</updated>
        <summary type="html"><![CDATA[The rise of big data analytics on top of NLP increases the computational
burden for text processing at scale. The problems faced in NLP are very high
dimensional text, so it takes a high computation resource. The MapReduce allows
parallelization of large computations and can improve the efficiency of text
processing. This research aims to study the effect of big data processing on
NLP tasks based on a deep learning approach. We classify a big text of news
topics with fine-tuning BERT used pre-trained models. Five pre-trained models
with a different number of parameters were used in this study. To measure the
efficiency of this method, we compared the performance of the BERT with the
pipelines from Spark NLP. The result shows that BERT without Spark NLP gives
higher accuracy compared to BERT with Spark NLP. The accuracy average and
training time of all models using BERT is 0.9187 and 35 minutes while using
BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will
take more computation resources and need a longer time to complete the tasks.
However, the accuracy of BERT with Spark NLP only decreased by an average of
5.7%, while the training time was reduced significantly by 62.9% compared to
BERT without Spark NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Optimization of Deep CNN for Image Denoising Using LSTM. (arXiv:2107.06845v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06845</id>
        <link href="http://arxiv.org/abs/2107.06845"/>
        <updated>2021-07-15T01:59:04.688Z</updated>
        <summary type="html"><![CDATA[The recent application of deep learning (DL) to various tasks has seen the
performance of classical techniques surpassed by their DL-based counterparts.
As a result, DL has equally seen application in the removal of noise from
images. In particular, the use of deep feed-forward convolutional neural
networks (DnCNNs) has been investigated for denoising. It utilizes advances in
DL techniques such as deep architecture, residual learning, and batch
normalization to achieve better denoising performance when compared with the
other classical state-of-the-art denoising algorithms. However, its deep
architecture resulted in a huge set of trainable parameters. Meta-optimization
is a training approach of enabling algorithms to learn to train themselves by
themselves. Training algorithms using meta-optimizers have been shown to enable
algorithms to achieve better performance when compared to the classical
gradient descent-based training approach. In this work, we investigate the
application of the meta-optimization training approach to the DnCNN denoising
algorithm to enhance its denoising capability. Our preliminary experiments on
simpler algorithms reveal the prospects of utilizing the meta-optimization
training approach towards the enhancement of the DnCNN denoising capability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alfarraj_M/0/1/0/all/0/1"&gt;Motaz Alfarraj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding. (arXiv:2010.14227v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14227</id>
        <link href="http://arxiv.org/abs/2010.14227"/>
        <updated>2021-07-15T01:59:04.682Z</updated>
        <summary type="html"><![CDATA[Negative sampling, which samples negative triplets from non-observed ones in
knowledge graph (KG), is an essential step in KG embedding. Recently,
generative adversarial network (GAN), has been introduced in negative sampling.
By sampling negative triplets with large gradients, these methods avoid the
problem of vanishing gradient and thus obtain better performance. However, they
make the original model more complex and harder to train. In this paper,
motivated by the observation that negative triplets with large gradients are
important but rare, we propose to directly keep track of them with the cache.
In this way, our method acts as a "distilled" version of previous GAN-based
methods, which does not waste training time on additional parameters to fit the
full distribution of negative triplets. However, how to sample from and update
the cache are two critical questions. We propose to solve these issues by
automated machine learning techniques. The automated version also covers
GAN-based methods as special cases. Theoretical explanation of NSCaching is
also provided, justifying the superior over fixed sampling scheme. Besides, we
further extend NSCaching with skip-gram model for graph embedding. Finally,
extensive experiments show that our method can gain significant improvements on
various KG embedding models and the skip-gram model, and outperforms the
state-of-the-art negative sampling methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do You Want Your Greedy: Simultaneous or Repeated?. (arXiv:2009.13998v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13998</id>
        <link href="http://arxiv.org/abs/2009.13998"/>
        <updated>2021-07-15T01:59:04.656Z</updated>
        <summary type="html"><![CDATA[We present SimultaneousGreedys, a deterministic algorithm for constrained
submodular maximization. At a high level, the algorithm maintains $\ell$
solutions and greedily updates them in a simultaneous fashion.
SimultaneousGreedys achieves the tightest known approximation guarantees for
both $k$-extendible systems and the more general $k$-systems, which are
$(k+1)^2/k = k + \mathcal{O}(1)$ and $(1 + \sqrt{k+2})^2 = k +
\mathcal{O}(\sqrt{k})$, respectively. This is in contrast to previous
algorithms, which are designed to provide tight approximation guarantees in one
setting, but not both. We also improve the analysis of RepeatedGreedy, showing
that it achieves an approximation ratio of $k + \mathcal{O}(\sqrt{k})$ for
$k$-systems when allowed to run for $\mathcal{O}(\sqrt{k})$ iterations, an
improvement in both the runtime and approximation over previous analyses. We
demonstrate that both algorithms may be modified to run in nearly linear time
with an arbitrarily small loss in the approximation.

Both SimultaneousGreedys and RepeatedGreedy are flexible enough to
incorporate the intersection of $m$ additional knapsack constraints, while
retaining similar approximation guarantees: both algorithms yield an
approximation guarantee of roughly $k + 2m + \mathcal{O}(\sqrt{k+m})$ for
$k$-systems and SimultaneousGreedys enjoys an improved approximation guarantee
of $k+2m + \mathcal{O}(\sqrt{m})$ for $k$-extendible systems. To complement our
algorithmic contributions, we provide a hardness result which states that no
algorithm making polynomially many oracle queries can achieve an approximation
better than $k + 1/2 + \varepsilon$. We also present SubmodularGreedy.jl, a
Julia package which implements these algorithms and may be downloaded at
https://github.com/crharshaw/SubmodularGreedy.jl . Finally, we test the
effectiveness of these algorithms on real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_M/0/1/0/all/0/1"&gt;Moran Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harshaw_C/0/1/0/all/0/1"&gt;Christopher Harshaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04019</id>
        <link href="http://arxiv.org/abs/2105.04019"/>
        <updated>2021-07-15T01:59:04.650Z</updated>
        <summary type="html"><![CDATA[Sorting and ranking supervision is a method for training neural networks
end-to-end based on ordering constraints. That is, the ground truth order of
sets of samples is known, while their absolute values remain unsupervised. For
that, we propose differentiable sorting networks by relaxing their pairwise
conditional swap operations. To address the problems of vanishing gradients and
extensive blurring that arise with larger numbers of layers, we propose mapping
activations to regions with moderate gradients. We consider odd-even as well as
bitonic sorting networks, which outperform existing relaxations of the sorting
operation. We show that bitonic sorting networks can achieve stable training on
large input sets of up to 1024 elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1"&gt;Felix Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1"&gt;Christian Borgelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1"&gt;Oliver Deussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03158</id>
        <link href="http://arxiv.org/abs/2103.03158"/>
        <updated>2021-07-15T01:59:04.644Z</updated>
        <summary type="html"><![CDATA[Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis. Inference in the SCM generates counterfactual images that
show what an MR image of the brain would look like if demographic or disease
covariates are changed. These images can be used for modeling disease
progression or used for image processing tasks where controlling for
confounders is necessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1"&gt;Jacob C. Reinhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1"&gt;Aaron Carass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Core-set Sampling for Efficient Neural Architecture Search. (arXiv:2107.06869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06869</id>
        <link href="http://arxiv.org/abs/2107.06869"/>
        <updated>2021-07-15T01:59:04.638Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS), an important branch of automatic machine
learning, has become an effective approach to automate the design of deep
learning models. However, the major issue in NAS is how to reduce the large
search time imposed by the heavy computational burden. While most recent
approaches focus on pruning redundant sets or developing new search
methodologies, this paper attempts to formulate the problem based on the data
curation manner. Our key strategy is to search the architecture using
summarized data distribution, i.e., core-set. Typically, many NAS algorithms
separate searching and training stages, and the proposed core-set methodology
is only used in search stage, thus their performance degradation can be
minimized. In our experiments, we were able to save overall computational time
from 30.8 hours to 3.5 hours, 8.8x reduction, on a single RTX 3090 GPU without
sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shim_J/0/1/0/all/0/1"&gt;Jae-hun Shim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kyeongbo Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;Suk-Ju Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interaction-Grounded Learning. (arXiv:2106.04887v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04887</id>
        <link href="http://arxiv.org/abs/2106.04887"/>
        <updated>2021-07-15T01:59:04.623Z</updated>
        <summary type="html"><![CDATA[Consider a prosthetic arm, learning to adapt to its user's control signals.
We propose Interaction-Grounded Learning for this novel setting, in which a
learner's goal is to interact with the environment with no grounding or
explicit reward to optimize its policies. Such a problem evades common RL
solutions which require an explicit reward. The learning agent observes a
multidimensional context vector, takes an action, and then observes a
multidimensional feedback vector. This multidimensional feedback vector has no
explicit reward information. In order to succeed, the algorithm must learn how
to evaluate the feedback vector to discover a latent reward signal, with which
it can ground its policies without supervision. We show that in an
Interaction-Grounded Learning setting, with certain natural assumptions, a
learner can discover the latent reward and ground its policy for successful
interaction. We provide theoretical guarantees and a proof-of-concept empirical
evaluation to demonstrate the effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tengyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1"&gt;Ida Momennejad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel approach for modelling and classifying sit-to-stand kinematics using inertial sensors. (arXiv:2107.06859v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06859</id>
        <link href="http://arxiv.org/abs/2107.06859"/>
        <updated>2021-07-15T01:59:04.617Z</updated>
        <summary type="html"><![CDATA[Sit-to-stand transitions are an important part of activities of daily living
and play a key role in functional mobility in humans. The sit-to-stand movement
is often affected in older adults due to frailty and in patients with motor
impairments such as Parkinson's disease leading to falls. Studying kinematics
of sit-to-stand transitions can provide insight in assessment, monitoring and
developing rehabilitation strategies for the affected populations. We propose a
three-segment body model for estimating sit-to-stand kinematics using only two
wearable inertial sensors, placed on the shank and back. Reducing the number of
sensors to two instead of one per body segment facilitates monitoring and
classifying movements over extended periods, making it more comfortable to wear
while reducing the power requirements of sensors. We applied this model on 10
younger healthy adults (YH), 12 older healthy adults (OH) and 12 people with
Parkinson's disease (PwP). We have achieved this by incorporating unique
sit-to-stand classification technique using unsupervised learning in the model
based reconstruction of angular kinematics using extended Kalman filter. Our
proposed model showed that it was possible to successfully estimate thigh
kinematics despite not measuring the thigh motion with inertial sensor. We
classified sit-to-stand transitions, sitting and standing states with the
accuracies of 98.67%, 94.20% and 91.41% for YH, OH and PwP respectively. We
have proposed a novel integrated approach of modelling and classification for
estimating the body kinematics during sit-to-stand motion and successfully
applied it on YH, OH and PwP groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wairagkar_M/0/1/0/all/0/1"&gt;Maitreyee Wairagkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villeneuve_E/0/1/0/all/0/1"&gt;Emma Villeneuve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1"&gt;Rachel King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janko_B/0/1/0/all/0/1"&gt;Balazs Janko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnett_M/0/1/0/all/0/1"&gt;Malcolm Burnett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashburn_A/0/1/0/all/0/1"&gt;Ann Ashburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_V/0/1/0/all/0/1"&gt;Veena Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sherratt_R/0/1/0/all/0/1"&gt;R. Simon Sherratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holderbaum_W/0/1/0/all/0/1"&gt;William Holderbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwin_W/0/1/0/all/0/1"&gt;William Harwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Model and Data Driven Algorithm for Online Learning of Any-to-Any Path Loss Maps. (arXiv:2107.06677v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06677</id>
        <link href="http://arxiv.org/abs/2107.06677"/>
        <updated>2021-07-15T01:59:04.609Z</updated>
        <summary type="html"><![CDATA[Learning any-to-any (A2A) path loss maps, where the objective is the
reconstruction of path loss between any two given points in a map, might be a
key enabler for many applications that rely on device-to-device (D2D)
communication. Such applications include machine-type communications (MTC) or
vehicle-to-vehicle (V2V) communications. Current approaches for learning A2A
maps are either model-based methods, or pure data-driven methods. Model-based
methods have the advantage that they can generate reliable estimations with low
computational complexity, but they cannot exploit information coming from data.
Pure data-driven methods can achieve good performance without assuming any
physical model, but their complexity and their lack of robustness is not
acceptable for many applications. In this paper, we propose a novel hybrid
model and data-driven approach that fuses information obtained from datasets
and models in an online fashion. To that end, we leverage the framework of
stochastic learning to deal with the sequential arrival of samples and propose
an online algorithm that alternatively and sequentially minimizes the original
non-convex problem. A proof of convergence is presented, along with experiments
based firstly on synthetic data, and secondly on a more realistic dataset for
V2X, with both experiments showing promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1"&gt;M. A. Gutierrez-Estevez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kasparick_M/0/1/0/all/0/1"&gt;Martin Kasparick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cavalvante_R/0/1/0/all/0/1"&gt;Renato L. G. Cavalvante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stanczak_S/0/1/0/all/0/1"&gt;S&amp;#x142;awomir Sta&amp;#x144;czak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00120</id>
        <link href="http://arxiv.org/abs/2104.00120"/>
        <updated>2021-07-15T01:59:04.603Z</updated>
        <summary type="html"><![CDATA[Stream fusion, also known as system combination, is a common technique in
automatic speech recognition for traditional hybrid hidden Markov model
approaches, yet mostly unexplored for modern deep neural network end-to-end
model architectures. Here, we investigate various fusion techniques for the
all-attention-based encoder-decoder architecture known as the transformer,
striving to achieve optimal fusion by investigating different fusion levels in
an example single-microphone setting with fusion of standard magnitude and
phase features. We introduce a novel multi-encoder learning method that
performs a weighted combination of two encoder-decoder multi-head attention
outputs only during training. Employing then only the magnitude feature encoder
in inference, we are able to show consistent improvement on Wall Street Journal
(WSJ) with language model and on Librispeech, without increase in runtime or
parameters. Combining two such multi-encoder trained models by a simple late
fusion in inference, we achieve state-of-the-art performance for
transformer-based models on WSJ with a significant WER reduction of 19%
relative compared to the current benchmark approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1"&gt;Timo Lohrenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1"&gt;Tim Fingscheidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Supervised Contrastive Learning. (arXiv:2102.08817v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08817</id>
        <link href="http://arxiv.org/abs/2102.08817"/>
        <updated>2021-07-15T01:59:04.598Z</updated>
        <summary type="html"><![CDATA[Minimizing cross-entropy over the softmax scores of a linear map composed
with a high-capacity encoder is arguably the most popular choice for training
neural networks on supervised learning tasks. However, recent works show that
one can directly optimize the encoder instead, to obtain equally (or even more)
discriminative representations via a supervised variant of a contrastive
objective. In this work, we address the question whether there are fundamental
differences in the sought-for representation geometry in the output space of
the encoder at minimal loss. Specifically, we prove, under mild assumptions,
that both losses attain their minimum once the representations of each class
collapse to the vertices of a regular simplex, inscribed in a hypersphere. We
provide empirical evidence that this configuration is attained in practice and
that reaching a close-to-optimal state typically indicates good generalization
performance. Yet, the two losses show remarkably different optimization
behavior. The number of iterations required to perfectly fit to data scales
superlinearly with the amount of randomly flipped labels for the supervised
contrastive loss. This is in contrast to the approximately linear scaling
previously reported for networks trained with cross-entropy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1"&gt;Florian Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1"&gt;Christoph D. Hofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels. (arXiv:2102.05291v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05291</id>
        <link href="http://arxiv.org/abs/2102.05291"/>
        <updated>2021-07-15T01:59:04.592Z</updated>
        <summary type="html"><![CDATA[The label noise transition matrix, characterizing the probabilities of a
training instance being wrongly annotated, is crucial to designing popular
solutions to learning with noisy labels. Existing works heavily rely on finding
"anchor points" or their approximates, defined as instances belonging to a
particular class almost surely. Nonetheless, finding anchor points remains a
non-trivial task, and the estimation accuracy is also often throttled by the
number of available anchor points. In this paper, we propose an alternative
option to the above task. Our main contribution is the discovery of an
efficient estimation procedure based on a clusterability condition. We prove
that with clusterable representations of features, using up to third-order
consensuses of noisy labels among neighbor representations is sufficient to
estimate a unique transition matrix. Compared with methods using anchor points,
our approach uses substantially more instances and benefits from a much better
sample complexity. We demonstrate the estimation accuracy and advantages of our
estimates using both synthetic noisy labels (on CIFAR-10/100) and real
human-level noisy labels (on Clothing1M and our self-collected human-annotated
CIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at
https://github.com/UCSC-REAL/HOC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaowei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yiwen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential-Critic GAN: Generating What You Want by a Cue of Preferences. (arXiv:2107.06700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06700</id>
        <link href="http://arxiv.org/abs/2107.06700"/>
        <updated>2021-07-15T01:59:04.558Z</updated>
        <summary type="html"><![CDATA[This paper proposes Differential-Critic Generative Adversarial Network
(DiCGAN) to learn the distribution of user-desired data when only partial
instead of the entire dataset possesses the desired property, which generates
desired data that meets user's expectations and can assist in designing
biological products with desired properties. Existing approaches select the
desired samples first and train regular GANs on the selected samples to derive
the user-desired data distribution. However, the selection of the desired data
relies on an expert criterion and supervision over the entire dataset. DiCGAN
introduces a differential critic that can learn the preference direction from
the pairwise preferences, which is amateur knowledge and can be defined on part
of the training data. The resultant critic guides the generation of the desired
data instead of the whole data. Specifically, apart from the Wasserstein GAN
loss, a ranking loss of the pairwise preferences is defined over the critic. It
endows the difference of critic values between each pair of samples with the
pairwise preference relation. The higher critic value indicates that the sample
is preferred by the user. Thus training the generative model for higher critic
values encourages the generation of user-preferred samples. Extensive
experiments show that our DiCGAN achieves state-of-the-art performance in
learning the user-desired data distributions, especially in the cases of
insufficient desired data and limited supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yinghua Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yuangang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W.Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xin Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Contrastive Learning. (arXiv:2106.15499v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15499</id>
        <link href="http://arxiv.org/abs/2106.15499"/>
        <updated>2021-07-15T01:59:04.552Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel contrastive learning framework, coined as
Self-Contrastive (SelfCon) Learning, that self-contrasts within multiple
outputs from the different levels of a network. We confirmed that SelfCon loss
guarantees the lower bound of mutual information (MI) between the intermediate
and last representations. Besides, we empirically showed, via various MI
estimators, that SelfCon loss highly correlates to the increase of MI and
better classification performance. In our experiments, SelfCon surpasses
supervised contrastive (SupCon) learning without the need for a multi-viewed
batch and with the cheaper computational cost. Especially on ResNet-18, we
achieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,
which is 2.87% and 4.36% higher than SupCon and cross-entropy loss,
respectively. We found that mitigating both vanishing gradient and overfitting
issue makes our method outperform the counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1"&gt;Sangmin Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungnyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1"&gt;Jongwoo Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Gihun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1"&gt;Seungjong Noh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slow-Growing Trees. (arXiv:2103.01926v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01926</id>
        <link href="http://arxiv.org/abs/2103.01926"/>
        <updated>2021-07-15T01:59:04.532Z</updated>
        <summary type="html"><![CDATA[Random Forest's performance can be matched by a single slow-growing tree
(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits
the view that CART is an extreme case of an iterative weighted least square
procedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests
(RF) is presented. Greedy ML algorithms' outcomes can be improved using either
"slow learning" or diversification. SGT applies the former to estimate a single
deep tree, and Booging (bagging stochastic BT with a high learning rate) uses
the latter with additive shallow trees. The performance of this tree ensemble
quaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Coulombe_P/0/1/0/all/0/1"&gt;Philippe Goulet Coulombe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Learning of Pinball TWSVM using Privileged Information and its applications. (arXiv:2107.06744v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06744</id>
        <link href="http://arxiv.org/abs/2107.06744"/>
        <updated>2021-07-15T01:59:04.461Z</updated>
        <summary type="html"><![CDATA[In any learning framework, an expert knowledge always plays a crucial role.
But, in the field of machine learning, the knowledge offered by an expert is
rarely used. Moreover, machine learning algorithms (SVM based) generally use
hinge loss function which is sensitive towards the noise. Thus, in order to get
the advantage from an expert knowledge and to reduce the sensitivity towards
the noise, in this paper, we propose privileged information based Twin Pinball
Support Vector Machine classifier (Pin-TWSVMPI) where expert's knowledge is in
the form of privileged information. The proposed Pin-TWSVMPI incorporates
privileged information by using correcting function so as to obtain two
nonparallel decision hyperplanes. Further, in order to make computations more
efficient and fast, we use Sequential Minimal Optimization (SMO) technique for
obtaining the classifier and have also shown its application for Pedestrian
detection and Handwritten digit recognition. Further, for UCI datasets, we
first implement a procedure which extracts privileged information from the
features of the dataset which are then further utilized by Pin-TWSVMPI that
leads to enhancement in classification accuracy with comparatively lesser
computational time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_R/0/1/0/all/0/1"&gt;Reshma Rastogi&lt;/a&gt; (nee. Khemchandani), &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1"&gt;Aman Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Modeling of Emerging Device-based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search. (arXiv:2107.06871v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2107.06871</id>
        <link href="http://arxiv.org/abs/2107.06871"/>
        <updated>2021-07-15T01:59:04.454Z</updated>
        <summary type="html"><![CDATA[Emerging device-based Computing-in-memory (CiM) has been proved to be a
promising candidate for high-energy efficiency deep neural network (DNN)
computations. However, most emerging devices suffer uncertainty issues,
resulting in a difference between actual data stored and the weight value it is
designed to be. This leads to an accuracy drop from trained models to actually
deployed platforms. In this work, we offer a thorough analysis of the effect of
such uncertainties-induced changes in DNN models. To reduce the impact of
device uncertainties, we propose UAE, an uncertainty-aware Neural Architecture
Search scheme to identify a DNN model that is both accurate and robust against
device uncertainties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zheyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1"&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaobo Sharon Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yiyu Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Adaptive Multi-Intention Inverse Reinforcement Learning. (arXiv:2107.06692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06692</id>
        <link href="http://arxiv.org/abs/2107.06692"/>
        <updated>2021-07-15T01:59:04.448Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep Inverse Reinforcement Learning (IRL) framework
that can learn an a priori unknown number of nonlinear reward functions from
unlabeled experts' demonstrations. For this purpose, we employ the tools from
Dirichlet processes and propose an adaptive approach to simultaneously account
for both complex and unknown number of reward functions. Using the conditional
maximum entropy principle, we model the experts' multi-intention behaviors as a
mixture of latent intention distributions and derive two algorithms to estimate
the parameters of the deep reward network along with the number of experts'
intentions from unlabeled demonstrations. The proposed algorithms are evaluated
on three benchmarks, two of which have been specifically extended in this study
for multi-intention IRL, and compared with well-known baselines. We demonstrate
through several experiments the advantages of our algorithms over the existing
approaches and the benefits of online inferring, rather than fixing beforehand,
the number of expert's intentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bighashdel_A/0/1/0/all/0/1"&gt;Ariyan Bighashdel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meletis_P/0/1/0/all/0/1"&gt;Panagiotis Meletis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1"&gt;Pavol Jancura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1"&gt;Gijs Dubbelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Machine Learning of Model Error in Dynamical Systems. (arXiv:2107.06658v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06658</id>
        <link href="http://arxiv.org/abs/2107.06658"/>
        <updated>2021-07-15T01:59:04.443Z</updated>
        <summary type="html"><![CDATA[The development of data-informed predictive models for dynamical systems is
of widespread interest in many disciplines. We present a unifying framework for
blending mechanistic and machine-learning approaches to identify dynamical
systems from data. We compare pure data-driven learning with hybrid models
which incorporate imperfect domain knowledge. We cast the problem in both
continuous- and discrete-time, for problems in which the model error is
memoryless and in which it has significant memory, and we compare data-driven
and hybrid approaches experimentally. Our formulation is agnostic to the chosen
machine learning model.

Using Lorenz '63 and Lorenz '96 Multiscale systems, we find that hybrid
methods substantially outperform solely data-driven approaches in terms of data
hunger, demands for model complexity, and overall predictive performance. We
also find that, while a continuous-time framing allows for robustness to
irregular sampling and desirable domain-interpretability, a discrete-time
framing can provide similar or better predictive performance, especially when
data are undersampled and the vector field cannot be resolved.

We study model error from the learning theory perspective, defining excess
risk and generalization error; for a linear model of the error used to learn
about ergodic dynamical systems, both errors are bounded by terms that diminish
with the square-root of T. We also illustrate scenarios that benefit from
modeling with memory, proving that continuous-time recurrent neural networks
(RNNs) can, in principle, learn memory-dependent model error and reconstruct
the original system arbitrarily well; numerical results depict challenges in
representing memory by this approach. We also connect RNNs to reservoir
computing and thereby relate the learning of memory-dependent error to recent
work on supervised learning between Banach spaces using random features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Levine_M/0/1/0/all/0/1"&gt;Matthew E. Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reactive Long Horizon Task Execution via Visual Skill and Precondition Models. (arXiv:2011.08694v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08694</id>
        <link href="http://arxiv.org/abs/2011.08694"/>
        <updated>2021-07-15T01:59:04.437Z</updated>
        <summary type="html"><![CDATA[Zero-shot execution of unseen robotic tasks is important to allowing robots
to perform a wide variety of tasks in human environments, but collecting the
amounts of data necessary to train end-to-end policies in the real-world is
often infeasible. We describe an approach for sim-to-real training that can
accomplish unseen robotic tasks using models learned in simulation to ground
components of a simple task planner. We learn a library of parameterized
skills, along with a set of predicates-based preconditions and termination
conditions, entirely in simulation. We explore a block-stacking task because it
has a clear structure, where multiple skills must be chained together, but our
methods are applicable to a wide range of other problems and domains, and can
transfer from simulation to the real-world with no fine tuning. The system is
able to recognize failures and accomplish long-horizon tasks from perceptual
input, which is critical for real-world execution. We evaluate our proposed
approach in both simulation and in the real-world, showing an increase in
success rate from 91.6% to 98% in simulation and from 10% to 80% success rate
in the real-world as compared with naive baselines. For experiment videos
including both real-world and simulation, see:
https://www.youtube.com/playlist?list=PL-oD0xHUngeLfQmpngYkGFZarstfPOXqX]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Shohin Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fishman_A/0/1/0/all/0/1"&gt;Adam Fishman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Likhachev_M/0/1/0/all/0/1"&gt;Maxim Likhachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Self-Training for Semi-Supervised Audio Recognition. (arXiv:2107.06877v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06877</id>
        <link href="http://arxiv.org/abs/2107.06877"/>
        <updated>2021-07-15T01:59:04.431Z</updated>
        <summary type="html"><![CDATA[Federated Learning is a distributed machine learning paradigm dealing with
decentralized and personal datasets. Since data reside on devices like
smartphones and virtual assistants, labeling is entrusted to the clients, or
labels are extracted in an automated way. Specifically, in the case of audio
data, acquiring semantic annotations can be prohibitively expensive and
time-consuming. As a result, an abundance of audio data remains unlabeled and
unexploited on users' devices. Most existing federated learning approaches
focus on supervised learning without harnessing the unlabeled data. In this
work, we study the problem of semi-supervised learning of audio models via
self-training in conjunction with federated learning. We propose FedSTAR to
exploit large-scale on-device unlabeled data to improve the generalization of
audio recognition models. We further demonstrate that self-supervised
pre-trained models can accelerate the training of on-device models,
significantly improving convergence to within fewer training rounds. We conduct
experiments on diverse public audio classification datasets and investigate the
performance of our models under varying percentages of labeled and unlabeled
data. Notably, we show that with as little as 3% labeled data available,
FedSTAR on average can improve the recognition rate by 13.28% compared to the
fully supervised federated model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1"&gt;Vasileios Tsouvalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1"&gt;Aaqib Saeed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozcelebi_T/0/1/0/all/0/1"&gt;Tanir Ozcelebi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05732</id>
        <link href="http://arxiv.org/abs/2006.05732"/>
        <updated>2021-07-15T01:59:04.425Z</updated>
        <summary type="html"><![CDATA[Object detection in images has reached unprecedented performances. The
state-of-the-art methods rely on deep architectures that extract salient
features and predict bounding boxes enclosing the objects of interest. These
methods essentially run on RGB images. However, the RGB images are often
compressed by the acquisition devices for storage purpose and transfer
efficiency. Hence, their decompression is required for object detectors. To
gain in efficiency, this paper proposes to take advantage of the compressed
representation of images to carry out object detection usable in constrained
resources conditions.

Specifically, we focus on JPEG images and propose a thorough analysis of
detection architectures newly designed in regard of the peculiarities of the
JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard
RGB-based architecture, while only reducing the detection performance by 5.5%.
Additionally, our empirical findings demonstrate that only part of the
compressed JPEG information, namely the luminance component, may be required to
match detection accuracy of the full input methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1"&gt;Benjamin Deguerre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1"&gt;Clement Chatelain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous vs. Discrete Optimization of Deep Neural Networks. (arXiv:2107.06608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06608</id>
        <link href="http://arxiv.org/abs/2107.06608"/>
        <updated>2021-07-15T01:59:04.408Z</updated>
        <summary type="html"><![CDATA[Existing analyses of optimization in deep learning are either continuous,
focusing on (variants of) gradient flow, or discrete, directly treating
(variants of) gradient descent. Gradient flow is amenable to theoretical
analysis, but is stylized and disregards computational efficiency. The extent
to which it represents gradient descent is an open question in deep learning
theory. The current paper studies this question. Viewing gradient descent as an
approximate numerical solution to the initial value problem of gradient flow,
we find that the degree of approximation depends on the curvature along the
latter's trajectory. We then show that over deep neural networks with
homogeneous activations, gradient flow trajectories enjoy favorable curvature,
suggesting they are well approximated by gradient descent. This finding allows
us to translate an analysis of gradient flow over deep linear neural networks
into a guarantee that gradient descent efficiently converges to global minimum
almost surely under random initialization. Experiments suggest that over simple
deep neural networks, gradient descent with conventional step size is indeed
close to the continuous limit. We hypothesize that the theory of gradient flows
will be central to unraveling mysteries behind deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elkabetz_O/0/1/0/all/0/1"&gt;Omer Elkabetz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1"&gt;Nadav Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00649</id>
        <link href="http://arxiv.org/abs/2107.00649"/>
        <updated>2021-07-15T01:59:04.389Z</updated>
        <summary type="html"><![CDATA[A set of novel approaches for estimating epistemic uncertainty in deep neural
networks with a single forward pass has recently emerged as a valid alternative
to Bayesian Neural Networks. On the premise of informative representations,
these deterministic uncertainty methods (DUMs) achieve strong performance on
detecting out-of-distribution (OOD) data while adding negligible computational
costs at inference time. However, it remains unclear whether DUMs are well
calibrated and can seamlessly scale to real-world applications - both
prerequisites for their practical deployment. To this end, we first provide a
taxonomy of DUMs, evaluate their calibration under continuous distributional
shifts and their performance on OOD detection for image classification tasks.
Then, we extend the most promising approaches to semantic segmentation. We find
that, while DUMs scale to realistic vision tasks and perform well on OOD
detection, the practicality of current methods is undermined by poor
calibration under realistic distributional shifts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1"&gt;Janis Postels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1"&gt;Mattia Segu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network. (arXiv:2107.06846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06846</id>
        <link href="http://arxiv.org/abs/2107.06846"/>
        <updated>2021-07-15T01:59:04.368Z</updated>
        <summary type="html"><![CDATA[An impact of climate change is the increase in frequency and intensity of
extreme precipitation events. However, confidently predicting the likelihood of
extreme precipitation at seasonal scales remains an outstanding challenge.
Here, we present an approach to forecasting the quantiles of the maximum daily
precipitation in each week up to six months ahead using the temporal fusion
transformer (TFT) model. Through experiments in two regions, we compare TFT
predictions with those of two baselines: climatology and a calibrated ECMWF
SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk
at six month lead time, the TFT predictions significantly outperform those from
S5 and show an overall small improvement compared to climatology. The TFT also
responds positively to departures from normal that climatology cannot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Civitarese_D/0/1/0/all/0/1"&gt;Daniel Salles Civitarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szwarcman_D/0/1/0/all/0/1"&gt;Daniela Szwarcman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1"&gt;Bianca Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1"&gt;Campbell Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conservative Objective Models for Effective Offline Model-Based Optimization. (arXiv:2107.06882v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06882</id>
        <link href="http://arxiv.org/abs/2107.06882"/>
        <updated>2021-07-15T01:59:04.363Z</updated>
        <summary type="html"><![CDATA[Computational design problems arise in a number of settings, from synthetic
biology to computer architectures. In this paper, we aim to solve data-driven
model-based optimization (MBO) problems, where the goal is to find a design
input that maximizes an unknown objective function provided access to only a
static dataset of prior experiments. Such data-driven optimization procedures
are the only practical methods in many real-world domains where active data
collection is expensive (e.g., when optimizing over proteins) or dangerous
(e.g., when optimizing over aircraft designs). Typical methods for MBO that
optimize the design against a learned model suffer from distributional shift:
it is easy to find a design that "fools" the model into predicting a high
value. To overcome this, we propose conservative objective models (COMs), a
method that learns a model of the objective function that lower bounds the
actual value of the ground-truth objective on out-of-distribution inputs, and
uses it for optimization. Structurally, COMs resemble adversarial training
methods used to overcome adversarial examples. COMs are simple to implement and
outperform a number of existing methods on a wide range of MBO problems,
including optimizing protein sequences, robot morphologies, neural network
weights, and superconducting materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trabucco_B/0/1/0/all/0/1"&gt;Brandon Trabucco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aviral Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xinyang Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Deep Distribution Network for Bid Shading in First-Price Auctions. (arXiv:2107.06650v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.06650</id>
        <link href="http://arxiv.org/abs/2107.06650"/>
        <updated>2021-07-15T01:59:04.356Z</updated>
        <summary type="html"><![CDATA[Since 2019, most ad exchanges and sell-side platforms (SSPs), in the online
advertising industry, shifted from second to first price auctions. Due to the
fundamental difference between these auctions, demand-side platforms (DSPs)
have had to update their bidding strategies to avoid bidding unnecessarily high
and hence overpaying. Bid shading was proposed to adjust the bid price intended
for second-price auctions, in order to balance cost and winning probability in
a first-price auction setup. In this study, we introduce a novel deep
distribution network for optimal bidding in both open (non-censored) and closed
(censored) online first-price auctions. Offline and online A/B testing results
show that our algorithm outperforms previous state-of-art algorithms in terms
of both surplus and effective cost per action (eCPX) metrics. Furthermore, the
algorithm is optimized in run-time and has been deployed into VerizonMedia DSP
as production algorithm, serving hundreds of billions of bid requests per day.
Online A/B test shows that advertiser's ROI are improved by +2.4%, +2.4%, and
+8.6% for impression based (CPM), click based (CPC), and conversion based (CPA)
campaigns respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shengjun Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_N/0/1/0/all/0/1"&gt;Niklas Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_B/0/1/0/all/0/1"&gt;Bharatbhushan Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitts_B/0/1/0/all/0/1"&gt;Brendan Kitts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gligorijevic_D/0/1/0/all/0/1"&gt;Djordje Gligorijevic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gultekin_S/0/1/0/all/0/1"&gt;San Gultekin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1"&gt;Tingyu Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Junwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flores_A/0/1/0/all/0/1"&gt;Aaron Flores&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03464</id>
        <link href="http://arxiv.org/abs/1908.03464"/>
        <updated>2021-07-15T01:59:04.348Z</updated>
        <summary type="html"><![CDATA[Feature selection, an effective technique for dimensionality reduction, plays
an important role in many machine learning systems. Supervised knowledge can
significantly improve the performance. However, faced with the rapid growth of
newly emerging concepts, existing supervised methods might easily suffer from
the scarcity and validity of labeled data for training. In this paper, the
authors study the problem of zero-shot feature selection (i.e., building a
feature selection model that generalizes well to "unseen" concepts with limited
training data of "seen" concepts). Specifically, they adopt class-semantic
descriptions (i.e., attributes) as supervision for feature selection, so as to
utilize the supervised knowledge transferred from the seen concepts. For more
reliable discriminative features, they further propose the
center-characteristic loss which encourages the selected features to capture
the central characteristics of seen concepts. Extensive experiments conducted
on various real-world datasets demonstrate the effectiveness of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tingzhang Zhao&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojun Ye&lt;/a&gt; (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. (arXiv:2012.08791v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08791</id>
        <link href="http://arxiv.org/abs/2012.08791"/>
        <updated>2021-07-15T01:59:04.340Z</updated>
        <summary type="html"><![CDATA[Until recently, the most accurate methods for time series classification were
limited by high computational complexity. ROCKET achieves state-of-the-art
accuracy with a fraction of the computational expense of most existing methods
by transforming input time series using random convolutional kernels, and using
the transformed features to train a linear classifier. We reformulate ROCKET
into a new method, MINIROCKET, making it up to 75 times faster on larger
datasets, and making it almost deterministic (and optionally, with additional
computational expense, fully deterministic), while maintaining essentially the
same accuracy. Using this method, it is possible to train and test a classifier
on all of 109 datasets from the UCR archive to state-of-the-art accuracy in
less than 10 minutes. MINIROCKET is significantly faster than any other method
of comparable accuracy (including ROCKET), and significantly more accurate than
any other method of even roughly-similar computational expense. As such, we
suggest that MINIROCKET should now be considered and used as the default
variant of ROCKET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dempster_A/0/1/0/all/0/1"&gt;Angus Dempster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1"&gt;Daniel F. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I. Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06876</id>
        <link href="http://arxiv.org/abs/2107.06876"/>
        <updated>2021-07-15T01:59:04.324Z</updated>
        <summary type="html"><![CDATA[The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1"&gt;Marten Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DULA: A Differentiable Ergonomics Model for Postural Optimization in Physical HRI. (arXiv:2107.06875v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06875</id>
        <link href="http://arxiv.org/abs/2107.06875"/>
        <updated>2021-07-15T01:59:04.318Z</updated>
        <summary type="html"><![CDATA[Ergonomics and human comfort are essential concerns in physical human-robot
interaction applications. Defining an accurate and easy-to-use ergonomic
assessment model stands as an important step in providing feedback for postural
correction to improve operator health and comfort. In order to enable efficient
computation, previously proposed automated ergonomic assessment and correction
tools make approximations or simplifications to gold-standard assessment tools
used by ergonomists in practice. In order to retain assessment quality, while
improving computational considerations, we introduce DULA, a differentiable and
continuous ergonomics model learned to replicate the popular and scientifically
validated RULA assessment. We show that DULA provides assessment comparable to
RULA while providing computational benefits. We highlight DULA's strength in a
demonstration of gradient-based postural optimization for a simulated
teleoperation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yazdani_A/0/1/0/all/0/1"&gt;Amir Yazdani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novin_R/0/1/0/all/0/1"&gt;Roya Sabbagh Novin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merryweather_A/0/1/0/all/0/1"&gt;Andrew Merryweather&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1"&gt;Tucker Hermans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06720</id>
        <link href="http://arxiv.org/abs/2107.06720"/>
        <updated>2021-07-15T01:59:04.312Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as an important consideration in algorithmic
decision-making. Unfairness occurs when an agent with higher merit obtains a
worse outcome than an agent with lower merit. Our central point is that a
primary cause of unfairness is uncertainty. A principal or algorithm making
decisions never has access to the agents' true merit, and instead uses proxy
features that only imperfectly predict merit (e.g., GPA, star ratings,
recommendation letters). None of these ever fully capture an agent's merit; yet
existing approaches have mostly been defining fairness notions directly based
on observed features and outcomes.

Our primary point is that it is more principled to acknowledge and model the
uncertainty explicitly. The role of observed features is to give rise to a
posterior distribution of the agents' merits. We use this viewpoint to define a
notion of approximate fairness in ranking. We call an algorithm $\phi$-fair
(for $\phi \in [0,1]$) if it has the following property for all agents $x$ and
all $k$: if agent $x$ is among the top $k$ agents with respect to merit with
probability at least $\rho$ (according to the posterior merit distribution),
then the algorithm places the agent among the top $k$ agents in its ranking
with probability at least $\phi \rho$.

We show how to compute rankings that optimally trade off approximate fairness
against utility to the principal. In addition to the theoretical
characterization, we present an empirical analysis of the potential impact of
the approach in simulation studies. For real-world validation, we applied the
approach in the context of a paper recommendation system that we built and
fielded at a large conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ashudeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1"&gt;David Kempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thinkback: Task-SpecificOut-of-Distribution Detection. (arXiv:2107.06668v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06668</id>
        <link href="http://arxiv.org/abs/2107.06668"/>
        <updated>2021-07-15T01:59:04.306Z</updated>
        <summary type="html"><![CDATA[The increased success of Deep Learning (DL) has recently sparked large-scale
deployment of DL models in many diverse industry segments. Yet, a crucial
weakness of supervised model is the inherent difficulty in handling
out-of-distribution samples, i.e., samples belonging to classes that were not
presented to the model at training time. We propose in this paper a novel way
to formulate the out-of-distribution detection problem, tailored for DL models.
Our method does not require fine tuning process on training data, yet is
significantly more accurate than the state of the art for out-of-distribution
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1"&gt;Dario Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Closure Models for Dynamical Systems. (arXiv:2012.13869v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13869</id>
        <link href="http://arxiv.org/abs/2012.13869"/>
        <updated>2021-07-15T01:59:04.300Z</updated>
        <summary type="html"><![CDATA[Complex dynamical systems are used for predictions in many domains. Because
of computational costs, models are truncated, coarsened, or aggregated. As the
neglected and unresolved terms become important, the utility of model
predictions diminishes. We develop a novel, versatile, and rigorous methodology
to learn non-Markovian closure parameterizations for known-physics/low-fidelity
models using data from high-fidelity simulations. The new "neural closure
models" augment low-fidelity models with neural delay differential equations
(nDDEs), motivated by the Mori-Zwanzig formulation and the inherent delays in
complex dynamical systems. We demonstrate that neural closures efficiently
account for truncated modes in reduced-order-models, capture the effects of
subgrid-scale processes in coarse models, and augment the simplification of
complex biological and physical-biogeochemical models. We find that using
non-Markovian over Markovian closures improves long-term prediction accuracy
and requires smaller networks. We derive adjoint equations and network
architectures needed to efficiently implement the new discrete and distributed
nDDEs, for any time-integration schemes and allowing nonuniformly-spaced
temporal training data. The performance of discrete over distributed delays in
closure models is explained using information theory, and we find an optimal
amount of past information for a specified architecture. Finally, we analyze
computational complexity and explain the limited additional cost due to neural
closure models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Abhinav Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lermusiaux_P/0/1/0/all/0/1"&gt;Pierre F.J. Lermusiaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOSNet: Deep Learning based Objective Assessment for Voice Conversion. (arXiv:1904.08352v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.08352</id>
        <link href="http://arxiv.org/abs/1904.08352"/>
        <updated>2021-07-15T01:59:04.294Z</updated>
        <summary type="html"><![CDATA[Existing objective evaluation metrics for voice conversion (VC) are not
always correlated with human perception. Therefore, training VC models with
such criteria may not effectively improve naturalness and similarity of
converted speech. In this paper, we propose deep learning-based assessment
models to predict human ratings of converted speech. We adopt the convolutional
and recurrent neural network models to build a mean opinion score (MOS)
predictor, termed as MOSNet. The proposed models are tested on large-scale
listening test results of the Voice Conversion Challenge (VCC) 2018.
Experimental results show that the predicted scores of the proposed MOSNet are
highly correlated with human MOS ratings at the system level while being fairly
correlated with human MOS ratings at the utterance level. Meanwhile, we have
modified MOSNet to predict the similarity scores, and the preliminary results
show that the predicted scores are also fairly correlated with human ratings.
These results confirm that the proposed models could be used as a computational
evaluator to measure the MOS of VC systems to reduce the need for expensive
human rating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chen-Chou Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1"&gt;Szu-Wei Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06802</id>
        <link href="http://arxiv.org/abs/2107.06802"/>
        <updated>2021-07-15T01:59:04.278Z</updated>
        <summary type="html"><![CDATA[User reviews have an essential role in the success of the developed mobile
apps. User reviews in the textual form are unstructured data, creating a very
high complexity when processed for sentiment analysis. Previous approaches that
have been used often ignore the context of reviews. In addition, the relatively
small data makes the model overfitting. A new approach, BERT, has been
introduced as a transfer learning model with a pre-trained model that has
previously been trained to have a better context representation. This study
examines the effectiveness of fine-tuning BERT for sentiment analysis using two
different pre-trained models. Besides the multilingual pre-trained model, we
use the pre-trained model that only has been trained in Indonesian. The dataset
used is Indonesian user reviews of the ten best apps in 2020 in Google Play
sites. We also perform hyper-parameter tuning to find the optimum trained
model. Two training data labeling approaches were also tested to determine the
effectiveness of the model, which is score-based and lexicon-based. The
experimental results show that pre-trained models trained in Indonesian have
better average accuracy on lexicon-based data. The pre-trained Indonesian model
highest accuracy is 84%, with 25 epochs and a training time of 24 minutes.
These results are better than all of the machine learning and multilingual
pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1"&gt;Anantha Yullian Sukmadewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1"&gt;Haftittah Wuswilahaken DW&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1"&gt;Fitra Abdurrachman Bachtiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06862</id>
        <link href="http://arxiv.org/abs/2107.06862"/>
        <updated>2021-07-15T01:59:04.272Z</updated>
        <summary type="html"><![CDATA[Reaction-Diffusion (RD) systems provide a computational framework that
governs many pattern formation processes in nature. Current RD system design
practices boil down to trial-and-error parameter search. We propose a
differentiable optimization method for learning the RD system parameters to
perform example-based texture synthesis on a 2D plane. We do this by
representing the RD system as a variant of Neural Cellular Automata and using
task-specific differentiable loss functions. RD systems generated by our method
exhibit robust, non-trivial 'life-like' behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1"&gt;Alexander Mordvintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1"&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1"&gt;Eyvind Niklasson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings. (arXiv:2107.05585v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05585</id>
        <link href="http://arxiv.org/abs/2107.05585"/>
        <updated>2021-07-15T01:59:04.266Z</updated>
        <summary type="html"><![CDATA[We study differentially private stochastic optimization in convex and
non-convex settings. For the convex case, we focus on the family of non-smooth
generalized linear losses (GLLs). Our algorithm for the $\ell_2$ setting
achieves optimal excess population risk in near-linear time, while the best
known differentially private algorithms for general convex losses run in
super-linear time. Our algorithm for the $\ell_1$ setting has nearly-optimal
excess population risk $\tilde{O}\big(\sqrt{\frac{\log{d}}{n}}\big)$, and
circumvents the dimension dependent lower bound of [AFKT21] for general
non-smooth convex losses. In the differentially private non-convex setting, we
provide several new algorithms for approximating stationary points of the
population risk. For the $\ell_1$-case with smooth losses and polyhedral
constraint, we provide the first nearly dimension independent rate, $\tilde
O\big(\frac{\log^{2/3}{d}}{{n^{1/3}}}\big)$ in linear time. For the constrained
$\ell_2$-case, with smooth losses, we obtain a linear-time algorithm with rate
$\tilde O\big(\frac{1}{n^{3/10}d^{1/10}}+\big(\frac{d}{n^2}\big)^{1/5}\big)$.
Finally, for the $\ell_2$-case we provide the first method for {\em non-smooth
weakly convex} stochastic optimization with rate $\tilde
O\big(\frac{1}{n^{1/4}}+\big(\frac{d}{n^2}\big)^{1/6}\big)$ which matches the
best existing non-private algorithm when $d= O(\sqrt{n})$. We also extend all
our results above for the non-convex $\ell_2$ setting to the $\ell_p$ setting,
where $1 < p \leq 2$, with only polylogarithmic (in the dimension) overhead in
the rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bassily_R/0/1/0/all/0/1"&gt;Raef Bassily&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menart_M/0/1/0/all/0/1"&gt;Michael Menart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCDNet: An Interpretable Rain Convolutional Dictionary Network for Single Image Deraining. (arXiv:2107.06808v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06808</id>
        <link href="http://arxiv.org/abs/2107.06808"/>
        <updated>2021-07-15T01:59:04.260Z</updated>
        <summary type="html"><![CDATA[As a common weather, rain streaks adversely degrade the image quality. Hence,
removing rains from an image has become an important issue in the field. To
handle such an ill-posed single image deraining task, in this paper, we
specifically build a novel deep architecture, called rain convolutional
dictionary network (RCDNet), which embeds the intrinsic priors of rain streaks
and has clear interpretability. In specific, we first establish a RCD model for
representing rain streaks and utilize the proximal gradient descent technique
to design an iterative algorithm only containing simple operators for solving
the model. By unfolding it, we then build the RCDNet in which every network
module has clear physical meanings and corresponds to each operation involved
in the algorithm. This good interpretability greatly facilitates an easy
visualization and analysis on what happens inside the network and why it works
well in inference process. Moreover, taking into account the domain gap issue
in real scenarios, we further design a novel dynamic RCDNet, where the rain
kernels can be dynamically inferred corresponding to input rainy images and
then help shrink the space for rain layer estimation with few rain maps so as
to ensure a fine generalization performance in the inconsistent scenarios of
rain types between training and testing data. By end-to-end training such an
interpretable network, all involved rain kernels and proximal operators can be
automatically extracted, faithfully characterizing the features of both rain
and clean background layers, and thus naturally lead to better deraining
performance. Comprehensive experiments substantiate the superiority of our
method, especially on its well generality to diverse testing scenarios and good
interpretability for all its modules. Code is available in
\emph{\url{https://github.com/hongwang01/DRCDNet}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_Q/0/1/0/all/0/1"&gt;Qi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yong Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting Criminal Charge Prediction and Its Algorithmic Bias via Quantum-Inspired Complex Valued Networks. (arXiv:2106.13456v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13456</id>
        <link href="http://arxiv.org/abs/2106.13456"/>
        <updated>2021-07-15T01:59:04.254Z</updated>
        <summary type="html"><![CDATA[While predictive policing has become increasingly common in assisting with
decisions in the criminal justice system, the use of these results is still
controversial. Some software based on deep learning lacks accuracy (e.g., in
F-1), and importantly many decision processes are not transparent, causing
doubt about decision bias, such as perceived racial and age disparities. This
paper addresses bias issues with post-hoc explanations to provide a trustable
prediction of whether a person will receive future criminal charges given one's
previous criminal records by learning temporal behavior patterns over twenty
years. Bi-LSTM relieves the vanishing gradient problem, attentional mechanisms
allow learning and interpretation of feature importance, and complex-valued
networks inspired quantum physics to facilitate a certain level of transparency
in modeling the decision process. Our approach shows a consistent and reliable
prediction precision and recall on a real-life dataset. Our analysis of the
importance of each input feature shows the critical causal impact on
decision-making, suggesting that criminal histories are statistically
significant factors, while identifiers, such as race and age, are not. Finally,
our algorithm indicates that a suspect tends to rather than suddenly increase
crime severity level over time gradually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Abdul Rafae Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varsanyi_P/0/1/0/all/0/1"&gt;Peter Varsanyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pabreja_R/0/1/0/all/0/1"&gt;Rachit Pabreja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Mixture of Experts. (arXiv:2107.06724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06724</id>
        <link href="http://arxiv.org/abs/2107.06724"/>
        <updated>2021-07-15T01:59:04.236Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has emerged as the predominant approach for
collaborative training of neural network models across multiple users, without
the need to gather the data at a central location. One of the important
challenges in this setting is data heterogeneity, i.e. different users have
different data characteristics. For this reason, training and using a single
global model might be suboptimal when considering the performance of each of
the individual user's data. In this work, we tackle this problem via Federated
Mixture of Experts, FedMix, a framework that allows us to train an ensemble of
specialized models. FedMix adaptively selects and trains a user-specific
selection of the ensemble members. We show that users with similar data
characteristics select the same members and therefore share statistical
strength while mitigating the effect of non-i.i.d data. Empirically, we show
through an extensive experimental evaluation that FedMix improves performance
compared to using a single global model across a variety of different sources
of non-i.i.d.-ness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reisser_M/0/1/0/all/0/1"&gt;Matthias Reisser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1"&gt;Christos Louizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M5 Competition Uncertainty: Overdispersion, distributional forecasting, GAMLSS and beyond. (arXiv:2107.06675v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.06675</id>
        <link href="http://arxiv.org/abs/2107.06675"/>
        <updated>2021-07-15T01:59:04.229Z</updated>
        <summary type="html"><![CDATA[The M5 competition uncertainty track aims for probabilistic forecasting of
sales of thousands of Walmart retail goods. We show that the M5 competition
data faces strong overdispersion and sporadic demand, especially zero demand.
We discuss resulting modeling issues concerning adequate probabilistic
forecasting of such count data processes. Unfortunately, the majority of
popular prediction methods used in the M5 competition (e.g. lightgbm and
xgboost GBMs) fails to address the data characteristics due to the considered
objective functions. The distributional forecasting provides a suitable
modeling approach for to the overcome those problems. The GAMLSS framework
allows flexible probabilistic forecasting using low dimensional distributions.
We illustrate, how the GAMLSS approach can be applied for the M5 competition
data by modeling the location and scale parameter of various distributions,
e.g. the negative binomial distribution. Finally, we discuss software packages
for distributional modeling and their drawback, like the R package gamlss with
its package extensions, and (deep) distributional forecasting libraries such as
TensorFlow Probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1"&gt;Florian Ziel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modelling Neuronal Behaviour with Time Series Regression: Recurrent Neural Networks on C. Elegans Data. (arXiv:2107.06762v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06762</id>
        <link href="http://arxiv.org/abs/2107.06762"/>
        <updated>2021-07-15T01:59:04.223Z</updated>
        <summary type="html"><![CDATA[Given the inner complexity of the human nervous system, insight into the
dynamics of brain activity can be gained from understanding smaller and simpler
organisms, such as the nematode C. Elegans. The behavioural and structural
biology of these organisms is well-known, making them prime candidates for
benchmarking modelling and simulation techniques. In these complex neuronal
collections, classical, white-box modelling techniques based on intrinsic
structural or behavioural information are either unable to capture the profound
nonlinearities of the neuronal response to different stimuli or generate
extremely complex models, which are computationally intractable. In this paper
we show how the nervous system of C. Elegans can be modelled and simulated with
data-driven models using different neural network architectures. Specifically,
we target the use of state of the art recurrent neural networks architectures
such as LSTMs and GRUs and compare these architectures in terms of their
properties and their accuracy as well as the complexity of the resulting
models. We show that GRU models with a hidden layer size of 4 units are able to
accurately reproduce with high accuracy the system's response to very different
stimuli.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mestre_G/0/1/0/all/0/1"&gt;Gon&amp;#xe7;alo Mestre&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barbulescu_R/0/1/0/all/0/1"&gt;Ruxandra Barbulescu&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/q-bio/1/au:+Silveira_L/0/1/0/all/0/1"&gt;L. Miguel Silveira&lt;/a&gt; (1 and 2) ((1) INESC-ID, Rua Alves Redol 9, 1000-029 Lisboa, (2) IST Tecnico Lisboa, Universidade de Lisboa, Av. Rovisco Pais 1, 1049-001 Lisboa)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Potential of Low-bit Training of Convolutional Neural Networks. (arXiv:2006.02804v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02804</id>
        <link href="http://arxiv.org/abs/2006.02804"/>
        <updated>2021-07-15T01:59:04.217Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a low-bit training framework for convolutional
neural networks, which is built around a novel multi-level scaling (MLS) tensor
format. Our framework focuses on reducing the energy consumption of convolution
operations by quantizing all the convolution operands to low bit-width format.
Specifically, we propose the MLS tensor format, in which the element-wise
bit-width can be largely reduced. Then, we describe the dynamic quantization
and the low-bit tensor convolution arithmetic to leverage the MLS tensor format
efficiently. Experiments show that our framework achieves a superior trade-off
between the accuracy and the bit-width than previous low-bit training
frameworks. For training a variety of models on CIFAR-10, using 1-bit mantissa
and 2-bit exponent is adequate to keep the accuracy loss within $1\%$. And on
larger datasets like ImageNet, using 4-bit mantissa and 2-bit exponent is
adequate to keep the accuracy loss within $1\%$. Through the energy consumption
simulation of the computing units, we can estimate that training a variety of
models with our framework could achieve $8.3\sim10.2\times$ and
$1.9\sim2.3\times$ higher energy efficiency than training with full-precision
and 8-bit floating-point arithmetic, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1"&gt;Xuefei Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1"&gt;Guohao Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhenhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shulin Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huazhong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14696</id>
        <link href="http://arxiv.org/abs/2011.14696"/>
        <updated>2021-07-15T01:59:04.210Z</updated>
        <summary type="html"><![CDATA[Active Learning (AL) techniques aim to minimize the training data required to
train a model for a given task. Pool-based AL techniques start with a small
initial labeled pool and then iteratively pick batches of the most informative
samples for labeling. Generally, the initial pool is sampled randomly and
labeled to seed the AL iterations. While recent studies have focused on
evaluating the robustness of various query functions in AL, little to no
attention has been given to the design of the initial labeled pool for deep
active learning. Given the recent successes of learning representations in
self-supervised/unsupervised ways, we study if an intelligently sampled initial
labeled pool can improve deep AL performance. We investigate the effect of
intelligently sampled initial labeled pools, including the use of
self-supervised and unsupervised strategies, on deep AL methods. The setup,
hypotheses, methodology, and implementation details were evaluated by peer
review before experiments were conducted. Experimental results could not
conclusively prove that intelligently sampled initial pools are better for AL
than random initial pools in the long run, although a Variational
Autoencoder-based initial pool sampling strategy showed interesting trends that
merit deeper investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1"&gt;Akshay L Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sai Vikas Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v4 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08721</id>
        <link href="http://arxiv.org/abs/2105.08721"/>
        <updated>2021-07-15T01:59:04.193Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast
dominant wave periods in oceanic waters. First, we use the data collected from
CDIP buoys and apply various data filtering methods. The data filtering methods
allow us to obtain a high-quality dataset for training and validation purposes.
We then extract various wave-based features like wave heights, periods,
skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and
air temperature for the buoys. Afterward, we train algorithms that use LightGBM
and Extra Trees through a hv-block cross-validation scheme to forecast dominant
wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,
and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,
Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,
15-day ahead, and 30 day ahead prediction. In case of the test dataset,
LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and
30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day
ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both
training and the test dataset suggests that the machine learning models
developed in this paper are robust. Since the LightGBM algorithm outperforms ET
for all the windows tested, it is taken as the final algorithm. Note that the
performance of both methods does not decrease significantly as the forecast
horizon increases. Likewise, the proposed method outperforms the numerical
approaches included in this paper in the test dataset. For 1 day ahead
prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,
0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre
for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the
other methods in the test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04217</id>
        <link href="http://arxiv.org/abs/2103.04217"/>
        <updated>2021-07-15T01:59:04.186Z</updated>
        <summary type="html"><![CDATA[We study low-rank parameterizations of weight matrices with embedded spectral
properties in the Deep Learning context. The low-rank property leads to
parameter efficiency and permits taking computational shortcuts when computing
mappings. Spectral properties are often subject to constraints in optimization
problems, leading to better models and stability of optimization. We start by
looking at the compact SVD parameterization of weight matrices and identifying
redundancy sources in the parameterization. We further apply the Tensor Train
(TT) decomposition to the compact SVD components, and propose a non-redundant
differentiable parameterization of fixed TT-rank tensor manifolds, termed the
Spectral Tensor Train Parameterization (STTP). We demonstrate the effects of
neural network compression in the image classification setting and both
compression and improved training stability in the generative adversarial
training setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1"&gt;Alexander Liniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance Analysis on Machine Learning-Based Channel Estimation. (arXiv:1911.03886v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03886</id>
        <link href="http://arxiv.org/abs/1911.03886"/>
        <updated>2021-07-15T01:59:04.178Z</updated>
        <summary type="html"><![CDATA[Recently, machine learning-based channel estimation has attracted much
attention. The performance of machine learning-based estimation has been
validated by simulation experiments. However, little attention has been paid to
the theoretical performance analysis. In this paper, we investigate the mean
square error (MSE) performance of machine learning-based estimation. Hypothesis
testing is employed to analyze its MSE upper bound. Furthermore, we build a
statistical model for hypothesis testing, which holds when the linear learning
module with a low input dimension is used in machine learning-based channel
estimation, and derive a clear analytical relation between the size of the
training data and performance. Then, we simulate the machine learning-based
channel estimation in orthogonal frequency division multiplexing (OFDM) systems
to verify our analysis results. Finally, the design considerations for the
situation where only limited training data is available are discussed. In this
situation, our analysis results can be applied to assess the performance and
support the design of machine learning-based channel estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mei_K/0/1/0/all/0/1"&gt;Kai Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaochen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajatheva_N/0/1/0/all/0/1"&gt;Nandana Rajatheva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jibo Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Behavior of Adversarial Training in Binary Classification. (arXiv:2010.13275v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13275</id>
        <link href="http://arxiv.org/abs/2010.13275"/>
        <updated>2021-07-15T01:59:04.172Z</updated>
        <summary type="html"><![CDATA[It has been consistently reported that many machine learning models are
susceptible to adversarial attacks i.e., small additive adversarial
perturbations applied to data points can cause misclassification. Adversarial
training using empirical risk minimization is considered to be the
state-of-the-art method for defense against adversarial attacks. Despite being
successful in practice, several problems in understanding generalization
performance of adversarial training remain open. In this paper, we derive
precise theoretical predictions for the performance of adversarial training in
binary classification. We consider the high-dimensional regime where the
dimension of data grows with the size of the training data-set at a constant
ratio. Our results provide exact asymptotics for standard and adversarial test
errors of the estimators obtained by adversarial training with $\ell_q$-norm
bounded perturbations ($q \ge 1$) for both discriminative binary models and
generative Gaussian-mixture models with correlated features. Furthermore, we
use these sharp predictions to uncover several intriguing observations on the
role of various parameters including the over-parameterization ratio, the data
model, and the attack budget on the adversarial and standard errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Taheri_H/0/1/0/all/0/1"&gt;Hossein Taheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pedarsani_R/0/1/0/all/0/1"&gt;Ramtin Pedarsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1"&gt;Christos Thrampoulidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks. (arXiv:2107.06661v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06661</id>
        <link href="http://arxiv.org/abs/2107.06661"/>
        <updated>2021-07-15T01:59:04.166Z</updated>
        <summary type="html"><![CDATA[In high-dimensional state spaces, the usefulness of Reinforcement Learning
(RL) is limited by the problem of exploration. This issue has been addressed
using potential-based reward shaping (PB-RS) previously. In the present work,
we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the
strict optimality guarantees of PB-RS to a guarantee of preserved long-term
behavior. Being less restrictive, FV-RS allows for reward shaping functions
that are even better suited for improving the sample efficiency of RL
algorithms. In particular, we consider settings in which the agent has access
to an approximate plan. Here, we use examples of simulated robotic manipulation
tasks to demonstrate that plan-based FV-RS can indeed significantly improve the
sample efficiency of RL over plan-based PB-RS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_I/0/1/0/all/0/1"&gt;Ingmar Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_O/0/1/0/all/0/1"&gt;Ozgur S. Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1"&gt;Marc Toussaint&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU. (arXiv:2107.06433v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06433</id>
        <link href="http://arxiv.org/abs/2107.06433"/>
        <updated>2021-07-15T01:59:04.149Z</updated>
        <summary type="html"><![CDATA[The Word Movers Distance (WMD) measures the semantic dissimilarity between
two text documents by computing the cost of optimally moving all words of a
source/query document to the most similar words of a target document. Computing
WMD between two documents is costly because it requires solving an optimization
problem that costs $O (V^3 \log(V)) $ where $V$ is the number of unique words
in the document. Fortunately, WMD can be framed as an Earth Mover's Distance
(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding
an entropy penalty to the optimization problem and solving it using the
Sinkhorn-Knopp algorithm. Additionally, the computation can be made highly
parallel by computing the WMD of a single query document against multiple
target documents at once, for example by finding whether a given tweet is
similar to any other tweets of a given day.

In this paper, we first present a shared-memory parallel Sinkhorn-Knopp
algorithm to compute the WMD of one document against many other documents by
adopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the
original $O(V^2)$ dense compute-heavy version into an equivalent sparse one
which is mapped onto the new Intel Programmable Integrated Unified Memory
Architecture (PIUMA) system. The WMD parallel implementation achieves 67x
speedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We
also show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on
Sinkhorn-WMD and also provide better strong scaling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1"&gt;Jesmin Jahan Tithi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_F/0/1/0/all/0/1"&gt;Fabrizio Petrini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Large Language Models Trained on Code. (arXiv:2107.03374v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03374</id>
        <link href="http://arxiv.org/abs/2107.03374"/>
        <updated>2021-07-15T01:59:04.143Z</updated>
        <summary type="html"><![CDATA[We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mark Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tworek_J/0/1/0/all/0/1"&gt;Jerry Tworek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_H/0/1/0/all/0/1"&gt;Heewoo Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Q/0/1/0/all/0/1"&gt;Qiming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_H/0/1/0/all/0/1"&gt;Henrique Ponde de Oliveira Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1"&gt;Jared Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_H/0/1/0/all/0/1"&gt;Harri Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burda_Y/0/1/0/all/0/1"&gt;Yuri Burda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_N/0/1/0/all/0/1"&gt;Nicholas Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockman_G/0/1/0/all/0/1"&gt;Greg Brockman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1"&gt;Alex Ray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1"&gt;Raul Puri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1"&gt;Gretchen Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrov_M/0/1/0/all/0/1"&gt;Michael Petrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khlaaf_H/0/1/0/all/0/1"&gt;Heidy Khlaaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_G/0/1/0/all/0/1"&gt;Girish Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1"&gt;Pamela Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1"&gt;Brooke Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_S/0/1/0/all/0/1"&gt;Scott Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryder_N/0/1/0/all/0/1"&gt;Nick Ryder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlov_M/0/1/0/all/0/1"&gt;Mikhail Pavlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1"&gt;Alethea Power&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1"&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bavarian_M/0/1/0/all/0/1"&gt;Mohammad Bavarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tillet_P/0/1/0/all/0/1"&gt;Philippe Tillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Such_F/0/1/0/all/0/1"&gt;Felipe Petroski Such&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1"&gt;Dave Cummings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plappert_M/0/1/0/all/0/1"&gt;Matthias Plappert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chantzis_F/0/1/0/all/0/1"&gt;Fotios Chantzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_E/0/1/0/all/0/1"&gt;Elizabeth Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herbert_Voss_A/0/1/0/all/0/1"&gt;Ariel Herbert-Voss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guss_W/0/1/0/all/0/1"&gt;William Hebgen Guss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1"&gt;Alex Nichol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paino_A/0/1/0/all/0/1"&gt;Alex Paino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tezak_N/0/1/0/all/0/1"&gt;Nikolas Tezak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuschkin_I/0/1/0/all/0/1"&gt;Igor Babuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1"&gt;Suchir Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shantanu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1"&gt;William Saunders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1"&gt;Christopher Hesse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carr_A/0/1/0/all/0/1"&gt;Andrew N. Carr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1"&gt;Jan Leike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achiam_J/0/1/0/all/0/1"&gt;Josh Achiam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1"&gt;Vedant Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morikawa_E/0/1/0/all/0/1"&gt;Evan Morikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1"&gt;Alec Radford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1"&gt;Matthew Knight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1"&gt;Miles Brundage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murati_M/0/1/0/all/0/1"&gt;Mira Murati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1"&gt;Katie Mayer&lt;/a&gt;, et al. (6 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Round Active Learning. (arXiv:2107.06703v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06703</id>
        <link href="http://arxiv.org/abs/2107.06703"/>
        <updated>2021-07-15T01:59:04.135Z</updated>
        <summary type="html"><![CDATA[Active learning (AL) aims at reducing labeling effort by identifying the most
valuable unlabeled data points from a large pool. Traditional AL frameworks
have two limitations: First, they perform data selection in a multi-round
manner, which is time-consuming and impractical. Second, they usually assume
that there are a small amount of labeled data points available in the same
domain as the data in the unlabeled pool. Recent work proposes a solution for
one-round active learning based on data utility learning and optimization,
which fixes the first issue but still requires the initially labeled data
points in the same domain. In this paper, we propose $\mathrm{D^2ULO}$ as a
solution that solves both issues. Specifically, $\mathrm{D^2ULO}$ leverages the
idea of domain adaptation (DA) to train a data utility model which can
effectively predict the utility for any given unlabeled data in the target
domain once labeled. The trained data utility model can then be used to select
high-utility data and at the same time, provide an estimate for the utility of
the selected data. Our algorithm does not rely on any feedback from annotators
in the target domain and hence, can be used to perform zero-round active
learning or warm-start existing multi-round active learning strategies. Our
experiments show that $\mathrm{D^2ULO}$ outperforms the existing
state-of-the-art AL strategies equipped with domain adaptation over various
domain shift settings (e.g., real-to-real data and synthetic-to-real data).
Particularly, $\mathrm{D^2ULO}$ are applicable to the scenario where source and
target labels have mismatches, which is not supported by the existing works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15158</id>
        <link href="http://arxiv.org/abs/2106.15158"/>
        <updated>2021-07-15T01:59:04.130Z</updated>
        <summary type="html"><![CDATA[As communication systems are foreseen to enable new services such as joint
communication and sensing and utilize parts of the sub-THz spectrum, the design
of novel waveforms that can support these emerging applications becomes
increasingly challenging. We present in this work an end-to-end learning
approach to design waveforms through joint learning of pulse shaping and
constellation geometry, together with a neural network (NN)-based receiver.
Optimization is performed to maximize an achievable information rate, while
satisfying constraints on out-of-band emission and power envelope. Our results
show that the proposed approach enables up to orders of magnitude smaller
adjacent channel leakage ratios (ACLRs) with peak-to-average power ratios
(PAPRs) competitive with traditional filters, without significant loss of
information rate on an additive white Gaussian noise (AWGN) channel, and no
additional complexity at the transmitter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1"&gt;Fay&amp;#xe7;al Ait Aoudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1"&gt;Jakob Hoydis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental limits and algorithms for sparse linear regression with sublinear sparsity. (arXiv:2101.11156v4 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11156</id>
        <link href="http://arxiv.org/abs/2101.11156"/>
        <updated>2021-07-15T01:59:04.124Z</updated>
        <summary type="html"><![CDATA[We establish exact asymptotic expressions for the normalized mutual
information and minimum mean-square-error (MMSE) of sparse linear regression in
the sub-linear sparsity regime. Our result is achieved by a generalization of
the adaptive interpolation method in Bayesian inference for linear regimes to
sub-linear ones. A modification of the well-known approximate message passing
algorithm to approach the MMSE fundamental limit is also proposed, and its
state evolution is rigorously analyzed. Our results show that the traditional
linear assumption between the signal dimension and number of observations in
the replica and adaptive interpolation methods is not necessary for sparse
signals. They also show how to modify the existing well-known AMP algorithms
for linear regimes to sub-linear ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-07-15T01:59:04.103Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMutants: Training neural bug detectors with contextual mutations. (arXiv:2107.06657v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.06657</id>
        <link href="http://arxiv.org/abs/2107.06657"/>
        <updated>2021-07-15T01:59:04.096Z</updated>
        <summary type="html"><![CDATA[Learning-based bug detectors promise to find bugs in large code bases by
exploiting natural hints such as names of variables and functions or comments.
Still, existing techniques tend to underperform when presented with realistic
bugs. We believe bug detector learning to currently suffer from a lack of
realistic defective training examples. In fact, real world bugs are scarce
which has driven existing methods to train on artificially created and mostly
unrealistic mutants. In this work, we propose a novel contextual mutation
operator which incorporates knowledge about the mutation context to dynamically
inject natural and more realistic faults into code. Our approach employs a
masked language model to produce a context-dependent distribution over feasible
token replacements. The evaluation shows that sampling from a language model
does not only produce mutants which more accurately represent real bugs but
also lead to better performing bug detectors, both on artificial benchmarks and
on real world source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1"&gt;Cedric Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehrheim_H/0/1/0/all/0/1"&gt;Heike Wehrheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalisation in Neural Networks Does not Require Feature Overlap. (arXiv:2107.06872v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06872</id>
        <link href="http://arxiv.org/abs/2107.06872"/>
        <updated>2021-07-15T01:59:04.091Z</updated>
        <summary type="html"><![CDATA[That shared features between train and test data are required for
generalisation in artificial neural networks has been a common assumption of
both proponents and critics of these models. Here, we show that convolutional
architectures avoid this limitation by applying them to two well known
challenges, based on learning the identity function and learning rules
governing sequences of words. In each case, successful performance on the test
set requires generalising to features that were not present in the training
data, which is typically not feasible for standard connectionist models.
However, our experiments demonstrate that neural networks can succeed on such
problems when they incorporate the weight sharing employed by convolutional
architectures. In the image processing domain, such architectures are intended
to reflect the symmetry under spatial translations of the natural world that
such images depict. We discuss the role of symmetry in the two tasks and its
connection to generalisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1"&gt;Jeff Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1"&gt;Jeffrey S. Bowers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05630</id>
        <link href="http://arxiv.org/abs/2103.05630"/>
        <updated>2021-07-15T01:59:04.085Z</updated>
        <summary type="html"><![CDATA[The rapid progress of photorealistic synthesis techniques has reached at a
critical point where the boundary between real and manipulated images starts to
blur. Thus, benchmarking and advancing digital forgery analysis have become a
pressing issue. However, existing face forgery datasets either have limited
diversity or only support coarse-grained analysis. To counter this emerging
threat, we construct the ForgeryNet dataset, an extremely large face forgery
dataset with unified annotations in image- and video-level data across four
tasks: 1) Image Forgery Classification, including two-way (real / fake),
three-way (real / fake with identity-replaced forgery approaches / fake with
identity-remained forgery approaches), and n-way (real and 15 respective
forgery approaches) classification. 2) Spatial Forgery Localization, which
segments the manipulated area of fake images compared to their corresponding
source real images. 3) Video Forgery Classification, which re-defines the
video-level forgery classification with manipulated frames in random positions.
This task is important because attackers in real world are free to manipulate
any target frame. and 4) Temporal Forgery Localization, to localize the
temporal segments which are manipulated. ForgeryNet is by far the largest
publicly available deep face forgery dataset in terms of data-scale (2.9
million images, 221,247 videos), manipulations (7 image-level approaches, 8
video-level approaches), perturbations (36 independent and more mixed
perturbations) and annotations (6.3 million classification labels, 2.9 million
manipulated area annotations and 221,247 temporal forgery segment labels). We
perform extensive benchmarking and studies of existing face forensics methods
and obtain several valuable observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yinan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1"&gt;Bei Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1"&gt;Guojun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1"&gt;Lu Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jing Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed functional-based gradient algorithms for off-policy reinforcement learning: A non-asymptotic viewpoint. (arXiv:2101.02137v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02137</id>
        <link href="http://arxiv.org/abs/2101.02137"/>
        <updated>2021-07-15T01:59:04.078Z</updated>
        <summary type="html"><![CDATA[We propose two policy gradient algorithms for solving the problem of control
in an off-policy reinforcement learning (RL) context. Both algorithms
incorporate a smoothed functional (SF) based gradient estimation scheme. The
first algorithm is a straightforward combination of importance sampling-based
off-policy evaluation with SF-based gradient estimation. The second algorithm,
inspired by the stochastic variance-reduced gradient (SVRG) algorithm,
incorporates variance reduction in the update iteration. For both algorithms,
we derive non-asymptotic bounds that establish convergence to an approximate
stationary point. From these results, we infer that the first algorithm
converges at a rate that is comparable to the well-known REINFORCE algorithm in
an off-policy RL context, while the second algorithm exhibits an improved rate
of convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1"&gt;Nithia Vijayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1"&gt;Prashanth L. A&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce Advertising. (arXiv:2106.03593v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03593</id>
        <link href="http://arxiv.org/abs/2106.03593"/>
        <updated>2021-07-15T01:59:04.062Z</updated>
        <summary type="html"><![CDATA[In e-commerce advertising, it is crucial to jointly consider various
performance metrics, e.g., user experience, advertiser utility, and platform
revenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be
suboptimal due to their fixed allocation rules to optimize a single performance
metric (e.g., revenue or social welfare). Recently, data-driven auctions,
learned directly from auction outcomes to optimize multiple performance
metrics, have attracted increasing research interests. However, the procedure
of auction mechanisms involves various discrete calculation operations, making
it challenging to be compatible with continuous optimization pipelines in
machine learning. In this paper, we design \underline{D}eep \underline{N}eural
\underline{A}uctions (DNAs) to enable end-to-end auction learning by proposing
a differentiable model to relax the discrete sorting operation, a key component
in auctions. We optimize the performance metrics by developing deep models to
efficiently extract contexts from auctions, providing rich features for auction
design. We further integrate the game theoretical conditions within the model
design, to guarantee the stability of the auctions. DNAs have been successfully
deployed in the e-commerce advertising system at Taobao. Experimental
evaluation results on both large-scale data set as well as online A/B test
demonstrated that DNAs significantly outperformed other mechanisms widely
adopted in industry.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhilin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhenzhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1"&gt;Hongtao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1"&gt;Da Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yiqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dagui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guihai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoqiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATTACC the Quadratic Bottleneck of Attention Layers. (arXiv:2107.06419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06419</id>
        <link href="http://arxiv.org/abs/2107.06419"/>
        <updated>2021-07-15T01:59:04.056Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms form the backbone of state-of-the-art machine learning
models for a variety of tasks. Deploying them on deep neural network (DNN)
accelerators, however, is prohibitively challenging especially under long
sequences. Operators in attention layers exhibit limited reuse and quadratic
growth in memory footprint, leading to severe memory-boundedness. This paper
introduces a new attention-tailored dataflow, termed FLAT, which leverages
operator fusion, loop-nest optimizations, and interleaved execution. It
increases the effective memory bandwidth by efficiently utilizing the
high-bandwidth, low-capacity on-chip buffer and thus achieves better run time
and compute resource utilization. We term FLAT-compatible accelerators ATTACC.
In our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of
energy reduction comparing to state-of-the-art edge and cloud accelerators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1"&gt;Sheng-Chun Kao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1"&gt;Suvinay Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_G/0/1/0/all/0/1"&gt;Gaurav Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tushar Krishna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disparity Between Batches as a Signal for Early Stopping. (arXiv:2107.06665v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06665</id>
        <link href="http://arxiv.org/abs/2107.06665"/>
        <updated>2021-07-15T01:59:04.050Z</updated>
        <summary type="html"><![CDATA[We propose a metric for evaluating the generalization ability of deep neural
networks trained with mini-batch gradient descent. Our metric, called gradient
disparity, is the $\ell_2$ norm distance between the gradient vectors of two
mini-batches drawn from the training set. It is derived from a probabilistic
upper bound on the difference between the classification errors over a given
mini-batch, when the network is trained on this mini-batch and when the network
is trained on another mini-batch of points sampled from the same dataset. We
empirically show that gradient disparity is a very promising early-stopping
criterion (i) when data is limited, as it uses all the samples for training and
(ii) when available data has noisy labels, as it signals overfitting better
than the validation data. Furthermore, we show in a wide range of experimental
settings that gradient disparity is strongly related to the generalization
error between the training and test sets, and that it is also very informative
about the level of label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Forouzesh_M/0/1/0/all/0/1"&gt;Mahsa Forouzesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1"&gt;Patrick Thiran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning. (arXiv:2107.06865v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06865</id>
        <link href="http://arxiv.org/abs/2107.06865"/>
        <updated>2021-07-15T01:59:04.044Z</updated>
        <summary type="html"><![CDATA[Biological spiking neurons with intrinsic dynamics underlie the powerful
representation and learning capabilities of the brain for processing multimodal
information in complex environments. Despite recent tremendous progress in
spiking neural networks (SNNs) for handling Euclidean-space tasks, it still
remains challenging to exploit SNNs in processing non-Euclidean-space data
represented by graph data, mainly due to the lack of effective modeling
framework and useful training techniques. Here we present a general spike-based
modeling framework that enables the direct training of SNNs for graph learning.
Through spatial-temporal unfolding for spiking data flows of node features, we
incorporate graph convolution filters into spiking dynamics and formalize a
synergistic learning paradigm. Considering the unique features of spike
representation and spiking dynamics, we propose a spatial-temporal feature
normalization (STFN) technique suitable for SNN to accelerate convergence. We
instantiate our methods into two spiking graph models, including graph
convolution SNNs and graph attention SNNs, and validate their performance on
three node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our
model can achieve comparable performance with the state-of-the-art graph neural
network (GNN) models with much lower computation costs, demonstrating great
benefits for the execution on neuromorphic hardware and prompting neuromorphic
applications in graphical scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingkun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yujie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Faqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jing Pei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Games: Multi-Agent Learning with Side Information. (arXiv:2107.06327v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2107.06327</id>
        <link href="http://arxiv.org/abs/2107.06327"/>
        <updated>2021-07-15T01:59:04.038Z</updated>
        <summary type="html"><![CDATA[We formulate the novel class of contextual games, a type of repeated games
driven by contextual information at each round. By means of kernel-based
regularity assumptions, we model the correlation between different contexts and
game outcomes and propose a novel online (meta) algorithm that exploits such
correlations to minimize the contextual regret of individual players. We define
game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and
optimal contextual welfare for this new class of games and show that c-CCEs and
optimal welfare can be approached whenever players' contextual regrets vanish.
Finally, we empirically validate our results in a traffic routing experiment,
where our algorithm leads to better performance and higher welfare compared to
baselines that do not exploit the available contextual information or the
correlations present in the game.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sessa_P/0/1/0/all/0/1"&gt;Pier Giuseppe Sessa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1"&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamgarpour_M/0/1/0/all/0/1"&gt;Maryam Kamgarpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06456</id>
        <link href="http://arxiv.org/abs/2107.06456"/>
        <updated>2021-07-15T01:59:04.021Z</updated>
        <summary type="html"><![CDATA[We propose an AID-purifier that can boost the robustness of
adversarially-trained networks by purifying their inputs. AID-purifier is an
auxiliary network that works as an add-on to an already trained main
classifier. To keep it computationally light, it is trained as a discriminator
with a binary cross-entropy loss. To obtain additionally useful information
from the adversarial examples, the architecture design is closely related to
information maximization principles where two layers of the main classification
network are piped to the auxiliary network. To assist the iterative
optimization procedure of purification, the auxiliary network is trained with
AVmixup. AID-purifier can be used together with other purifiers such as
PixelDefend for an extra enhancement. The overall results indicate that the
best performing adversarially-trained networks can be enhanced by the best
performing purification networks, where AID-purifier is a competitive candidate
that is light and robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Duhun Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunjung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-time simulations with high fidelity on quantum hardware. (arXiv:2102.04313v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04313</id>
        <link href="http://arxiv.org/abs/2102.04313"/>
        <updated>2021-07-15T01:59:04.014Z</updated>
        <summary type="html"><![CDATA[Moderate-size quantum computers are now publicly accessible over the cloud,
opening the exciting possibility of performing dynamical simulations of quantum
systems. However, while rapidly improving, these devices have short coherence
times, limiting the depth of algorithms that may be successfully implemented.
Here we demonstrate that, despite these limitations, it is possible to
implement long-time, high fidelity simulations on current hardware.
Specifically, we simulate an XY-model spin chain on the Rigetti and IBM quantum
computers, maintaining a fidelity of at least 0.9 for over 600 time steps. This
is a factor of 150 longer than is possible using the iterated Trotter method.
Our simulations are performed using a new algorithm that we call the fixed
state Variational Fast Forwarding (fsVFF) algorithm. This algorithm decreases
the circuit depth and width required for a quantum simulation by finding an
approximate diagonalization of a short time evolution unitary. Crucially, fsVFF
only requires finding a diagonalization on the subspace spanned by the initial
state, rather than on the total Hilbert space as with previous methods,
substantially reducing the required resources. We further demonstrate the
viability of fsVFF through large numerical implementations of the algorithm, as
well as an analysis of its noise resilience and the scaling of simulation
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gibbs_J/0/1/0/all/0/1"&gt;Joe Gibbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gili_K/0/1/0/all/0/1"&gt;Kaitlin Gili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Holmes_Z/0/1/0/all/0/1"&gt;Zo&amp;#xeb; Holmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Commeau_B/0/1/0/all/0/1"&gt;Benjamin Commeau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Arrasmith_A/0/1/0/all/0/1"&gt;Andrew Arrasmith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Cincio_L/0/1/0/all/0/1"&gt;Lukasz Cincio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1"&gt;Patrick J. Coles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1"&gt;Andrew Sornborger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Write Thrice: Creating Documents, Computational Notebooks and Presentations From a Single Source. (arXiv:2107.06639v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2107.06639</id>
        <link href="http://arxiv.org/abs/2107.06639"/>
        <updated>2021-07-15T01:59:04.007Z</updated>
        <summary type="html"><![CDATA[Academic trade requires juggling multiple variants of the same content
published in different formats: manuscripts, presentations, posters and
computational notebooks. The need to track versions to accommodate for the
write--review--rebut--revise life-cycle adds another layer of complexity. We
propose to significantly reduce this burden by maintaining a single source
document in a version-controlled environment (such as git), adding
functionality to generate a collection of output formats popular in academia.
To this end, we utilise various open-source tools from the Jupyter scientific
computing ecosystem and operationalise selected software engineering concepts.
We offer a proof-of-concept workflow that composes Jupyter Book (an online
document), Jupyter Notebook (a computational narrative) and reveal.js slides
from a single markdown source file. Hosted on GitHub, our approach supports
change tracking and versioning, as well as a transparent review process based
on the underlying code issue management infrastructure. An exhibit of our
workflow can be previewed at https://so-cool.github.io/you-only-write-thrice/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1"&gt;Kacper Sokol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flach_P/0/1/0/all/0/1"&gt;Peter Flach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Parallel Model Selection for Deep Learning Systems. (arXiv:2107.06469v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06469</id>
        <link href="http://arxiv.org/abs/2107.06469"/>
        <updated>2021-07-15T01:59:03.992Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes more expensive, both in terms of time and compute,
inefficiencies in machine learning (ML) training prevent practical usage of
state-of-the-art models for most users. The newest model architectures are
simply too large to be fit onto a single processor. To address the issue, many
ML practitioners have turned to model parallelism as a method of distributing
the computational requirements across several devices. Unfortunately, the
sequential nature of neural networks causes very low efficiency and device
utilization in model parallel training jobs. We propose a new form of "shard
parallelism" combining task and model parallelism, then package it into a
framework we name Hydra. Hydra recasts the problem of model parallelism in the
multi-model context to produce a fine-grained parallel workload of independent
model shards, rather than independent models. This new parallel design promises
dramatic speedups relative to the traditional model parallelism paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagrecha_K/0/1/0/all/0/1"&gt;Kabir Nagrecha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interval Universal Approximation for Neural Networks. (arXiv:2007.06093v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06093</id>
        <link href="http://arxiv.org/abs/2007.06093"/>
        <updated>2021-07-15T01:59:03.984Z</updated>
        <summary type="html"><![CDATA[To verify safety and robustness of neural networks, researchers have
successfully applied abstract interpretation, primarily using the interval
abstract domain. In this paper, we study the theoretical power and limits of
the interval domain for neural-network verification.

First, we introduce the interval universal approximation (IUA) theorem. IUA
shows that neural networks not only can approximate any continuous function $f$
(universal approximation) as we have known for decades, but we can find a
neural network, using any well-behaved activation function, whose interval
bounds are an arbitrarily close approximation of the set semantics of $f$ (the
result of applying $f$ to a set of inputs). We call this notion of
approximation interval approximation. Our theorem generalizes the recent result
of Baader et al. (2020) from ReLUs to a rich class of activation functions that
we call squashable functions. Additionally, the IUA theorem implies that we can
always construct provably robust neural networks under $\ell_\infty$-norm using
almost any practical activation function.

Second, we study the computational complexity of constructing neural networks
that are amenable to precise interval analysis. This is a crucial question, as
our constructive proof of IUA is exponential in the size of the approximation
domain. We boil this question down to the problem of approximating the range of
a neural network with squashable activation functions. We show that the range
approximation problem (RA) is a $\Delta_2$-intermediate problem, which is
strictly harder than $\mathsf{NP}$-complete problems, assuming
$\mathsf{coNP}\not\subset \mathsf{NP}$. As a result, IUA is an inherently hard
problem: No matter what abstract domain or computational tools we consider to
achieve interval approximation, there is no efficient construction of such a
universal approximator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1"&gt;Aws Albarghouthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakriya_G/0/1/0/all/0/1"&gt;Gautam Prakriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00904</id>
        <link href="http://arxiv.org/abs/2102.00904"/>
        <updated>2021-07-15T01:59:03.938Z</updated>
        <summary type="html"><![CDATA[In this paper, we studied whether models based on BiLSTM and BERT can predict
hashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a
sizable financial impact on Ecommerce. We processed a corpus of Ecommerce
reviews as inputs, and predicted hashtags as outputs. We evaluated the results
using four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A
word cloud was used as a qualitative metric. While all computer-generated
metrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results
produced amazing scores. We concluded that the texts predicted by the neural
networks are very promising for use as hashtags for products on Ecommerce
websites. The code for this work is available at
https://github.com/augustocamargo/text-to-hashtag.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1"&gt;Augusto Camargo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wesley Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1"&gt;Felipe Peressim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1"&gt;Alan Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1"&gt;Marcelo Finger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higgs Boson Classification: Brain-inspired BCPNN Learning with StreamBrain. (arXiv:2107.06676v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06676</id>
        <link href="http://arxiv.org/abs/2107.06676"/>
        <updated>2021-07-15T01:59:03.932Z</updated>
        <summary type="html"><![CDATA[One of the most promising approaches for data analysis and exploration of
large data sets is Machine Learning techniques that are inspired by brain
models. Such methods use alternative learning rules potentially more
efficiently than established learning rules. In this work, we focus on the
potential of brain-inspired ML for exploiting High-Performance Computing (HPC)
resources to solve ML problems: we discuss the BCPNN and an HPC implementation,
called StreamBrain, its computational cost, suitability to HPC systems. As an
example, we use StreamBrain to analyze the Higgs Boson dataset from High Energy
Physics and discriminate between background and signal classes in collisions of
high-energy particle colliders. Overall, we reach up to 69.15% accuracy and
76.4% Area Under the Curve (AUC) performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Svedin_M/0/1/0/all/0/1"&gt;Martin Svedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podobas_A/0/1/0/all/0/1"&gt;Artur Podobas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Steven W. D. Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markidis_S/0/1/0/all/0/1"&gt;Stefano Markidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TEACHING -- Trustworthy autonomous cyber-physical applications through human-centred intelligence. (arXiv:2107.06543v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06543</id>
        <link href="http://arxiv.org/abs/2107.06543"/>
        <updated>2021-07-15T01:59:03.922Z</updated>
        <summary type="html"><![CDATA[This paper discusses the perspective of the H2020 TEACHING project on the
next generation of autonomous applications running in a distributed and highly
heterogeneous environment comprising both virtual and physical resources
spanning the edge-cloud continuum. TEACHING puts forward a human-centred vision
leveraging the physiological, emotional, and cognitive state of the users as a
driver for the adaptation and optimization of the autonomous applications. It
does so by building a distributed, embedded and federated learning system
complemented by methods and tools to enforce its dependability, security and
privacy preservation. The paper discusses the main concepts of the TEACHING
approach and singles out the main AI-related research challenges associated
with it. Further, we provide a discussion of the design choices for the
TEACHING system to tackle the aforementioned challenges]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akarmazyan_S/0/1/0/all/0/1"&gt;Siranush Akarmazyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armengaud_E/0/1/0/all/0/1"&gt;Eric Armengaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacco_M/0/1/0/all/0/1"&gt;Manlio Bacco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravos_G/0/1/0/all/0/1"&gt;George Bravos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_C/0/1/0/all/0/1"&gt;Calogero Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_E/0/1/0/all/0/1"&gt;Emanuele Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassara_P/0/1/0/all/0/1"&gt;Pietro Cassara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coppola_M/0/1/0/all/0/1"&gt;Massimo Coppola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davalas_C/0/1/0/all/0/1"&gt;Charalampos Davalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dazzi_P/0/1/0/all/0/1"&gt;Patrizio Dazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Degennaro_M/0/1/0/all/0/1"&gt;Maria Carmela Degennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarli_D/0/1/0/all/0/1"&gt;Daniele Di Sarli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobaj_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Dobaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1"&gt;Claudio Gallicchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girbal_S/0/1/0/all/0/1"&gt;Sylvain Girbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotta_A/0/1/0/all/0/1"&gt;Alberto Gotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groppo_R/0/1/0/all/0/1"&gt;Riccardo Groppo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macher_G/0/1/0/all/0/1"&gt;Georg Macher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazzei_D/0/1/0/all/0/1"&gt;Daniele Mazzei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mencagli_G/0/1/0/all/0/1"&gt;Gabriele Mencagli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1"&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1"&gt;Alessio Micheli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroglio_R/0/1/0/all/0/1"&gt;Roberta Peroglio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petroni_S/0/1/0/all/0/1"&gt;Salvatore Petroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potenza_R/0/1/0/all/0/1"&gt;Rosaria Potenza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pourdanesh_F/0/1/0/all/0/1"&gt;Farank Pourdanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardianos_C/0/1/0/all/0/1"&gt;Christos Sardianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1"&gt;Konstantinos Tserpes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabo_F/0/1/0/all/0/1"&gt;Fulvio Tagliab&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valtl_J/0/1/0/all/0/1"&gt;Jakob Valtl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varlamis_I/0/1/0/all/0/1"&gt;Iraklis Varlamis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veledar_O/0/1/0/all/0/1"&gt;Omar Veledar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Referring Transformer: A One-step Approach to Multi-task Visual Grounding. (arXiv:2106.03089v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03089</id>
        <link href="http://arxiv.org/abs/2106.03089"/>
        <updated>2021-07-15T01:59:03.915Z</updated>
        <summary type="html"><![CDATA[As an important step towards visual reasoning, visual grounding (e.g., phrase
localization, referring expression comprehension/segmentation) has been widely
explored Previous approaches to referring expression comprehension (REC) or
segmentation (RES) either suffer from limited performance, due to a two-stage
setup, or require the designing of complex task-specific one-stage
architectures. In this paper, we propose a simple one-stage multi-task
framework for visual grounding tasks. Specifically, we leverage a transformer
architecture, where two modalities are fused in a visual-lingual encoder. In
the decoder, the model learns to generate contextualized lingual queries which
are then decoded and used to directly regress the bounding box and produce a
segmentation mask for the corresponding referred regions. With this simple but
highly contextualized model, we outperform state-of-the-arts methods by a large
margin on both REC and RES tasks. We also show that a simple pre-training
schedule (on an external dataset) further improves the performance. Extensive
experiments and ablations illustrate that our model benefits greatly from
contextualized information and multi-task training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Muchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many-to-Many Voice Conversion based Feature Disentanglement using Variational Autoencoder. (arXiv:2107.06642v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.06642</id>
        <link href="http://arxiv.org/abs/2107.06642"/>
        <updated>2021-07-15T01:59:03.909Z</updated>
        <summary type="html"><![CDATA[Voice conversion is a challenging task which transforms the voice
characteristics of a source speaker to a target speaker without changing
linguistic content. Recently, there have been many works on many-to-many Voice
Conversion (VC) based on Variational Autoencoder (VAEs) achieving good results,
however, these methods lack the ability to disentangle speaker identity and
linguistic content to achieve good performance on unseen speaker scenarios. In
this paper, we propose a new method based on feature disentanglement to tackle
many to many voice conversion. The method has the capability to disentangle
speaker identity and linguistic content from utterances, it can convert from
many source speakers to many target speakers with a single autoencoder network.
Moreover, it naturally deals with the unseen target speaker scenarios. We
perform both objective and subjective evaluations to show the competitive
performance of our proposed method compared with other state-of-the-art models
in terms of naturalness and target speaker similarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luong_M/0/1/0/all/0/1"&gt;Manh Luong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1"&gt;Viet Anh Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oblivious sketching for logistic regression. (arXiv:2107.06615v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06615</id>
        <link href="http://arxiv.org/abs/2107.06615"/>
        <updated>2021-07-15T01:59:03.883Z</updated>
        <summary type="html"><![CDATA[What guarantees are possible for solving logistic regression in one pass over
a data stream? To answer this question, we present the first data oblivious
sketch for logistic regression. Our sketch can be computed in input sparsity
time over a turnstile data stream and reduces the size of a $d$-dimensional
data set from $n$ to only $\operatorname{poly}(\mu d\log n)$ weighted points,
where $\mu$ is a useful parameter which captures the complexity of compressing
the data. Solving (weighted) logistic regression on the sketch gives an $O(\log
n)$-approximation to the original problem on the full data set. We also show
how to obtain an $O(1)$-approximation with slight modifications. Our sketches
are fast, simple, easy to implement, and our experiments demonstrate their
practicality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munteanu_A/0/1/0/all/0/1"&gt;Alexander Munteanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omlor_S/0/1/0/all/0/1"&gt;Simon Omlor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for Practical Measures. (arXiv:2107.06626v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2107.06626</id>
        <link href="http://arxiv.org/abs/2107.06626"/>
        <updated>2021-07-15T01:59:03.876Z</updated>
        <summary type="html"><![CDATA[It is well known that the Johnson-Lindenstrauss dimensionality reduction
method is optimal for worst case distortion. While in practice many other
methods and heuristics are used, not much is known in terms of bounds on their
performance. The question of whether the JL method is optimal for practical
measures of distortion was recently raised in \cite{BFN19} (NeurIPS'19). They
provided upper bounds on its quality for a wide range of practical measures and
showed that indeed these are best possible in many cases. Yet, some of the most
important cases, including the fundamental case of average distortion were left
open. In particular, they show that the JL transform has $1+\epsilon$ average
distortion for embedding into $k$-dimensional Euclidean space, where
$k=O(1/\eps^2)$, and for more general $q$-norms of distortion, $k =
O(\max\{1/\eps^2,q/\eps\})$, whereas tight lower bounds were established only
for large values of $q$ via reduction to the worst case.

In this paper we prove that these bounds are best possible for any
dimensionality reduction method, for any $1 \leq q \leq O(\frac{\log (2\eps^2
n)}{\eps})$ and $\epsilon \geq \frac{1}{\sqrt{n}}$, where $n$ is the size of
the subset of Euclidean space.

Our results imply that the JL method is optimal for various distortion
measures commonly used in practice, such as {\it stress, energy} and {\it
relative error}. We prove that if any of these measures is bounded by $\eps$
then $k=\Omega(1/\eps^2)$, for any $\epsilon \geq \frac{1}{\sqrt{n}}$, matching
the upper bounds of \cite{BFN19} and extending their tightness results for the
full range moment analysis.

Our results may indicate that the JL dimensionality reduction method should
be considered more often in practical applications, and the bounds we provide
for its quality should be served as a measure for comparison when evaluating
the performance of other methods and heuristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartal_Y/0/1/0/all/0/1"&gt;Yair Bartal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fandina_O/0/1/0/all/0/1"&gt;Ora Nova Fandina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1"&gt;Kasper Green Larsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnability of Learning Performance and Its Application to Data Valuation. (arXiv:2107.06336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06336</id>
        <link href="http://arxiv.org/abs/2107.06336"/>
        <updated>2021-07-15T01:59:03.863Z</updated>
        <summary type="html"><![CDATA[For most machine learning (ML) tasks, evaluating learning performance on a
given dataset requires intensive computation. On the other hand, the ability to
efficiently estimate learning performance may benefit a wide spectrum of
applications, such as active learning, data quality management, and data
valuation. Recent empirical studies show that for many common ML models, one
can accurately learn a parametric model that predicts learning performance for
any given input datasets using a small amount of samples. However, the
theoretical underpinning of the learnability of such performance prediction
models is still missing. In this work, we develop the first theoretical
analysis of the ML performance learning problem. We propose a relaxed notion
for submodularity that can well describe the behavior of learning performance
as a function of input datasets. We give a learning algorithm that achieves a
constant-factor approximation under certain assumptions. Further, we give a
learning algorithm that achieves arbitrarily small error based on a newly
derived structural result. We then discuss a natural, important use case of
learning performance learning -- data valuation, which is known to suffer
computational challenges due to the requirement of estimating learning
performance for many data combinations. We show that performance learning can
significantly improve the accuracy of data valuation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distance-based Hyperspherical Classification for Multi-source Open-Set Domain Adaptation. (arXiv:2107.02067v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02067</id>
        <link href="http://arxiv.org/abs/2107.02067"/>
        <updated>2021-07-15T01:59:03.857Z</updated>
        <summary type="html"><![CDATA[Vision systems trained in closed-world scenarios will inevitably fail when
presented with new environmental conditions, new data distributions and novel
classes at deployment time. How to move towards open-world learning is a long
standing research question, but the existing solutions mainly focus on specific
aspects of the problem (single domain Open-Set, multi-domain Closed-Set), or
propose complex strategies which combine multiple losses and manually tuned
hyperparameters. In this work we tackle multi-source Open-Set domain adaptation
by introducing HyMOS: a straightforward supervised model that exploits the
power of contrastive learning and the properties of its hyperspherical feature
space to correctly predict known labels on the target, while rejecting samples
belonging to any unknown class. HyMOS includes a tailored data balancing to
enforce cross-source alignment and introduces style transfer among the instance
transformations of contrastive learning for source-target adaptation, avoiding
the risk of negative transfer. Finally a self-training strategy refines the
model without the need for handcrafted thresholds. We validate our method over
three challenging datasets and provide an extensive quantitative and
qualitative experimental analysis. The obtained results show that HyMOS
outperforms several Open-Set and universal domain adaptation approaches,
defining the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1"&gt;Silvia Bucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1"&gt;Francesco Cappio Borlino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1"&gt;Tatiana Tommasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer. (arXiv:2107.06573v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06573</id>
        <link href="http://arxiv.org/abs/2107.06573"/>
        <updated>2021-07-15T01:59:03.848Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks for language models like long short-term memory
(LSTM) have been utilized as a tool for modeling and predicting long term
dynamics of complex stochastic molecular systems. Recently successful examples
on learning slow dynamics by LSTM are given with simulation data of low
dimensional reaction coordinate. However, in this report we show that the
following three key factors significantly affect the performance of language
model learning, namely dimensionality of reaction coordinates, temporal
resolution and state partition. When applying recurrent neural networks to
molecular dynamics simulation trajectories of high dimensionality, we find that
rare events corresponding to the slow dynamics might be obscured by other
faster dynamics of the system, and cannot be efficiently learned. Under such
conditions, we find that coarse graining the conformational space into
metastable states and removing recrossing events when estimating transition
probabilities between states could greatly help improve the accuracy of slow
dynamics learning in molecular dynamics. Moreover, we also explore other models
like Transformer, which do not show superior performance than LSTM in
overcoming these issues. Therefore, to learn rare events of slow molecular
dynamics by LSTM and Transformer, it is critical to choose proper temporal
resolution (i.e., saving intervals of MD simulation trajectories) and state
partition in high resolution data, since deep neural network models might not
automatically disentangle slow dynamics from fast dynamics when both are
present in data influencing each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenqi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Siqin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuhui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:03.826Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IFedAvg: Interpretable Data-Interoperability for Federated Learning. (arXiv:2107.06580v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06580</id>
        <link href="http://arxiv.org/abs/2107.06580"/>
        <updated>2021-07-15T01:59:03.817Z</updated>
        <summary type="html"><![CDATA[Recently, the ever-growing demand for privacy-oriented machine learning has
motivated researchers to develop federated and decentralized learning
techniques, allowing individual clients to train models collaboratively without
disclosing their private datasets. However, widespread adoption has been
limited in domains relying on high levels of user trust, where assessment of
data compatibility is essential. In this work, we define and address low
interoperability induced by underlying client data inconsistencies in federated
learning for tabular data. The proposed method, iFedAvg, builds on federated
averaging adding local element-wise affine layers to allow for a personalized
and granular understanding of the collaborative learning process. Thus,
enabling the detection of outlier datasets in the federation and also learning
the compensation for local data distribution shifts without sharing any
original data. We evaluate iFedAvg using several public benchmarks and a
previously unstudied collection of real-world datasets from the 2014 - 2016
West African Ebola epidemic, jointly forming the largest such dataset in the
world. In all evaluations, iFedAvg achieves competitive average performance
with negligible overhead. It additionally shows substantial improvement on
outlier clients, highlighting increased robustness to individual dataset
shifts. Most importantly, our method provides valuable client-specific insights
at a fine-grained level to guide interoperable federated learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roschewitz_D/0/1/0/all/0/1"&gt;David Roschewitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_M/0/1/0/all/0/1"&gt;Mary-Anne Hartley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corinzia_L/0/1/0/all/0/1"&gt;Luca Corinzia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Jigsaw Learning for Cartoon Face Recognition. (arXiv:2107.06532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06532</id>
        <link href="http://arxiv.org/abs/2107.06532"/>
        <updated>2021-07-15T01:59:03.810Z</updated>
        <summary type="html"><![CDATA[Cartoon face recognition is challenging as they typically have smooth color
regions and emphasized edges, the key to recognize cartoon faces is to
precisely perceive their sparse and critical shape patterns. However, it is
quite difficult to learn a shape-oriented representation for cartoon face
recognition with convolutional neural networks (CNNs). To mitigate this issue,
we propose the GraphJigsaw that constructs jigsaw puzzles at various stages in
the classification network and solves the puzzles with the graph convolutional
network (GCN) in a progressive manner. Solving the puzzles requires the model
to spot the shape patterns of the cartoon faces as the texture information is
quite limited. The key idea of GraphJigsaw is constructing a jigsaw puzzle by
randomly shuffling the intermediate convolutional feature maps in the spatial
dimension and exploiting the GCN to reason and recover the correct layout of
the jigsaw fragments in a self-supervised manner. The proposed GraphJigsaw
avoids training the classification model with the deconstructed images that
would introduce noisy patterns and are harmful for the final classification.
Specially, GraphJigsaw can be incorporated at various stages in a top-down
manner within the classification model, which facilitates propagating the
learned shape patterns gradually. GraphJigsaw does not rely on any extra manual
annotation during the training process and incorporates no extra computation
burden at inference time. Both quantitative and qualitative experimental
results have verified the feasibility of our proposed GraphJigsaw, which
consistently outperforms other face recognition or jigsaw-based methods on two
popular cartoon face datasets with considerable improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lao_L/0/1/0/all/0/1"&gt;Lingjie Lao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style-Restricted GAN: Multi-Modal Translation with Style Restriction Using Generative Adversarial Networks. (arXiv:2105.07621v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07621</id>
        <link href="http://arxiv.org/abs/2105.07621"/>
        <updated>2021-07-15T01:59:03.804Z</updated>
        <summary type="html"><![CDATA[Unpaired image-to-image translation using Generative Adversarial Networks
(GAN) is successful in converting images among multiple domains. Moreover,
recent studies have shown a way to diversify the outputs of the generator.
However, since there are no restrictions on how the generator diversifies the
results, it is likely to translate some unexpected features. In this paper, we
propose Style-Restricted GAN (SRGAN) to demonstrate the importance of
controlling the encoded features used in style diversifying process. More
specifically, instead of KL divergence loss, we adopt three new losses to
restrict the distribution of the encoded features: batch KL divergence loss,
correlation loss, and histogram imitation loss. Further, the encoder is
pre-trained with classification tasks before being used in translation process.
The study reports quantitative as well as qualitative results with Precision,
Recall, Density, and Coverage. The proposed three losses lead to the
enhancement of the level of diversity compared to the conventional KL loss. In
particular, SRGAN is found to be successful in translating with higher
diversity and without changing the class-unrelated features in the CelebA face
dataset. To conclude, the importance of the encoded features being
well-regulated was proven with two experiments. Our implementation is available
at https://github.com/shinshoji01/Style-Restricted_GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inoue_S/0/1/0/all/0/1"&gt;Sho Inoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonsalves_T/0/1/0/all/0/1"&gt;Tad Gonsalves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06475</id>
        <link href="http://arxiv.org/abs/2107.06475"/>
        <updated>2021-07-15T01:59:03.790Z</updated>
        <summary type="html"><![CDATA[Understanding the strengths and weaknesses of machine learning (ML)
algorithms is crucial for determine their scope of application. Here, we
introduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of
synthetic datasets for comprehensive, reproducible, and interpretable
benchmarking of machine learning algorithms for classification of binary
outcomes. The DIGEN resource consists of 40 mathematical functions which map
continuous features to discrete endpoints for creating synthetic datasets.
These 40 functions were discovered using a heuristic algorithm designed to
maximize the diversity of performance among multiple popular machine learning
algorithms thus providing a useful test suite for evaluating and comparing new
methods. Access to the generative functions facilitates understanding of why a
method performs poorly compared to other algorithms thus providing ideas for
improvement. The resource with extensive documentation and analyses is
open-source and available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1"&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1"&gt;Jason H. Moore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks. (arXiv:2107.06405v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06405</id>
        <link href="http://arxiv.org/abs/2107.06405"/>
        <updated>2021-07-15T01:59:03.772Z</updated>
        <summary type="html"><![CDATA[We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the
agent's trajectory that improves the sample efficiency in sparse-reward MDPs.
We show that any optimal policy necessarily satisfies the k-SP constraint.
Notably, the k-SP constraint prevents the policy from exploring state-action
pairs along the non-k-SP trajectories (e.g., going back and forth). However, in
practice, excluding state-action pairs may hinder the convergence of RL
algorithms. To overcome this, we propose a novel cost function that penalizes
the policy violating SP constraint, instead of completely excluding it. Our
numerical experiment in a tabular RL setting demonstrates that the SP
constraint can significantly reduce the trajectory space of policy. As a
result, our constraint enables more sample efficient learning by suppressing
redundant exploration and exploitation. Our experiments on MiniGrid, DeepMind
Lab, Atari, and Fetch show that the proposed method significantly improves
proximal policy optimization (PPO) and outperforms existing novelty-seeking
exploration methods including count-based exploration even in continuous
control tasks, indicating that it improves the sample efficiency by preventing
the agent from taking redundant actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1"&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungtae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jongwook Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seijen_H/0/1/0/all/0/1"&gt;Harm van Seijen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1"&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06501</id>
        <link href="http://arxiv.org/abs/2107.06501"/>
        <updated>2021-07-15T01:59:03.766Z</updated>
        <summary type="html"><![CDATA[High-level representation-guided pixel denoising and adversarial training are
independent solutions to enhance the robustness of CNNs against adversarial
attacks by pre-processing input data and re-training models, respectively. Most
recently, adversarial training techniques have been widely studied and improved
while the pixel denoising-based method is getting less attractive. However, it
is still questionable whether there exists a more advanced pixel
denoising-based method and whether the combination of the two solutions
benefits each other. To this end, we first comprehensively investigate two
kinds of pixel denoising methods for adversarial robustness enhancement (i.e.,
existing additive-based and unexplored filtering-based methods) under the loss
functions of image-level and semantic-level restorations, respectively, showing
that pixel-wise filtering can obtain much higher image quality (e.g., higher
PSNR) as well as higher robustness (e.g., higher accuracy on adversarial
examples) than existing pixel-wise additive-based method. However, we also
observe that the robustness results of the filtering-based method rely on the
perturbation amplitude of adversarial examples used for training. To address
this problem, we propose predictive perturbation-aware pixel-wise filtering,
where dual-perturbation filtering and an uncertainty-aware fusion module are
designed and employed to automatically perceive the perturbation amplitude
during the training and testing process. The proposed method is termed as
AdvFilter. Moreover, we combine adversarial pixel denoising methods with three
adversarial training-based methods, hinting that considering data and models
jointly is able to achieve more robust CNNs. The experiments conduct on
NeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over
enhancing CNNs' robustness, high generalization to different models, and noise
levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1"&gt;Weikai Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;Geguang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06393</id>
        <link href="http://arxiv.org/abs/2107.06393"/>
        <updated>2021-07-15T01:59:03.756Z</updated>
        <summary type="html"><![CDATA[Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective
compositional scene understanding in images. Here, we propose Hybrid Memoised
Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid
discrete-continuous models. Prior approaches to learning suffer as they need to
perform repeated expensive inner-loop discrete inference. We build on a recent
approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by
memoising discrete variables, and extend it to allow for a principled and
effective way to handle continuous variables by learning a separate recognition
model used for importance-sampling based approximate inference and
marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene
understanding domains, and show that it outperforms current state-of-the-art
inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1"&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1"&gt;Luke Hewitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1"&gt;Kevin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Siddharth N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.06396</id>
        <link href="http://arxiv.org/abs/2107.06396"/>
        <updated>2021-07-15T01:59:03.748Z</updated>
        <summary type="html"><![CDATA[The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD
operated by the DLR Institute of Space Propulsion is a research platform that
allows the study of thermoacoustic instabilities under realistic conditions,
representative of small upper stage rocket engines. We use data from BKD
experimental campaigns in which the static chamber pressure and fuel-oxidizer
ratio are varied such that the first tangential mode of the combustor is
excited under some conditions. We train an autoregressive Bayesian neural
network model to forecast the amplitude of the dynamic pressure time series,
inputting multiple sensor measurements (injector pressure/ temperature
measurements, static chamber pressure, high-frequency dynamic pressure
measurements, high-frequency OH* chemiluminescence measurements) and future
flow rate control signals. The Bayesian nature of our algorithms allows us to
work with a dataset whose size is restricted by the expense of each
experimental run, without making overconfident extrapolations. We find that the
networks are able to accurately forecast the evolution of the pressure
amplitude and anticipate instability events on unseen experimental runs 500
milliseconds in advance. We compare the predictive accuracy of multiple models
using different combinations of sensor inputs. We find that the high-frequency
dynamic pressure signal is particularly informative. We also use the technique
of integrated gradients to interpret the influence of different sensor inputs
on the model prediction. The negative log-likelihood of data points in the test
dataset indicates that predictive uncertainties are well-characterized by our
Bayesian model and simulating a sensor failure event results as expected in a
dramatic increase in the epistemic component of the uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Waxenegger-Wilfing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1"&gt;Justin Hardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1"&gt;Matthew P. Juniper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Beyond Linear RL: Sample Efficient Neural Function Approximation. (arXiv:2107.06466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06466</id>
        <link href="http://arxiv.org/abs/2107.06466"/>
        <updated>2021-07-15T01:59:03.741Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (RL) powered by neural net approximation of the Q
function has had enormous empirical success. While the theory of RL has
traditionally focused on linear function approximation (or eluder dimension)
approaches, little is known about nonlinear RL with neural net approximations
of the Q functions. This is the focus of this work, where we study function
approximation with two-layer neural networks (considering both ReLU and
polynomial activation functions). Our first result is a computationally and
statistically efficient algorithm in the generative model setting under
completeness for two-layer neural networks. Our second result considers this
setting but under only realizability of the neural net function class. Here,
assuming deterministic dynamics, the sample complexity scales linearly in the
algebraic dimension. In all cases, our results significantly improve upon what
can be attained with linear (or eluder dimension) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectrum Gaussian Processes Based On Tunable Basis Functions. (arXiv:2107.06473v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.06473</id>
        <link href="http://arxiv.org/abs/2107.06473"/>
        <updated>2021-07-15T01:59:03.734Z</updated>
        <summary type="html"><![CDATA[Spectral approximation and variational inducing learning for the Gaussian
process are two popular methods to reduce computational complexity. However, in
previous research, those methods always tend to adopt the orthonormal basis
functions, such as eigenvectors in the Hilbert space, in the spectrum method,
or decoupled orthogonal components in the variational framework. In this paper,
inspired by quantum physics, we introduce a novel basis function, which is
tunable, local and bounded, to approximate the kernel function in the Gaussian
process. There are two adjustable parameters in these functions, which control
their orthogonality to each other and limit their boundedness. And we conduct
extensive experiments on open-source datasets to testify its performance.
Compared to several state-of-the-art methods, it turns out that the proposed
method can obtain satisfactory or even better results, especially with poorly
chosen kernel functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wenqi Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guanlin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ping_Y/0/1/0/all/0/1"&gt;Yang Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06796</id>
        <link href="http://arxiv.org/abs/2107.06796"/>
        <updated>2021-07-15T01:59:03.710Z</updated>
        <summary type="html"><![CDATA[Fake news is a problem faced by society in this era. It is not rare for fake
news to cause provocation and problem for the people. Indonesia, as a country
with the 4th largest population, has a problem in dealing with fake news. More
than 30% of rural and urban population are deceived by this fake news problem.
As we have been studying, there is only few literatures on preventing the
spread of fake news in Bahasa Indonesia. So, this research is conducted to
prevent these problems. The dataset used in this research was obtained from a
news portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on
this page, we got 1116 data consisting of valid news and fake news. The dataset
can be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This
dataset will be combined with other available datasets. The methods used are
CNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This
research shows that the BERT method with Transformer Network has the best
results with an accuracy of up to 90%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1"&gt;Aisyah Awalina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1"&gt;Jibran Fawaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1"&gt;Rifky Yunus Krisnabayu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-15T01:59:03.703Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction. (arXiv:2107.06511v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06511</id>
        <link href="http://arxiv.org/abs/2107.06511"/>
        <updated>2021-07-15T01:59:03.695Z</updated>
        <summary type="html"><![CDATA[Accurate capacitance extraction is becoming more important for designing
integrated circuits under advanced process technology. The pattern matching
based full-chip extraction methodology delivers fast computational speed, but
suffers from large error, and tedious efforts on building capacitance models of
the increasing structure patterns. In this work, we propose an effective method
for building convolutional neural network (CNN) based capacitance models
(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance
extraction. With a novel grid-based data representation, the proposed method is
able to model the pattern with a variable number of conductors, so that largely
reduce the number of patterns. Based on the ability of ResNet architecture on
capturing spatial information and the proposed training skills, the obtained
CNN-Cap exhibits much better performance over the multilayer perception neural
network based capacitance model while being more versatile. Extensive
experiments on a 55nm and a 15nm process technologies have demonstrated that
the error of total capacitance produced with CNN-Cap is always within 1.3% and
the error of produced coupling capacitance is less than 10% in over 99.5%
probability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU
server, while it consumes negligible memory compared to the look-up table based
capacitance model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dingcheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenjian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuanbo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Wenjie Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06481</id>
        <link href="http://arxiv.org/abs/2107.06481"/>
        <updated>2021-07-15T01:59:03.667Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning approach for the classification of
Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to
the availability of large annotated datasets and also enough computational
power in the form of GPUs, many deep learning-based solutions for object
classification have been proposed of late, especially in the domain of images
and graphical models. Nevertheless, very few solutions have been proposed for
the task of functional classification of CAD models. Hence, for this research,
CAD models have been collected from Engineering Shape Benchmark (ESB), National
Design Repository (NDR) and augmented with newer models created using a
modelling software to form a dataset - 'CADNET'. It is proposed to use a
residual network architecture for CADNET, inspired by the popular ResNet. A
weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature
extraction, and the generated images are fed as inputs to the CNN. The problem
of class imbalance in the dataset is addressed using a class weights approach.
Experiments have been conducted with other signatures such as geodesic distance
etc. using deep networks as well as other network architectures on the CADNET.
The LFD-based CNN approach using the proposed network architecture, along with
gradient boosting yielded the best classification accuracy on CADNET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1"&gt;Pranjal Bhaskare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Evaluation Methods for the Causal Effect of Recommendations. (arXiv:2107.06630v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06630</id>
        <link href="http://arxiv.org/abs/2107.06630"/>
        <updated>2021-07-15T01:59:03.658Z</updated>
        <summary type="html"><![CDATA[Evaluating the causal effect of recommendations is an important objective
because the causal effect on user interactions can directly leads to an
increase in sales and user engagement. To select an optimal recommendation
model, it is common to conduct A/B testing to compare model performance.
However, A/B testing of causal effects requires a large number of users, making
such experiments costly and risky. We therefore propose the first interleaving
methods that can efficiently compare recommendation models in terms of causal
effects. In contrast to conventional interleaving methods, we measure the
outcomes of both items on an interleaved list and items not on the interleaved
list, since the causal effect is the difference between outcomes with and
without recommendations. To ensure that the evaluations are unbiased, we either
select items with equal probability or weight the outcomes using inverse
propensity scores. We then verify the unbiasedness and efficiency of online
evaluation methods through simulated online experiments. The results indicate
that our proposed methods are unbiased and that they have superior efficiency
to A/B testing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sato_M/0/1/0/all/0/1"&gt;Masahiro Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration. (arXiv:2107.06629v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06629</id>
        <link href="http://arxiv.org/abs/2107.06629"/>
        <updated>2021-07-15T01:59:03.650Z</updated>
        <summary type="html"><![CDATA[In this work we present a general, two-stage reinforcement learning approach
for going from a single demonstration trajectory to a robust policy that can be
deployed on hardware without any additional training. The demonstration is used
in the first stage as a starting point to facilitate initial exploration. In
the second stage, the relevant task reward is optimized directly and a policy
robust to environment uncertainties is computed. We demonstrate and examine in
detail performance and robustness of our approach on highly dynamic hopping and
bounding tasks on a real quadruped robot.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogdanovic_M/0/1/0/all/0/1"&gt;Miroslav Bogdanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khadiv_M/0/1/0/all/0/1"&gt;Majid Khadiv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1"&gt;Ludovic Righetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06618</id>
        <link href="http://arxiv.org/abs/2107.06618"/>
        <updated>2021-07-15T01:59:03.643Z</updated>
        <summary type="html"><![CDATA[Chest radiography has been a recommended procedure for patient triaging and
resource management in intensive care units (ICUs) throughout the COVID-19
pandemic. The machine learning efforts to augment this workflow have been long
challenged due to deficiencies in reporting, model evaluation, and failure mode
analysis. To address some of those shortcomings, we model radiological features
with a human-interpretable class hierarchy that aligns with the radiological
decision process. Also, we propose the use of a data-driven error analysis
methodology to uncover the blind spots of our model, providing further
transparency on its clinical utility. For example, our experiments show that
model failures highly correlate with ICU imaging conditions and with the
inherent difficulty in distinguishing certain types of radiological features.
Also, our hierarchical interpretation and analysis facilitates the comparison
with respect to radiologists' findings and inter-variability, which in return
helps us to better assess the clinical applicability of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1"&gt;Shruthi Bannur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1"&gt;Ozan Oktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1"&gt;Melanie Bernhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1"&gt;Anton Schwaighofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rajesh Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1"&gt;Besmira Nushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1"&gt;Sharan Wadhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1"&gt;Aditya Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1"&gt;Kal Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1"&gt;Shazad Ashraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1"&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1"&gt;Daniel C. Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06356</id>
        <link href="http://arxiv.org/abs/2107.06356"/>
        <updated>2021-07-15T01:59:03.628Z</updated>
        <summary type="html"><![CDATA[Roads are connecting line between different places, and used daily. Roads'
periodic maintenance keeps them safe and functional. Detecting and reporting
the existence of potholes to responsible departments can help in eliminating
them. This study deployed and tested on different deep learning architecture to
detect potholes. The images used for training were collected by cellphone
mounted on the windshield of the car, in addition to many images downloaded
from the internet to increase the size and variability of the database. Second,
various object detection algorithms are employed and compared to detect
potholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.
YOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%
mean Average Precision (mAP). The speed of processing was 20 frame per second.
The system was able to detect potholes from a range on 100 meters away from the
camera. The system can increase the safety of drivers and improve the
performance of self-driving cars by detecting pothole time ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1"&gt;Anas Al Shaghouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1"&gt;Rami Alkhatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1"&gt;Samir Berjaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.06578</id>
        <link href="http://arxiv.org/abs/2107.06578"/>
        <updated>2021-07-15T01:59:03.611Z</updated>
        <summary type="html"><![CDATA[To enable process analysis based on an event log without compromising the
privacy of individuals involved in process execution, a log may be anonymized.
Such anonymization strives to transform a log so that it satisfies provable
privacy guarantees, while largely maintaining its utility for process analysis.
Existing techniques perform anonymization using simple, syntactic measures to
identify suitable transformation operations. This way, the semantics of the
activities referenced by the events in a trace are neglected, potentially
leading to transformations in which events of unrelated activities are merged.
To avoid this and incorporate the semantics of activities during anonymization,
we propose to instead incorporate a distance measure based on feature learning.
Specifically, we show how embeddings of events enable the definition of a
distance measure for traces to guide event log anonymization. Our experiments
with real-world data indicate that anonymization using this measure, compared
to a syntactic one, yields logs that are closer to the original log in various
dimensions and, hence, have higher utility for process analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1"&gt;Fabian R&amp;#xf6;sel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1"&gt;Stephan A. Fahrenkrog-Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1"&gt;Han van der Aa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1"&gt;Matthias Weidlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06546</id>
        <link href="http://arxiv.org/abs/2107.06546"/>
        <updated>2021-07-15T01:59:03.596Z</updated>
        <summary type="html"><![CDATA[We present the visually-grounded language modelling track that was introduced
in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the
new track and discuss participation rules in detail. We also present the two
baseline systems that were developed for this track.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1"&gt;Afra Alishahia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1"&gt;Grzegorz Chrupa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1"&gt;Alejandrina Cristia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1"&gt;Bertrand Higy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1"&gt;Marvin Lavechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06499</id>
        <link href="http://arxiv.org/abs/2107.06499"/>
        <updated>2021-07-15T01:59:03.560Z</updated>
        <summary type="html"><![CDATA[We find that existing language modeling datasets contain many near-duplicate
examples and long repetitive substrings. As a result, over 1% of the unprompted
output of language models trained on these datasets is copied verbatim from the
training data. We develop two tools that allow us to deduplicate training
datasets -- for example removing from C4 a single 61 word English sentence that
is repeated over 60,000 times. Deduplication allows us to train models that
emit memorized text ten times less frequently and require fewer train steps to
achieve the same or better accuracy. We can also reduce train-test overlap,
which affects over 4% of the validation set of standard datasets, thus allowing
for more accurate evaluation. We release code for reproducing our work and
performing dataset deduplication at
https://github.com/google-research/deduplicate-text-datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1"&gt;Andrew Nystrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1"&gt;Chris Callison-Burch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[For high-dimensional hierarchical models, consider exchangeability of effects across covariates instead of across datasets. (arXiv:2107.06428v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2107.06428</id>
        <link href="http://arxiv.org/abs/2107.06428"/>
        <updated>2021-07-15T01:59:03.550Z</updated>
        <summary type="html"><![CDATA[Hierarchical Bayesian methods enable information sharing across multiple
related regression problems. While standard practice is to model regression
parameters (effects) as (1) exchangeable across datasets and (2) correlated to
differing degrees across covariates, we show that this approach exhibits poor
statistical performance when the number of covariates exceeds the number of
datasets. For instance, in statistical genetics, we might regress dozens of
traits (defining datasets) for thousands of individuals (responses) on up to
millions of genetic variants (covariates). When an analyst has more covariates
than datasets, we argue that it is often more natural to instead model effects
as (1) exchangeable across covariates and (2) correlated to differing degrees
across datasets. To this end, we propose a hierarchical model expressing our
alternative perspective. We devise an empirical Bayes estimator for learning
the degree of correlation between datasets. We develop theory that demonstrates
that our method outperforms the classic approach when the number of covariates
dominates the number of datasets, and corroborate this result empirically on
several high-dimensional multiple regression and classification problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Trippe_B/0/1/0/all/0/1"&gt;Brian L. Trippe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Finucane_H/0/1/0/all/0/1"&gt;Hilary K. Finucane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Broderick_T/0/1/0/all/0/1"&gt;Tamara Broderick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry and Generalization: Eigenvalues as predictors of where a network will fail to generalize. (arXiv:2107.06386v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06386</id>
        <link href="http://arxiv.org/abs/2107.06386"/>
        <updated>2021-07-15T01:59:03.539Z</updated>
        <summary type="html"><![CDATA[We study the deformation of the input space by a trained autoencoder via the
Jacobians of the trained weight matrices. In doing so, we prove bounds for the
mean squared errors for points in the input space, under assumptions regarding
the orthogonality of the eigenvectors. We also show that the trace and the
product of the eigenvalues of the Jacobian matrices is a good predictor of the
MSE on test points. This is a dataset independent means of testing an
autoencoder's ability to generalize on new input. Namely, no knowledge of the
dataset on which the network was trained is needed, only the parameters of the
trained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwala_S/0/1/0/all/0/1"&gt;Susama Agarwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dees_B/0/1/0/all/0/1"&gt;Benjamin Dees&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gearhart_A/0/1/0/all/0/1"&gt;Andrew Gearhart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lowman_C/0/1/0/all/0/1"&gt;Corey Lowman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06409</id>
        <link href="http://arxiv.org/abs/2107.06409"/>
        <updated>2021-07-15T01:59:03.528Z</updated>
        <summary type="html"><![CDATA[Datasets often contain input dimensions that are unnecessary to predict the
output label, e.g. background in object recognition, which lead to more
trainable parameters. Deep Neural Networks (DNNs) are robust to increasing the
number of parameters in the hidden layers, but it is unclear whether this holds
true for the input layer. In this letter, we investigate the impact of
unnecessary input dimensions on a central issue of DNNs: their data efficiency,
ie. the amount of examples needed to achieve certain generalization
performance. Our results show that unnecessary input dimensions that are
task-unrelated substantially degrade data efficiency. This highlights the need
for mechanisms that remove {task-unrelated} dimensions to enable data
efficiency gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safer Reinforcement Learning through Transferable Instinct Networks. (arXiv:2107.06686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06686</id>
        <link href="http://arxiv.org/abs/2107.06686"/>
        <updated>2021-07-15T01:59:03.517Z</updated>
        <summary type="html"><![CDATA[Random exploration is one of the main mechanisms through which reinforcement
learning (RL) finds well-performing policies. However, it can lead to
undesirable or catastrophic outcomes when learning online in safety-critical
environments. In fact, safe learning is one of the major obstacles towards
real-world agents that can learn during deployment. One way of ensuring that
agents respect hard limitations is to explicitly configure boundaries in which
they can operate. While this might work in some cases, we do not always have
clear a-priori information which states and actions can lead dangerously close
to hazardous states. Here, we present an approach where an additional policy
can override the main policy and offer a safer alternative action. In our
instinct-regulated RL (IR^2L) approach, an "instinctual" network is trained to
recognize undesirable situations, while guarding the learning policy against
entering them. The instinct network is pre-trained on a single task where it is
safe to make mistakes, and transferred to environments in which learning a new
task safely is critical. We demonstrate IR^2L in the OpenAI Safety gym domain,
in which it receives a significantly lower number of safety violations during
training than a baseline RL approach while reaching similar task performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1"&gt;Djordje Grbic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1"&gt;Sebastian Risi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Associative Memory. (arXiv:2107.06446v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06446</id>
        <link href="http://arxiv.org/abs/2107.06446"/>
        <updated>2021-07-15T01:59:03.494Z</updated>
        <summary type="html"><![CDATA[Dense Associative Memories or Modern Hopfield Networks have many appealing
properties of associative memory. They can do pattern completion, store a large
number of memories, and can be described using a recurrent neural network with
a degree of biological plausibility and rich feedback between the neurons. At
the same time, up until now all the models of this class have had only one
hidden layer, and have only been formulated with densely connected network
architectures, two aspects that hinder their machine learning applications.
This paper tackles this gap and describes a fully recurrent model of
associative memory with an arbitrary large number of layers, some of which can
be locally connected (convolutional), and a corresponding energy function that
decreases on the dynamical trajectory of the neurons' activations. The memories
of the full network are dynamically "assembled" using primitives encoded in the
synaptic weights of the lower layers, with the "assembling rules" encoded in
the synaptic weights of the higher layers. In addition to the bottom-up
propagation of information, typical of commonly used feedforward neural
networks, the model described has rich top-down feedback from higher layers
that help the lower-layer neurons to decide on their response to the input
stimuli.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krotov_D/0/1/0/all/0/1"&gt;Dmitry Krotov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tourbillon: a Physically Plausible Neural Architecture. (arXiv:2107.06424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06424</id>
        <link href="http://arxiv.org/abs/2107.06424"/>
        <updated>2021-07-15T01:59:03.480Z</updated>
        <summary type="html"><![CDATA[In a physical neural system, backpropagation is faced with a number of
obstacles including: the need for labeled data, the violation of the locality
learning principle, the need for symmetric connections, and the lack of
modularity. Tourbillon is a new architecture that addresses all these
limitations. At its core, it consists of a stack of circular autoencoders
followed by an output layer. The circular autoencoders are trained in
self-supervised mode by recirculation algorithms and the top layer in
supervised mode by stochastic gradient descent, with the option of propagating
error information through the entire stack using non-symmetric connections.
While the Tourbillon architecture is meant primarily to address physical
constraints, and not to improve current engineering applications of deep
learning, we demonstrate its viability on standard benchmark datasets including
MNIST, Fashion MNIST, and CIFAR10. We show that Tourbillon can achieve
comparable performance to models trained with backpropagation and outperform
models that are trained with other physically plausible algorithms, such as
feedback alignment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_M/0/1/0/all/0/1"&gt;Mohammadamin Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldi_P/0/1/0/all/0/1"&gt;Pierre Baldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadowski_P/0/1/0/all/0/1"&gt;Peter Sadowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlated Stochastic Block Models: Exact Graph Matching with Applications to Recovering Communities. (arXiv:2107.06767v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2107.06767</id>
        <link href="http://arxiv.org/abs/2107.06767"/>
        <updated>2021-07-15T01:59:03.461Z</updated>
        <summary type="html"><![CDATA[We consider the task of learning latent community structure from multiple
correlated networks. First, we study the problem of learning the latent vertex
correspondence between two edge-correlated stochastic block models, focusing on
the regime where the average degree is logarithmic in the number of vertices.
We derive the precise information-theoretic threshold for exact recovery: above
the threshold there exists an estimator that outputs the true correspondence
with probability close to 1, while below it no estimator can recover the true
correspondence with probability bounded away from 0. As an application of our
results, we show how one can exactly recover the latent communities using
multiple correlated graphs in parameter regimes where it is
information-theoretically impossible to do so using just a single graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Racz_M/0/1/0/all/0/1"&gt;Miklos Z. Racz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sridhar_A/0/1/0/all/0/1"&gt;Anirudh Sridhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth and First Order Stochastic Frank-Wolfe Algorithms for Constrained Optimization. (arXiv:2107.06534v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.06534</id>
        <link href="http://arxiv.org/abs/2107.06534"/>
        <updated>2021-07-15T01:59:03.453Z</updated>
        <summary type="html"><![CDATA[This paper considers stochastic convex optimization problems with two sets of
constraints: (a) deterministic constraints on the domain of the optimization
variable, which are difficult to project onto; and (b) deterministic or
stochastic constraints that admit efficient projection. Problems of this form
arise frequently in the context of semidefinite programming as well as when
various NP-hard problems are solved approximately via semidefinite relaxation.
Since projection onto the first set of constraints is difficult, it becomes
necessary to explore projection-free algorithms, such as the stochastic
Frank-Wolfe (FW) algorithm. On the other hand, the second set of constraints
cannot be handled in the same way, and must be incorporated as an indicator
function within the objective function, thereby complicating the application of
FW methods. Similar problems have been studied before, and solved using
first-order stochastic FW algorithms by applying homotopy and Nesterov's
smoothing techniques to the indicator function. This work improves upon these
existing results and puts forth momentum-based first-order methods that yield
improved convergence rates, at par with the best known rates for problems
without the second set of constraints. Zeroth-order variants of the proposed
algorithms are also developed and again improve upon the state-of-the-art rate
results. The efficacy of the proposed algorithms is tested on relevant
applications of sparse matrix estimation, clustering via semidefinite
relaxation, and uniform sparsest cut problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Akhtar_Z/0/1/0/all/0/1"&gt;Zeeshan Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rajawat_K/0/1/0/all/0/1"&gt;Ketan Rajawat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Transformer Pruning. (arXiv:2104.08500v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08500</id>
        <link href="http://arxiv.org/abs/2104.08500"/>
        <updated>2021-07-15T01:59:03.447Z</updated>
        <summary type="html"><![CDATA[Vision transformer has achieved competitive performance on a variety of
computer vision applications. However, their storage, run-time memory, and
computational demands are hindering the deployment to mobile devices. Here we
present a vision transformer pruning approach, which identifies the impacts of
dimensions in each layer of transformer and then executes pruning accordingly.
By encouraging dimension-wise sparsity in the transformer, important dimensions
automatically emerge. A great number of dimensions with small importance scores
can be discarded to achieve a high pruning ratio without significantly
compromising accuracy. The pipeline for vision transformer pruning is as
follows: 1) training with sparsity regularization; 2) pruning dimensions of
linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of
the proposed algorithm are well evaluated and analyzed on ImageNet dataset to
demonstrate the effectiveness of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MESS: Manifold Embedding Motivated Super Sampling. (arXiv:2107.06566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06566</id>
        <link href="http://arxiv.org/abs/2107.06566"/>
        <updated>2021-07-15T01:59:03.441Z</updated>
        <summary type="html"><![CDATA[Many approaches in the field of machine learning and data analysis rely on
the assumption that the observed data lies on lower-dimensional manifolds. This
assumption has been verified empirically for many real data sets. To make use
of this manifold assumption one generally requires the manifold to be locally
sampled to a certain density such that features of the manifold can be
observed. However, for increasing intrinsic dimensionality of a data set the
required data density introduces the need for very large data sets, resulting
in one of the many faces of the curse of dimensionality. To combat the
increased requirement for local data density we propose a framework to generate
virtual data points that faithful to an approximate embedding function
underlying the manifold observable in the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thordsen_E/0/1/0/all/0/1"&gt;Erik Thordsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Contextual Bandits: Learning How Behavior Evolves over Time. (arXiv:2107.06317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06317</id>
        <link href="http://arxiv.org/abs/2107.06317"/>
        <updated>2021-07-15T01:59:03.424Z</updated>
        <summary type="html"><![CDATA[Understanding an agent's priorities by observing their behavior is critical
for transparency and accountability in decision processes, such as in
healthcare. While conventional approaches to policy learning almost invariably
assume stationarity in behavior, this is hardly true in practice: Medical
practice is constantly evolving, and clinical professionals are constantly
fine-tuning their priorities. We desire an approach to policy learning that
provides (1) interpretable representations of decision-making, accounts for (2)
non-stationarity in behavior, as well as operating in an (3) offline manner.
First, we model the behavior of learning agents in terms of contextual bandits,
and formalize the problem of inverse contextual bandits (ICB). Second, we
propose two algorithms to tackle ICB, each making varying degrees of
assumptions regarding the agent's learning strategy. Finally, through both real
and simulated data for liver transplantations, we illustrate the applicability
and explainability of our method, as well as validating its accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1"&gt;Alihan H&amp;#xfc;y&amp;#xfc;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1"&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06530</id>
        <link href="http://arxiv.org/abs/2107.06530"/>
        <updated>2021-07-15T01:59:03.417Z</updated>
        <summary type="html"><![CDATA[Due to the recent outbreak of COVID-19, many classes, exams, and meetings
have been conducted non-face-to-face. However, the foundation for video
conferencing solutions is still insufficient. So this technology has become an
important issue. In particular, these technologies are essential for
non-face-to-face testing, and technology dissemination is urgent. In this
paper, we present a single video conferencing solution using gaze estimation in
preparation for these problems. Gaze is an important cue for the tasks such as
analysis of human behavior. Hence, numerous studies have been proposed to solve
gaze estimation using deep learning, which is one of the most prominent methods
up to date. We use these gaze estimation methods to detect abnormal behavior of
video conferencing participants. Our contribution is as follows. i) We find and
apply the optimal network for the gaze estimation method and apply a
self-supervised method to improve accuracy. ii) For anomaly detection, we
present a new dataset that aggregates the values of a new gaze, head pose, etc.
iii) We train newly created data on Multi Layer Perceptron (MLP) models to
detect anomaly behavior based on deep learning. We demonstrate the robustness
of our method through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1"&gt;Suneung-Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributionally Robust Policy Learning via Adversarial Environment Generation. (arXiv:2107.06353v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06353</id>
        <link href="http://arxiv.org/abs/2107.06353"/>
        <updated>2021-07-15T01:59:03.411Z</updated>
        <summary type="html"><![CDATA[Our goal is to train control policies that generalize well to unseen
environments. Inspired by the Distributionally Robust Optimization (DRO)
framework, we propose DRAGEN - Distributionally Robust policy learning via
Adversarial Generation of ENvironments - for iteratively improving robustness
of policies to realistic distribution shifts by generating adversarial
environments. The key idea is to learn a generative model for environments
whose latent variables capture cost-predictive and realistic variations in
environments. We perform DRO with respect to a Wasserstein ball around the
empirical distribution of environments by generating realistic adversarial
environments via gradient ascent on the latent space. We demonstrate strong
Out-of-Distribution (OoD) generalization in simulation for (i) swinging up a
pendulum with onboard vision and (ii) grasping realistic 2D/3D objects.
Grasping experiments on hardware demonstrate better sim2real performance
compared to domain randomization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_A/0/1/0/all/0/1"&gt;Allen Z. Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Anirudha Majumdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06304</id>
        <link href="http://arxiv.org/abs/2107.06304"/>
        <updated>2021-07-15T01:59:03.404Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior and vulnerability of pre-trained deep neural
networks (DNNs) can help to improve them. Analysis can be performed via
reversing the network's flow to generate inputs from internal representations.
Most existing work relies on priors or data-intensive optimization to invert a
model, yet struggles to scale to deep architectures and complex datasets. This
paper presents a zero-shot direct model inversion framework that recovers the
input to the trained model given only the internal representation. The crux of
our method is to inverse the DNN in a divide-and-conquer manner while
re-syncing the inverted layers via cycle-consistency guidance with the help of
synthesized data. As a result, we obtain a single feed-forward model capable of
inversion with a single forward pass without seeing any real data of the
original task. With the proposed approach, we scale zero-shot direct inversion
to deep architectures and complex datasets. We empirically show that modern
classification models on ImageNet can, surprisingly, be inverted, allowing an
approximate recovery of the original 224x224px images from a representation
after more than 20 layers. Moreover, inversion of generators in GANs unveils
latent code of a given synthesized face image at 128x128px, which can even, in
turn, improve defective synthesized images from GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Hierarchical Federated Learning for IoT Heterogeneous Systems with Imbalanced Data. (arXiv:2107.06548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06548</id>
        <link href="http://arxiv.org/abs/2107.06548"/>
        <updated>2021-07-15T01:59:03.386Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed learning methodology that allows
multiple nodes to cooperatively train a deep learning model, without the need
to share their local data. It is a promising solution for telemonitoring
systems that demand intensive data collection, for detection, classification,
and prediction of future events, from different locations while maintaining a
strict privacy constraint. Due to privacy concerns and critical communication
bottlenecks, it can become impractical to send the FL updated models to a
centralized server. Thus, this paper studies the potential of hierarchical FL
in IoT heterogeneous systems and propose an optimized solution for user
assignment and resource allocation on multiple edge nodes. In particular, this
work focuses on a generic class of machine learning models that are trained
using gradient-descent-based schemes while considering the practical
constraints of non-uniformly distributed data across different users. We
evaluate the proposed system using two real-world datasets, and we show that it
outperforms state-of-the-art FL solutions. In particular, our numerical results
highlight the effectiveness of our approach and its ability to provide 4-6%
increase in the classification accuracy, with respect to hierarchical FL
schemes that consider distance-based user assignment. Furthermore, the proposed
approach could significantly accelerate FL training and reduce communication
overhead by providing 75-85% reduction in the communication rounds between edge
nodes and the centralized server, for the same model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1"&gt;Alaa Awad Abdellatif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1"&gt;Naram Mhaisen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawy_Z/0/1/0/all/0/1"&gt;Zaher Dawy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasreddine_W/0/1/0/all/0/1"&gt;Wassim Nasreddine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06351</id>
        <link href="http://arxiv.org/abs/2107.06351"/>
        <updated>2021-07-15T01:59:03.380Z</updated>
        <summary type="html"><![CDATA[Image annotation and large annotated datasets are crucial parts within the
Computer Vision and Artificial Intelligence fields.At the same time, it is
well-known and acknowledged by the research community that the image annotation
process is challenging, time-consuming and hard to scale. Therefore, the
researchers and practitioners are always seeking ways to perform the
annotations easier, faster, and at higher quality. Even though several widely
used tools exist and the tools' landscape evolved considerably, most of the
tools still require intricate technical setups and high levels of technical
savviness from its operators and crowdsource contributors.

In order to address such challenges, we develop and present BRIMA -- a
flexible and open-source browser extension that allows BRowser-only IMage
Annotation at considerably lower overheads. Once added to the browser, it
instantly allows the user to annotate images easily and efficiently directly
from the browser without any installation or setup on the client-side. It also
features cross-browser and cross-platform functionality thus presenting itself
as a neat tool for researchers within the Computer Vision, Artificial
Intelligence, and privacy-related fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Feature Selection via Transferring Supervised Knowledge. (arXiv:1908.03464v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03464</id>
        <link href="http://arxiv.org/abs/1908.03464"/>
        <updated>2021-07-15T01:59:03.372Z</updated>
        <summary type="html"><![CDATA[Feature selection, an effective technique for dimensionality reduction, plays
an important role in many machine learning systems. Supervised knowledge can
significantly improve the performance. However, faced with the rapid growth of
newly emerging concepts, existing supervised methods might easily suffer from
the scarcity and validity of labeled data for training. In this paper, the
authors study the problem of zero-shot feature selection (i.e., building a
feature selection model that generalizes well to "unseen" concepts with limited
training data of "seen" concepts). Specifically, they adopt class-semantic
descriptions (i.e., attributes) as supervision for feature selection, so as to
utilize the supervised knowledge transferred from the seen concepts. For more
reliable discriminative features, they further propose the
center-characteristic loss which encourages the selected features to capture
the central characteristics of seen concepts. Extensive experiments conducted
on various real-world datasets demonstrate the effectiveness of the method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tingzhang Zhao&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojun Ye&lt;/a&gt; (2) ((1) Department of Computer Science, University of Science and Technology Beijing (2) School of Software, Tsinghua University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06344</id>
        <link href="http://arxiv.org/abs/2107.06344"/>
        <updated>2021-07-15T01:59:03.329Z</updated>
        <summary type="html"><![CDATA[Drivers have unique and rich driving behaviors when operating vehicles in
traffic. This paper presents a novel driver behavior learning approach that
captures the uniqueness and richness of human driver behavior in realistic
driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach
is proposed to learn a distribution of cost function, which represents the
richness of the human driver behavior with a given set of driver-specific
demonstrations. Evaluations are conducted on the realistic driving data
collected from the 3D driver-in-the-loop driving simulation. The results show
that the learned stochastic driver model is capable of expressing the richness
of the human driving strategies under different realistic driving scenarios.
Compared to the deterministic baseline driver model, the results reveal that
the proposed stochastic driver behavior model can better replicate the driver's
unique and rich driving strategies in a variety of traffic conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1"&gt;Mehmet Fatih Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Performance Analysis of the Adversarial System Variant Approximation Method to Quantify Process Model Generalization. (arXiv:2107.06319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06319</id>
        <link href="http://arxiv.org/abs/2107.06319"/>
        <updated>2021-07-15T01:59:03.321Z</updated>
        <summary type="html"><![CDATA[Process mining algorithms discover a process model from an event log. The
resulting process model is supposed to describe all possible event sequences of
the underlying system. Generalization is a process model quality dimension of
interest. A generalization metric should quantify the extent to which a process
model represents the observed event sequences contained in the event log and
the unobserved event sequences of the system. Most of the available metrics in
the literature cannot properly quantify the generalization of a process model.
A recently published method [1] called Adversarial System Variant Approximation
leverages Generative Adversarial Networks to approximate the underlying event
sequence distribution of a system from an event log. While this method
demonstrated performance gains over existing methods in measuring the
generalization of process models, its experimental evaluations have been
performed under ideal conditions. This paper experimentally investigates the
performance of Adversarial System Variant Approximation under non-ideal
conditions such as biased and limited event logs. Moreover, experiments are
performed to investigate the originally proposed sampling hyperparameter value
of the method on its performance to measure the generalization. The results
confirm the need to raise awareness about the working conditions of the
Adversarial System Variant Approximation method. The outcomes of this paper
also serve to initiate future research directions.

[1] Theis, Julian, and Houshang Darabi. "Adversarial System Variant
Approximation to Quantify Process Model Generalization." IEEE Access 8 (2020):
194410-194427.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1"&gt;Julian Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mokhtarian_I/0/1/0/all/0/1"&gt;Ilia Mokhtarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1"&gt;Houshang Darabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06281</id>
        <link href="http://arxiv.org/abs/2107.06281"/>
        <updated>2021-07-15T01:59:03.285Z</updated>
        <summary type="html"><![CDATA[Brain graph synthesis marked a new era for predicting a target brain graph
from a source one without incurring the high acquisition cost and processing
time of neuroimaging data. However, existing multi-modal graph synthesis
frameworks have several limitations. First, they mainly focus on generating
graphs from the same domain (intra-modality), overlooking the rich multimodal
representations of brain connectivity (inter-modality). Second, they can only
handle isomorphic graph generation tasks, limiting their generalizability to
synthesizing target graphs with a different node size and topological structure
from those of the source one. More importantly, both target and source domains
might have different distributions, which causes a domain fracture between them
(i.e., distribution misalignment). To address such challenges, we propose an
inter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to
infer a target graph modality based on a given modality. Our three core
contributions lie in (i) predicting a target graph (e.g., functional) from a
source graph (e.g., morphological) based on a novel graph generative
adversarial network (gGAN); (ii) using non-isomorphic graphs for both source
and target domains with a different number of nodes, edges and structure; and
(iii) enforcing the predicted target distribution to match that of the ground
truth graphs using a graph autoencoder to relax the designed loss oprimization.
To handle the unstable behavior of gGAN, we design a new Ground
Truth-Preserving (GT-P) loss function to guide the generator in learning the
topological structure of ground truth brain graphs. Our comprehensive
experiments on predicting functional from morphological graphs demonstrate the
outperformance of IMANGraphNet in comparison with its variants. This can be
further leveraged for integrative and holistic brain mapping in health and
disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1"&gt;Islem Mhiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1"&gt;Ahmed Nebli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1"&gt;Mohamed Ali Mahjoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1"&gt;Islem Rekik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09124</id>
        <link href="http://arxiv.org/abs/2104.09124"/>
        <updated>2021-07-15T01:59:03.232Z</updated>
        <summary type="html"><![CDATA[While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of
ResNet-101/ResNet-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yuting Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaowei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean Embeddings with Test-Time Data Augmentation for Ensembling of Representations. (arXiv:2106.08038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08038</id>
        <link href="http://arxiv.org/abs/2106.08038"/>
        <updated>2021-07-15T01:59:03.226Z</updated>
        <summary type="html"><![CDATA[Averaging predictions over a set of models -- an ensemble -- is widely used
to improve predictive performance and uncertainty estimation of deep learning
models. At the same time, many machine learning systems, such as search,
matching, and recommendation systems, heavily rely on embeddings.
Unfortunately, due to misalignment of features of independently trained models,
embeddings, cannot be improved with a naive deep ensemble like approach. In
this work, we look at the ensembling of representations and propose mean
embeddings with test-time augmentation (MeTTA) simple yet well-performing
recipe for ensembling representations. Empirically we demonstrate that MeTTA
significantly boosts the quality of linear evaluation on ImageNet for both
supervised and self-supervised models. Even more exciting, we draw connections
between MeTTA, image retrieval, and transformation invariant models. We believe
that spreading the success of ensembles to inference higher-quality
representations is the important step that will open many new applications of
ensembling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ashukha_A/0/1/0/all/0/1"&gt;Arsenii Ashukha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1"&gt;Andrei Atanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. (arXiv:2106.03348v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03348</id>
        <link href="http://arxiv.org/abs/2106.03348"/>
        <updated>2021-07-15T01:59:03.220Z</updated>
        <summary type="html"><![CDATA[Transformers have shown great potential in various computer vision tasks
owing to their strong capability in modeling long-range dependency using the
self-attention mechanism. Nevertheless, vision transformers treat an image as
1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in
modeling local visual structures and dealing with scale variance.
Alternatively, they require large-scale training data and longer training
schedules to learn the IB implicitly. In this paper, we propose a novel Vision
Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.
Technically, ViTAE has several spatial pyramid reduction modules to downsample
and embed the input image into tokens with rich multi-scale context by using
multiple convolutions with different dilation rates. In this way, it acquires
an intrinsic scale invariance IB and is able to learn robust feature
representation for objects at various scales. Moreover, in each transformer
layer, ViTAE has a convolution block in parallel to the multi-head
self-attention module, whose features are fused and fed into the feed-forward
network. Consequently, it has the intrinsic locality IB and is able to learn
local features and global dependencies collaboratively. Experiments on ImageNet
as well as downstream tasks prove the superiority of ViTAE over the baseline
transformer and concurrent works. Source code and pretrained models will be
available at GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yufei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Person-MinkUNet: 3D Person Detection with LiDAR Point Cloud. (arXiv:2107.06780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06780</id>
        <link href="http://arxiv.org/abs/2107.06780"/>
        <updated>2021-07-15T01:59:03.202Z</updated>
        <summary type="html"><![CDATA[In this preliminary work we attempt to apply submanifold sparse convolution
to the task of 3D person detection. In particular, we present Person-MinkUNet,
a single-stage 3D person detection network based on Minkowski Engine with U-Net
architecture. The network achieves a 76.4% average precision (AP) on the JRDB
3D detection benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14117</id>
        <link href="http://arxiv.org/abs/2105.14117"/>
        <updated>2021-07-15T01:59:03.195Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning methods for computer vision have demonstrated the
effectiveness of pre-training feature representations, resulting in
well-generalizing Deep Neural Networks, even if the annotated data are limited.
However, representation learning techniques require a significant amount of
time for model training, with most of it time spent on precise hyper-parameter
optimization and selection of augmentation techniques. We hypothesized that if
the annotated dataset has enough morphological diversity to capture the general
population's as is common in medical imaging, for example, due to conserved
similarities of tissue mythologies, the variance error of the trained model is
the prevalent component of the Bias-Variance Trade-off. We propose the Variance
Aware Training (VAT) method that exploits this property by introducing the
variance error into the model loss function, i.e., enabling minimizing the
variance explicitly. Additionally, we provide the theoretical formulation and
proof of the proposed method to aid in interpreting the approach. Our method
requires selecting only one hyper-parameter and was able to match or improve
the state-of-the-art performance of self-supervised methods while achieving an
order of magnitude reduction in the GPU training time. We validated VAT on
three medical imaging datasets from diverse domains and various learning
objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the
heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography
dataset for ordinary regression of diabetic retinopathy progression (Kaggle
2019 APTOS Blindness Detection challenge), and classification of
histopathologic scans of lymph node sections (PatchCamelyon dataset).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1"&gt;Dmitrii Shubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1"&gt;Sebastian D. Goodfellow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Pothole Detection Using Deep Learning. (arXiv:2107.06356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06356</id>
        <link href="http://arxiv.org/abs/2107.06356"/>
        <updated>2021-07-15T01:59:03.187Z</updated>
        <summary type="html"><![CDATA[Roads are connecting line between different places, and used daily. Roads'
periodic maintenance keeps them safe and functional. Detecting and reporting
the existence of potholes to responsible departments can help in eliminating
them. This study deployed and tested on different deep learning architecture to
detect potholes. The images used for training were collected by cellphone
mounted on the windshield of the car, in addition to many images downloaded
from the internet to increase the size and variability of the database. Second,
various object detection algorithms are employed and compared to detect
potholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53.
YOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39%
mean Average Precision (mAP). The speed of processing was 20 frame per second.
The system was able to detect potholes from a range on 100 meters away from the
camera. The system can increase the safety of drivers and improve the
performance of self-driving cars by detecting pothole time ahead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaghouri_A/0/1/0/all/0/1"&gt;Anas Al Shaghouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alkhatib_R/0/1/0/all/0/1"&gt;Rami Alkhatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berjaoui_S/0/1/0/all/0/1"&gt;Samir Berjaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSFNet:Multi-scale features network for monocular depth estimation. (arXiv:2107.06445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06445</id>
        <link href="http://arxiv.org/abs/2107.06445"/>
        <updated>2021-07-15T01:59:03.179Z</updated>
        <summary type="html"><![CDATA[In recent years, monocular depth estimation is applied to understand the
surrounding 3D environment and has made great progress. However, there is an
ill-posed problem on how to gain depth information directly from a single
image. With the rapid development of deep learning, this problem is possible to
be solved. Although more and more approaches are proposed one after another,
most of existing methods inevitably lost details due to continuous downsampling
when mapping from RGB space to depth space. To the end, we design a Multi-scale
Features Network (MSFNet), which consists of Enhanced Diverse Attention (EDA)
module and Upsample-Stage Fusion (USF) module. The EDA module employs the
spatial attention method to learn significant spatial information, while USF
module complements low-level detail information with high-level semantic
information from the perspective of multi-scale feature fusion to improve the
predicted effect. In addition, since the simple samples are always trained to a
better effect first, the hard samples are difficult to converge. Therefore, we
design a batch-loss to assign large loss factors to the harder samples in a
batch. Experiments on NYU-Depth V2 dataset and KITTI dataset demonstrate that
our proposed approach is more competitive with the state-of-the-art methods in
both qualitative and quantitative evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_M/0/1/0/all/0/1"&gt;Meiqi Pei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06492</id>
        <link href="http://arxiv.org/abs/2107.06492"/>
        <updated>2021-07-15T01:59:03.173Z</updated>
        <summary type="html"><![CDATA[COVID-19 leads to the high demand for remote interactive systems ever seen.
One of the key elements of these systems is video streaming, which requires a
very high network bandwidth due to its specific real-time demand, especially
with high-resolution video. Existing video compression methods are struggling
in the trade-off between video quality and the speed requirement. Addressed
that the background information rarely changes in most remote meeting cases, we
introduce a Region-Of-Interests (ROI) based video compression framework (named
RCLC) that leverages the cutting-edge learning-based and conventional
technologies. In RCLC, each coming frame is marked as a background-updating
(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the
BU frame is compressed with low-quality and high-compression, while the ROI
from RU-frame is compressed with high-quality and low-compression. The
learning-based methods are applied to detect the ROI, blend background-ROI, and
enhance video quality. The experimental results show that our RCLC can reduce
up to 32.55\% BD-rate for the ROI region compared to H.265 video codec under a
similar compression time with 1080p resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Trinh Man Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjia Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09908</id>
        <link href="http://arxiv.org/abs/2106.09908"/>
        <updated>2021-07-15T01:59:03.157Z</updated>
        <summary type="html"><![CDATA[A significant amount of work has been done on adversarial attacks that inject
imperceptible noise to images to deteriorate the image classification
performance of deep models. However, most of the existing studies consider
attacks in the digital (pixel) domain where an image acquired by an image
sensor with sampling and quantization has been recorded. This paper, for the
first time, introduces an optical adversarial attack, which physically alters
the light field information arriving at the image sensor so that the
classification model yields misclassification. More specifically, we modulate
the phase of the light in the Fourier domain using a spatial light modulator
placed in the photographic system. The operative parameters of the modulator
are obtained by gradient-based optimization to maximize cross-entropy and
minimize distortions. We present experiments based on both simulation and a
real hardware optical system, from which the feasibility of the proposed
optical attack is demonstrated. It is also verified that the proposed attack is
completely different from common optical-domain distortions such as spherical
aberration, defocus, and astigmatism in terms of both perturbation patterns and
classification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyulim Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;JeongSoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Seungri Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1"&gt;Chulmin Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Event Camera Calibration. (arXiv:2107.06749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06749</id>
        <link href="http://arxiv.org/abs/2107.06749"/>
        <updated>2021-07-15T01:59:03.151Z</updated>
        <summary type="html"><![CDATA[Camera calibration is an important prerequisite towards the solution of 3D
computer vision problems. Traditional methods rely on static images of a
calibration pattern. This raises interesting challenges towards the practical
usage of event cameras, which notably require image change to produce
sufficient measurements. The current standard for event camera calibration
therefore consists of using flashing patterns. They have the advantage of
simultaneously triggering events in all reprojected pattern feature locations,
but it is difficult to construct or use such patterns in the field. We present
the first dynamic event camera calibration algorithm. It calibrates directly
from events captured during relative motion between camera and calibration
pattern. The method is propelled by a novel feature extraction mechanism for
calibration patterns, and leverages existing calibration tools before
optimizing all parameters through a multi-segment continuous-time formulation.
As demonstrated through our results on real data, the obtained calibration
method is highly convenient and reliably calibrates from data sequences
spanning less than 10 seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kneip_L/0/1/0/all/0/1"&gt;Laurent Kneip&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. (arXiv:2103.05630v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05630</id>
        <link href="http://arxiv.org/abs/2103.05630"/>
        <updated>2021-07-15T01:59:03.144Z</updated>
        <summary type="html"><![CDATA[The rapid progress of photorealistic synthesis techniques has reached at a
critical point where the boundary between real and manipulated images starts to
blur. Thus, benchmarking and advancing digital forgery analysis have become a
pressing issue. However, existing face forgery datasets either have limited
diversity or only support coarse-grained analysis. To counter this emerging
threat, we construct the ForgeryNet dataset, an extremely large face forgery
dataset with unified annotations in image- and video-level data across four
tasks: 1) Image Forgery Classification, including two-way (real / fake),
three-way (real / fake with identity-replaced forgery approaches / fake with
identity-remained forgery approaches), and n-way (real and 15 respective
forgery approaches) classification. 2) Spatial Forgery Localization, which
segments the manipulated area of fake images compared to their corresponding
source real images. 3) Video Forgery Classification, which re-defines the
video-level forgery classification with manipulated frames in random positions.
This task is important because attackers in real world are free to manipulate
any target frame. and 4) Temporal Forgery Localization, to localize the
temporal segments which are manipulated. ForgeryNet is by far the largest
publicly available deep face forgery dataset in terms of data-scale (2.9
million images, 221,247 videos), manipulations (7 image-level approaches, 8
video-level approaches), perturbations (36 independent and more mixed
perturbations) and annotations (6.3 million classification labels, 2.9 million
manipulated area annotations and 221,247 temporal forgery segment labels). We
perform extensive benchmarking and studies of existing face forensics methods
and obtain several valuable observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yinan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_B/0/1/0/all/0/1"&gt;Bei Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yichun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1"&gt;Guojun Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luchuan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1"&gt;Lu Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jing Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06777</id>
        <link href="http://arxiv.org/abs/2107.06777"/>
        <updated>2021-07-15T01:59:03.136Z</updated>
        <summary type="html"><![CDATA[One of the most pressing problems in the automated analysis of historical
documents is the availability of annotated training data. In this paper, we
propose a novel method for the synthesis of training data for semantic
segmentation of document images. We utilize clusters found in intermediate
features of a StyleGAN generator for the synthesis of RGB and label images at
the same time. Our model can be applied to any dataset of scanned documents
without the need for manual annotation of individual images, as each model is
custom-fit to the dataset. In our experiments, we show that models trained on
our synthetic data can reach competitive performance on open benchmark datasets
for line segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1"&gt;Christian Bartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratz_H/0/1/0/all/0/1"&gt;Hendrik R&amp;#xe4;tz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_J/0/1/0/all/0/1"&gt;Joseph Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faces in the Wild: Efficient Gender Recognition in Surveillance Conditions. (arXiv:2107.06847v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06847</id>
        <link href="http://arxiv.org/abs/2107.06847"/>
        <updated>2021-07-15T01:59:03.128Z</updated>
        <summary type="html"><![CDATA[Soft biometrics inference in surveillance scenarios is a topic of interest
for various applications, particularly in security-related areas. However, soft
biometric analysis is not extensively reported in wild conditions. In
particular, previous works on gender recognition report their results in face
datasets, with relatively good image quality and frontal poses. Given the
uncertainty of the availability of the facial region in wild conditions, we
consider that these methods are not adequate for surveillance settings. To
overcome these limitations, we: 1) present frontal and wild face versions of
three well-known surveillance datasets; and 2) propose a model that effectively
and dynamically combines facial and body information, which makes it suitable
for gender recognition in wild conditions. The frontal and wild face datasets
derive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA,
PA-100K, and RAP), using a pose-based approach to filter the frontal samples
and facial regions. This approach retrieves the facial region of images with
varying image/subject conditions, where the state-of-the-art face detectors
often fail. Our model combines facial and body information through a learnable
fusion matrix and a channel-attention sub-network, focusing on the most
influential body parts according to the specific image/subject features. We
compare it with five PAR methods, consistently obtaining state-of-the-art
results on gender recognition, and reducing the prediction errors by up to 24%
in frontal samples. The announced PAR datasets versions and model serve as the
basis for wild soft biometrics classification and are available in
https://github.com/Tiago-Roxo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roxo_T/0/1/0/all/0/1"&gt;Tiago Roxo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1"&gt;Hugo Proen&amp;#xe7;a&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14132</id>
        <link href="http://arxiv.org/abs/2106.14132"/>
        <updated>2021-07-15T01:59:03.111Z</updated>
        <summary type="html"><![CDATA[Pose transfer of human videos aims to generate a high fidelity video of a
target person imitating actions of a source person. A few studies have made
great progress either through image translation with deep latent features or
neural rendering with explicit 3D features. However, both of them rely on large
amounts of training data to generate realistic results, and the performance
degrades on more accessible internet videos due to insufficient training
frames. In this paper, we demonstrate that the dynamic details can be preserved
even trained from short monocular videos. Overall, we propose a neural video
rendering framework coupled with an image-translation-based dynamic details
generation network (D2G-Net), which fully utilizes both the stability of
explicit 3D features and the capacity of learning components. To be specific, a
novel texture representation is presented to encode both the static and
pose-varying appearance characteristics, which is then mapped to the image
space and rendered as a detail-rich frame in the neural rendering stage.
Moreover, we introduce a concise temporal loss in the training stage to
suppress the detail flickering that is made more visible due to high-quality
dynamic details generated by our method. Through extensive comparisons, we
demonstrate that our neural human video renderer is capable of achieving both
clearer dynamic details and more robust performance even on accessible short
videos with only 2k - 4k frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yang-tian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao-zhi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yu-kun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15754</id>
        <link href="http://arxiv.org/abs/2106.15754"/>
        <updated>2021-07-15T01:59:03.100Z</updated>
        <summary type="html"><![CDATA[Long-range context information is crucial for the semantic segmentation of
High-Resolution (HR) Remote Sensing Images (RSIs). The image cropping
operations, commonly used for training neural networks, limit the perception of
long-range context information in large RSIs. To break this limitation, we
propose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR
RSIs. In the WiCoNet, apart from a conventional feature extraction network that
aggregates the local information, an extra context branch is designed to
explicitly model the spatial information in a larger image area. The
information between the two branches is communicated through a Context
Transformer, which is a novel design derived from the Vision Transformer to
model the long-range context correlations. Ablation studies and comparative
experiments conducted on several benchmark datasets prove the effectiveness of
the proposed method. In addition, we present a new Beijing Land-Use (BLU)
dataset. This is a large-scale HR satellite dataset provided with high-quality
and fine-grained reference labels, which can boost future studies in this
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Lei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaofu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaojie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuebin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1"&gt;Lorenzo Bruzzone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Structural Causal Model for MR Images of Multiple Sclerosis. (arXiv:2103.03158v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03158</id>
        <link href="http://arxiv.org/abs/2103.03158"/>
        <updated>2021-07-15T01:59:03.094Z</updated>
        <summary type="html"><![CDATA[Precision medicine involves answering counterfactual questions such as "Would
this patient respond better to treatment A or treatment B?" These types of
questions are causal in nature and require the tools of causal inference to be
answered, e.g., with a structural causal model (SCM). In this work, we develop
an SCM that models the interaction between demographic information, disease
covariates, and magnetic resonance (MR) images of the brain for people with
multiple sclerosis. Inference in the SCM generates counterfactual images that
show what an MR image of the brain would look like if demographic or disease
covariates are changed. These images can be used for modeling disease
progression or used for image processing tasks where controlling for
confounders is necessary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reinhold_J/0/1/0/all/0/1"&gt;Jacob C. Reinhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carass_A/0/1/0/all/0/1"&gt;Aaron Carass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developmental Stage Classification of EmbryosUsing Two-Stream Neural Network with Linear-Chain Conditional Random Field. (arXiv:2107.06360v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06360</id>
        <link href="http://arxiv.org/abs/2107.06360"/>
        <updated>2021-07-15T01:59:03.089Z</updated>
        <summary type="html"><![CDATA[The developmental process of embryos follows a monotonic order. An embryo can
progressively cleave from one cell to multiple cells and finally transform to
morula and blastocyst. For time-lapse videos of embryos, most existing
developmental stage classification methods conduct per-frame predictions using
an image frame at each time step. However, classification using only images
suffers from overlapping between cells and imbalance between stages. Temporal
information can be valuable in addressing this problem by capturing movements
between neighboring frames. In this work, we propose a two-stream model for
developmental stage classification. Unlike previous methods, our two-stream
model accepts both temporal and image information. We develop a linear-chain
conditional random field (CRF) on top of neural network features extracted from
the temporal and image streams to make use of both modalities. The linear-chain
CRF formulation enables tractable training of global sequential models over
multiple frames while also making it possible to inject monotonic development
order constraints into the learning process explicitly. We demonstrate our
algorithm on two time-lapse embryo video datasets: i) mouse and ii) human
embryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human
embryo stage classification, respectively. Our approach will enable more
profound clinical and biological studies and suggests a new direction for
developmental stage classification by utilizing temporal information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukyanenko_S/0/1/0/all/0/1"&gt;Stanislav Lukyanenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1"&gt;Won-Dong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Donglai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Struyven_R/0/1/0/all/0/1"&gt;Robbert Struyven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leahy_B/0/1/0/all/0/1"&gt;Brian Leahy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Helen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1"&gt;Alexander Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Yosef_D/0/1/0/all/0/1"&gt;Dalit Ben-Yosef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needleman_D/0/1/0/all/0/1"&gt;Daniel Needleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Guided Mixup for Semi-Supervised Domain Adaptation without Source Data. (arXiv:2107.06707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06707</id>
        <link href="http://arxiv.org/abs/2107.06707"/>
        <updated>2021-07-15T01:59:03.083Z</updated>
        <summary type="html"><![CDATA[Present domain adaptation methods usually perform explicit representation
alignment by simultaneously accessing the source data and target data. However,
the source data are not always available due to the privacy preserving
consideration or bandwidth limitation. Source-free domain adaptation aims to
solve the above problem by performing domain adaptation without accessing the
source data. The adaptation paradigm is receiving more and more attention in
recent years, and multiple works have been proposed for unsupervised
source-free domain adaptation. However, without utilizing any supervised signal
and source data at the adaptation stage, the optimization of the target model
is unstable and fragile. To alleviate the problem, we focus on semi-supervised
domain adaptation under source-free setting. More specifically, we propose
uncertainty-guided Mixup to reduce the representation's intra-domain
discrepancy and perform inter-domain alignment without directly accessing the
source data. Finally, we conduct extensive semi-supervised domain adaptation
experiments on various datasets. Our method outperforms the recent
semi-supervised baselines and the unsupervised variant also achieves
competitive performance. The experiment codes will be released in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06538</id>
        <link href="http://arxiv.org/abs/2107.06538"/>
        <updated>2021-07-15T01:59:03.066Z</updated>
        <summary type="html"><![CDATA[Fine-grained image recognition is challenging because discriminative clues
are usually fragmented, whether from a single image or multiple images. Despite
their significant improvements, most existing methods still focus on the most
discriminative parts from a single image, ignoring informative details in other
regions and lacking consideration of clues from other associated images. In
this paper, we analyze the difficulties of fine-grained image recognition from
a new perspective and propose a transformer architecture with the peak
suppression module and knowledge guidance module, which respects the
diversification of discriminative features in a single image and the
aggregation of discriminative clues among multiple images. Specifically, the
peak suppression module first utilizes a linear projection to convert the input
image into sequential tokens. It then blocks the token based on the attention
response generated by the transformer encoder. This module penalizes the
attention to the most discriminative parts in the feature learning process,
therefore, enhancing the information exploitation of the neglected regions. The
knowledge guidance module compares the image-based representation generated
from the peak suppression module with the learnable knowledge embedding set to
obtain the knowledge response coefficients. Afterwards, it formalizes the
knowledge learning as a classification problem using response coefficients as
the classification scores. Knowledge embeddings and image-based representations
are updated during training so that the knowledge embedding includes
discriminative clues for different images. Finally, we incorporate the acquired
knowledge embeddings into the image-based representations as comprehensive
representations, leading to significantly higher performance. Extensive
evaluations on the six popular datasets demonstrate the advantage of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinda Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based Augmented Reality for Surgical Guidance. (arXiv:2107.06397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06397</id>
        <link href="http://arxiv.org/abs/2107.06397"/>
        <updated>2021-07-15T01:59:03.060Z</updated>
        <summary type="html"><![CDATA[We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1"&gt;Mitchell Doughty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Karan Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1"&gt;Nilesh R. Ghugre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DVMN: Dense Validity Mask Network for Depth Completion. (arXiv:2107.06709v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06709</id>
        <link href="http://arxiv.org/abs/2107.06709"/>
        <updated>2021-07-15T01:59:03.055Z</updated>
        <summary type="html"><![CDATA[LiDAR depth maps provide environmental guidance in a variety of applications.
However, such depth maps are typically sparse and insufficient for complex
tasks such as autonomous navigation. State of the art methods use image guided
neural networks for dense depth completion. We develop a guided convolutional
neural network focusing on gathering dense and valid information from sparse
depth maps. To this end, we introduce a novel layer with spatially variant and
content-depended dilation to include additional data from sparse input.
Furthermore, we propose a sparsity invariant residual bottleneck block. We
evaluate our Dense Validity Mask Network (DVMN) on the KITTI depth completion
benchmark and achieve state of the art results. At the time of submission, our
network is the leading method using sparsity invariant convolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reichardt_L/0/1/0/all/0/1"&gt;Laurenz Reichardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1"&gt;Patrick Mangat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1"&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Image Compression with Discretized Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules. (arXiv:2107.06463v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06463</id>
        <link href="http://arxiv.org/abs/2107.06463"/>
        <updated>2021-07-15T01:59:03.048Z</updated>
        <summary type="html"><![CDATA[Recently deep learning-based image compression methods have achieved
significant achievements and gradually outperformed traditional approaches
including the latest standard Versatile Video Coding (VVC) in both PSNR and
MS-SSIM metrics. Two key components of learned image compression frameworks are
the entropy model of the latent representations and the encoding/decoding
network architectures. Various models have been proposed, such as
autoregressive, softmax, logistic mixture, Gaussian mixture, and Laplacian.
Existing schemes only use one of these models. However, due to the vast
diversity of images, it is not optimal to use one model for all images, even
different regions of one image. In this paper, we propose a more flexible
discretized Gaussian-Laplacian-Logistic mixture model (GLLMM) for the latent
representations, which can adapt to different contents in different images and
different regions of one image more accurately. Besides, in the
encoding/decoding network design part, we propose a concatenated residual
blocks (CRB), where multiple residual blocks are serially connected with
additional shortcut connections. The CRB can improve the learning ability of
the network, which can further improve the compression performance.
Experimental results using the Kodak and Tecnick datasets show that the
proposed scheme outperforms all the state-of-the-art learning-based methods and
existing compression standards including VVC intra coding (4:4:4 and 4:2:0) in
terms of the PSNR and MS-SSIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1"&gt;Haisheng Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_F/0/1/0/all/0/1"&gt;Feng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1"&gt;Mohammad Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guohe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_C/0/1/0/all/0/1"&gt;Chengjie Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1"&gt;Jingning Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06652</id>
        <link href="http://arxiv.org/abs/2107.06652"/>
        <updated>2021-07-15T01:59:03.042Z</updated>
        <summary type="html"><![CDATA[This paper explores the use of self-supervised deep learning in medical
imaging in cases where two scan modalities are available for the same subject.
Specifically, we use a large publicly-available dataset of over 20,000 subjects
from the UK Biobank with both whole body Dixon technique magnetic resonance
(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three
contributions: (i) We introduce a multi-modal image-matching contrastive
framework, that is able to learn to match different-modality scans of the same
subject with high accuracy. (ii) Without any adaption, we show that the
correspondences learnt during this contrastive training step can be used to
perform automatic cross-modal scan registration in a completely unsupervised
manner. (iii) Finally, we use these registrations to transfer segmentation maps
from the DXA scans to the MR scans where they are used to train a network to
segment anatomical regions without requiring ground-truth MR examples. To aid
further research, our code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1"&gt;Rhydian Windsor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Human Motion Prediction via A Bayesian Neural Network. (arXiv:2107.06564v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.06564</id>
        <link href="http://arxiv.org/abs/2107.06564"/>
        <updated>2021-07-15T01:59:03.027Z</updated>
        <summary type="html"><![CDATA[Human motion prediction is an important and challenging topic that has
promising prospects in efficient and safe human-robot-interaction systems.
Currently, the majority of the human motion prediction algorithms are based on
deterministic models, which may lead to risky decisions for robots. To solve
this problem, we propose a probabilistic model for human motion prediction in
this paper. The key idea of our approach is to extend the conventional
deterministic motion prediction neural network to a Bayesian one. On one hand,
our model could generate several future motions when given an observed motion
sequence. On the other hand, by calculating the Epistemic Uncertainty and the
Heteroscedastic Aleatoric Uncertainty, our model could tell the robot if the
observation has been seen before and also give the optimal result among all
possible predictions. We extensively validate our approach on a large scale
benchmark dataset Human3.6m. The experiments show that our approach performs
better than deterministic methods. We further evaluate our approach in a
Human-Robot-Interaction (HRI) scenario. The experimental results show that our
approach makes the interaction more efficient and safer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1"&gt;Xuguang Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in PET: an Industry Perspective. (arXiv:2107.06747v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06747</id>
        <link href="http://arxiv.org/abs/2107.06747"/>
        <updated>2021-07-15T01:59:03.021Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has significant potential to positively impact
and advance medical imaging, including positron emission tomography (PET)
imaging applications. AI has the ability to enhance and optimize all aspects of
the PET imaging chain from patient scheduling, patient setup, protocoling, data
acquisition, detector signal processing, reconstruction, image processing and
interpretation. AI poses industry-specific challenges which will need to be
addressed and overcome to maximize the future potentials of AI in PET. This
paper provides an overview of these industry-specific challenges for the
development, standardization, commercialization, and clinical adoption of AI,
and explores the potential enhancements to PET imaging brought on by AI in the
near future. In particular, the combination of on-demand image reconstruction,
AI, and custom designed data processing workflows may open new possibilities
for innovation which would positively impact the industry and ultimately
patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1"&gt;Arkadiusz Sitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sangtae Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asma_E/0/1/0/all/0/1"&gt;Evren Asma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandler_A/0/1/0/all/0/1"&gt;Adam Chandler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihsani_A/0/1/0/all/0/1"&gt;Alvin Ihsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prevrhal_S/0/1/0/all/0/1"&gt;Sven Prevrhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1"&gt;Arman Rahmim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saboury_B/0/1/0/all/0/1"&gt;Babak Saboury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thielemans_K/0/1/0/all/0/1"&gt;Kris Thielemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative and reproducible benchmarks for comprehensive evaluation of machine learning classifiers. (arXiv:2107.06475v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06475</id>
        <link href="http://arxiv.org/abs/2107.06475"/>
        <updated>2021-07-15T01:59:03.015Z</updated>
        <summary type="html"><![CDATA[Understanding the strengths and weaknesses of machine learning (ML)
algorithms is crucial for determine their scope of application. Here, we
introduce the DIverse and GENerative ML Benchmark (DIGEN) - a collection of
synthetic datasets for comprehensive, reproducible, and interpretable
benchmarking of machine learning algorithms for classification of binary
outcomes. The DIGEN resource consists of 40 mathematical functions which map
continuous features to discrete endpoints for creating synthetic datasets.
These 40 functions were discovered using a heuristic algorithm designed to
maximize the diversity of performance among multiple popular machine learning
algorithms thus providing a useful test suite for evaluating and comparing new
methods. Access to the generative functions facilitates understanding of why a
method performs poorly compared to other algorithms thus providing ideas for
improvement. The resource with extensive documentation and analyses is
open-source and available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orzechowski_P/0/1/0/all/0/1"&gt;Patryk Orzechowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1"&gt;Jason H. Moore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convolutional Neural Network Approach to the Classification of Engineering Models. (arXiv:2107.06481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06481</id>
        <link href="http://arxiv.org/abs/2107.06481"/>
        <updated>2021-07-15T01:59:03.008Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning approach for the classification of
Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to
the availability of large annotated datasets and also enough computational
power in the form of GPUs, many deep learning-based solutions for object
classification have been proposed of late, especially in the domain of images
and graphical models. Nevertheless, very few solutions have been proposed for
the task of functional classification of CAD models. Hence, for this research,
CAD models have been collected from Engineering Shape Benchmark (ESB), National
Design Repository (NDR) and augmented with newer models created using a
modelling software to form a dataset - 'CADNET'. It is proposed to use a
residual network architecture for CADNET, inspired by the popular ResNet. A
weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature
extraction, and the generated images are fed as inputs to the CNN. The problem
of class imbalance in the dataset is addressed using a class weights approach.
Experiments have been conducted with other signatures such as geodesic distance
etc. using deep networks as well as other network architectures on the CADNET.
The LFD-based CNN approach using the proposed network architecture, along with
gradient boosting yielded the best classification accuracy on CADNET.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskare_P/0/1/0/all/0/1"&gt;Pranjal Bhaskare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10271</id>
        <link href="http://arxiv.org/abs/2106.10271"/>
        <updated>2021-07-15T01:59:03.002Z</updated>
        <summary type="html"><![CDATA[Temporal action detection (TAD) aims to determine the semantic label and the
boundaries of every action instance in an untrimmed video. It is a fundamental
and challenging task in video understanding and significant progress has been
made. Previous methods involve multiple stages or networks and hand-designed
rules or operations, which fall short in efficiency and flexibility. In this
paper, we propose an end-to-end framework for TAD upon Transformer, termed
\textit{TadTR}, which maps a set of learnable embeddings to action instances in
parallel. TadTR is able to adaptively extract temporal context information
required for making action predictions, by selectively attending to a sparse
set of snippets in a video. As a result, it simplifies the pipeline of TAD and
requires lower computation cost than previous detectors, while preserving
remarkable detection performance. TadTR achieves state-of-the-art performance
on HACS Segments (+3.35% average mAP). As a single-network detector, TadTR runs
10$\times$ faster than its comparable competitor. It outperforms existing
single-network detectors by a large margin on THUMOS14 (+5.0% average mAP) and
ActivityNet (+7.53% average mAP). When combined with other detectors, it
reports 54.1% mAP at IoU=0.5 on THUMOS14, and 34.55% average mAP on
ActivityNet-1.3. Our code will be released at
\url{https://github.com/xlliu7/TadTR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qimeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Neural Rendering for Image Hazing. (arXiv:2107.06681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06681</id>
        <link href="http://arxiv.org/abs/2107.06681"/>
        <updated>2021-07-15T01:59:02.996Z</updated>
        <summary type="html"><![CDATA[Image hazing aims to render a hazy image from a given clean one, which could
be applied to a variety of practical applications such as gaming, filming,
photographic filtering, and image dehazing. To generate plausible haze, we
study two less-touched but challenging problems in hazy image rendering,
namely, i) how to estimate the transmission map from a single image without
auxiliary information, and ii) how to adaptively learn the airlight from
exemplars, i.e., unpaired real hazy images. To this end, we propose a neural
rendering method for image hazing, dubbed as HazeGEN. To be specific, HazeGEN
is a knowledge-driven neural network which estimates the transmission map by
leveraging a new prior, i.e., there exists the structure similarity (e.g.,
contour and luminance) between the transmission map and the input clean image.
To adaptively learn the airlight, we build a neural module based on another new
prior, i.e., the rendered hazy image and the exemplar are similar in the
airlight distribution. To the best of our knowledge, this could be the first
attempt to deeply rendering hazy images in an unsupervised fashion. Comparing
with existing haze generation methods, HazeGEN renders the hazy images in an
unsupervised, learnable, and controllable manner, thus avoiding the
labor-intensive efforts in paired data collection and the domain-shift issue in
haze generation. Extensive experiments show the promising performance of our
method comparing with some baselines in both qualitative and quantitative
comparisons. The code will be released on GitHub after acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yijie Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Peng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. (arXiv:2104.14540v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14540</id>
        <link href="http://arxiv.org/abs/2104.14540"/>
        <updated>2021-07-15T01:59:02.978Z</updated>
        <summary type="html"><![CDATA[Self-supervised monocular depth estimation networks are trained to predict
scene depth using nearby frames as a supervision signal during training.
However, for many applications, sequence information in the form of video
frames is also available at test time. The vast majority of monocular networks
do not make use of this extra signal, thus ignoring valuable information that
could be used to improve the predicted depth. Those that do, either use
computationally expensive test-time refinement techniques or off-the-shelf
recurrent networks, which only indirectly make use of the geometric information
that is inherently available.

We propose ManyDepth, an adaptive approach to dense depth estimation that can
make use of sequence information at test time, when it is available. Taking
inspiration from multi-view stereo, we propose a deep end-to-end cost volume
based approach that is trained using self-supervision only. We present a novel
consistency loss that encourages the network to ignore the cost volume when it
is deemed unreliable, e.g. in the case of moving objects, and an augmentation
scheme to cope with static cameras. Our detailed experiments on both KITTI and
Cityscapes show that we outperform all published self-supervised baselines,
including those that use single or multiple frames at test time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1"&gt;Jamie Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1"&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1"&gt;Victor Prisacariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brostow_G/0/1/0/all/0/1"&gt;Gabriel Brostow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1"&gt;Michael Firman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease Localization in X-ray images. (arXiv:2107.06442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06442</id>
        <link href="http://arxiv.org/abs/2107.06442"/>
        <updated>2021-07-15T01:59:02.971Z</updated>
        <summary type="html"><![CDATA[Locating diseases in chest X-ray images with few careful annotations saves
large human effort. Recent works approached this task with innovative
weakly-supervised algorithms such as multi-instance learning (MIL) and class
activation maps (CAM), however, these methods often yield inaccurate or
incomplete regions. One of the reasons is the neglection of the pathological
implications hidden in the relationship across anatomical regions within each
image and the relationship across images. In this paper, we argue that the
cross-region and cross-image relationship, as contextual and compensating
information, is vital to obtain more consistent and integral regions. To model
the relationship, we propose the Graph Regularized Embedding Network (GREN),
which leverages the intra-image and inter-image information to locate diseases
on chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes,
and then models the intra-image relationship between the lung lobes using an
intra-image graph to compare different regions. Meanwhile, the relationship
between in-batch images is modeled by an inter-image graph to compare multiple
images. This process mimics the training and decision-making process of a
radiologist: comparing multiple regions and images for diagnosis. In order for
the deep embedding layers of the neural network to retain structural
information (important in the localization task), we use the Hash coding and
Hamming distance to compute the graphs, which are used as regularizers to
facilitate training. By means of this, our approach achieves the
state-of-the-art result on NIH chest X-ray dataset for weakly-supervised
disease localization. Our codes are accessible online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1"&gt;Baolian Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1"&gt;Chaowei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1"&gt;Chengwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Huiguang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Analysis of Visual COVID-19 Features from Chest Radiographs. (arXiv:2107.06618v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06618</id>
        <link href="http://arxiv.org/abs/2107.06618"/>
        <updated>2021-07-15T01:59:02.965Z</updated>
        <summary type="html"><![CDATA[Chest radiography has been a recommended procedure for patient triaging and
resource management in intensive care units (ICUs) throughout the COVID-19
pandemic. The machine learning efforts to augment this workflow have been long
challenged due to deficiencies in reporting, model evaluation, and failure mode
analysis. To address some of those shortcomings, we model radiological features
with a human-interpretable class hierarchy that aligns with the radiological
decision process. Also, we propose the use of a data-driven error analysis
methodology to uncover the blind spots of our model, providing further
transparency on its clinical utility. For example, our experiments show that
model failures highly correlate with ICU imaging conditions and with the
inherent difficulty in distinguishing certain types of radiological features.
Also, our hierarchical interpretation and analysis facilitates the comparison
with respect to radiologists' findings and inter-variability, which in return
helps us to better assess the clinical applicability of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bannur_S/0/1/0/all/0/1"&gt;Shruthi Bannur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oktay_O/0/1/0/all/0/1"&gt;Ozan Oktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bernhardt_M/0/1/0/all/0/1"&gt;Melanie Bernhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schwaighofer_A/0/1/0/all/0/1"&gt;Anton Schwaighofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jena_R/0/1/0/all/0/1"&gt;Rajesh Jena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nushi_B/0/1/0/all/0/1"&gt;Besmira Nushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wadhwani_S/0/1/0/all/0/1"&gt;Sharan Wadhwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nori_A/0/1/0/all/0/1"&gt;Aditya Nori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Natarajan_K/0/1/0/all/0/1"&gt;Kal Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashraf_S/0/1/0/all/0/1"&gt;Shazad Ashraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alvarez_Valle_J/0/1/0/all/0/1"&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Castro_D/0/1/0/all/0/1"&gt;Daniel C. Castro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Abnormal Behavior with Self-Supervised Gaze Estimation. (arXiv:2107.06530v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06530</id>
        <link href="http://arxiv.org/abs/2107.06530"/>
        <updated>2021-07-15T01:59:02.959Z</updated>
        <summary type="html"><![CDATA[Due to the recent outbreak of COVID-19, many classes, exams, and meetings
have been conducted non-face-to-face. However, the foundation for video
conferencing solutions is still insufficient. So this technology has become an
important issue. In particular, these technologies are essential for
non-face-to-face testing, and technology dissemination is urgent. In this
paper, we present a single video conferencing solution using gaze estimation in
preparation for these problems. Gaze is an important cue for the tasks such as
analysis of human behavior. Hence, numerous studies have been proposed to solve
gaze estimation using deep learning, which is one of the most prominent methods
up to date. We use these gaze estimation methods to detect abnormal behavior of
video conferencing participants. Our contribution is as follows. i) We find and
apply the optimal network for the gaze estimation method and apply a
self-supervised method to improve accuracy. ii) For anomaly detection, we
present a new dataset that aggregates the values of a new gaze, head pose, etc.
iii) We train newly created data on Multi Layer Perceptron (MLP) models to
detect anomaly behavior based on deep learning. We demonstrate the robustness
of our method through experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneung-Kim/0/1/0/all/0/1"&gt;Suneung-Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06831</id>
        <link href="http://arxiv.org/abs/2107.06831"/>
        <updated>2021-07-15T01:59:02.942Z</updated>
        <summary type="html"><![CDATA[As a key component of talking face generation, lip movements generation
determines the naturalness and coherence of the generated talking face video.
Prior literature mainly focuses on speech-to-lip generation while there is a
paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing
end-to-end works depend on the attention mechanism and autoregressive (AR)
decoding manner. However, the AR decoding manner generates current lip frame
conditioned on frames generated previously, which inherently hinders the
inference speed, and also has a detrimental effect on the quality of generated
lip frames due to error propagation. This encourages the research of parallel
T2L generation. In this work, we propose a novel parallel decoding model for
high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we
predict the duration of the encoded linguistic features and model the target
lip frames conditioned on the encoded linguistic features with their duration
in a non-autoregressive manner. Furthermore, we incorporate the structural
similarity index loss and adversarial learning to improve perceptual quality of
generated lip frames and alleviate the blurry prediction problem. Extensive
experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L
generates lip movements with competitive quality compared with the
state-of-the-art AR T2L model DualLip and exceeds the baseline AR model
TransformerT2L by a notable margin benefiting from the mitigation of the error
propagation problem; and 2) exhibits distinct superiority in inference speed
(an average speedup of 19$\times$ than DualLip on TCD-TIMIT).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhiying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03857</id>
        <link href="http://arxiv.org/abs/2105.03857"/>
        <updated>2021-07-15T01:59:02.936Z</updated>
        <summary type="html"><![CDATA[Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to
effectively train 3D-CNN by some slices from 3D seismic data, so that the model
can learn the segmentation of 3D seismic data from a few 2D slices. In order to
fully extract information from limited data and suppress seismic noise, we
propose an attention module that can be used for active supervision training
and embedded in the network. The attention heatmap label is generated by the
original label, and letting it supervise the attention module using the
lambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss
function, the method can extract 3D seismic features from a few 2D slice
labels. And it also shows the advanced performance of the attention module,
which can significantly suppress the noise in the seismic data while increasing
the model's sensitivity to the foreground. Finally, on the public test set, we
only use the 2D slice labels training that accounts for 3.3% of the 3D volume
label, and achieve similar performance to the 3D volume label training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1"&gt;YiMin Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kewen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianbing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1"&gt;Yingjie Xi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Initial Pools for Deep Active Learning. (arXiv:2011.14696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14696</id>
        <link href="http://arxiv.org/abs/2011.14696"/>
        <updated>2021-07-15T01:59:02.929Z</updated>
        <summary type="html"><![CDATA[Active Learning (AL) techniques aim to minimize the training data required to
train a model for a given task. Pool-based AL techniques start with a small
initial labeled pool and then iteratively pick batches of the most informative
samples for labeling. Generally, the initial pool is sampled randomly and
labeled to seed the AL iterations. While recent studies have focused on
evaluating the robustness of various query functions in AL, little to no
attention has been given to the design of the initial labeled pool for deep
active learning. Given the recent successes of learning representations in
self-supervised/unsupervised ways, we study if an intelligently sampled initial
labeled pool can improve deep AL performance. We investigate the effect of
intelligently sampled initial labeled pools, including the use of
self-supervised and unsupervised strategies, on deep AL methods. The setup,
hypotheses, methodology, and implementation details were evaluated by peer
review before experiments were conducted. Experimental results could not
conclusively prove that intelligently sampled initial pools are better for AL
than random initial pools in the long run, although a Variational
Autoencoder-based initial pool sampling strategy showed interesting trends that
merit deeper investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1"&gt;Akshay L Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sai Vikas Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive Representations of Weather Scenes for Strategic Air Traffic Flow Management. (arXiv:2107.06394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06394</id>
        <link href="http://arxiv.org/abs/2107.06394"/>
        <updated>2021-07-15T01:59:02.923Z</updated>
        <summary type="html"><![CDATA[Terse representation of high-dimensional weather scene data is explored, in
support of strategic air traffic flow management objectives. Specifically, we
consider whether aviation-relevant weather scenes are compressible, in the
sense that each scene admits a possibly-different sparse representation in a
basis of interest. Here, compression of weather scenes extracted from METAR
data (including temperature, flight categories, and visibility profiles for the
contiguous United States) is examined, for the graph-spectral basis. The scenes
are found to be compressible, with 75-95% of the scene content captured using
0.5-4% of the basis vectors. Further, the dominant basis vectors for each scene
are seen to identify time-varying spatial characteristics of the weather, and
reconstruction from the compressed representation is demonstrated. Finally,
potential uses of the compressive representations in strategic TFM design are
briefly scoped.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Sandip Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering. (arXiv:2107.06325v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06325</id>
        <link href="http://arxiv.org/abs/2107.06325"/>
        <updated>2021-07-15T01:59:02.917Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering (VQA) is concerned with answering free-form
questions about an image. Since it requires a deep semantic and linguistic
understanding of the question and the ability to associate it with various
objects that are present in the image, it is an ambitious task and requires
multi-modal reasoning from both computer vision and natural language
processing. We propose Graphhopper, a novel method that approaches the task by
integrating knowledge graph reasoning, computer vision, and natural language
processing techniques. Concretely, our method is based on performing
context-driven, sequential reasoning based on the scene entities and their
semantic and spatial relationships. As a first step, we derive a scene graph
that describes the objects in the image, as well as their attributes and their
mutual relationships. Subsequently, a reinforcement learning agent is trained
to autonomously navigate in a multi-hop manner over the extracted scene graph
to generate reasoning paths, which are the basis for deriving answers. We
conduct an experimental study on the challenging dataset GQA, based on both
manually curated and automatically generated scene graphs. Our results show
that we keep up with a human performance on manually curated scene graphs.
Moreover, we find that Graphhopper outperforms another state-of-the-art scene
graph reasoning model on both manually curated and automatically generated
scene graphs by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1"&gt;Rajat Koner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hildebrandt_M/0/1/0/all/0/1"&gt;Marcel Hildebrandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1"&gt;Deepan Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Detection in the DCT Domain: is Luminance the Solution?. (arXiv:2006.05732v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05732</id>
        <link href="http://arxiv.org/abs/2006.05732"/>
        <updated>2021-07-15T01:59:02.911Z</updated>
        <summary type="html"><![CDATA[Object detection in images has reached unprecedented performances. The
state-of-the-art methods rely on deep architectures that extract salient
features and predict bounding boxes enclosing the objects of interest. These
methods essentially run on RGB images. However, the RGB images are often
compressed by the acquisition devices for storage purpose and transfer
efficiency. Hence, their decompression is required for object detectors. To
gain in efficiency, this paper proposes to take advantage of the compressed
representation of images to carry out object detection usable in constrained
resources conditions.

Specifically, we focus on JPEG images and propose a thorough analysis of
detection architectures newly designed in regard of the peculiarities of the
JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard
RGB-based architecture, while only reducing the detection performance by 5.5%.
Additionally, our empirical findings demonstrate that only part of the
compressed JPEG information, namely the luminance component, may be required to
match detection accuracy of the full input methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deguerre_B/0/1/0/all/0/1"&gt;Benjamin Deguerre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatelain_C/0/1/0/all/0/1"&gt;Clement Chatelain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. (arXiv:2107.06505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06505</id>
        <link href="http://arxiv.org/abs/2107.06505"/>
        <updated>2021-07-15T01:59:02.895Z</updated>
        <summary type="html"><![CDATA[Recent neural rendering approaches for human activities achieve remarkable
view synthesis results, but still rely on dense input views or dense training
with all the capture frames, leading to deployment difficulty and inefficient
training overload. However, existing advances will be ill-posed if the input is
both spatially and temporally sparse. To fill this gap, in this paper we
propose a few-shot neural human rendering approach (FNHR) from only sparse RGBD
inputs, which exploits the temporal and spatial redundancy to generate
photo-realistic free-view output of human activities. Our FNHR is trained only
on the key-frames which expand the motion manifold in the input sequences. We
introduce a two-branch neural blending to combine the neural point render and
classical graphics texturing pipeline, which integrates reliable observations
over sparse key-frames. Furthermore, we adopt a patch-based adversarial
training process to make use of the local redundancy and avoids over-fitting to
the key-frames, which generates fine-detailed rendering results. Extensive
experiments demonstrate the effectiveness of our approach to generate
high-quality free view-point results for challenging human performances under
the sparse setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haimin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Minye Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense. (arXiv:2107.06456v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06456</id>
        <link href="http://arxiv.org/abs/2107.06456"/>
        <updated>2021-07-15T01:59:02.889Z</updated>
        <summary type="html"><![CDATA[We propose an AID-purifier that can boost the robustness of
adversarially-trained networks by purifying their inputs. AID-purifier is an
auxiliary network that works as an add-on to an already trained main
classifier. To keep it computationally light, it is trained as a discriminator
with a binary cross-entropy loss. To obtain additionally useful information
from the adversarial examples, the architecture design is closely related to
information maximization principles where two layers of the main classification
network are piped to the auxiliary network. To assist the iterative
optimization procedure of purification, the auxiliary network is trained with
AVmixup. AID-purifier can be used together with other purifiers such as
PixelDefend for an extra enhancement. The overall results indicate that the
best performing adversarially-trained networks can be enhanced by the best
performing purification networks, where AID-purifier is a competitive candidate
that is light and robust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1"&gt;Duhun Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunjung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Ultrasound Frame to Volume Registration. (arXiv:2107.06449v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06449</id>
        <link href="http://arxiv.org/abs/2107.06449"/>
        <updated>2021-07-15T01:59:02.883Z</updated>
        <summary type="html"><![CDATA[Fusing intra-operative 2D transrectal ultrasound (TRUS) image with
pre-operative 3D magnetic resonance (MR) volume to guide prostate biopsy can
significantly increase the yield. However, such a multimodal 2D/3D registration
problem is a very challenging task. In this paper, we propose an end-to-end
frame-to-volume registration network (FVR-Net), which can efficiently bridge
the previous research gaps by aligning a 2D TRUS frame with a 3D TRUS volume
without requiring hardware tracking. The proposed FVR-Net utilizes a
dual-branch feature extraction module to extract the information from TRUS
frame and volume to estimate transformation parameters. We also introduce a
differentiable 2D slice sampling module which allows gradients backpropagating
from an unsupervised image similarity loss for content correspondence learning.
Our model shows superior efficiency for real-time interventional guidance with
highly competitive registration accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengtao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"&gt;Sheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wood_B/0/1/0/all/0/1"&gt;Bradford J. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06501</id>
        <link href="http://arxiv.org/abs/2107.06501"/>
        <updated>2021-07-15T01:59:02.877Z</updated>
        <summary type="html"><![CDATA[High-level representation-guided pixel denoising and adversarial training are
independent solutions to enhance the robustness of CNNs against adversarial
attacks by pre-processing input data and re-training models, respectively. Most
recently, adversarial training techniques have been widely studied and improved
while the pixel denoising-based method is getting less attractive. However, it
is still questionable whether there exists a more advanced pixel
denoising-based method and whether the combination of the two solutions
benefits each other. To this end, we first comprehensively investigate two
kinds of pixel denoising methods for adversarial robustness enhancement (i.e.,
existing additive-based and unexplored filtering-based methods) under the loss
functions of image-level and semantic-level restorations, respectively, showing
that pixel-wise filtering can obtain much higher image quality (e.g., higher
PSNR) as well as higher robustness (e.g., higher accuracy on adversarial
examples) than existing pixel-wise additive-based method. However, we also
observe that the robustness results of the filtering-based method rely on the
perturbation amplitude of adversarial examples used for training. To address
this problem, we propose predictive perturbation-aware pixel-wise filtering,
where dual-perturbation filtering and an uncertainty-aware fusion module are
designed and employed to automatically perceive the perturbation amplitude
during the training and testing process. The proposed method is termed as
AdvFilter. Moreover, we combine adversarial pixel denoising methods with three
adversarial training-based methods, hinting that considering data and models
jointly is able to achieve more robust CNNs. The experiments conduct on
NeurIPS-2017DEV, SVHN, and CIFAR10 datasets and show the advantages over
enhancing CNNs' robustness, high generalization to different models, and noise
levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_W/0/1/0/all/0/1"&gt;Weikai Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;Geguang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDC: Piecewise Depth Completion utilizing Superpixels. (arXiv:2107.06711v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06711</id>
        <link href="http://arxiv.org/abs/2107.06711"/>
        <updated>2021-07-15T01:59:02.861Z</updated>
        <summary type="html"><![CDATA[Depth completion from sparse LiDAR and high-resolution RGB data is one of the
foundations for autonomous driving techniques. Current approaches often rely on
CNN-based methods with several known drawbacks: flying pixel at depth
discontinuities, overfitting to both a given data set as well as error metric,
and many more. Thus, we propose our novel Piecewise Depth Completion (PDC),
which works completely without deep learning. PDC segments the RGB image into
superpixels corresponding the regions with similar depth value. Superpixels
corresponding to same objects are gathered using a cost map. At the end, we
receive detailed depth images with state of the art accuracy. In our
evaluation, we can show both the influence of the individual proposed
processing steps and the overall performance of our method on the challenging
KITTI dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teutscher_D/0/1/0/all/0/1"&gt;Dennis Teutscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangat_P/0/1/0/all/0/1"&gt;Patrick Mangat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasenmuller_O/0/1/0/all/0/1"&gt;Oliver Wasenm&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Lottery Ticket Hypothesis. (arXiv:2107.06825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06825</id>
        <link href="http://arxiv.org/abs/2107.06825"/>
        <updated>2021-07-15T01:59:02.855Z</updated>
        <summary type="html"><![CDATA[We introduce a generalization to the lottery ticket hypothesis in which the
notion of "sparsity" is relaxed by choosing an arbitrary basis in the space of
parameters. We present evidence that the original results reported for the
canonical basis continue to hold in this broader setting. We describe how
structured pruning methods, including pruning units or factorizing
fully-connected layers into products of low-rank matrices, can be cast as
particular instances of this "generalized" lottery ticket hypothesis. The
investigations reported here are preliminary and are provided to encourage
further research along this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1"&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markeeva_L/0/1/0/all/0/1"&gt;Larisa Markeeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keysers_D/0/1/0/all/0/1"&gt;Daniel Keysers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolstikhin_I/0/1/0/all/0/1"&gt;Ilya Tolstikhin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Novel View Synthesis. (arXiv:2107.06812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06812</id>
        <link href="http://arxiv.org/abs/2107.06812"/>
        <updated>2021-07-15T01:59:02.849Z</updated>
        <summary type="html"><![CDATA[Predicting novel views of a scene from real-world images has always been a
challenging task. In this work, we propose a deep convolutional neural network
(CNN) which learns to predict novel views of a scene from given collection of
images. In comparison to prior deep learning based approaches, which can handle
only a fixed number of input images to predict novel view, proposed approach
works with different numbers of input images. The proposed model explicitly
performs feature extraction and matching from a given pair of input images and
estimates, at each pixel, the probability distribution (pdf) over possible
depth levels in the scene. This pdf is then used for estimating the novel view.
The model estimates multiple predictions of novel view, one estimate per input
image pair, from given image collection. The model also estimates an occlusion
mask and combines multiple novel view estimates in to a single optimal
prediction. The finite number of depth levels used in the analysis may cause
occasional blurriness in the estimated view. We mitigate this issue with simple
multi-resolution analysis which improves the quality of the estimates. We
substantiate the performance on different datasets and show competitive
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+More_A/0/1/0/all/0/1"&gt;Amit More&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Hypothesis Transfer for Source-Free Domain Adaptation. (arXiv:2107.06735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06735</id>
        <link href="http://arxiv.org/abs/2107.06735"/>
        <updated>2021-07-15T01:59:02.843Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation has been widely used to deal with the distribution shift in
vision, language, multimedia etc. Most domain adaptation methods learn
domain-invariant features with data from both domains available. However, such
a strategy might be infeasible in practice when source data are unavailable due
to data-privacy concerns. To address this issue, we propose a novel adaptation
method via hypothesis transfer without accessing source data at adaptation
stage. In order to fully use the limited target data, a semi-supervised mutual
enhancement method is proposed, in which entropy minimization and augmented
label propagation are used iteratively to perform inter-domain and intra-domain
alignments. Compared with state-of-the-art methods, the experimental results on
three public datasets demonstrate that our method gets up to 19.9% improvements
on semi-supervised adaptation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Ning Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jiajun Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Lixian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jun Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Foes of Neural Network's Data Efficiency Among Unnecessary Input Dimensions. (arXiv:2107.06409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06409</id>
        <link href="http://arxiv.org/abs/2107.06409"/>
        <updated>2021-07-15T01:59:02.836Z</updated>
        <summary type="html"><![CDATA[Datasets often contain input dimensions that are unnecessary to predict the
output label, e.g. background in object recognition, which lead to more
trainable parameters. Deep Neural Networks (DNNs) are robust to increasing the
number of parameters in the hidden layers, but it is unclear whether this holds
true for the input layer. In this letter, we investigate the impact of
unnecessary input dimensions on a central issue of DNNs: their data efficiency,
ie. the amount of examples needed to achieve certain generalization
performance. Our results show that unnecessary input dimensions that are
task-unrelated substantially degrade data efficiency. This highlights the need
for mechanisms that remove {task-unrelated} dimensions to enable data
efficiency gains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1"&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-isomorphic Inter-modality Graph Alignment and Synthesis for Holistic Brain Mapping. (arXiv:2107.06281v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2107.06281</id>
        <link href="http://arxiv.org/abs/2107.06281"/>
        <updated>2021-07-15T01:59:02.828Z</updated>
        <summary type="html"><![CDATA[Brain graph synthesis marked a new era for predicting a target brain graph
from a source one without incurring the high acquisition cost and processing
time of neuroimaging data. However, existing multi-modal graph synthesis
frameworks have several limitations. First, they mainly focus on generating
graphs from the same domain (intra-modality), overlooking the rich multimodal
representations of brain connectivity (inter-modality). Second, they can only
handle isomorphic graph generation tasks, limiting their generalizability to
synthesizing target graphs with a different node size and topological structure
from those of the source one. More importantly, both target and source domains
might have different distributions, which causes a domain fracture between them
(i.e., distribution misalignment). To address such challenges, we propose an
inter-modality aligner of non-isomorphic graphs (IMANGraphNet) framework to
infer a target graph modality based on a given modality. Our three core
contributions lie in (i) predicting a target graph (e.g., functional) from a
source graph (e.g., morphological) based on a novel graph generative
adversarial network (gGAN); (ii) using non-isomorphic graphs for both source
and target domains with a different number of nodes, edges and structure; and
(iii) enforcing the predicted target distribution to match that of the ground
truth graphs using a graph autoencoder to relax the designed loss oprimization.
To handle the unstable behavior of gGAN, we design a new Ground
Truth-Preserving (GT-P) loss function to guide the generator in learning the
topological structure of ground truth brain graphs. Our comprehensive
experiments on predicting functional from morphological graphs demonstrate the
outperformance of IMANGraphNet in comparison with its variants. This can be
further leveraged for integrative and holistic brain mapping in health and
disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Mhiri_I/0/1/0/all/0/1"&gt;Islem Mhiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebli_A/0/1/0/all/0/1"&gt;Ahmed Nebli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mahjoub_M/0/1/0/all/0/1"&gt;Mohamed Ali Mahjoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1"&gt;Islem Rekik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04408</id>
        <link href="http://arxiv.org/abs/2011.04408"/>
        <updated>2021-07-15T01:59:02.821Z</updated>
        <summary type="html"><![CDATA[Different environments pose a great challenge on the outdoor robust visual
perception for long-term autonomous driving and the generalization of
learning-based algorithms on different environmental effects is still an open
problem. Although monocular depth prediction has been well studied recently,
there is few work focusing on the robust learning-based depth prediction across
different environments, e.g., changing illumination and seasons, owing to the
lack of such a multi-environment real-world dataset and benchmark. To this end,
the first cross-season monocular depth prediction dataset and benchmark
SeasonDepth (available on https://seasondepth.github.io/) is built based on CMU
Visual Localization dataset. To benchmark the depth estimation performance
under different environments, we investigate representative and recent
state-of-the-art open-source supervised, self-supervised and domain adaptation
depth prediction methods from KITTI benchmark using several newly-formulated
metrics. Through extensive experimental evaluation on the proposed dataset, the
influence of multiple environments on performance and robustness is analyzed
both qualitatively and quantitatively, showing that the long-term monocular
depth prediction is far from solved even with fine-tuning. We further give
promising avenues that self-supervised training and stereo geometry constraint
help to enhance the robustness to changing environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Baoquan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Attention Generative Adversarial Network for Remote Sensing Image Super-Resolution. (arXiv:2107.06536v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06536</id>
        <link href="http://arxiv.org/abs/2107.06536"/>
        <updated>2021-07-15T01:59:02.814Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) methods can generate remote sensing images with
high spatial resolution without increasing the cost, thereby providing a
feasible way to acquire high-resolution remote sensing images, which are
difficult to obtain due to the high cost of acquisition equipment and complex
weather. Clearly, image super-resolution is a severe ill-posed problem.
Fortunately, with the development of deep learning, the powerful fitting
ability of deep neural networks has solved this problem to some extent. In this
paper, we propose a network based on the generative adversarial network (GAN)
to generate high resolution remote sensing images, named the multi-attention
generative adversarial network (MA-GAN). We first designed a GAN-based
framework for the image SR task. The core to accomplishing the SR task is the
image generator with post-upsampling that we designed. The main body of the
generator contains two blocks; one is the pyramidal convolution in the
residual-dense block (PCRDB), and the other is the attention-based upsample
(AUP) block. The attentioned pyramidal convolution (AttPConv) in the PCRDB
block is a module that combines multi-scale convolution and channel attention
to automatically learn and adjust the scaling of the residuals for better
results. The AUP block is a module that combines pixel attention (PA) to
perform arbitrary multiples of upsampling. These two blocks work together to
help generate better quality images. For the loss function, we design a loss
function based on pixel loss and introduce both adversarial loss and feature
loss to guide the generator learning. We have compared our method with several
state-of-the-art methods on a remote sensing scene image dataset, and the
experimental results consistently demonstrate the effectiveness of the proposed
MA-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Meng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiasong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiuping Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Sen Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Generalizability in Limited-Angle CT Reconstruction with Sinogram Extrapolation. (arXiv:2103.05255v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05255</id>
        <link href="http://arxiv.org/abs/2103.05255"/>
        <updated>2021-07-15T01:59:02.789Z</updated>
        <summary type="html"><![CDATA[Computed tomography (CT) reconstruction from X-ray projections acquired
within a limited angle range is challenging, especially when the angle range is
extremely small. Both analytical and iterative models need more projections for
effective modeling. Deep learning methods have gained prevalence due to their
excellent reconstruction performances, but such success is mainly limited
within the same dataset and does not generalize across datasets with different
distributions. Hereby we propose ExtraPolationNetwork for limited-angle CT
reconstruction via the introduction of a sinogram extrapolation module, which
is theoretically justified. The module complements extra sinogram information
and boots model generalizability. Extensive experimental results show that our
reconstruction model achieves state-of-the-art performance on NIH-AAPM dataset,
similar to existing approaches. More importantly, we show that using such a
sinogram extrapolation module significantly improves the generalization
capability of the model on unseen datasets (e.g., COVID-19 and LIDC datasets)
when compared to existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Ce Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haimiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shang_K/0/1/0/all/0/1"&gt;Kun Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yuanyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S. Kevin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs. (arXiv:2107.06563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06563</id>
        <link href="http://arxiv.org/abs/2107.06563"/>
        <updated>2021-07-15T01:59:02.775Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep neural networks in chest X-ray (CXR) diagnosis,
supervised learning only allows the prediction of disease classes that were
seen during training. At inference, these networks cannot predict an unseen
disease class. Incorporating a new class requires the collection of labeled
data, which is not a trivial task, especially for less frequently-occurring
diseases. As a result, it becomes inconceivable to build a model that can
diagnose all possible disease classes. Here, we propose a multi-label
generalized zero shot learning (CXR-ML-GZSL) network that can simultaneously
predict multiple seen and unseen diseases in CXR images. Given an input image,
CXR-ML-GZSL learns a visual representation guided by the input's corresponding
semantics extracted from a rich medical text corpus. Towards this ambitious
goal, we propose to map both visual and semantic modalities to a latent feature
space using a novel learning objective. The objective ensures that (i) the most
relevant labels for the query image are ranked higher than irrelevant labels,
(ii) the network learns a visual representation that is aligned with its
semantics in the latent feature space, and (iii) the mapped semantics preserve
their original inter-class representation. The network is end-to-end trainable
and requires no independent pre-training for the offline feature extractor.
Experiments on the NIH Chest X-ray dataset show that our network outperforms
two strong baselines in terms of recall, precision, f1 score, and area under
the receiver operating characteristic curve. Our code is publicly available at:
https://github.com/nyuad-cai/CXR-ML-GZSL.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1"&gt;Nasir Hayat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lashen_H/0/1/0/all/0/1"&gt;Hazem Lashen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1"&gt;Farah E. Shamout&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07409</id>
        <link href="http://arxiv.org/abs/2106.07409"/>
        <updated>2021-07-15T01:59:02.759Z</updated>
        <summary type="html"><![CDATA[This is a short technical report introducing the solution of Team Rat for
Short-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)
Workshop and Challenge at CVPR 2021.

In this report, we propose an Edge-Aware Network (EANet) that uses edge
information to refine the segmentation edge. To further obtain the finer edge
results, we introduce edge attention loss that only compute cross entropy on
the edges, it can effectively reduce the classification error around edge and
get more smooth boundary. Benefiting from the edge information and edge
attention loss, the proposed EANet achieves 86.16\% accuracy in the Short-video
Face Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,
ranked the third place.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1"&gt;Xiaofei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiangtao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Tensor Train Parameterization of Deep Learning Layers. (arXiv:2103.04217v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04217</id>
        <link href="http://arxiv.org/abs/2103.04217"/>
        <updated>2021-07-15T01:59:02.743Z</updated>
        <summary type="html"><![CDATA[We study low-rank parameterizations of weight matrices with embedded spectral
properties in the Deep Learning context. The low-rank property leads to
parameter efficiency and permits taking computational shortcuts when computing
mappings. Spectral properties are often subject to constraints in optimization
problems, leading to better models and stability of optimization. We start by
looking at the compact SVD parameterization of weight matrices and identifying
redundancy sources in the parameterization. We further apply the Tensor Train
(TT) decomposition to the compact SVD components, and propose a non-redundant
differentiable parameterization of fixed TT-rank tensor manifolds, termed the
Spectral Tensor Train Parameterization (STTP). We demonstrate the effects of
neural network compression in the image classification setting and both
compression and improved training stability in the generative adversarial
training setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1"&gt;Alexander Liniger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiSTF: Bilateral-Branch Self-Training Framework for Semi-Supervised Large-scale Fine-Grained Recognition. (arXiv:2107.06768v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06768</id>
        <link href="http://arxiv.org/abs/2107.06768"/>
        <updated>2021-07-15T01:59:02.736Z</updated>
        <summary type="html"><![CDATA[Semi-supervised Fine-Grained Recognition is a challenge task due to the
difficulty of data imbalance, high inter-class similarity and domain mismatch.
Recent years, this field has witnessed great progress and many methods has
gained great performance. However, these methods can hardly generalize to the
large-scale datasets, such as Semi-iNat, as they are prone to suffer from noise
in unlabeled data and the incompetence for learning features from imbalanced
fine-grained data. In this work, we propose Bilateral-Branch Self-Training
Framework (BiSTF), a simple yet effective framework to improve existing
semi-supervised learning methods on class-imbalanced and domain-shifted
fine-grained data. By adjusting the update frequency through stochastic epoch
update, BiSTF iteratively retrains a baseline SSL model with a labeled set
expanded by selectively adding pseudo-labeled samples from an unlabeled set,
where the distribution of pseudo-labeled samples are the same as the labeled
data. We show that BiSTF outperforms the existing state-of-the-art SSL
algorithm on Semi-iNat dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guochen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1"&gt;Qiang Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization with Pseudo-Domain Label for Face Anti-Spoofing. (arXiv:2107.06552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06552</id>
        <link href="http://arxiv.org/abs/2107.06552"/>
        <updated>2021-07-15T01:59:02.730Z</updated>
        <summary type="html"><![CDATA[Face anti-spoofing (FAS) plays an important role in protecting face
recognition systems from face representation attacks. Many recent studies in
FAS have approached this problem with domain generalization technique. Domain
generalization aims to increase generalization performance to better detect
various types of attacks and unseen attacks. However, previous studies in this
area have defined each domain simply as an anti-spoofing datasets and focused
on developing learning techniques. In this paper, we proposed a method that
enables network to judge its domain by itself with the clustered convolutional
feature statistics from intermediate layers of the network, without labeling
domains as datasets. We obtained pseudo-domain labels by not only using the
network extracting features, but also using depth estimators, which were
previously used only as an auxiliary task in FAS. In our experiments, we
trained with three datasets and evaluated the performance with the remaining
one dataset to demonstrate the effectiveness of the proposed method by
conducting a total of four sets of experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young Eun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks are Surprisingly Reversible: A Baseline for Zero-Shot Inversion. (arXiv:2107.06304v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06304</id>
        <link href="http://arxiv.org/abs/2107.06304"/>
        <updated>2021-07-15T01:59:02.724Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior and vulnerability of pre-trained deep neural
networks (DNNs) can help to improve them. Analysis can be performed via
reversing the network's flow to generate inputs from internal representations.
Most existing work relies on priors or data-intensive optimization to invert a
model, yet struggles to scale to deep architectures and complex datasets. This
paper presents a zero-shot direct model inversion framework that recovers the
input to the trained model given only the internal representation. The crux of
our method is to inverse the DNN in a divide-and-conquer manner while
re-syncing the inverted layers via cycle-consistency guidance with the help of
synthesized data. As a result, we obtain a single feed-forward model capable of
inversion with a single forward pass without seeing any real data of the
original task. With the proposed approach, we scale zero-shot direct inversion
to deep architectures and complex datasets. We empirically show that modern
classification models on ImageNet can, surprisingly, be inverted, allowing an
approximate recovery of the original 224x224px images from a representation
after more than 20 layers. Moreover, inversion of generators in GANs unveils
latent code of a given synthesized face image at 128x128px, which can even, in
turn, improve defective synthesized images from GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongxu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1"&gt;Pavlo Molchanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint). (arXiv:2107.06351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06351</id>
        <link href="http://arxiv.org/abs/2107.06351"/>
        <updated>2021-07-15T01:59:02.718Z</updated>
        <summary type="html"><![CDATA[Image annotation and large annotated datasets are crucial parts within the
Computer Vision and Artificial Intelligence fields.At the same time, it is
well-known and acknowledged by the research community that the image annotation
process is challenging, time-consuming and hard to scale. Therefore, the
researchers and practitioners are always seeking ways to perform the
annotations easier, faster, and at higher quality. Even though several widely
used tools exist and the tools' landscape evolved considerably, most of the
tools still require intricate technical setups and high levels of technical
savviness from its operators and crowdsource contributors.

In order to address such challenges, we develop and present BRIMA -- a
flexible and open-source browser extension that allows BRowser-only IMage
Annotation at considerably lower overheads. Once added to the browser, it
instantly allows the user to annotate images easily and efficiently directly
from the browser without any installation or setup on the client-side. It also
features cross-browser and cross-platform functionality thus presenting itself
as a neat tool for researchers within the Computer Vision, Artificial
Intelligence, and privacy-related fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lahtinen_T/0/1/0/all/0/1"&gt;Tuomo Lahtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turtiainen_H/0/1/0/all/0/1"&gt;Hannu Turtiainen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costin_A/0/1/0/all/0/1"&gt;Andrei Costin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface. (arXiv:2107.06393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06393</id>
        <link href="http://arxiv.org/abs/2107.06393"/>
        <updated>2021-07-15T01:59:02.702Z</updated>
        <summary type="html"><![CDATA[Modeling complex phenomena typically involves the use of both discrete and
continuous variables. Such a setting applies across a wide range of problems,
from identifying trends in time-series data to performing effective
compositional scene understanding in images. Here, we propose Hybrid Memoised
Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid
discrete-continuous models. Prior approaches to learning suffer as they need to
perform repeated expensive inner-loop discrete inference. We build on a recent
approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by
memoising discrete variables, and extend it to allow for a principled and
effective way to handle continuous variables by learning a separate recognition
model used for importance-sampling based approximate inference and
marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene
understanding domains, and show that it outperforms current state-of-the-art
inference methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tuan Anh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_K/0/1/0/all/0/1"&gt;Katherine M. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hewitt_L/0/1/0/all/0/1"&gt;Luke Hewitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1"&gt;Kevin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1"&gt;Siddharth N&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gershman_S/0/1/0/all/0/1"&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:02.694Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06307</id>
        <link href="http://arxiv.org/abs/2107.06307"/>
        <updated>2021-07-15T01:59:02.688Z</updated>
        <summary type="html"><![CDATA[High-definition map (HD map) construction is a crucial problem for autonomous
driving. This problem typically involves collecting high-quality point clouds,
fusing multiple point clouds of the same scene, annotating map elements, and
updating maps constantly. This pipeline, however, requires a vast amount of
human efforts and resources which limits its scalability. Additionally,
traditional HD maps are coupled with centimeter-level accurate localization
which is unreliable in many scenarios. In this paper, we argue that online map
learning, which dynamically constructs the HD maps based on local sensor
observations, is a more scalable way to provide semantic and geometry priors to
self-driving vehicles than traditional pre-annotated HD maps. Meanwhile, we
introduce an online map learning method, titled HDMapNet. It encodes image
features from surrounding cameras and/or point clouds from LiDAR, and predicts
vectorized map elements in the bird's-eye view. We benchmark HDMapNet on the
nuScenes dataset and show that in all settings, it performs better than
baseline methods. Of note, our fusion-based HDMapNet outperforms existing
methods by more than 50% in all metrics. To accelerate future research, we
develop customized metrics to evaluate map learning performance, including both
semantic-level and instance-level ones. By introducing this method and metrics,
we invite the community to study this novel map learning problem. We will
release our code and evaluation kit to facilitate future development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yilun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Programming of Reaction-Diffusion Patterns. (arXiv:2107.06862v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.06862</id>
        <link href="http://arxiv.org/abs/2107.06862"/>
        <updated>2021-07-15T01:59:02.641Z</updated>
        <summary type="html"><![CDATA[Reaction-Diffusion (RD) systems provide a computational framework that
governs many pattern formation processes in nature. Current RD system design
practices boil down to trial-and-error parameter search. We propose a
differentiable optimization method for learning the RD system parameters to
perform example-based texture synthesis on a 2D plane. We do this by
representing the RD system as a variant of Neural Cellular Automata and using
task-specific differentiable loss functions. RD systems generated by our method
exhibit robust, non-trivial 'life-like' behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mordvintsev_A/0/1/0/all/0/1"&gt;Alexander Mordvintsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randazzo_E/0/1/0/all/0/1"&gt;Ettore Randazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niklasson_E/0/1/0/all/0/1"&gt;Eyvind Niklasson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Algebraic Recombination for Compositional Generalization. (arXiv:2107.06516v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06516</id>
        <link href="http://arxiv.org/abs/2107.06516"/>
        <updated>2021-07-15T01:59:02.545Z</updated>
        <summary type="html"><![CDATA[Neural sequence models exhibit limited compositional generalization ability
in semantic parsing tasks. Compositional generalization requires algebraic
recombination, i.e., dynamically recombining structured expressions in a
recursive manner. However, most previous studies mainly concentrate on
recombining lexical units, which is an important but not sufficient part of
algebraic recombination. In this paper, we propose LeAR, an end-to-end neural
model to learn algebraic recombination for compositional generalization. The
key insight is to model the semantic parsing task as a homomorphism between a
latent syntactic algebra and a semantic algebra, thus encouraging algebraic
recombination. Specifically, we learn two modules jointly: a Composer for
producing latent syntax, and an Interpreter for assigning semantic operations.
Experiments on two realistic and comprehensive compositional generalization
benchmarks demonstrate the effectiveness of our model. The source code is
publicly available at https://github.com/microsoft/ContextualSP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chenyao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1"&gt;Shengnan An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zeqi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1"&gt;Jian-Guang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Lijie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reference Knowledgeable Network for Machine Reading Comprehension. (arXiv:2012.03709v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03709</id>
        <link href="http://arxiv.org/abs/2012.03709"/>
        <updated>2021-07-15T01:59:02.527Z</updated>
        <summary type="html"><![CDATA[Multi-choice Machine Reading Comprehension (MRC) as a challenge requires
model to select the most appropriate answer from a set of candidates given
passage and question. Most of the existing researches focus on the modeling of
the task datasets without explicitly referring to external fine-grained
knowledge sources, which is supposed to greatly make up the deficiency of the
given passage. Thus we propose a novel reference-based knowledge enhancement
model called Reference Knowledgeable Network (RekNet), which refines critical
information from the passage and quote explicit knowledge in necessity. In
detail, RekNet refines fine-grained critical information and defines it as
Reference Span, then quotes explicit knowledge quadruples by the co-occurrence
information of Reference Span and candidates. The proposed RekNet is evaluated
on three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, which shows
consistent and remarkable performance improvement with observable statistical
significance level over strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yilin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation. (arXiv:2107.06779v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06779</id>
        <link href="http://arxiv.org/abs/2107.06779"/>
        <updated>2021-07-15T01:59:02.521Z</updated>
        <summary type="html"><![CDATA[Emotion recognition in conversation (ERC) is a crucial component in affective
dialogue systems, which helps the system understand users' emotions and
generate empathetic responses. However, most works focus on modeling speaker
and contextual information primarily on the textual modality or simply
leveraging multimodal information through feature concatenation. In order to
explore a more effective way of utilizing both multimodal and long-distance
contextual information, we propose a new model based on multimodal fused graph
convolutional network, MMGCN, in this work. MMGCN can not only make use of
multimodal dependencies effectively, but also leverage speaker information to
model inter-speaker and intra-speaker dependency. We evaluate our proposed
model on two public benchmark datasets, IEMOCAP and MELD, and the results prove
the effectiveness of MMGCN, which outperforms other SOTA methods by a
significant margin under the multimodal conversation setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jingwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jinming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indonesia's Fake News Detection using Transformer Network. (arXiv:2107.06796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06796</id>
        <link href="http://arxiv.org/abs/2107.06796"/>
        <updated>2021-07-15T01:59:02.515Z</updated>
        <summary type="html"><![CDATA[Fake news is a problem faced by society in this era. It is not rare for fake
news to cause provocation and problem for the people. Indonesia, as a country
with the 4th largest population, has a problem in dealing with fake news. More
than 30% of rural and urban population are deceived by this fake news problem.
As we have been studying, there is only few literatures on preventing the
spread of fake news in Bahasa Indonesia. So, this research is conducted to
prevent these problems. The dataset used in this research was obtained from a
news portal that identifies fake news, turnbackhoax.id. Using Web Scrapping on
this page, we got 1116 data consisting of valid news and fake news. The dataset
can be accessed at https://github.com/JibranFawaid/turnbackhoax-dataset. This
dataset will be combined with other available datasets. The methods used are
CNN, BiLSTM, Hybrid CNN-BiLSTM, and BERT with Transformer Network. This
research shows that the BERT method with Transformer Network has the best
results with an accuracy of up to 90%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awalina_A/0/1/0/all/0/1"&gt;Aisyah Awalina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaid_J/0/1/0/all/0/1"&gt;Jibran Fawaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krisnabayu_R/0/1/0/all/0/1"&gt;Rifky Yunus Krisnabayu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParCourE: A Parallel Corpus Explorer fora Massively Multilingual Corpus. (arXiv:2107.06632v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06632</id>
        <link href="http://arxiv.org/abs/2107.06632"/>
        <updated>2021-07-15T01:59:02.507Z</updated>
        <summary type="html"><![CDATA[With more than 7000 languages worldwide, multilingual natural language
processing (NLP) is essential both from an academic and commercial perspective.
Researching typological properties of languages is fundamental for progress in
multilingual NLP. Examples include assessing language similarity for effective
transfer learning, injecting inductive biases into machine learning models or
creating resources such as dictionaries and inflection tables. We provide
ParCourE, an online tool that allows to browse a word-aligned parallel corpus,
covering 1334 languages. We give evidence that this is useful for typological
research. ParCourE can be set up for any parallel corpus and can thus be used
for typological research on other corpora as well as for exploring their
quality and properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1"&gt;Ayyoob Imani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1"&gt;Masoud Jalili Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1"&gt;Philipp Dufter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cysouw_M/0/1/0/all/0/1"&gt;Michael Cysouw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deduplicating Training Data Makes Language Models Better. (arXiv:2107.06499v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06499</id>
        <link href="http://arxiv.org/abs/2107.06499"/>
        <updated>2021-07-15T01:59:02.453Z</updated>
        <summary type="html"><![CDATA[We find that existing language modeling datasets contain many near-duplicate
examples and long repetitive substrings. As a result, over 1% of the unprompted
output of language models trained on these datasets is copied verbatim from the
training data. We develop two tools that allow us to deduplicate training
datasets -- for example removing from C4 a single 61 word English sentence that
is repeated over 60,000 times. Deduplication allows us to train models that
emit memorized text ten times less frequently and require fewer train steps to
achieve the same or better accuracy. We can also reduce train-test overlap,
which affects over 4% of the validation set of standard datasets, thus allowing
for more accurate evaluation. We release code for reproducing our work and
performing dataset deduplication at
https://github.com/google-research/deduplicate-text-datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Katherine Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1"&gt;Daphne Ippolito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nystrom_A/0/1/0/all/0/1"&gt;Andrew Nystrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1"&gt;Chris Callison-Burch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text. (arXiv:2107.06483v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06483</id>
        <link href="http://arxiv.org/abs/2107.06483"/>
        <updated>2021-07-15T01:59:02.435Z</updated>
        <summary type="html"><![CDATA[Generating code-switched text is a problem of growing interest, especially
given the scarcity of corpora containing large volumes of real code-switched
text. In this work, we adapt a state-of-the-art neural machine translation
model to generate Hindi-English code-switched sentences starting from
monolingual Hindi sentences. We outline a carefully designed curriculum of
pretraining steps, including the use of synthetic code-switched text, that
enable the model to generate high-quality code-switched text. Using text
generated from our model as data augmentation, we show significant reductions
in perplexity on a language modeling task, compared to using text from other
generative models of CS text. We also show improvements using our text for a
downstream code-switched natural language inference task. Our generated text is
further subjected to a rigorous evaluation using a human evaluation study and a
range of objective metrics, where we show performance comparable (and sometimes
even superior) to code-switched text obtained via crowd workers who are native
Hindi speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tarunesh_I/0/1/0/all/0/1"&gt;Ishan Tarunesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Syamantak Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance-based Neuron Allocation for Multilingual Neural Machine Translation. (arXiv:2107.06569v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06569</id>
        <link href="http://arxiv.org/abs/2107.06569"/>
        <updated>2021-07-15T01:59:02.429Z</updated>
        <summary type="html"><![CDATA[Multilingual neural machine translation with a single model has drawn much
attention due to its capability to deal with multiple languages. However, the
current multilingual translation paradigm often makes the model tend to
preserve the general knowledge, but ignore the language-specific knowledge.
Some previous works try to solve this problem by adding various kinds of
language-specific modules to the model, but they suffer from the parameter
explosion problem and require specialized manual design. To solve these
problems, we propose to divide the model neurons into general and
language-specific parts based on their importance across languages. The general
part is responsible for preserving the general knowledge and participating in
the translation of all the languages, while the language-specific part is
responsible for preserving the language-specific knowledge and participating in
the translation of some specific languages. Experimental results on several
language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the
effectiveness and universality of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wanying Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shuhao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Fine-Tuning for Sentiment Analysis on Indonesian Mobile Apps Reviews. (arXiv:2107.06802v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06802</id>
        <link href="http://arxiv.org/abs/2107.06802"/>
        <updated>2021-07-15T01:59:02.403Z</updated>
        <summary type="html"><![CDATA[User reviews have an essential role in the success of the developed mobile
apps. User reviews in the textual form are unstructured data, creating a very
high complexity when processed for sentiment analysis. Previous approaches that
have been used often ignore the context of reviews. In addition, the relatively
small data makes the model overfitting. A new approach, BERT, has been
introduced as a transfer learning model with a pre-trained model that has
previously been trained to have a better context representation. This study
examines the effectiveness of fine-tuning BERT for sentiment analysis using two
different pre-trained models. Besides the multilingual pre-trained model, we
use the pre-trained model that only has been trained in Indonesian. The dataset
used is Indonesian user reviews of the ten best apps in 2020 in Google Play
sites. We also perform hyper-parameter tuning to find the optimum trained
model. Two training data labeling approaches were also tested to determine the
effectiveness of the model, which is score-based and lexicon-based. The
experimental results show that pre-trained models trained in Indonesian have
better average accuracy on lexicon-based data. The pre-trained Indonesian model
highest accuracy is 84%, with 25 epochs and a training time of 24 minutes.
These results are better than all of the machine learning and multilingual
pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukmadewa_A/0/1/0/all/0/1"&gt;Anantha Yullian Sukmadewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DW_H/0/1/0/all/0/1"&gt;Haftittah Wuswilahaken DW&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachtiar_F/0/1/0/all/0/1"&gt;Fitra Abdurrachman Bachtiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Composing Conversational Negation. (arXiv:2107.06820v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06820</id>
        <link href="http://arxiv.org/abs/2107.06820"/>
        <updated>2021-07-15T01:59:02.397Z</updated>
        <summary type="html"><![CDATA[Negation in natural language does not follow Boolean logic and is therefore
inherently difficult to model. In particular, it takes into account the broader
understanding of what is being negated. In previous work, we proposed a
framework for negation of words that accounts for `worldly context'. In this
paper, we extend that proposal now accounting for the compositional structure
inherent in language, within the DisCoCirc framework. We compose the negations
of single words to capture the negation of sentences. We also describe how to
model the negation of words whose meanings evolve in the text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_R/0/1/0/all/0/1"&gt;Razin A. Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_L/0/1/0/all/0/1"&gt;Lia Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodatz_B/0/1/0/all/0/1"&gt;Benjamin Rodatz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1"&gt;Bob Coecke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale News Classification using BERT Language Model: Spark NLP Approach. (arXiv:2107.06785v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06785</id>
        <link href="http://arxiv.org/abs/2107.06785"/>
        <updated>2021-07-15T01:59:02.381Z</updated>
        <summary type="html"><![CDATA[The rise of big data analytics on top of NLP increases the computational
burden for text processing at scale. The problems faced in NLP are very high
dimensional text, so it takes a high computation resource. The MapReduce allows
parallelization of large computations and can improve the efficiency of text
processing. This research aims to study the effect of big data processing on
NLP tasks based on a deep learning approach. We classify a big text of news
topics with fine-tuning BERT used pre-trained models. Five pre-trained models
with a different number of parameters were used in this study. To measure the
efficiency of this method, we compared the performance of the BERT with the
pipelines from Spark NLP. The result shows that BERT without Spark NLP gives
higher accuracy compared to BERT with Spark NLP. The accuracy average and
training time of all models using BERT is 0.9187 and 35 minutes while using
BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will
take more computation resources and need a longer time to complete the tasks.
However, the accuracy of BERT with Spark NLP only decreased by an average of
5.7%, while the training time was reduced significantly by 62.9% compared to
BERT without Spark NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugroho_K/0/1/0/all/0/1"&gt;Kuncahyo Setyo Nugroho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1"&gt;Novanto Yudistira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to make qubits speak. (arXiv:2107.06776v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2107.06776</id>
        <link href="http://arxiv.org/abs/2107.06776"/>
        <updated>2021-07-15T01:59:02.375Z</updated>
        <summary type="html"><![CDATA[This is a story about making quantum computers speak, and doing so in a
quantum-native, compositional and meaning-aware manner. Recently we did
question-answering with an actual quantum computer. We explain what we did,
stress that this was all done in terms of pictures, and provide many pointers
to the related literature. In fact, besides natural language, many other things
can be implemented in a quantum-native, compositional and meaning-aware manner,
and we provide the reader with some indications of that broader pictorial
landscape, including our account on the notion of compositionality. We also
provide some guidance for the actual execution, so that the reader can give it
a go as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Coecke_B/0/1/0/all/0/1"&gt;Bob Coecke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Felice_G/0/1/0/all/0/1"&gt;Giovanni de Felice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Meichanetzidis_K/0/1/0/all/0/1"&gt;Konstantinos Meichanetzidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Toumi_A/0/1/0/all/0/1"&gt;Alexis Toumi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.06751</id>
        <link href="http://arxiv.org/abs/2107.06751"/>
        <updated>2021-07-15T01:59:02.369Z</updated>
        <summary type="html"><![CDATA[Probabilistic text generators have been used to produce fake scientific
papers for more than a decade. Such nonsensical papers are easily detected by
both human and machine. Now more complex AI-powered generation techniques
produce texts indistinguishable from that of humans and the generation of
scientific texts from a few keywords has been documented. Our study introduces
the concept of tortured phrases: unexpected weird phrases in lieu of
established ones, such as 'counterfeit consciousness' instead of 'artificial
intelligence.' We combed the literature for tortured phrases and study one
reputable journal where these concentrated en masse. Hypothesising the use of
advanced language models we ran a detector on the abstracts of recent articles
of this journal and on several control sets. The pairwise comparisons reveal a
concentration of abstracts flagged as 'synthetic' in the journal. We also
highlight irregularities in its operation, such as abrupt changes in editorial
timelines. We substantiate our call for investigation by analysing several
individual dubious articles, stressing questionable features: tortured writing
style, citation of non-existent literature, and unacknowledged image reuse.
Surprisingly, some websites offer to rewrite texts for free, generating
gobbledegook full of tortured phrases. We believe some authors used rewritten
texts to pad their manuscripts. We wish to raise the awareness on publications
containing such questionable AI-generated or rewritten texts that passed (poor)
peer review. Deception with synthetic texts threatens the integrity of the
scientific literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1"&gt;Guillaume Cabanac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1"&gt;Cyril Labb&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1"&gt;Alexander Magazinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition. (arXiv:2107.06546v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06546</id>
        <link href="http://arxiv.org/abs/2107.06546"/>
        <updated>2021-07-15T01:59:02.362Z</updated>
        <summary type="html"><![CDATA[We present the visually-grounded language modelling track that was introduced
in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the
new track and discuss participation rules in detail. We also present the two
baseline systems that were developed for this track.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alishahia_A/0/1/0/all/0/1"&gt;Afra Alishahia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1"&gt;Grzegorz Chrupa&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristia_A/0/1/0/all/0/1"&gt;Alejandrina Cristia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1"&gt;Emmanuel Dupoux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Higy_B/0/1/0/all/0/1"&gt;Bertrand Higy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavechin_M/0/1/0/all/0/1"&gt;Marvin Lavechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Chen Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Training of Acoustic Encoders for Speech Recognition. (arXiv:2106.08960v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08960</id>
        <link href="http://arxiv.org/abs/2106.08960"/>
        <updated>2021-07-15T01:59:02.356Z</updated>
        <summary type="html"><![CDATA[On-device speech recognition requires training models of different sizes for
deploying on devices with various computational budgets. When building such
different models, we can benefit from training them jointly to take advantage
of the knowledge shared between them. Joint training is also efficient since it
reduces the redundancy in the training procedure's data handling operations. We
propose a method for collaboratively training acoustic encoders of different
sizes for speech recognition. We use a sequence transducer setup where
different acoustic encoders share a common predictor and joiner modules. The
acoustic encoders are also trained using co-distillation through an auxiliary
task for frame level chenone prediction, along with the transducer loss. We
perform experiments using the LibriSpeech corpus and demonstrate that the
collaboratively trained acoustic encoders can provide up to a 11% relative
improvement in the word error rate on both the test partitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraja_V/0/1/0/all/0/1"&gt;Varun Nagaraja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Michael L. Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. (arXiv:2106.01040v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01040</id>
        <link href="http://arxiv.org/abs/2106.01040"/>
        <updated>2021-07-15T01:59:02.349Z</updated>
        <summary type="html"><![CDATA[Transformer is important for text modeling. However, it has difficulty in
handling long documents due to the quadratic complexity with input text length.
In order to handle this problem, we propose a hierarchical interactive
Transformer (Hi-Transformer) for efficient and effective long document
modeling. Hi-Transformer models documents in a hierarchical way, i.e., first
learns sentence representations and then learns document representations. It
can effectively reduce the complexity and meanwhile capture global document
context in the modeling of each sentence. More specifically, we first use a
sentence Transformer to learn the representations of each sentence. Then we use
a document Transformer to model the global document context from these sentence
representations. Next, we use another sentence Transformer to enhance sentence
modeling using the global document context. Finally, we use hierarchical
pooling method to obtain document embedding. Extensive experiments on three
benchmark datasets validate the efficiency and effectiveness of Hi-Transformer
in long document modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03158</id>
        <link href="http://arxiv.org/abs/2107.03158"/>
        <updated>2021-07-15T01:59:02.329Z</updated>
        <summary type="html"><![CDATA[Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1"&gt;Markus Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1"&gt;Marc-Andr&amp;#xe9; Kaufhold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1"&gt;Christian Reuter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14150</id>
        <link href="http://arxiv.org/abs/2105.14150"/>
        <updated>2021-07-15T01:59:02.318Z</updated>
        <summary type="html"><![CDATA[MultiWOZ is one of the most popular multi-domain task-oriented dialog
datasets, containing 10K+ annotated dialogs covering eight domains. It has been
widely accepted as a benchmark for various dialog tasks, e.g., dialog state
tracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog
modeling. In this work, we identify an overlooked issue with dialog state
annotation inconsistencies in the dataset, where a slot type is tagged
inconsistently across similar dialogs leading to confusion for DST modeling. We
propose an automated correction for this issue, which is present in a whopping
70% of the dialogs. Additionally, we notice that there is significant entity
bias in the dataset (e.g., "cambridge" appears in 50% of the destination cities
in the train domain). The entity bias can potentially lead to named entity
memorization in generative models, which may go unnoticed as the test set
suffers from a similar entity bias as well. We release a new test set with all
entities replaced with unseen entities. Finally, we benchmark joint goal
accuracy (JGA) of the state-of-the-art DST baselines on these modified versions
of the data. Our experiments show that the annotation inconsistency corrections
lead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in
JGA when models are evaluated on the new test set with unseen entities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1"&gt;Kun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1"&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouhan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Ankita De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1"&gt;Alborz Geramifard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1"&gt;Chinnadhurai Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition. (arXiv:2104.00120v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00120</id>
        <link href="http://arxiv.org/abs/2104.00120"/>
        <updated>2021-07-15T01:59:02.309Z</updated>
        <summary type="html"><![CDATA[Stream fusion, also known as system combination, is a common technique in
automatic speech recognition for traditional hybrid hidden Markov model
approaches, yet mostly unexplored for modern deep neural network end-to-end
model architectures. Here, we investigate various fusion techniques for the
all-attention-based encoder-decoder architecture known as the transformer,
striving to achieve optimal fusion by investigating different fusion levels in
an example single-microphone setting with fusion of standard magnitude and
phase features. We introduce a novel multi-encoder learning method that
performs a weighted combination of two encoder-decoder multi-head attention
outputs only during training. Employing then only the magnitude feature encoder
in inference, we are able to show consistent improvement on Wall Street Journal
(WSJ) with language model and on Librispeech, without increase in runtime or
parameters. Combining two such multi-encoder trained models by a simple late
fusion in inference, we achieve state-of-the-art performance for
transformer-based models on WSJ with a significant WER reduction of 19%
relative compared to the current benchmark approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lohrenz_T/0/1/0/all/0/1"&gt;Timo Lohrenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhengyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fingscheidt_T/0/1/0/all/0/1"&gt;Tim Fingscheidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-hashtag Generation using Seq2seq Learning. (arXiv:2102.00904v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00904</id>
        <link href="http://arxiv.org/abs/2102.00904"/>
        <updated>2021-07-15T01:59:02.301Z</updated>
        <summary type="html"><![CDATA[In this paper, we studied whether models based on BiLSTM and BERT can predict
hashtags in Brazilian Portuguese for Ecommerce websites. Hashtags have a
sizable financial impact on Ecommerce. We processed a corpus of Ecommerce
reviews as inputs, and predicted hashtags as outputs. We evaluated the results
using four quantitative metrics: NIST, BLEU, METEOR and a crowdsourced score. A
word cloud was used as a qualitative metric. While all computer-generated
metrics (NIST, BLEU and METEOR) indicated bad results, the crowdsourced results
produced amazing scores. We concluded that the texts predicted by the neural
networks are very promising for use as hashtags for products on Ecommerce
websites. The code for this work is available at
https://github.com/augustocamargo/text-to-hashtag.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Camargo_A/0/1/0/all/0/1"&gt;Augusto Camargo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carvalho_W/0/1/0/all/0/1"&gt;Wesley Carvalho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peressim_F/0/1/0/all/0/1"&gt;Felipe Peressim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_A/0/1/0/all/0/1"&gt;Alan Barzilay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finger_M/0/1/0/all/0/1"&gt;Marcelo Finger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06876</id>
        <link href="http://arxiv.org/abs/2107.06876"/>
        <updated>2021-07-15T01:59:02.294Z</updated>
        <summary type="html"><![CDATA[The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klicpera_J/0/1/0/all/0/1"&gt;Johannes Klicpera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1"&gt;Marten Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1"&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["How to best say it?" : Translating Directives in Machine Language into Natural Language in the Blocks World. (arXiv:2107.06886v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06886</id>
        <link href="http://arxiv.org/abs/2107.06886"/>
        <updated>2021-07-15T01:59:02.276Z</updated>
        <summary type="html"><![CDATA[We propose a method to generate optimal natural language for block placement
directives generated by a machine's planner during human-agent interactions in
the blocks world. A non user-friendly machine directive, e.g., move(ObjId,
toPos), is transformed into visually and contextually grounded referring
expressions that are much easier for the user to comprehend. We describe an
algorithm that progressively and generatively transforms the machine's
directive in ECI (Elementary Composable Ideas)-space, generating many
alternative versions of the directive. We then define a cost function to
evaluate the ease of comprehension of these alternatives and select the best
option. The parameters for this cost function were derived empirically from a
user study that measured utterance-to-action timings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sujeong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1"&gt;Amir Tamrakar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06472</id>
        <link href="http://arxiv.org/abs/2107.06472"/>
        <updated>2021-07-15T01:59:02.269Z</updated>
        <summary type="html"><![CDATA[Accurately linking news articles to scientific research works is a critical
component in a number of applications, such as measuring the social impact of a
research work and detecting inaccuracies or distortions in science news.
Although the lack of links between news and literature has been a challenge in
these applications, it is a relatively unexplored research problem. In this
paper we designed and evaluated a new approach that consists of (1) augmenting
latest named-entity recognition techniques to extract various metadata, and (2)
designing a new elastic search engine that can facilitate the use of enriched
metadata queries. To evaluate our approach, we constructed two datasets of
paired news articles and research papers: one is used for training models to
extract metadata, and the other for evaluation. Our experiments showed that the
new approach performed significantly better than a baseline approach used by
altmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate
the effectiveness of the approach, we also conducted a study on 37,600
health-related press releases published on EurekAlert!, which showed that our
approach was able to identify the corresponding research papers with a top-1
accuracy of at least 0.97.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code. (arXiv:2106.00411v2 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00411</id>
        <link href="http://arxiv.org/abs/2106.00411"/>
        <updated>2021-07-15T01:59:02.249Z</updated>
        <summary type="html"><![CDATA[Math informational retrieval (MIR) search engines are absent in the
wide-spread production use, even though documents in the STEM fields contain
many mathematical formulae, which are sometimes more important than text for
understanding. We have developed and open-sourced the WebMIaS MIR search engine
that has been successfully deployed in the European Digital Mathematics Library
(EuDML). However, its deployment is difficult to automate due to the complexity
of this task. Moreover, the solutions developed so far to tackle this challenge
are imperfect in terms of speed, maintenance, and robustness. In this paper, we
will describe the virtualization of WebMIaS using Docker that solves all three
problems and allows anyone to deploy containerized WebMIaS in a single line of
code. The publicly available Docker image will also help the community push the
development of math-aware search engines in the ARQMath workshop series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Lupt&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1"&gt;V&amp;#xed;t Novotn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1"&gt;Michal &amp;#x160;tef&amp;#xe1;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1"&gt;Petr Sojka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises, Future Directions, and Applications. (arXiv:2107.06835v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.06835</id>
        <link href="http://arxiv.org/abs/2107.06835"/>
        <updated>2021-07-15T01:59:02.242Z</updated>
        <summary type="html"><![CDATA[Edge technology aims to bring Cloud resources (specifically, the compute,
storage, and network) to the closed proximity of the Edge devices, i.e., smart
devices where the data are produced and consumed. Embedding computing and
application in Edge devices lead to emerging of two new concepts in Edge
technology, namely, Edge computing and Edge analytics. Edge analytics uses some
techniques or algorithms to analyze the data generated by the Edge devices.
With the emerging of Edge analytics, the Edge devices have become a complete
set. Currently, Edge analytics is unable to provide full support for the
execution of the analytic techniques. The Edge devices cannot execute advanced
and sophisticated analytic algorithms following various constraints such as
limited power supply, small memory size, limited resources, etc. This article
aims to provide a detailed discussion on Edge analytics. A clear explanation to
distinguish between the three concepts of Edge technology, namely, Edge
devices, Edge computing, and Edge analytics, along with their issues.
Furthermore, the article discusses the implementation of Edge analytics to
solve many problems in various areas such as retail, agriculture, industry, and
healthcare. In addition, the research papers of the state-of-the-art edge
analytics are rigorously reviewed in this article to explore the existing
issues, emerging challenges, research opportunities and their directions, and
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1"&gt;Sabuzima Nayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1"&gt;Ripon Patgiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waikhom_L/0/1/0/all/0/1"&gt;Lilapati Waikhom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Arif Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Much Can CLIP Benefit Vision-and-Language Tasks?. (arXiv:2107.06383v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06383</id>
        <link href="http://arxiv.org/abs/2107.06383"/>
        <updated>2021-07-15T01:59:02.234Z</updated>
        <summary type="html"><![CDATA[Most existing Vision-and-Language (V&L) models rely on pre-trained visual
encoders, using a relatively small set of manually-annotated data (as compared
to web-crawled data), to perceive the visual world. However, it has been
observed that large-scale pretraining usually can result in better
generalization performance, e.g., CLIP (Contrastive Language-Image
Pre-training), trained on a massive amount of image-caption pairs, has shown a
strong zero-shot capability on various vision tasks. To further study the
advantage brought by CLIP, we propose to use CLIP as the visual encoder in
various V&L models in two typical scenarios: 1) plugging CLIP into
task-specific fine-tuning; 2) combining CLIP with V&L pre-training and
transferring to downstream tasks. We show that CLIP significantly outperforms
widely-used visual encoders trained with in-domain annotated data, such as
BottomUp-TopDown. We achieve competitive or better results on diverse V&L
tasks, while establishing new state-of-the-art results on Visual Question
Answering, Visual Entailment, and V&L Navigation tasks. We release our code at
https://github.com/clip-vil/CLIP-ViL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liunian Harold Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1"&gt;Anna Rohrbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Speed and High-Quality Text-to-Lip Generation. (arXiv:2107.06831v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06831</id>
        <link href="http://arxiv.org/abs/2107.06831"/>
        <updated>2021-07-15T01:59:02.205Z</updated>
        <summary type="html"><![CDATA[As a key component of talking face generation, lip movements generation
determines the naturalness and coherence of the generated talking face video.
Prior literature mainly focuses on speech-to-lip generation while there is a
paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing
end-to-end works depend on the attention mechanism and autoregressive (AR)
decoding manner. However, the AR decoding manner generates current lip frame
conditioned on frames generated previously, which inherently hinders the
inference speed, and also has a detrimental effect on the quality of generated
lip frames due to error propagation. This encourages the research of parallel
T2L generation. In this work, we propose a novel parallel decoding model for
high-speed and high-quality text-to-lip generation (HH-T2L). Specifically, we
predict the duration of the encoded linguistic features and model the target
lip frames conditioned on the encoded linguistic features with their duration
in a non-autoregressive manner. Furthermore, we incorporate the structural
similarity index loss and adversarial learning to improve perceptual quality of
generated lip frames and alleviate the blurry prediction problem. Extensive
experiments conducted on GRID and TCD-TIMIT datasets show that 1) HH-T2L
generates lip movements with competitive quality compared with the
state-of-the-art AR T2L model DualLip and exceeds the baseline AR model
TransformerT2L by a notable margin benefiting from the mitigation of the error
propagation problem; and 2) exhibits distinct superiority in inference speed
(an average speedup of 19$\times$ than DualLip on TCD-TIMIT).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhiying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision. (arXiv:2105.04019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04019</id>
        <link href="http://arxiv.org/abs/2105.04019"/>
        <updated>2021-07-15T01:59:02.195Z</updated>
        <summary type="html"><![CDATA[Sorting and ranking supervision is a method for training neural networks
end-to-end based on ordering constraints. That is, the ground truth order of
sets of samples is known, while their absolute values remain unsupervised. For
that, we propose differentiable sorting networks by relaxing their pairwise
conditional swap operations. To address the problems of vanishing gradients and
extensive blurring that arise with larger numbers of layers, we propose mapping
activations to regions with moderate gradients. We consider odd-even as well as
bitonic sorting networks, which outperform existing relaxations of the sorting
operation. We show that bitonic sorting networks can achieve stable training on
large input sets of up to 1024 elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1"&gt;Felix Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgelt_C/0/1/0/all/0/1"&gt;Christian Borgelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1"&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deussen_O/0/1/0/all/0/1"&gt;Oliver Deussen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Health News to Research Literature. (arXiv:2107.06472v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06472</id>
        <link href="http://arxiv.org/abs/2107.06472"/>
        <updated>2021-07-15T01:59:02.187Z</updated>
        <summary type="html"><![CDATA[Accurately linking news articles to scientific research works is a critical
component in a number of applications, such as measuring the social impact of a
research work and detecting inaccuracies or distortions in science news.
Although the lack of links between news and literature has been a challenge in
these applications, it is a relatively unexplored research problem. In this
paper we designed and evaluated a new approach that consists of (1) augmenting
latest named-entity recognition techniques to extract various metadata, and (2)
designing a new elastic search engine that can facilitate the use of enriched
metadata queries. To evaluate our approach, we constructed two datasets of
paired news articles and research papers: one is used for training models to
extract metadata, and the other for evaluation. Our experiments showed that the
new approach performed significantly better than a baseline approach used by
altmetric.com (0.89 vs 0.32 in terms of top-1 accuracy). To further demonstrate
the effectiveness of the approach, we also conducted a study on 37,600
health-related press releases published on EurekAlert!, which showed that our
approach was able to identify the corresponding research papers with a top-1
accuracy of at least 0.97.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bei Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Recommendation for Cold-start Users with Meta Transitional Learning. (arXiv:2107.06427v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06427</id>
        <link href="http://arxiv.org/abs/2107.06427"/>
        <updated>2021-07-15T01:59:02.178Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge for sequential recommenders is to capture the
sequential patterns of users toward modeling how users transit among items. In
many practical scenarios, however, there are a great number of cold-start users
with only minimal logged interactions. As a result, existing sequential
recommendation models will lose their predictive power due to the difficulties
in learning sequential patterns over users with only limited interactions. In
this work, we aim to improve sequential recommendation for cold-start users
with a novel framework named MetaTL, which learns to model the transition
patterns of users through meta-learning. Specifically, the proposed MetaTL: (i)
formulates sequential recommendation for cold-start users as a few-shot
learning problem; (ii) extracts the dynamic transition patterns among users
with a translation-based architecture; and (iii) adopts meta transitional
learning to enable fast learning for cold-start users with only limited
interactions, leading to accurate inference of sequential interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1"&gt;James Caverlee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-15T01:59:02.170Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Ranking under Uncertainty. (arXiv:2107.06720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06720</id>
        <link href="http://arxiv.org/abs/2107.06720"/>
        <updated>2021-07-15T01:59:02.151Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as an important consideration in algorithmic
decision-making. Unfairness occurs when an agent with higher merit obtains a
worse outcome than an agent with lower merit. Our central point is that a
primary cause of unfairness is uncertainty. A principal or algorithm making
decisions never has access to the agents' true merit, and instead uses proxy
features that only imperfectly predict merit (e.g., GPA, star ratings,
recommendation letters). None of these ever fully capture an agent's merit; yet
existing approaches have mostly been defining fairness notions directly based
on observed features and outcomes.

Our primary point is that it is more principled to acknowledge and model the
uncertainty explicitly. The role of observed features is to give rise to a
posterior distribution of the agents' merits. We use this viewpoint to define a
notion of approximate fairness in ranking. We call an algorithm $\phi$-fair
(for $\phi \in [0,1]$) if it has the following property for all agents $x$ and
all $k$: if agent $x$ is among the top $k$ agents with respect to merit with
probability at least $\rho$ (according to the posterior merit distribution),
then the algorithm places the agent among the top $k$ agents in its ranking
with probability at least $\phi \rho$.

We show how to compute rankings that optimally trade off approximate fairness
against utility to the principal. In addition to the theoretical
characterization, we present an empirical analysis of the potential impact of
the approach in simulation studies. For real-world validation, we applied the
approach in the context of a paper recommendation system that we built and
fielded at a large conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ashudeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kempe_D/0/1/0/all/0/1"&gt;David Kempe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCLC: ROI-based joint conventional and learning video compression. (arXiv:2107.06492v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06492</id>
        <link href="http://arxiv.org/abs/2107.06492"/>
        <updated>2021-07-15T01:59:02.136Z</updated>
        <summary type="html"><![CDATA[COVID-19 leads to the high demand for remote interactive systems ever seen.
One of the key elements of these systems is video streaming, which requires a
very high network bandwidth due to its specific real-time demand, especially
with high-resolution video. Existing video compression methods are struggling
in the trade-off between video quality and the speed requirement. Addressed
that the background information rarely changes in most remote meeting cases, we
introduce a Region-Of-Interests (ROI) based video compression framework (named
RCLC) that leverages the cutting-edge learning-based and conventional
technologies. In RCLC, each coming frame is marked as a background-updating
(BU) or ROI-updating (RU) frame. By applying the conventional video codec, the
BU frame is compressed with low-quality and high-compression, while the ROI
from RU-frame is compressed with high-quality and low-compression. The
learning-based methods are applied to detect the ROI, blend background-ROI, and
enhance video quality. The experimental results show that our RCLC can reduce
up to 32.55\% BD-rate for the ROI region compared to H.265 video codec under a
similar compression time with 1080p resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Trinh Man Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjia Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using BERT Encoding to Tackle the Mad-lib Attack in SMS Spam Detection. (arXiv:2107.06400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06400</id>
        <link href="http://arxiv.org/abs/2107.06400"/>
        <updated>2021-07-15T01:59:02.123Z</updated>
        <summary type="html"><![CDATA[One of the stratagems used to deceive spam filters is to substitute vocables
with synonyms or similar words that turn the message unrecognisable by the
detection algorithms. In this paper we investigate whether the recent
development of language models sensitive to the semantics and context of words,
such as Google's BERT, may be useful to overcome this adversarial attack
(called "Mad-lib" as per the word substitution game). Using a dataset of 5572
SMS spam messages, we first established a baseline of detection performance
using widely known document representation models (BoW and TFIDF) and the novel
BERT model, coupled with a variety of classification algorithms (Decision Tree,
kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron). Then, we
built a thesaurus of the vocabulary contained in these messages, and set up a
Mad-lib attack experiment in which we modified each message of a held out
subset of data (not used in the baseline experiment) with different rates of
substitution of original words with synonyms from the thesaurus. Lastly, we
evaluated the detection performance of the three representation models (BoW,
TFIDF and BERT) coupled with the best classifier from the baseline experiment
(SVM). We found that the classic models achieved a 94% Balanced Accuracy (BA)
in the original dataset, whereas the BERT model obtained 96%. On the other
hand, the Mad-lib attack experiment showed that BERT encodings manage to
maintain a similar BA performance of 96% with an average substitution rate of
1.82 words per message, and 95% with 3.34 words substituted per message. In
contrast, the BA performance of the BoW and TFIDF encoders dropped to chance.
These results hint at the potential advantage of BERT models to combat these
type of ingenious attacks, offsetting to some extent for the inappropriate use
of semantic relationships in language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rojas_Galeano_S/0/1/0/all/0/1"&gt;Sergio Rojas-Galeano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSCAN : Dialog Structure discovery using SCAN. (arXiv:2107.06426v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06426</id>
        <link href="http://arxiv.org/abs/2107.06426"/>
        <updated>2021-07-15T01:59:02.099Z</updated>
        <summary type="html"><![CDATA[Can we discover dialog structure by dividing utterances into labelled
clusters. Can these labels be generated from the data. Typically for dialogs we
need an ontology and use that to discover structure, however by using
unsupervised classification and self-labelling we are able to intuit this
structure without any labels or ontology. In this paper we apply SCAN (Semantic
Clustering using Nearest Neighbors) to dialog data. We used BERT for pretext
task and an adaptation of SCAN for clustering and self labeling. These clusters
are used to identify transition probabilities and create the dialog structure.
The self-labelling method used for SCAN makes these structures interpretable as
every cluster has a label. As the approach is unsupervised, evaluation metrics
is a challenge, we use statistical measures as proxies for structure quality]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nath_A/0/1/0/all/0/1"&gt;Apurba Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kubba_A/0/1/0/all/0/1"&gt;Aayush Kubba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What do writing features tell us about AI papers?. (arXiv:2107.06310v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06310</id>
        <link href="http://arxiv.org/abs/2107.06310"/>
        <updated>2021-07-15T01:59:02.084Z</updated>
        <summary type="html"><![CDATA[As the numbers of submissions to conferences grow quickly, the task of
assessing the quality of academic papers automatically, convincingly, and with
high accuracy attracts increasing attention. We argue that studying
interpretable dimensions of these submissions could lead to scalable solutions.
We extract a collection of writing features, and construct a suite of
prediction tasks to assess the usefulness of these features in predicting
citation counts and the publication of AI-related papers. Depending on the
venues, the writing features can predict the conference vs. workshop appearance
with F1 scores up to 60-90, sometimes even outperforming the content-based
tf-idf features and RoBERTa. We show that the features describe writing style
more than content. To further understand the results, we estimate the causal
impact of the most indicative features. Our analysis on writing features
provides a perspective to assessing and refining the writing of academic
articles at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zining Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1"&gt;Frank Rudzicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06416</id>
        <link href="http://arxiv.org/abs/2107.06416"/>
        <updated>2021-07-15T01:59:02.063Z</updated>
        <summary type="html"><![CDATA[Recommendations with personalized explanations have been shown to increase
user trust and perceived quality and help users make better decisions.
Moreover, such explanations allow users to provide feedback by critiquing them.
Several algorithms for recommendation systems with multi-step critiquing have
therefore been developed. However, providing a user-friendly interface based on
personalized explanations and critiquing has not been addressed in the last
decade. In this paper, we introduce four different web interfaces (available
under https://lia.epfl.ch/critiquing/) helping users making decisions and
finding their ideal item. We have chosen the hotel recommendation domain as a
use case even though our approach is trivially adaptable for other domains.
Moreover, our system is model-agnostic (for both recommender systems and
critiquing models) allowing a great flexibility and further extensions. Our
interfaces are above all a useful tool to help research in recommendation with
critiquing. They allow to test such systems on a real use case and also to
highlight some limitations of these approaches to find solutions to overcome
them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1"&gt;Diana Petrescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition. (arXiv:2107.06538v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.06538</id>
        <link href="http://arxiv.org/abs/2107.06538"/>
        <updated>2021-07-15T01:59:02.048Z</updated>
        <summary type="html"><![CDATA[Fine-grained image recognition is challenging because discriminative clues
are usually fragmented, whether from a single image or multiple images. Despite
their significant improvements, most existing methods still focus on the most
discriminative parts from a single image, ignoring informative details in other
regions and lacking consideration of clues from other associated images. In
this paper, we analyze the difficulties of fine-grained image recognition from
a new perspective and propose a transformer architecture with the peak
suppression module and knowledge guidance module, which respects the
diversification of discriminative features in a single image and the
aggregation of discriminative clues among multiple images. Specifically, the
peak suppression module first utilizes a linear projection to convert the input
image into sequential tokens. It then blocks the token based on the attention
response generated by the transformer encoder. This module penalizes the
attention to the most discriminative parts in the feature learning process,
therefore, enhancing the information exploitation of the neglected regions. The
knowledge guidance module compares the image-based representation generated
from the peak suppression module with the learnable knowledge embedding set to
obtain the knowledge response coefficients. Afterwards, it formalizes the
knowledge learning as a classification problem using response coefficients as
the classification scores. Knowledge embeddings and image-based representations
are updated during training so that the knowledge embedding includes
discriminative clues for different images. Finally, we incorporate the acquired
knowledge embeddings into the image-based representations as comprehensive
representations, leading to significantly higher performance. Extensive
evaluations on the six popular datasets demonstrate the advantage of the
proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinda Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lili Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Recommend Items to Wikidata Editors. (arXiv:2107.06423v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06423</id>
        <link href="http://arxiv.org/abs/2107.06423"/>
        <updated>2021-07-15T01:59:02.033Z</updated>
        <summary type="html"><![CDATA[Wikidata is an open knowledge graph built by a global community of
volunteers. As it advances in scale, it faces substantial challenges around
editor engagement. These challenges are in terms of both attracting new editors
to keep up with the sheer amount of work and retaining existing editors.
Experience from other online communities and peer-production systems, including
Wikipedia, suggests that personalised recommendations could help, especially
newcomers, who are sometimes unsure about how to contribute best to an ongoing
effort. For this reason, we propose a recommender system WikidataRec for
Wikidata items. The system uses a hybrid of content-based and collaborative
filtering techniques to rank items for editors relying on both item features
and item-editor previous interaction. A neural network, named a neural mixture
of representations, is designed to learn fine weights for the combination of
item-based representations and optimize them with editor-based representation
by item-editor interaction. To facilitate further research in this space, we
also create two benchmark datasets, a general-purpose one with 220,000 editors
responsible for 14 million interactions with 4 million items and a second one
focusing on the contributions of more than 8,000 more active editors. We
perform an offline evaluation of the system on both datasets with promising
results. Our code and datasets are available at
https://github.com/WikidataRec-developer/Wikidata_Recommender.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+AlGhamdi_K/0/1/0/all/0/1"&gt;Kholoud AlGhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1"&gt;Miaojing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simperl_E/0/1/0/all/0/1"&gt;Elena Simperl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals. (arXiv:2107.06751v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.06751</id>
        <link href="http://arxiv.org/abs/2107.06751"/>
        <updated>2021-07-15T01:59:02.007Z</updated>
        <summary type="html"><![CDATA[Probabilistic text generators have been used to produce fake scientific
papers for more than a decade. Such nonsensical papers are easily detected by
both human and machine. Now more complex AI-powered generation techniques
produce texts indistinguishable from that of humans and the generation of
scientific texts from a few keywords has been documented. Our study introduces
the concept of tortured phrases: unexpected weird phrases in lieu of
established ones, such as 'counterfeit consciousness' instead of 'artificial
intelligence.' We combed the literature for tortured phrases and study one
reputable journal where these concentrated en masse. Hypothesising the use of
advanced language models we ran a detector on the abstracts of recent articles
of this journal and on several control sets. The pairwise comparisons reveal a
concentration of abstracts flagged as 'synthetic' in the journal. We also
highlight irregularities in its operation, such as abrupt changes in editorial
timelines. We substantiate our call for investigation by analysing several
individual dubious articles, stressing questionable features: tortured writing
style, citation of non-existent literature, and unacknowledged image reuse.
Surprisingly, some websites offer to rewrite texts for free, generating
gobbledegook full of tortured phrases. We believe some authors used rewritten
texts to pad their manuscripts. We wish to raise the awareness on publications
containing such questionable AI-generated or rewritten texts that passed (poor)
peer review. Deception with synthetic texts threatens the integrity of the
scientific literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cabanac_G/0/1/0/all/0/1"&gt;Guillaume Cabanac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbe_C/0/1/0/all/0/1"&gt;Cyril Labb&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magazinov_A/0/1/0/all/0/1"&gt;Alexander Magazinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identification of Dynamical Systems using Symbolic Regression. (arXiv:2107.06131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06131</id>
        <link href="http://arxiv.org/abs/2107.06131"/>
        <updated>2021-07-14T01:41:52.131Z</updated>
        <summary type="html"><![CDATA[We describe a method for the identification of models for dynamical systems
from observational data. The method is based on the concept of symbolic
regression and uses genetic programming to evolve a system of ordinary
differential equations (ODE). The novelty is that we add a step of
gradient-based optimization of the ODE parameters. For this we calculate the
sensitivities of the solution to the initial value problem (IVP) using
automatic differentiation. The proposed approach is tested on a set of 19
problem instances taken from the literature which includes datasets from
simulated systems as well as datasets captured from mechanical systems. We find
that gradient-based optimization of parameters improves predictive accuracy of
the models. The best results are obtained when we first fit the individual
equations to the numeric differences and then subsequently fine-tune the
identified parameter values by fitting the IVP solution to the observed
variable values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1"&gt;Gabriel Kronberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kammerer_L/0/1/0/all/0/1"&gt;Lukas Kammerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1"&gt;Michael Kommenda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoWaF: Shuffling of Weights and Feature Maps: A Novel Hardware Intrinsic Attack (HIA) on Convolutional Neural Network (CNN). (arXiv:2103.09327v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09327</id>
        <link href="http://arxiv.org/abs/2103.09327"/>
        <updated>2021-07-14T01:41:52.125Z</updated>
        <summary type="html"><![CDATA[Security of inference phase deployment of Convolutional neural network (CNN)
into resource constrained embedded systems (e.g. low end FPGAs) is a growing
research area. Using secure practices, third party FPGA designers can be
provided with no knowledge of initial and final classification layers. In this
work, we demonstrate that hardware intrinsic attack (HIA) in such a "secure"
design is still possible. Proposed HIA is inserted inside mathematical
operations of individual layers of CNN, which propagates erroneous operations
in all the subsequent CNN layers that lead to misclassification. The attack is
non-periodic and completely random, hence it becomes difficult to detect. Five
different attack scenarios with respect to each CNN layer are designed and
evaluated based on the overhead resources and the rate of triggering in
comparison to the original implementation. Our results for two CNN
architectures show that in all the attack scenarios, additional latency is
negligible (<0.61%), increment in DSP, LUT, FF is also less than 2.36%. Three
attack scenarios do not require any additional BRAM resources, while in two
scenarios BRAM increases, which compensates with the corresponding decrease in
FF and LUTs. To the authors' best knowledge this work is the first to address
the hardware intrinsic CNN attack with the attacker does not have knowledge of
the full CNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1"&gt;Tolulope A. Odetola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Syed Rafay Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instrumental Variable Value Iteration for Causal Offline Reinforcement Learning. (arXiv:2102.09907v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09907</id>
        <link href="http://arxiv.org/abs/2102.09907"/>
        <updated>2021-07-14T01:41:52.107Z</updated>
        <summary type="html"><![CDATA[In offline reinforcement learning (RL) an optimal policy is learnt solely
from a priori collected observational data. However, in observational data,
actions are often confounded by unobserved variables. Instrumental variables
(IVs), in the context of RL, are the variables whose influence on the state
variables are all mediated through the action. When a valid instrument is
present, we can recover the confounded transition dynamics through
observational data. We study a confounded Markov decision process where the
transition dynamics admit an additive nonlinear functional form. Using IVs, we
derive a conditional moment restriction (CMR) through which we can identify
transition dynamics based on observational data. We propose a provably
efficient IV-aided Value Iteration (IVVI) algorithm based on a primal-dual
reformulation of CMR. To the best of our knowledge, this is the first provably
efficient algorithm for instrument-aided offline RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liao_L/0/1/0/all/0/1"&gt;Luofeng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zuyue Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1"&gt;Mladen Kolar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoran Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00773</id>
        <link href="http://arxiv.org/abs/1911.00773"/>
        <updated>2021-07-14T01:41:52.100Z</updated>
        <summary type="html"><![CDATA[This paper analyzes challenges in cloze-style reading comprehension on
multiparty dialogue and suggests two new tasks for more comprehensive
predictions of personal entities in daily conversations. We first demonstrate
that there are substantial limitations to the evaluation methods of previous
work, namely that randomized assignment of samples to training and test data
substantially decreases the complexity of cloze-style reading comprehension.
According to our analysis, replacing the random data split with a chronological
data split reduces test accuracy on previous single-variable passage completion
task from 72\% to 34\%, that leaves much more room to improve. Our proposed
tasks extend the previous single-variable passage completion task by replacing
more character mentions with variables. Several deep learning models are
developed to validate these three tasks. A thorough error analysis is provided
to understand the challenges and guide the future direction of this research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changmao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Modular Implicit Differentiation. (arXiv:2105.15183v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15183</id>
        <link href="http://arxiv.org/abs/2105.15183"/>
        <updated>2021-07-14T01:41:52.043Z</updated>
        <summary type="html"><![CDATA[Automatic differentiation (autodiff) has revolutionized machine learning. It
allows expressing complex computations by composing elementary ones in creative
ways and removes the burden of computing their derivatives by hand. More
recently, differentiation of optimization problem solutions has attracted
widespread attention with applications such as optimization as a layer, and in
bi-level problems such as hyper-parameter optimization and meta-learning.
However, the formulas for these derivatives often involve case-by-case tedious
mathematical derivations. In this paper, we propose a unified, efficient and
modular approach for implicit differentiation of optimization problems. In our
approach, the user defines (in Python in the case of our implementation) a
function $F$ capturing the optimality conditions of the problem to be
differentiated. Once this is done, we leverage autodiff of $F$ and implicit
differentiation to automatically differentiate the optimization problem. Our
approach thus combines the benefits of implicit differentiation and autodiff.
It is efficient as it can be added on top of any state-of-the-art solver and
modular as the optimality condition specification is decoupled from the
implicit differentiation mechanism. We show that seemingly simple principles
allow to recover many recently proposed implicit differentiation methods and
create new ones easily. We demonstrate the ease of formulating and solving
bi-level optimization problems using our framework. We also showcase an
application to the sensitivity analysis of molecular dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1"&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berthet_Q/0/1/0/all/0/1"&gt;Quentin Berthet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frostig_R/0/1/0/all/0/1"&gt;Roy Frostig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1"&gt;Stephan Hoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llinares_Lopez_F/0/1/0/all/0/1"&gt;Felipe Llinares-L&amp;#xf3;pez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vert_J/0/1/0/all/0/1"&gt;Jean-Philippe Vert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00138</id>
        <link href="http://arxiv.org/abs/2104.00138"/>
        <updated>2021-07-14T01:41:52.030Z</updated>
        <summary type="html"><![CDATA[Quantitative lung measures derived from computed tomography (CT) have been
demonstrated to improve prognostication in coronavirus disease (COVID-19)
patients, but are not part of the clinical routine since required manual
segmentation of lung lesions is prohibitively time-consuming. We propose a new
fully automated deep learning framework for rapid quantification and
differentiation between lung lesions in COVID-19 pneumonia from both contrast
and non-contrast CT images using convolutional Long Short-Term Memory
(ConvLSTM) networks. Utilizing the expert annotations, model training was
performed 5 times with separate hold-out sets using 5-fold cross-validation to
segment ground-glass opacity and high opacity (including consolidation and
pleural effusion). The performance of the method was evaluated on CT data sets
from 197 patients with positive reverse transcription polymerase chain reaction
test result for SARS-CoV-2. Strong agreement between expert manual and
automatic segmentation was obtained for lung lesions with a Dice score
coefficient of 0.876 $\pm$ 0.005; excellent correlations of 0.978 and 0.981 for
ground-glass opacity and high opacity volumes. In the external validation set
of 67 patients, there was dice score coefficient of 0.767 $\pm$ 0.009 as well
as excellent correlations of 0.989 and 0.996 for ground-glass opacity and high
opacity volumes. Computations for a CT scan comprising 120 slices were
performed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX
graphics processing unit. Therefore, our deep learning-based method allows
rapid fully-automated quantitative measurement of pneumonia burden from CT and
may generate results with an accuracy similar to the expert readers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1"&gt;Kajetan Grodecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1"&gt;Aditya Killekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Andrew Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1"&gt;Sebastien Cadet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1"&gt;Priscilla McElhinney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1"&gt;Aryabod Razipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1"&gt;Cato Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1"&gt;Barry D. Pressman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1"&gt;Peter Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1"&gt;Judit Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1"&gt;Pal Maurovich-Horvat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1"&gt;Nicola Gaibazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1"&gt;Udit Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1"&gt;Elisabetta Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1"&gt;Cecilia Agalbato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1"&gt;Jiro Munechika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1"&gt;Hidenari Matsumoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1"&gt;Roberto Men&amp;#xe8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1"&gt;Gianfranco Parati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1"&gt;Franco Cernigliaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1"&gt;Nitesh Nerlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1"&gt;Camilla Torlasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1"&gt;Gianluca Pontone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1"&gt;Damini Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1"&gt;Piotr J. Slomka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability and Generalization of Stochastic Gradient Methods for Minimax Problems. (arXiv:2105.03793v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03793</id>
        <link href="http://arxiv.org/abs/2105.03793"/>
        <updated>2021-07-14T01:41:52.022Z</updated>
        <summary type="html"><![CDATA[Many machine learning problems can be formulated as minimax problems such as
Generative Adversarial Networks (GANs), AUC maximization and robust estimation,
to mention but a few. A substantial amount of studies are devoted to studying
the convergence behavior of their stochastic gradient-type algorithms. In
contrast, there is relatively little work on their generalization, i.e., how
the learning models built from training examples would behave on test examples.
In this paper, we provide a comprehensive generalization analysis of stochastic
gradient methods for minimax problems under both convex-concave and
nonconvex-nonconcave cases through the lens of algorithmic stability. We
establish a quantitative connection between stability and several
generalization measures both in expectation and with high probability. For the
convex-concave setting, our stability analysis shows that stochastic gradient
descent ascent attains optimal generalization bounds for both smooth and
nonsmooth minimax problems. We also establish generalization bounds for both
weakly-convex-weakly-concave and gradient-dominated problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenhuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1"&gt;Yiming Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06631</id>
        <link href="http://arxiv.org/abs/2007.06631"/>
        <updated>2021-07-14T01:41:52.014Z</updated>
        <summary type="html"><![CDATA[We introduce T-Basis, a novel concept for a compact representation of a set
of tensors, each of an arbitrary shape, which is often seen in Neural Networks.
Each of the tensors in the set is modeled using Tensor Rings, though the
concept applies to other Tensor Networks. Owing its name to the T-shape of
nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally
shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such
representation allows us to parameterize the tensor set with a small number of
parameters (coefficients of the T-Basis tensors), scaling logarithmically with
each tensor's size in the set and linearly with the dimensionality of T-Basis.
We evaluate the proposed approach on the task of neural network compression and
demonstrate that it reaches high compression rates at acceptable performance
drops. Finally, we analyze memory and operation requirements of the compressed
networks and conclude that T-Basis networks are equally well suited for
training and inference in resource-constrained environments and usage on the
edge devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1"&gt;Menelaos Kanakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06197</id>
        <link href="http://arxiv.org/abs/2107.06197"/>
        <updated>2021-07-14T01:41:51.995Z</updated>
        <summary type="html"><![CDATA[We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for
generative adversarial learning. KDD GAN formulates the training as a
likelihood ratio optimization problem where the data distributions are written
explicitly via (local) Kernel Density Estimates (KDE). This is inspired by the
recent progress in contrastive learning and its relation to KDE. We define the
KDEs directly in feature space and forgo the requirement of invertibility of
the kernel feature mappings. In our approach, features are no longer optimized
for linear separability, as in the original GAN formulation, but for the more
general discrimination of distributions in the feature space. We analyze the
gradient of our loss with respect to the feature representation and show that
it is better behaved than that of the original hinge loss. We perform
experiments with the proposed KDE-based loss, used either as a training loss or
a regularization term, on both CIFAR10 and scaled versions of ImageNet. We use
BigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the
architecture of the networks. We show a boost in the quality of generated
samples with respect to FID from 10% to 40% compared to the baseline. Code will
be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1"&gt;Abdelhak Lemkhenter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1"&gt;Adam Bielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1"&gt;Alp Eren Sari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06257</id>
        <link href="http://arxiv.org/abs/2107.06257"/>
        <updated>2021-07-14T01:41:51.983Z</updated>
        <summary type="html"><![CDATA[Geo-localizing static objects from street images is challenging but also very
important for road asset mapping and autonomous driving. In this paper we
present a two-stage framework that detects and geolocalizes traffic signs from
low frame rate street videos. Our proposed system uses a modified version of
RetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign
relative to the camera, in addition to performing the standard classification
and bounding box regression. Candidate sign detections from GPS-RetinaNet are
condensed into geolocalized signs by our custom tracker, which consists of a
learned metric network and a variant of the Hungarian Algorithm. Our metric
network estimates the similarity between pairs of detections, then the
Hungarian Algorithm matches detections across images using the similarity
scores provided by the metric network. Our models were trained using an updated
version of the ARTS dataset, which contains 25,544 images and 47.589 sign
annotations ~\cite{arts}. The proposed dataset covers a diverse set of
environments gathered from a broad selection of roads. Each annotaiton contains
a sign class label, its geospatial location, an assembly label, a side of road
indicator, and unique identifiers that aid in the evaluation. This dataset will
support future progress in the field, and the proposed system demonstrates how
to take advantage of some of the unique characteristics of a realistic
geolocalization dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1"&gt;Thayer Alshaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1"&gt;Colin Van Oort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1"&gt;Jonathan Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1"&gt;Safwan Wshah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Sampling to Optimization on Discrete Domains withApplications to Determinant Maximization. (arXiv:2102.05347v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05347</id>
        <link href="http://arxiv.org/abs/2102.05347"/>
        <updated>2021-07-14T01:41:51.976Z</updated>
        <summary type="html"><![CDATA[We show a connection between sampling and optimization on discrete domains.
For a family of distributions $\mu$ defined on size $k$ subsets of a ground set
of elements that is closed under external fields, we show that rapid mixing of
natural local random walks implies the existence of simple approximation
algorithms to find $\max \mu(\cdot)$. More precisely we show that if
(multi-step) down-up random walks have spectral gap at least inverse
polynomially large in $k$, then (multi-step) local search can find $\max
\mu(\cdot)$ within a factor of $k^{O(k)}$. As the main application of our
result, we show a simple nearly-optimal $k^{O(k)}$-factor approximation
algorithm for MAP inference on nonsymmetric DPPs. This is the first nontrivial
multiplicative approximation for finding the largest size $k$ principal minor
of a square (not-necessarily-symmetric) matrix $L$ with $L+L^\intercal\succeq
0$.

We establish the connection between sampling and optimization by showing that
an exchange inequality, a concept rooted in discrete convex analysis, can be
derived from fast mixing of local random walks. We further connect exchange
inequalities with composable core-sets for optimization, generalizing recent
results on composable core-sets for DPP maximization to arbitrary distributions
that satisfy either the strongly Rayleigh property or that have a log-concave
generating polynomial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anari_N/0/1/0/all/0/1"&gt;Nima Anari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1"&gt;Thuy-Duong Vuong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying variety of customer's online engagement for churn prediction with mixed-penalty logistic regression. (arXiv:2105.07671v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07671</id>
        <link href="http://arxiv.org/abs/2105.07671"/>
        <updated>2021-07-14T01:41:51.968Z</updated>
        <summary type="html"><![CDATA[Using big data to analyze consumer behavior can provide effective
decision-making tools for preventing customer attrition (churn) in customer
relationship management (CRM). Focusing on a CRM dataset with several different
categories of factors that impact customer heterogeneity (i.e., usage of
self-care service channels, duration of service, and responsiveness to
marketing actions), we provide new predictive analytics of customer churn rate
based on a machine learning method that enhances the classification of logistic
regression by adding a mixed penalty term. The proposed penalized logistic
regression can prevent overfitting when dealing with big data and minimize the
loss function when balancing the cost from the median (absolute value) and mean
(squared value) regularization. We show the analytical properties of the
proposed method and its computational advantage in this research. In addition,
we investigate the performance of the proposed method with a CRM data set (that
has a large number of features) under different settings by efficiently
eliminating the disturbance of (1) least important features and (2) sensitivity
from the minority (churn) class. Our empirical results confirm the expected
performance of the proposed method in full compliance with the common
classification criteria (i.e., accuracy, precision, and recall) for evaluating
machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Simovic_P/0/1/0/all/0/1"&gt;Petra Posedel &amp;#x160;imovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Horvatic_D/0/1/0/all/0/1"&gt;Davor Horvatic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_E/0/1/0/all/0/1"&gt;Edward W. Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant Graph Neural Networks for 3D Macromolecular Structure. (arXiv:2106.03843v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03843</id>
        <link href="http://arxiv.org/abs/2106.03843"/>
        <updated>2021-07-14T01:41:51.961Z</updated>
        <summary type="html"><![CDATA[Representing and reasoning about 3D structures of macromolecules is emerging
as a distinct challenge in machine learning. Here, we extend recent work on
geometric vector perceptrons and apply equivariant graph neural networks to a
wide range of tasks from structural biology. Our method outperforms all
reference architectures on three out of eight tasks in the ATOM3D benchmark, is
tied for first on two others, and is competitive with equivariant networks
using higher-order representations and spherical harmonic convolutions. In
addition, we demonstrate that transfer learning can further improve performance
on certain downstream tasks. Code is available at
https://github.com/drorlab/gvp-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Bowen Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eismann_S/0/1/0/all/0/1"&gt;Stephan Eismann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soni_P/0/1/0/all/0/1"&gt;Pratham N. Soni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dror_R/0/1/0/all/0/1"&gt;Ron O. Dror&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05379</id>
        <link href="http://arxiv.org/abs/2104.05379"/>
        <updated>2021-07-14T01:41:51.943Z</updated>
        <summary type="html"><![CDATA[Recent publications on automatic-speech-recognition (ASR) have a strong focus
on attention encoder-decoder (AED) architectures which tend to suffer from
over-fitting in low resource scenarios. One solution to tackle this issue is to
generate synthetic data with a trained text-to-speech system (TTS) if
additional text is available. This was successfully applied in many
publications with AED systems, but only very limited in the context of other
ASR architectures. We investigate the effect of varying pre-processing, the
speaker embedding and input encoding of the TTS system w.r.t. the effectiveness
of the synthesized data for AED-ASR training. Additionally, we also consider
internal language model subtraction for the first time, resulting in up to 38%
relative improvement. We compare the AED results to a state-of-the-art hybrid
ASR system, a monophone based system using
connectionist-temporal-classification (CTC) and a monotonic transducer based
system. We show that for the later systems the addition of synthetic data has
no relevant effect, but they still outperform the AED systems on
LibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a
hybrid system on the clean/noisy test-sets, surpassing any previous
state-of-the-art systems on Librispeech-100h that do not include unlabeled
audio data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1"&gt;Nick Rossenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1"&gt;Mohammad Zeineldeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1"&gt;Benedikt Hilmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Machine Learning for Time-Varying Systems: Low Dimensional Latent Space Tuning. (arXiv:2107.06207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06207</id>
        <link href="http://arxiv.org/abs/2107.06207"/>
        <updated>2021-07-14T01:41:51.936Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) tools such as encoder-decoder convolutional neural
networks (CNN) can represent incredibly complex nonlinear functions which map
between combinations of images and scalars. For example, CNNs can be used to
map combinations of accelerator parameters and images which are 2D projections
of the 6D phase space distributions of charged particle beams as they are
transported between various particle accelerator locations. Despite their
strengths, applying ML to time-varying systems, or systems with shifting
distributions, is an open problem, especially for large systems for which
collecting new data for re-training is impractical or interrupts operations.
Particle accelerators are one example of large time-varying systems for which
collecting detailed training data requires lengthy dedicated beam measurements
which may no longer be available during regular operations. We present a
recently developed method of adaptive ML for time-varying systems. Our approach
is to map very high (N>100k) dimensional inputs (a combination of scalar
parameters and images) into the low dimensional (N~2) latent space at the
output of the encoder section of an encoder-decoder CNN. We then actively tune
the low dimensional latent space-based representation of complex system
dynamics by the addition of an adaptively tuned feedback vector directly before
the decoder sections builds back up to our image-based high-dimensional phase
space density representations. This method allows us to learn correlations
within and to quickly tune the characteristics of incredibly high parameter
systems and to track their evolution in real time based on feedback without
massive new data sets for re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scheinker_A/0/1/0/all/0/1"&gt;Alexander Scheinker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effects of personality traits in predicting grade retention of Brazilian students. (arXiv:2107.05767v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2107.05767</id>
        <link href="http://arxiv.org/abs/2107.05767"/>
        <updated>2021-07-14T01:41:51.922Z</updated>
        <summary type="html"><![CDATA[Student's grade retention is a key issue faced by many education systems,
especially those in developing countries. In this paper, we seek to gauge the
relevance of students' personality traits in predicting grade retention in
Brazil. For that, we used data collected in 2012 and 2017, in the city of
Sertaozinho, countryside of the state of Sao Paulo, Brazil. The surveys taken
in Sertaozinho included several socioeconomic questions, standardized tests,
and a personality test. Moreover, students were in grades 4, 5, and 6 in 2012.
Our approach was based on training machine learning models on the surveys' data
to predict grade retention between 2012 and 2017 using information from 2012 or
before, and then using some strategies to quantify personality traits'
predictive power. We concluded that, besides proving to be fairly better than a
random classifier when isolated, personality traits contribute to prediction
even when using socioeconomic variables and standardized tests results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Toledo_C/0/1/0/all/0/1"&gt;Carmen Melo Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bassedon_G/0/1/0/all/0/1"&gt;Guilherme Mendes Bassedon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ferreira_J/0/1/0/all/0/1"&gt;Jonathan Batista Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gianvechio_L/0/1/0/all/0/1"&gt;Lucka de Godoy Gianvechio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guatimosim_C/0/1/0/all/0/1"&gt;Carlos Guatimosim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Renato Vicente&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategic Instrumental Variable Regression: Recovering Causal Relationships From Strategic Responses. (arXiv:2107.05762v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05762</id>
        <link href="http://arxiv.org/abs/2107.05762"/>
        <updated>2021-07-14T01:41:51.912Z</updated>
        <summary type="html"><![CDATA[Machine Learning algorithms often prompt individuals to strategically modify
their observable attributes to receive more favorable predictions. As a result,
the distribution the predictive model is trained on may differ from the one it
operates on in deployment. While such distribution shifts, in general, hinder
accurate predictions, our work identifies a unique opportunity associated with
shifts due to strategic responses: We show that we can use strategic responses
effectively to recover causal relationships between the observable features and
outcomes we wish to predict. More specifically, we study a game-theoretic model
in which a principal deploys a sequence of models to predict an outcome of
interest (e.g., college GPA) for a sequence of strategic agents (e.g., college
applicants). In response, strategic agents invest efforts and modify their
features for better predictions. In such settings, unobserved confounding
variables can influence both an agent's observable features (e.g., high school
records) and outcomes. Therefore, standard regression methods generally produce
biased estimators. In order to address this issue, our work establishes a novel
connection between strategic responses to machine learning models and
instrumental variable (IV) regression, by observing that the sequence of
deployed models can be viewed as an instrument that affects agents' observable
features but does not directly influence their outcomes. Therefore, two-stage
least squares (2SLS) regression can recover the causal relationships between
observable features and outcomes. Beyond causal recovery, we can build on our
2SLS method to address two additional relevant optimization objectives: agent
outcome maximization and predictive risk minimization. Finally, our numerical
simulations on semi-synthetic data show that our methods significantly
outperform OLS regression in causal relationship estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harris_K/0/1/0/all/0/1"&gt;Keegan Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1"&gt;Daniel Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stapleton_L/0/1/0/all/0/1"&gt;Logan Stapleton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1"&gt;Hoda Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A resource-efficient method for repeated HPO and NAS problems. (arXiv:2103.16111v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16111</id>
        <link href="http://arxiv.org/abs/2103.16111"/>
        <updated>2021-07-14T01:41:51.905Z</updated>
        <summary type="html"><![CDATA[In this work we consider the problem of repeated hyperparameter and neural
architecture search (HNAS). We propose an extension of Successive Halving that
is able to leverage information gained in previous HNAS problems with the goal
of saving computational resources. We empirically demonstrate that our solution
is able to drastically decrease costs while maintaining accuracy and being
robust to negative transfer. Our method is significantly simpler than competing
transfer learning approaches, setting a new baseline for transfer learning in
HNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1"&gt;Giovanni Zappella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salinas_D/0/1/0/all/0/1"&gt;David Salinas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v4 [cs.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.08962</id>
        <link href="http://arxiv.org/abs/1904.08962"/>
        <updated>2021-07-14T01:41:51.886Z</updated>
        <summary type="html"><![CDATA[This paper studies a class of constrained restless multi-armed bandits
(CRMAB). The constraints are in the form of time varying set of actions (set of
available arms). This variation can be either stochastic or semi-deterministic.
Given a set of arms, a fixed number of them can be chosen to be played in each
decision interval. The play of each arm yields a state dependent reward. The
current states of arms are partially observable through binary feedback signals
from arms that are played. The current availability of arms is fully
observable. The objective is to maximize long term cumulative reward. The
uncertainty about future availability of arms along with partial state
information makes this objective challenging. Applications for CRMAB abound in
the domain of cyber-physical systems. First, this optimization problem is
analyzed using Whittle's index policy. To this end, a constrained restless
single-armed bandit is studied. It is shown to admit a threshold-type optimal
policy and is also indexable. An algorithm to compute Whittle's index is
presented. An alternate solution method with lower complexity is also presented
in the form of an online rollout policy. Further, upper bounds on the value
function are derived in order to estimate the degree of sub-optimality of
various solutions. The simulation study compares the performance of Whittle's
index, online rollout, myopic and modified Whittle's index policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaza_K/0/1/0/all/0/1"&gt;Kesav Kaza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meshram_R/0/1/0/all/0/1"&gt;Rahul Meshram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1"&gt;Varun Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merchant_S/0/1/0/all/0/1"&gt;S.N.Merchant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Casual Inference using Deep Bayesian Dynamic Survival Model (CDS). (arXiv:2101.10643v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10643</id>
        <link href="http://arxiv.org/abs/2101.10643"/>
        <updated>2021-07-14T01:41:51.880Z</updated>
        <summary type="html"><![CDATA[Causal inference in longitudinal observational health data often requires the
accurate estimation of treatment effects on time-to-event outcomes in the
presence of time-varying covariates. To tackle this sequential treatment effect
estimation problem, we have developed a causal dynamic survival (CDS) model
that uses the potential outcomes framework with the recurrent sub-networks with
random seed ensembles to estimate the difference in survival curves of its
confidence interval. Using simulated survival datasets, the CDS model has shown
good causal effect estimation performance across scenarios of sample dimension,
event rate, confounding and overlapping. However, increasing the sample size is
not effective to alleviate the adverse impact from high level of confounding.
In two large clinical cohort studies, our model identified the expected
conditional average treatment effect and detected individual effect
heterogeneity over time and patient subgroups. CDS provides individualised
absolute treatment effect estimations to improve clinical decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jie Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gallego_B/0/1/0/all/0/1"&gt;Blanca Gallego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07868</id>
        <link href="http://arxiv.org/abs/2102.07868"/>
        <updated>2021-07-14T01:41:51.873Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows. (arXiv:2002.10516v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10516</id>
        <link href="http://arxiv.org/abs/2002.10516"/>
        <updated>2021-07-14T01:41:51.866Z</updated>
        <summary type="html"><![CDATA[Normalizing flows transform a simple base distribution into a complex target
distribution and have proved to be powerful models for data generation and
density estimation. In this work, we propose a novel type of normalizing flow
driven by a differential deformation of the Wiener process. As a result, we
obtain a rich time series model whose observable process inherits many of the
appealing properties of its base process, such as efficient computation of
likelihoods and marginals. Furthermore, our continuous treatment provides a
natural framework for irregular time series with an independent arrival
process, including straightforward interpolation. We illustrate the desirable
properties of the proposed model on popular stochastic processes and
demonstrate its superior flexibility to variational RNN and latent ODE
baselines in a series of experiments on synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruizhi Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Bo Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1"&gt;Greg Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1"&gt;Andreas Lehrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphSVX: Shapley Value Explanations for Graph Neural Networks. (arXiv:2104.10482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10482</id>
        <link href="http://arxiv.org/abs/2104.10482"/>
        <updated>2021-07-14T01:41:51.858Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) achieve significant performance for various
learning tasks on geometric data due to the incorporation of graph structure
into the learning of node representations, which renders their comprehension
challenging. In this paper, we first propose a unified framework satisfied by
most existing GNN explainers. Then, we introduce GraphSVX, a post hoc local
model-agnostic explanation method specifically designed for GNNs. GraphSVX is a
decomposition technique that captures the "fair" contribution of each feature
and node towards the explained prediction by constructing a surrogate model on
a perturbed dataset. It extends to graphs and ultimately provides as
explanation the Shapley Values from game theory. Experiments on real-world and
synthetic datasets demonstrate that GraphSVX achieves state-of-the-art
performance compared to baseline models while presenting core theoretical and
human-centric properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duval_A/0/1/0/all/0/1"&gt;Alexandre Duval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1"&gt;Fragkiskos D. Malliaros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Interpretable Models of Theory of Mind. (arXiv:2104.02938v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02938</id>
        <link href="http://arxiv.org/abs/2104.02938"/>
        <updated>2021-07-14T01:41:51.840Z</updated>
        <summary type="html"><![CDATA[When developing AI systems that interact with humans, it is essential to
design both a system that can understand humans, and a system that humans can
understand. Most deep network based agent-modeling approaches are 1) not
interpretable and 2) only model external behavior, ignoring internal mental
states, which potentially limits their capability for assistance,
interventions, discovering false beliefs, etc. To this end, we develop an
interpretable modular neural framework for modeling the intentions of other
observed entities. We demonstrate the efficacy of our approach with experiments
on data from human participants on a search and rescue task in Minecraft, and
show that incorporating interpretability can significantly increase predictive
performance under the right conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oguntola_I/0/1/0/all/0/1"&gt;Ini Oguntola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1"&gt;Dana Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1"&gt;Katia Sycara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated Uncertainty for Molecular Property Prediction using Ensembles of Message Passing Neural Networks. (arXiv:2107.06068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06068</id>
        <link href="http://arxiv.org/abs/2107.06068"/>
        <updated>2021-07-14T01:41:51.833Z</updated>
        <summary type="html"><![CDATA[Data-driven methods based on machine learning have the potential to
accelerate analysis of atomic structures. However, machine learning models can
produce overconfident predictions and it is therefore crucial to detect and
handle uncertainty carefully. Here, we extend a message passing neural network
designed specifically for predicting properties of molecules and materials with
a calibrated probabilistic predictive distribution. The method presented in
this paper differs from the previous work by considering both aleatoric and
epistemic uncertainty in a unified framework, and by re-calibrating the
predictive distribution on unseen data. Through computer experiments, we show
that our approach results in accurate models for predicting molecular formation
energies with calibrated uncertainty in and out of the training data
distribution on two public molecular benchmark datasets, QM9 and PC9. The
proposed method provides a general framework for training and evaluating neural
network ensemble models that are able to produce accurate predictions of
properties of molecules with calibrated uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Busk_J/0/1/0/all/0/1"&gt;Jonas Busk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorgensen_P/0/1/0/all/0/1"&gt;Peter Bj&amp;#xf8;rn J&amp;#xf8;rgensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhowmik_A/0/1/0/all/0/1"&gt;Arghya Bhowmik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mikkel N. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1"&gt;Ole Winther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vegge_T/0/1/0/all/0/1"&gt;Tejs Vegge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for OS-ELM Digital Circuit. (arXiv:2103.09791v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09791</id>
        <link href="http://arxiv.org/abs/2103.09791"/>
        <updated>2021-07-14T01:41:51.827Z</updated>
        <summary type="html"><![CDATA[Currently there has been increasing demand for real-time training on
resource-limited IoT devices such as smart sensors, which realizes standalone
online adaptation for streaming data without data transfers to remote servers.
OS-ELM (Online Sequential Extreme Learning Machine) has been one of promising
neural-network-based online algorithms for on-chip learning because it can
perform online training at low computational cost and is easy to implement as a
digital circuit. Existing OS-ELM digital circuits employ fixed-point data
format and the bit-widths are often manually tuned, however, this may cause
overflow or underflow which can lead to unexpected behavior of the circuit. For
on-chip learning systems, an overflow/underflow-free design has a great impact
since online training is continuously performed and the intervals of
intermediate variables will dynamically change as time goes by. In this paper,
we propose an overflow/underflow-free bit-width optimization method for
fixed-point digital circuits of OS-ELM. Experimental results show that our
method realizes overflow/underflow-free OS-ELM digital circuits with 1.0x -
1.5x more area cost compared to the baseline simulation method where overflow
or underflow can happen.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1"&gt;Mineto Tsukada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1"&gt;Hiroki Matsutani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stress Classification and Personalization: Getting the most out of the least. (arXiv:2107.05666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05666</id>
        <link href="http://arxiv.org/abs/2107.05666"/>
        <updated>2021-07-14T01:41:51.820Z</updated>
        <summary type="html"><![CDATA[Stress detection and monitoring is an active area of research with important
implications for the personal, professional, and social health of an
individual. Current approaches for affective state classification use
traditional machine learning algorithms with features computed from multiple
sensor modalities. These methods are data-intensive and rely on hand-crafted
features which impede the practical applicability of these sensor systems in
daily lives. To overcome these shortcomings, we propose a novel Convolutional
Neural Network (CNN) based stress detection and classification framework
without any feature computation using data from only one sensor modality. Our
method is competitive and outperforms current state-of-the-art techniques and
achieves a classification accuracy of $92.85\%$ and an $f1$ score of $0.89$.
Through our leave-one-subject-out analysis, we also show the importance of
personalizing stress models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sah_R/0/1/0/all/0/1"&gt;Ramesh Kumar Sah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghasemzadeh_H/0/1/0/all/0/1"&gt;Hassan Ghasemzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability. (arXiv:2107.06277v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06277</id>
        <link href="http://arxiv.org/abs/2107.06277"/>
        <updated>2021-07-14T01:41:51.813Z</updated>
        <summary type="html"><![CDATA[Generalization is a central challenge for the deployment of reinforcement
learning (RL) systems in the real world. In this paper, we show that the
sequential structure of the RL problem necessitates new approaches to
generalization beyond the well-studied techniques used in supervised learning.
While supervised learning methods can generalize effectively without explicitly
accounting for epistemic uncertainty, we show that, perhaps surprisingly, this
is not the case in RL. We show that generalization to unseen test conditions
from a limited number of training conditions induces implicit partial
observability, effectively turning even fully-observed MDPs into POMDPs.
Informed by this observation, we recast the problem of generalization in RL as
solving the induced partially observed Markov decision process, which we call
the epistemic POMDP. We demonstrate the failure modes of algorithms that do not
appropriately handle this partial observability, and suggest a simple
ensemble-based technique for approximately solving the partially observed
problem. Empirically, we demonstrate that our simple algorithm derived from the
epistemic POMDP achieves significant gains in generalization over current
methods on the Procgen benchmark suite.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1"&gt;Dibya Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahme_J/0/1/0/all/0/1"&gt;Jad Rahme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aviral Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_R/0/1/0/all/0/1"&gt;Ryan P. Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04718</id>
        <link href="http://arxiv.org/abs/2106.04718"/>
        <updated>2021-07-14T01:41:51.795Z</updated>
        <summary type="html"><![CDATA[Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Ting Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1"&gt;Desheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1"&gt;Bingyu Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intermittent Jamming against Telemetry and Telecommand of Satellite Systems and A Learning-driven Detection Strategy. (arXiv:2107.06181v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06181</id>
        <link href="http://arxiv.org/abs/2107.06181"/>
        <updated>2021-07-14T01:41:51.788Z</updated>
        <summary type="html"><![CDATA[Towards sixth-generation networks (6G), satellite communication systems,
especially based on Low Earth Orbit (LEO) networks, become promising due to
their unique and comprehensive capabilities. These advantages are accompanied
by a variety of challenges such as security vulnerabilities, management of
hybrid systems, and high mobility. In this paper, firstly, a security
deficiency in the physical layer is addressed with a conceptual framework,
considering the cyber-physical nature of the satellite systems, highlighting
the potential attacks. Secondly, a learning-driven detection scheme is
proposed, and the lightweight convolutional neural network (CNN) is designed.
The performance of the designed CNN architecture is compared with a prevalent
machine learning algorithm, support vector machine (SVM). The results show that
deficiency attacks against the satellite systems can be detected by employing
the proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gecgel_S/0/1/0/all/0/1"&gt;Selen Gecgel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurt_G/0/1/0/all/0/1"&gt;Gunes Karabulut Kurt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09351</id>
        <link href="http://arxiv.org/abs/2102.09351"/>
        <updated>2021-07-14T01:41:51.781Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) is one of the vital image processing methods that
improve the resolution of an image in the field of computer vision. In the last
two decades, significant progress has been made in the field of
super-resolution, especially by utilizing deep learning methods. This survey is
an effort to provide a detailed survey of recent progress in single-image
super-resolution in the perspective of deep learning while also informing about
the initial classical methods used for image super-resolution. The survey
classifies the image SR methods into four categories, i.e., classical methods,
supervised learning-based methods, unsupervised learning-based methods, and
domain-specific SR methods. We also introduce the problem of SR to provide
intuition about image quality metrics, available reference datasets, and SR
challenges. Deep learning-based approaches of SR are evaluated using a
reference dataset. Some of the reviewed state-of-the-art image SR methods
include the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),
multiscale residual network (MSRN), meta residual dense network (Meta-RDN),
recurrent back-projection network (RBPN), second-order attention network (SAN),
SR feedback network (SRFBN) and the wavelet-based residual attention network
(WRAN). Finally, this survey is concluded with future directions and trends in
SR and open problems in SR to be addressed by the researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1"&gt;Syed Muhammad Arsalan Bashir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mahrukh Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1"&gt;Yilong Niu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Modeling: A Review. (arXiv:2104.12556v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12556</id>
        <link href="http://arxiv.org/abs/2104.12556"/>
        <updated>2021-07-14T01:41:51.774Z</updated>
        <summary type="html"><![CDATA[The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming challenges and opportunities to data and domain-driven modeling.
This paper makes a comprehensive review of the challenges, tasks, methods, gaps
and opportunities on modeling COVID-19 problems and data. It constructs a
research landscape of COVID-19 modeling, and further categorizes, compares and
discusses the related work on modeling COVID-19 epidemic transmission processes
and dynamics, case identification and tracing, infection diagnosis and trends,
medical treatments, non-pharmaceutical intervention effect, drug and vaccine
development, psychological, economic and social impact, and misinformation,
etc. The modeling methods involve mathematical and statistical models,
domain-driven modeling by epidemiological compartmental models, medical and
biomedical analysis, data-driven learning by shallow and deep machine learning,
simulation systems, social science methods, and hybrid methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning of Conjugate Mappings. (arXiv:2104.01874v2 [math.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01874</id>
        <link href="http://arxiv.org/abs/2104.01874"/>
        <updated>2021-07-14T01:41:51.767Z</updated>
        <summary type="html"><![CDATA[Despite many of the most common chaotic dynamical systems being continuous in
time, it is through discrete time mappings that much of the understanding of
chaos is formed. Henri Poincar\'e first made this connection by tracking
consecutive iterations of the continuous flow with a lower-dimensional,
transverse subspace. The mapping that iterates the dynamics through consecutive
intersections of the flow with the subspace is now referred to as a Poincar\'e
map, and it is the primary method available for interpreting and classifying
chaotic dynamics. Unfortunately, in all but the simplest systems, an explicit
form for such a mapping remains outstanding. This work proposes a method for
obtaining explicit Poincar\'e mappings by using deep learning to construct an
invertible coordinate transformation into a conjugate representation where the
dynamics are governed by a relatively simple chaotic mapping. The invertible
change of variable is based on an autoencoder, which allows for dimensionality
reduction, and has the advantage of classifying chaotic systems using the
equivalence relation of topological conjugacies. Indeed, the enforcement of
topological conjugacies is the critical neural network regularization for
learning the coordinate and dynamics pairing. We provide expository
applications of the method to low-dimensional systems such as the R\"ossler and
Lorenz systems, while also demonstrating the utility of the method on
infinite-dimensional systems, such as the Kuramoto--Sivashinsky equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bramburger_J/0/1/0/all/0/1"&gt;Jason J. Bramburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Brunton_S/0/1/0/all/0/1"&gt;Steven L. Brunton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Recovery of Clusters in Finite Metric Spaces Using Oracle Queries. (arXiv:2102.00504v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00504</id>
        <link href="http://arxiv.org/abs/2102.00504"/>
        <updated>2021-07-14T01:41:51.747Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of exact cluster recovery using oracle queries.
Previous results show that clusters in Euclidean spaces that are convex and
separated with a margin can be reconstructed exactly using only $O(\log n)$
same-cluster queries, where $n$ is the number of input points. In this work, we
study this problem in the more challenging non-convex setting. We introduce a
structural characterization of clusters, called $(\beta,\gamma)$-convexity,
that can be applied to any finite set of points equipped with a metric (or even
a semimetric, as the triangle inequality is not needed). Using
$(\beta,\gamma)$-convexity, we can translate natural density properties of
clusters (which include, for instance, clusters that are strongly non-convex in
$\mathbb{R}^d$) into a graph-theoretic notion of convexity. By exploiting this
convexity notion, we design a deterministic algorithm that recovers
$(\beta,\gamma)$-convex clusters using $O(k^2 \log n + k^2
(6/\beta\gamma)^{dens(X)})$ same-cluster queries, where $k$ is the number of
clusters and $dens(X)$ is the density dimension of the semimetric. We show that
an exponential dependence on the density dimension is necessary, and we also
show that, if we are allowed to make $O(k^2 + k\log n)$ additional queries to a
"cluster separation" oracle, then we can recover clusters that have different
and arbitrary scales, even when the scale of each cluster is unknown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bressan_M/0/1/0/all/0/1"&gt;Marco Bressan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1"&gt;Silvio Lattanzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1"&gt;Andrea Paudice&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automatic Instrumentation by Learning to Separate Parts in Symbolic Multitrack Music. (arXiv:2107.05916v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05916</id>
        <link href="http://arxiv.org/abs/2107.05916"/>
        <updated>2021-07-14T01:41:51.741Z</updated>
        <summary type="html"><![CDATA[Modern keyboards allow a musician to play multiple instruments at the same
time by assigning zones -- fixed pitch ranges of the keyboard -- to different
instruments. In this paper, we aim to further extend this idea and examine the
feasibility of automatic instrumentation -- dynamically assigning instruments
to notes in solo music during performance. In addition to the online,
real-time-capable setting for performative use cases, automatic instrumentation
can also find applications in assistive composing tools in an offline setting.
Due to the lack of paired data of original solo music and their full
arrangements, we approach automatic instrumentation by learning to separate
parts (e.g., voices, instruments and tracks) from their mixture in symbolic
multitrack music, assuming that the mixture is to be played on a keyboard. We
frame the task of part separation as a sequential multi-class classification
problem and adopt machine learning to map sequences of notes into sequences of
part labels. To examine the effectiveness of our proposed models, we conduct a
comprehensive empirical evaluation over four diverse datasets of different
genres and ensembles -- Bach chorales, string quartets, game music and pop
music. Our experiments show that the proposed models outperform various
baselines. We also demonstrate the potential for our proposed models to produce
alternative convincing instrumentations for an existing arrangement by
separating its mixture into parts. All source code and audio samples can be
found at https://salu133445.github.io/arranger/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05942</id>
        <link href="http://arxiv.org/abs/2104.05942"/>
        <updated>2021-07-14T01:41:51.734Z</updated>
        <summary type="html"><![CDATA[This paper introduces recurrent equilibrium networks (RENs), a new class of
nonlinear dynamical models for applications in machine learning, system
identification and control. The new model class has ``built in'' guarantees of
stability and robustness: all models in the class are contracting - a strong
form of nonlinear stability - and models can satisfy prescribed incremental
integral quadratic constraints (IQC), including Lipschitz bounds and
incremental passivity. RENs are otherwise very flexible: they can represent all
stable linear systems, all previously-known sets of contracting recurrent
neural networks and echo state networks, all deep feedforward neural networks,
and all stable Wiener/Hammerstein models. RENs are parameterized directly by a
vector in R^N, i.e. stability and robustness are ensured without parameter
constraints, which simplifies learning since generic methods for unconstrained
optimization can be used. The performance and robustness of the new model set
is evaluated on benchmark nonlinear system identification problems, and the
paper also presents applications in data-driven nonlinear observer design and
control with stability guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Revay_M/0/1/0/all/0/1"&gt;Max Revay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruigang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manchester_I/0/1/0/all/0/1"&gt;Ian R. Manchester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainability-based Backdoor Attacks Against Graph Neural Networks. (arXiv:2104.03674v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03674</id>
        <link href="http://arxiv.org/abs/2104.03674"/>
        <updated>2021-07-14T01:41:51.727Z</updated>
        <summary type="html"><![CDATA[Backdoor attacks represent a serious threat to neural network models. A
backdoored model will misclassify the trigger-embedded inputs into an
attacker-chosen target label while performing normally on other benign inputs.
There are already numerous works on backdoor attacks on neural networks, but
only a few works consider graph neural networks (GNNs). As such, there is no
intensive research on explaining the impact of trigger injecting position on
the performance of backdoor attacks on GNNs.

To bridge this gap, we conduct an experimental investigation on the
performance of backdoor attacks on GNNs. We apply two powerful GNN
explainability approaches to select the optimal trigger injecting position to
achieve two attacker objectives -- high attack success rate and low clean
accuracy drop. Our empirical results on benchmark datasets and state-of-the-art
neural network models demonstrate the proposed method's effectiveness in
selecting trigger injecting position for backdoor attacks on GNNs. For
instance, on the node classification task, the backdoor attack with trigger
injecting position selected by GraphLIME reaches over $84 \%$ attack success
rate with less than $2.5 \%$ accuracy drop]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minhui/0/1/0/all/0/1"&gt;Minhui&lt;/a&gt; (Jason)Xue, &lt;a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1"&gt;Stjepan Picek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06239</id>
        <link href="http://arxiv.org/abs/2107.06239"/>
        <updated>2021-07-14T01:41:51.720Z</updated>
        <summary type="html"><![CDATA[We consider the problem of obese human mesh recovery, i.e., fitting a
parametric human mesh to images of obese people. Despite obese person mesh
fitting being an important problem with numerous applications (e.g.,
healthcare), much recent progress in mesh recovery has been restricted to
images of non-obese people. In this work, we identify this crucial gap in the
current literature by presenting and discussing limitations of existing
algorithms. Next, we present a simple baseline to address this problem that is
scalable and can be easily used in conjunction with existing algorithms to
improve their performance. Finally, we present a generalized human mesh
optimization algorithm that substantially improves the performance of existing
methods on both obese person images as well as community-standard benchmark
datasets. A key innovation of this technique is that it does not rely on
supervision from expensive-to-create mesh parameters. Instead, starting from
widely and cheaply available 2D keypoints annotations, our method automatically
generates mesh parameters that can in turn be used to re-train and fine-tune
any existing mesh estimation algorithm. This way, we show our method acts as a
drop-in to improve the performance of a wide variety of contemporary mesh
estimation methods. We conduct extensive experiments on multiple datasets
comprising both standard and obese person images and demonstrate the efficacy
of our proposed techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ren Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Meng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1"&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Instance-Level Label Noise: Disparate Impacts and Treatments. (arXiv:2102.05336v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05336</id>
        <link href="http://arxiv.org/abs/2102.05336"/>
        <updated>2021-07-14T01:41:51.701Z</updated>
        <summary type="html"><![CDATA[This paper aims to provide understandings for the effect of an
over-parameterized model, e.g. a deep neural network, memorizing
instance-dependent noisy labels. We first quantify the harms caused by
memorizing noisy instances, and show the disparate impacts of noisy labels for
sample instances with different representation frequencies. We then analyze how
several popular solutions for learning with noisy labels mitigate this harm at
the instance level. Our analysis reveals that existing approaches lead to
disparate treatments when handling noisy instances. While higher-frequency
instances often enjoy a high probability of an improvement by applying these
solutions, lower-frequency instances do not. Our analysis reveals new
understandings for when these approaches work, and provides theoretical
justifications for previously reported empirical observations. This observation
requires us to rethink the distribution of label noise across instances and
calls for different treatments for instances in different regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework. (arXiv:2106.06046v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06046</id>
        <link href="http://arxiv.org/abs/2106.06046"/>
        <updated>2021-07-14T01:41:51.694Z</updated>
        <summary type="html"><![CDATA[Guidelines and principles of trustworthy AI should be adhered to in practice
during the development of AI systems. This work suggests a novel information
theoretic trustworthy AI framework based on the hypothesis that information
theory enables taking into account the ethical AI principles during the
development of machine learning and deep learning models via providing a way to
study and optimize the inherent tradeoffs between trustworthy AI principles.
Under the proposed framework, a unified approach to ``privacy-preserving
interpretable and transferable learning'' is considered to introduce the
information theoretic measures for privacy-leakage, interpretability, and
transferability. A technique based on variational optimization, employing
\emph{conditionally deep autoencoders}, is developed for practically
calculating the defined information theoretic measures for privacy-leakage,
interpretability, and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mohit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1"&gt;Bernhard A. Moser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1"&gt;Lukas Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freudenthaler_B/0/1/0/all/0/1"&gt;Bernhard Freudenthaler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stable Representations with Full Encoder. (arXiv:2103.14082v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14082</id>
        <link href="http://arxiv.org/abs/2103.14082"/>
        <updated>2021-07-14T01:41:51.688Z</updated>
        <summary type="html"><![CDATA[While the beta-VAE family is aiming to find disentangled representations and
acquire human-interpretable generative factors, like what an ICA (from the
linear domain) does, we propose Full Encoder, a novel unified autoencoder
framework as a correspondence to PCA in the non-linear domain. The idea is to
train an autoencoder with one latent variable first, then involve more latent
variables progressively to refine the reconstruction results. The Full Encoder
is also a latent variable predictive model that the latent variables acquired
are stable and robust, as they always learn the same representation regardless
of the network initial states. Full Encoder can be used to determine the
degrees of freedom in a simple non-linear system and can be useful for data
compression or anomaly detection. Full Encoder can also be combined with the
beta-VAE framework to sort out the importance of the generative factors,
providing more insights for non-linear system analysis. These qualities will
make FE useful for analyzing real-life industrial non-linear systems. To
validate, we created a toy dataset with a custom-made non-linear system to test
it and compare its properties to those of VAE and beta-VAE's.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhouzheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1"&gt;Kun Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks and End-to-End Learning for Audio Compression. (arXiv:2105.11681v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11681</id>
        <link href="http://arxiv.org/abs/2105.11681"/>
        <updated>2021-07-14T01:41:51.682Z</updated>
        <summary type="html"><![CDATA[Recent achievements in end-to-end deep learning have encouraged the
exploration of tasks dealing with highly structured data with unified deep
network models. Having such models for compressing audio signals has been
challenging since it requires discrete representations that are not easy to
train with end-to-end backpropagation. In this paper, we present an end-to-end
deep learning approach that combines recurrent neural networks (RNNs) within
the training strategy of variational autoencoders (VAEs) with a binary
representation of the latent space. We apply a reparametrization trick for the
Bernoulli distribution for the discrete representations, which allows smooth
backpropagation. In addition, our approach allows the separation of the encoder
and decoder, which is necessary for compression tasks. To our best knowledge,
this is the first end-to-end learning for a single audio compression model with
RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1"&gt;Daniela N. Rim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_I/0/1/0/all/0/1"&gt;Inseon Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Heeyoul Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiCOVA-Net: Diagnosing COVID-19 using Acoustics based on Deep Residual Network for the DiCOVA Challenge 2021. (arXiv:2107.06126v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.06126</id>
        <link href="http://arxiv.org/abs/2107.06126"/>
        <updated>2021-07-14T01:41:51.675Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep residual network-based method, namely the
DiCOVA-Net, to identify COVID-19 infected patients based on the acoustic
recording of their coughs. Since there are far more healthy people than
infected patients, this classification problem faces the challenge of
imbalanced data. To improve the model's ability to recognize minority class
(the infected patients), we introduce data augmentation and cost-sensitive
methods into our model. Besides, considering the particularity of this task, we
deploy some fine-tuning techniques to adjust the pre-training ResNet50.
Furthermore, to improve the model's generalizability, we use ensemble learning
to integrate prediction results from multiple base classifiers generated using
different random seeds. To evaluate the proposed DiCOVA-Net's performance, we
conducted experiments with the DiCOVA challenge dataset. The results show that
our method has achieved 85.43\% in AUC, among the top of all competing teams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jiangeng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shaoze Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Mengling Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Transformation Learning for Deep Anomaly Detection Beyond Images. (arXiv:2103.16440v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16440</id>
        <link href="http://arxiv.org/abs/2103.16440"/>
        <updated>2021-07-14T01:41:51.658Z</updated>
        <summary type="html"><![CDATA[Data transformations (e.g. rotations, reflections, and cropping) play an
important role in self-supervised learning. Typically, images are transformed
into different views, and neural networks trained on tasks involving these
views produce useful feature representations for downstream tasks, including
anomaly detection. However, for anomaly detection beyond image data, it is
often unclear which transformations to use. Here we present a simple end-to-end
procedure for anomaly detection with learnable transformations. The key idea is
to embed the transformed data into a semantic space such that the transformed
data still resemble their untransformed form, while different transformations
are easily distinguishable. Extensive experiments on time series demonstrate
that our proposed method outperforms existing approaches in the one-vs.-rest
setting and is competitive in the more challenging n-vs.-rest anomaly detection
task. On tabular datasets from the medical and cyber-security domains, our
method learns domain-specific transformations and detects anomalies more
accurately than previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1"&gt;Chen Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfrommer_T/0/1/0/all/0/1"&gt;Timo Pfrommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1"&gt;Marius Kloft&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1"&gt;Stephan Mandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1"&gt;Maja Rudolph&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Induced Domain Adaptation. (arXiv:2107.05911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05911</id>
        <link href="http://arxiv.org/abs/2107.05911"/>
        <updated>2021-07-14T01:41:51.651Z</updated>
        <summary type="html"><![CDATA[We formulate the problem of induced domain adaptation (IDA) when the
underlying distribution/domain shift is introduced by the model being deployed.
Our formulation is motivated by applications where the deployed machine
learning models interact with human agents, and will ultimately face responsive
and interactive data distributions. We formalize the discussions of the
transferability of learning in our IDA setting by studying how the model
trained on the available source distribution (data) would translate to the
performance on the induced domain. We provide both upper bounds for the
performance gap due to the induced domain shift, as well as lower bound for the
trade-offs a classifier has to suffer on either the source training
distribution or the induced target distribution. We provide further
instantiated analysis for two popular domain adaptation settings with covariate
shift and label shift. We highlight some key properties of IDA, as well as
computational and learning challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yatong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07936</id>
        <link href="http://arxiv.org/abs/1911.07936"/>
        <updated>2021-07-14T01:41:51.644Z</updated>
        <summary type="html"><![CDATA[Eye tracking is handled as one of the key technologies for applications that
assess and evaluate human attention, behavior, and biometrics, especially using
gaze, pupillary, and blink behaviors. One of the challenges with regard to the
social acceptance of eye tracking technology is however the preserving of
sensitive and personal information. To tackle this challenge, we employ a
privacy-preserving framework based on randomized encoding to train a Support
Vector Regression model using synthetic eye images privately to estimate the
human gaze. During the computation, none of the parties learn about the data or
the result that any other party has. Furthermore, the party that trains the
model cannot reconstruct pupil, blinks or visual scanpath. The experimental
results show that our privacy-preserving framework is capable of working in
real-time, with the same accuracy as compared to non-private version and could
be extended to other eye tracking related problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1"&gt;Efe Bozkir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1"&gt;Ali Burak &amp;#xdc;nal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1"&gt;Mete Akg&amp;#xfc;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1"&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1"&gt;Nico Pfeifer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05997</id>
        <link href="http://arxiv.org/abs/2107.05997"/>
        <updated>2021-07-14T01:41:51.636Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have an enormous potential to learn from complex
biomedical data. In particular, DNNs have been used to seamlessly fuse
heterogeneous information from neuroanatomy, genetics, biomarkers, and
neuropsychological tests for highly accurate Alzheimer's disease diagnosis. On
the other hand, their black-box nature is still a barrier for the adoption of
such a system in the clinic, where interpretability is absolutely essential. We
propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for
explaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of
the neuroanatomy and tabular biomarkers. Our explanations are based on the
Shapley value, which is the unique method that satisfies all fundamental axioms
for local explanations previously established in the literature. Thus, SVEHNN
has many desirable characteristics that previous work on interpretability for
medical decision making is lacking. To avoid the exponential time complexity of
the Shapley value, we propose to transform a given DNN into a Lightweight
Probabilistic Deep Network without re-training, thus achieving a complexity
only quadratic in the number of features. In our experiments on synthetic and
real data, we show that we can closely approximate the exact Shapley value with
a dramatically reduced runtime and can reveal the hidden knowledge the network
has learned from the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1"&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1"&gt;Christina Aigner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1"&gt;Christian Wachinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorical Foundations of Gradient-Based Learning. (arXiv:2103.01931v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01931</id>
        <link href="http://arxiv.org/abs/2103.01931"/>
        <updated>2021-07-14T01:41:51.629Z</updated>
        <summary type="html"><![CDATA[We propose a categorical semantics of gradient-based machine learning
algorithms in terms of lenses, parametrised maps, and reverse derivative
categories. This foundation provides a powerful explanatory and unifying
framework: it encompasses a variety of gradient descent algorithms such as
ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions
such as as MSE and Softmax cross-entropy, shedding new light on their
similarities and differences. Our approach to gradient-based learning has
examples generalising beyond the familiar continuous domains (modelled in
categories of smooth maps) and can be realized in the discrete setting of
boolean circuits. Finally, we demonstrate the practical significance of our
framework with an implementation in Python.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cruttwell_G/0/1/0/all/0/1"&gt;G.S.H. Cruttwell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavranovic_B/0/1/0/all/0/1"&gt;Bruno Gavranovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghani_N/0/1/0/all/0/1"&gt;Neil Ghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_P/0/1/0/all/0/1"&gt;Paul Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanasi_F/0/1/0/all/0/1"&gt;Fabio Zanasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in Drug Discovery: Applications and Techniques. (arXiv:2106.05386v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05386</id>
        <link href="http://arxiv.org/abs/2106.05386"/>
        <updated>2021-07-14T01:41:51.613Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) has been transforming the practice of drug
discovery in the past decade. Various AI techniques have been used in a wide
range of applications, such as virtual screening and drug design. In this
survey, we first give an overview on drug discovery and discuss related
applications, which can be reduced to two major tasks, i.e., molecular property
prediction and molecule generation. We then discuss common data resources,
molecule representations and benchmark platforms. Furthermore, to summarize the
progress of AI in drug discovery, we present the relevant AI techniques
including model architectures and learning paradigms in the papers surveyed. We
expect that this survey will serve as a guide for researchers who are
interested in working at the interface of artificial intelligence and drug
discovery. We also provide a GitHub repository
(https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection
of papers and codes, if applicable, as a learning resource, which is regularly
updated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jianyuan Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ojima_I/0/1/0/all/0/1"&gt;Iwao Ojima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samaras_D/0/1/0/all/0/1"&gt;Dimitris Samaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fusheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Meta-Prior Learning Using Empirical Bayes. (arXiv:2002.01129v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.01129</id>
        <link href="http://arxiv.org/abs/2002.01129"/>
        <updated>2021-07-14T01:41:51.606Z</updated>
        <summary type="html"><![CDATA[Adding domain knowledge to a learning system is known to improve results. In
multi-parameter Bayesian frameworks, such knowledge is incorporated as a prior.
On the other hand, various model parameters can have different learning rates
in real-world problems, especially with skewed data. Two often-faced challenges
in Operation Management and Management Science applications are the absence of
informative priors, and the inability to control parameter learning rates. In
this study, we propose a hierarchical Empirical Bayes approach that addresses
both challenges, and that can generalize to any Bayesian framework. Our method
learns empirical meta-priors from the data itself and uses them to decouple the
learning rates of first-order and second-order features (or any other given
feature grouping) in a Generalized Linear Model. As the first-order features
are likely to have a more pronounced effect on the outcome, focusing on
learning first-order weights first is likely to improve performance and
convergence time. Our Empirical Bayes method clamps features in each group
together and uses the deployed model's observed data to empirically compute a
hierarchical prior in hindsight. We report theoretical results for the
unbiasedness, strong consistency, and optimal frequentist cumulative regret
properties of our meta-prior variance estimator. We apply our method to a
standard supervised learning optimization problem, as well as an online
combinatorial optimization problem in a contextual bandit setting implemented
in an Amazon production system. Both during simulations and live experiments,
our method shows marked improvements, especially in cases of small traffic. Our
findings are promising, as optimizing over sparse data is often a challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabi_S/0/1/0/all/0/1"&gt;Sareh Nabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassif_H/0/1/0/all/0/1"&gt;Houssam Nassif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Joseph Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamani_H/0/1/0/all/0/1"&gt;Hamed Mamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imbens_G/0/1/0/all/0/1"&gt;Guido Imbens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03871</id>
        <link href="http://arxiv.org/abs/2009.03871"/>
        <updated>2021-07-14T01:41:51.600Z</updated>
        <summary type="html"><![CDATA[In this work we propose a method based on geometric deep learning to predict
the complete surface of the liver, given a partial point cloud of the organ
obtained during the surgical laparoscopic procedure. We introduce a new data
augmentation technique that randomly perturbs shapes in their frequency domain
to compensate the limited size of our dataset. The core of our method is a
variational autoencoder (VAE) that is trained to learn a latent space for
complete shapes of the liver. At inference time, the generative part of the
model is embedded in an optimisation procedure where the latent representation
is iteratively updated to generate a model that matches the intraoperative
partial point cloud. The effect of this optimisation is a progressive non-rigid
deformation of the initially generated shape. Our method is qualitatively
evaluated on real data and quantitatively evaluated on synthetic data. We
compared with a state-of-the-art rigid registration algorithm, that our method
outperformed in visible areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1"&gt;Simone Foti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1"&gt;Bongjin Koo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1"&gt;Thomas Dowrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1"&gt;Joao Ramalhinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1"&gt;Moustafa Allam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1"&gt;Brian Davidson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1"&gt;Matthew J. Clarkson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[POMO: Policy Optimization with Multiple Optima for Reinforcement Learning. (arXiv:2010.16011v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16011</id>
        <link href="http://arxiv.org/abs/2010.16011"/>
        <updated>2021-07-14T01:41:51.593Z</updated>
        <summary type="html"><![CDATA[In neural combinatorial optimization (CO), reinforcement learning (RL) can
turn a deep neural net into a fast, powerful heuristic solver of NP-hard
problems. This approach has a great potential in practical applications because
it allows near-optimal solutions to be found without expert guides armed with
substantial domain knowledge. We introduce Policy Optimization with Multiple
Optima (POMO), an end-to-end approach for building such a heuristic solver.
POMO is applicable to a wide range of CO problems. It is designed to exploit
the symmetries in the representation of a CO solution. POMO uses a modified
REINFORCE algorithm that forces diverse rollouts towards all optimal solutions.
Empirically, the low-variance baseline of POMO makes RL training fast and
stable, and it is more resistant to local minima compared to previous
approaches. We also introduce a new augmentation-based inference method, which
accompanies POMO nicely. We demonstrate the effectiveness of POMO by solving
three popular NP-hard problems, namely, traveling salesman (TSP), capacitated
vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based
on POMO shows a significant improvement in performance over all recent learned
heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100
while reducing inference time by more than an order of magnitude.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1"&gt;Yeong-Dae Kwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jinho Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Byoungjip Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1"&gt;Iljoo Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1"&gt;Youngjune Gwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1"&gt;Seungjai Min&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Proximal Policy Optimization's Heavy-tailed Gradients. (arXiv:2102.10264v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10264</id>
        <link href="http://arxiv.org/abs/2102.10264"/>
        <updated>2021-07-14T01:41:51.586Z</updated>
        <summary type="html"><![CDATA[Modern policy gradient algorithms such as Proximal Policy Optimization (PPO)
rely on an arsenal of heuristics, including loss clipping and gradient
clipping, to ensure successful learning. These heuristics are reminiscent of
techniques from robust statistics, commonly used for estimation in outlier-rich
(``heavy-tailed'') regimes. In this paper, we present a detailed empirical
study to characterize the heavy-tailed nature of the gradients of the PPO
surrogate reward function. We demonstrate that the gradients, especially for
the actor network, exhibit pronounced heavy-tailedness and that it increases as
the agent's policy diverges from the behavioral policy (i.e., as the agent goes
further off policy). Further examination implicates the likelihood ratios and
advantages in the surrogate reward as the main sources of the observed
heavy-tailedness. We then highlight issues arising due to the heavy-tailed
nature of the gradients. In this light, we study the effects of the standard
PPO clipping heuristics, demonstrating that these tricks primarily serve to
offset heavy-tailedness in gradients. Thus motivated, we propose incorporating
GMOM, a high-dimensional robust estimator, into PPO as a substitute for three
clipping tricks. Despite requiring less hyperparameter tuning, our method
matches the performance of PPO (with all heuristics enabled) on a battery of
MuJoCo continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Saurabh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhanson_J/0/1/0/all/0/1"&gt;Joshua Zhanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisotto_E/0/1/0/all/0/1"&gt;Emilio Parisotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1"&gt;Adarsh Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_S/0/1/0/all/0/1"&gt;Sivaraman Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Regrets for Learning the Prior in Bandits. (arXiv:2107.06196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06196</id>
        <link href="http://arxiv.org/abs/2107.06196"/>
        <updated>2021-07-14T01:41:51.568Z</updated>
        <summary type="html"><![CDATA[We propose ${\tt AdaTS}$, a Thompson sampling algorithm that adapts
sequentially to bandit tasks that it interacts with. The key idea in ${\tt
AdaTS}$ is to adapt to an unknown task prior distribution by maintaining a
distribution over its parameters. When solving a bandit task, that uncertainty
is marginalized out and properly accounted for. ${\tt AdaTS}$ is a
fully-Bayesian algorithm that can be implemented efficiently in several classes
of bandit problems. We derive upper bounds on its Bayes regret that quantify
the loss due to not knowing the task prior, and show that it is small. Our
theory is supported by experiments, where ${\tt AdaTS}$ outperforms prior
algorithms and works well even in challenging real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1"&gt;Soumya Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Platform Design Problem. (arXiv:2009.06117v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06117</id>
        <link href="http://arxiv.org/abs/2009.06117"/>
        <updated>2021-07-14T01:41:51.560Z</updated>
        <summary type="html"><![CDATA[On-line firms deploy suites of software platforms, where each platform is
designed to interact with users during a certain activity, such as browsing,
chatting, socializing, emailing, driving, etc. The economic and incentive
structure of this exchange, as well as its algorithmic nature, have not been
explored to our knowledge. We model this interaction as a Stackelberg game
between a Designer and one or more Agents. We model an Agent as a Markov chain
whose states are activities; we assume that the Agent's utility is a linear
function of the steady-state distribution of this chain. The Designer may
design a platform for each of these activities/states; if a platform is adopted
by the Agent, the transition probabilities of the Markov chain are affected,
and so is the objective of the Agent. The Designer's utility is a linear
function of the steady state probabilities of the accessible states minus the
development cost of the platforms. The underlying optimization problem of the
Agent -- how to choose the states for which to adopt the platform -- is an MDP.
If this MDP has a simple yet plausible structure (the transition probabilities
from one state to another only depend on the target state and the recurrent
probability of the current state) the Agent's problem can be solved by a greedy
algorithm. The Designer's optimization problem (designing a custom suite for
the Agent so as to optimize, through the Agent's optimum reaction, the
Designer's revenue), is NP-hard to approximate within any finite ratio;
however, the special case, while still NP-hard, has an FPTAS. These results
generalize from a single Agent to a distribution of Agents with finite support,
as well as to the setting where the Designer must find the best response to the
existing strategies of other Designers. We discuss other implications of our
results and directions of future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1"&gt;Christos Papadimitriou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1"&gt;Kiran Vodrahalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakakis_M/0/1/0/all/0/1"&gt;Mihalis Yannakakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00387</id>
        <link href="http://arxiv.org/abs/2101.00387"/>
        <updated>2021-07-14T01:41:51.552Z</updated>
        <summary type="html"><![CDATA[In recent times, BERT based transformer models have become an inseparable
part of the 'tech stack' of text processing models. Similar progress is being
observed in the speech domain with a multitude of models observing
state-of-the-art results by using audio transformer models to encode speech.
This begs the question of what are these audio transformer models learning.
Moreover, although the standard methodology is to choose the last layer
embedding for any downstream task, but is it the optimal choice? We try to
answer these questions for the two recent audio transformer models, Mockingjay
and wave2vec2.0. We compare them on a comprehensive set of language delivery
and structure features including audio, fluency and pronunciation features.
Additionally, we probe the audio models' understanding of textual surface,
syntax, and semantic features and compare them to BERT. We do this over
exhaustive settings for native, non-native, synthetic, read and spontaneous
speech datasets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Jui Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1"&gt;Yaman Kumar Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rajiv Ratn Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Magnetic Resonance Imaging data augmentation through conditional ICA. (arXiv:2107.06104v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06104</id>
        <link href="http://arxiv.org/abs/2107.06104"/>
        <updated>2021-07-14T01:41:51.545Z</updated>
        <summary type="html"><![CDATA[Advances in computational cognitive neuroimaging research are related to the
availability of large amounts of labeled brain imaging data, but such data are
scarce and expensive to generate. While powerful data generation mechanisms,
such as Generative Adversarial Networks (GANs), have been designed in the last
decade for computer vision, such improvements have not yet carried over to
brain imaging. A likely reason is that GANs training is ill-suited to the
noisy, high-dimensional and small-sample data available in functional
neuroimaging.In this paper, we introduce Conditional Independent Components
Analysis (Conditional ICA): a fast functional Magnetic Resonance Imaging (fMRI)
data augmentation technique, that leverages abundant resting-state data to
create images by sampling from an ICA decomposition. We then propose a
mechanism to condition the generator on classes observed with few samples. We
first show that the generative mechanism is successful at synthesizing data
indistinguishable from observations, and that it yields gains in classification
accuracy in brain decoding problems. In particular it outperforms GANs while
being much easier to optimize and interpret. Lastly, Conditional ICA enhances
classification accuracy in eight datasets without further parameters tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tajini_B/0/1/0/all/0/1"&gt;Badr Tajini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richard_H/0/1/0/all/0/1"&gt;Hugo Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thirion_B/0/1/0/all/0/1"&gt;Bertrand Thirion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Better Generalization Bounds with Locally Elastic Stability. (arXiv:2010.13988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13988</id>
        <link href="http://arxiv.org/abs/2010.13988"/>
        <updated>2021-07-14T01:41:51.538Z</updated>
        <summary type="html"><![CDATA[Algorithmic stability is a key characteristic to ensure the generalization
ability of a learning algorithm. Among different notions of stability,
\emph{uniform stability} is arguably the most popular one, which yields
exponential generalization bounds. However, uniform stability only considers
the worst-case loss change (or so-called sensitivity) by removing a single data
point, which is distribution-independent and therefore undesirable. There are
many cases that the worst-case sensitivity of the loss is much larger than the
average sensitivity taken over the single data point that is removed,
especially in some advanced models such as random feature models or neural
networks. Many previous works try to mitigate the distribution independent
issue by proposing weaker notions of stability, however, they either only yield
polynomial bounds or the bounds derived do not vanish as sample size goes to
infinity. Given that, we propose \emph{locally elastic stability} as a weaker
and distribution-dependent stability notion, which still yields exponential
generalization bounds. We further demonstrate that locally elastic stability
implies tighter generalization bounds than those derived based on uniform
stability in many situations by revisiting the examples of bounded support
vector machines, regularized least square regressions, and stochastic gradient
descent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hangfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03097</id>
        <link href="http://arxiv.org/abs/2103.03097"/>
        <updated>2021-07-14T01:41:51.520Z</updated>
        <summary type="html"><![CDATA[Machine learning systems generally assume that the training and testing
distributions are the same. To this end, a key requirement is to develop models
that can generalize to unseen distributions. Domain generalization (DG), i.e.,
out-of-distribution generalization, has attracted increasing interests in
recent years. Domain generalization deals with a challenging setting where one
or several different but related domain(s) are given, and the goal is to learn
a model that can generalize to an unseen test domain. Great progress has been
made in the area of domain generalization for years. This paper presents the
first review of recent advances in this area. First, we provide a formal
definition of domain generalization and discuss several related fields. We then
thoroughly review the theories related to domain generalization and carefully
analyze the theory behind generalization. We categorize recent algorithms into
three classes: data manipulation, representation learning, and learning
strategy, and present several popular algorithms in detail for each category.
Third, we introduce the commonly used datasets and applications. Finally, we
summarize existing literature and present some potential research topics for
the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1"&gt;Yidong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Batching for Gaussian Process Surrogates with Application in Noisy Level Set Estimation. (arXiv:2003.08579v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08579</id>
        <link href="http://arxiv.org/abs/2003.08579"/>
        <updated>2021-07-14T01:41:51.513Z</updated>
        <summary type="html"><![CDATA[We develop adaptive replicated designs for Gaussian process metamodels of
stochastic experiments. Adaptive batching is a natural extension of sequential
design heuristics with the benefit of replication growing as response features
are learned, inputs concentrate, and the metamodeling overhead rises. Motivated
by the problem of learning the level set of the mean simulator response we
develop four novel schemes: Multi-Level Batching (MLB), Ratchet Batching (RB),
Adaptive Batched Stepwise Uncertainty Reduction (ABSUR), Adaptive Design with
Stepwise Allocation (ADSA) and Deterministic Design with Stepwise Allocation
(DDSA). Our algorithms simultaneously (MLB, RB and ABSUR) or sequentially (ADSA
and DDSA) determine the sequential design inputs and the respective number of
replicates. Illustrations using synthetic examples and an application in
quantitative finance (Bermudan option pricing via Regression Monte Carlo) show
that adaptive batching brings significant computational speed-ups with minimal
loss of modeling fidelity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lyu_X/0/1/0/all/0/1"&gt;Xiong Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ludkovski_M/0/1/0/all/0/1"&gt;Mike Ludkovski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06209</id>
        <link href="http://arxiv.org/abs/2107.06209"/>
        <updated>2021-07-14T01:41:51.484Z</updated>
        <summary type="html"><![CDATA[Discriminative features play an important role in image and object
classification and also in other fields of research such as semi-supervised
learning, fine-grained classification, out of distribution detection. Inspired
by Linear Discriminant Analysis (LDA), we propose an optimization called Neural
Discriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA
transforms deep features to become more discriminative and, therefore, improves
the performances in various tasks. Our proposed optimization has two primary
goals for inter- and intra-class variances. The first one is to minimize
variances within each individual class. The second goal is to maximize pairwise
distances between features coming from different classes. We evaluate our NDA
optimization in different research fields: general supervised classification,
fine-grained classification, semi-supervised learning, and out of distribution
detection. We achieve performance improvements in all the fields compared to
baseline methods that do not use NDA. Besides, using NDA, we also surpass the
state of the art on the four tasks on various testing datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1"&gt;Mai Lan Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1"&gt;Emanuel Aldea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1"&gt;Volker Blanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oversampling Divide-and-conquer for Response-skewed Kernel Ridge Regression. (arXiv:2107.05834v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05834</id>
        <link href="http://arxiv.org/abs/2107.05834"/>
        <updated>2021-07-14T01:41:51.474Z</updated>
        <summary type="html"><![CDATA[The divide-and-conquer method has been widely used for estimating large-scale
kernel ridge regression estimates. Unfortunately, when the response variable is
highly skewed, the divide-and-conquer kernel ridge regression (dacKRR) may
overlook the underrepresented region and result in unacceptable results. We
develop a novel response-adaptive partition strategy to overcome the
limitation. In particular, we propose to allocate the replicates of some
carefully identified informative observations to multiple nodes (local
processors). The idea is analogous to the popular oversampling technique.
Although such a technique has been widely used for addressing discrete label
skewness, extending it to the dacKRR setting is nontrivial. We provide both
theoretical and practical guidance on how to effectively over-sample the
observations under the dacKRR setting. Furthermore, we show the proposed
estimate has a smaller asymptotic mean squared error (AMSE) than that of the
classical dacKRR estimate under mild conditions. Our theoretical findings are
supported by both simulated and real-data analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaoxiao Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust and Reliable Algorithmic Recourse. (arXiv:2102.13620v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13620</id>
        <link href="http://arxiv.org/abs/2102.13620"/>
        <updated>2021-07-14T01:41:51.451Z</updated>
        <summary type="html"><![CDATA[As predictive models are increasingly being deployed in high-stakes decision
making (e.g., loan approvals), there has been growing interest in post hoc
techniques which provide recourse to affected individuals. These techniques
generate recourses under the assumption that the underlying predictive model
does not change. However, in practice, models are often regularly updated for a
variety of reasons (e.g., dataset shifts), thereby rendering previously
prescribed recourses ineffective. To address this problem, we propose a novel
framework, RObust Algorithmic Recourse (ROAR), that leverages adversarial
training for finding recourses that are robust to model shifts. To the best of
our knowledge, this work proposes the first solution to this critical problem.
We also carry out detailed theoretical analysis which underscores the
importance of constructing recourses that are robust to model shifts: 1) we
derive a lower bound on the probability of invalidation of recourses generated
by existing approaches which are not robust to model shifts. 2) we prove that
the additional cost incurred due to the robust recourses output by our
framework is bounded. Experimental evaluation on multiple synthetic and
real-world datasets demonstrates the efficacy of the proposed framework and
supports our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shalmali Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conservative Offline Distributional Reinforcement Learning. (arXiv:2107.06106v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06106</id>
        <link href="http://arxiv.org/abs/2107.06106"/>
        <updated>2021-07-14T01:41:51.371Z</updated>
        <summary type="html"><![CDATA[Many reinforcement learning (RL) problems in practice are offline, learning
purely from observational data. A key challenge is how to ensure the learned
policy is safe, which requires quantifying the risk associated with different
actions. In the online setting, distributional RL algorithms do so by learning
the distribution over returns (i.e., cumulative rewards) instead of the
expected return; beyond quantifying risk, they have also been shown to learn
better representations for planning. We propose Conservative Offline
Distributional Actor Critic (CODAC), an offline RL algorithm suitable for both
risk-neutral and risk-averse domains. CODAC adapts distributional RL to the
offline setting by penalizing the predicted quantiles of the return for
out-of-distribution actions. We prove that CODAC learns a conservative return
distribution -- in particular, for finite MDPs, CODAC converges to an uniform
lower bound on the quantiles of the return distribution; our proof relies on a
novel analysis of the distributional Bellman operator. In our experiments, on
two challenging robot navigation tasks, CODAC successfully learns risk-averse
policies using offline data collected purely from risk-neutral agents.
Furthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of
both expected and risk-sensitive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yecheng Jason Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayaraman_D/0/1/0/all/0/1"&gt;Dinesh Jayaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1"&gt;Osbert Bastani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes on the Behavior of MC Dropout. (arXiv:2008.02627v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02627</id>
        <link href="http://arxiv.org/abs/2008.02627"/>
        <updated>2021-07-14T01:41:51.354Z</updated>
        <summary type="html"><![CDATA[Among the various options to estimate uncertainty in deep neural networks,
Monte-Carlo dropout is widely popular for its simplicity and effectiveness.
However the quality of the uncertainty estimated through this method varies and
choices in architecture design and in training procedures have to be carefully
considered and tested to obtain satisfactory results. In this paper we present
a study offering a different point of view on the behavior of Monte-Carlo
dropout, which enables us to observe a few interesting properties of the
technique to keep in mind when considering its use for uncertainty estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verdoja_F/0/1/0/all/0/1"&gt;Francesco Verdoja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1"&gt;Ville Kyrki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-07-14T01:41:51.347Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defence against adversarial attacks using classical and quantum-enhanced Boltzmann machines. (arXiv:2012.11619v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11619</id>
        <link href="http://arxiv.org/abs/2012.11619"/>
        <updated>2021-07-14T01:41:51.338Z</updated>
        <summary type="html"><![CDATA[We provide a robust defence to adversarial attacks on discriminative
algorithms. Neural networks are naturally vulnerable to small, tailored
perturbations in the input data that lead to wrong predictions. On the
contrary, generative models attempt to learn the distribution underlying a
dataset, making them inherently more robust to small perturbations. We use
Boltzmann machines for discrimination purposes as attack-resistant classifiers,
and compare them against standard state-of-the-art adversarial defences. We
find improvements ranging from 5% to 72% against attacks with Boltzmann
machines on the MNIST dataset. We furthermore complement the training with
quantum-enhanced sampling from the D-Wave 2000Q annealer, finding results
comparable with classical techniques and with marginal improvements in some
cases. These results underline the relevance of probabilistic methods in
constructing neural networks and highlight a novel scenario of practical
relevance where quantum computers, even with limited hardware capabilites,
could provide advantages over classical computers. This work is dedicated to
the memory of Peter Wittek.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kehoe_A/0/1/0/all/0/1"&gt;Aidan Kehoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wittek_P/0/1/0/all/0/1"&gt;Peter Wittek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yanbo Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1"&gt;Alejandro Pozas-Kerstjens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12970</id>
        <link href="http://arxiv.org/abs/2010.12970"/>
        <updated>2021-07-14T01:41:51.321Z</updated>
        <summary type="html"><![CDATA[Denoising is a fundamental challenge in scientific imaging. Deep
convolutional neural networks (CNNs) provide the current state of the art in
denoising natural images, where they produce impressive results. However, their
potential has barely been explored in the context of scientific imaging.
Denoising CNNs are typically trained on real natural images artificially
corrupted with simulated noise. In contrast, in scientific applications,
noiseless ground-truth images are usually not available. To address this issue,
we propose a simulation-based denoising (SBD) framework, in which CNNs are
trained on simulated images. We test the framework on data obtained from
transmission electron microscopy (TEM), an imaging technique with widespread
applications in material science, biology, and medicine. SBD outperforms
existing techniques by a wide margin on a simulated benchmark dataset, as well
as on real data. Apart from the denoised images, SBD generates likelihood maps
to visualize the agreement between the structure of the denoised image and the
observed data. Our results reveal shortcomings of state-of-the-art denoising
architectures, such as their small field-of-view: substantially increasing the
field-of-view of the CNNs allows them to exploit non-local periodic patterns in
the data, which is crucial at high noise levels. In addition, we analyze the
generalization capability of SBD, demonstrating that the trained networks are
robust to variations of imaging parameters and of the underlying signal
structure. Finally, we release the first publicly available benchmark dataset
of TEM images, containing 18,000 examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1"&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1"&gt;Ramon Manzorro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1"&gt;Joshua L. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Binh Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1"&gt;Dev Yashpal Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1"&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1"&gt;David S. Matteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1"&gt;Peter A. Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameterization of Forced Isotropic Turbulent Flow using Autoencoders and Generative Adversarial Networks. (arXiv:2107.06264v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.06264</id>
        <link href="http://arxiv.org/abs/2107.06264"/>
        <updated>2021-07-14T01:41:51.313Z</updated>
        <summary type="html"><![CDATA[Autoencoders and generative neural network models have recently gained
popularity in fluid mechanics due to their spontaneity and low processing time
instead of high fidelity CFD simulations. Auto encoders are used as model order
reduction tools in applications of fluid mechanics by compressing input
high-dimensional data using an encoder to map the input space into a
lower-dimensional latent space. Whereas, generative models such as Variational
Auto-encoders (VAEs) and Generative Adversarial Networks (GANs) are proving to
be effective in generating solutions to chaotic models with high 'randomness'
such as turbulent flows. In this study, forced isotropic turbulence flow is
generated by parameterizing into some basic statistical characteristics. The
models trained on pre-simulated data from dependencies on these characteristics
and the flow generation is then affected by varying these parameters. The
latent vectors pushed along the generator models like the decoders and
generators contain independent entries which can be used to create different
outputs with similar properties. The use of neural network-based architecture
removes the need for dependency on the classical mesh-based Navier-Stoke
equation estimation which is prominent in many CFD softwares.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Kanishk/0/1/0/all/0/1"&gt;Kanishk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nandal_T/0/1/0/all/0/1"&gt;Tanishk Nandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tyagi_P/0/1/0/all/0/1"&gt;Prince Tyagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Singh_R/0/1/0/all/0/1"&gt;Raj Kumar Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-14T01:41:51.306Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predictive models for wind speed using artificial intelligence and copula. (arXiv:2107.06182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06182</id>
        <link href="http://arxiv.org/abs/2107.06182"/>
        <updated>2021-07-14T01:41:51.299Z</updated>
        <summary type="html"><![CDATA[Electricity generation from burning fossil fuels is one of the major
contributors to global warming. Renewable energy sources are a viable
alternative to produce electrical energy and to reduce the emission from the
power industry. These energy sources are the building blocks of green energy,
which all have different characteristics. Their availabilities are also
diverse, depending on geographical locations and other parameters. Low
implementation cost and distributed availability all over the world uplifts
their popularity exponentially. Therefore, it has unlocked opportunities for
consumers to produce electricity locally and use it on-site, which reduces
dependency on centralized utility companies. The research considers two main
objectives: the prediction of wind speed that simplifies wind farm planning and
feasibility study. Secondly, the need to understand the dependency structure of
the wind speeds of multiple distant locations. To address the first objective,
twelve artificial intelligence algorithms were used for wind speed prediction
from collected meteorological parameters. The model performances were compared
to determine the wind speed prediction accuracy. The results show a deep
learning approach, long short-term memory (LSTM) outperforms other models with
the highest accuracy of 97.8%. For dependency, a multivariate cumulative
distribution function, Copula, was used to find the joint distribution of two
or more distant location wind speeds, followed by a case study. We found that
the appropriate copula family and the parameters vary based on the distance in
between. For the case study, Joe-Frank (BB8) copula shows an efficient joint
distribution fit for a wind speed pair with a standard error of 0.0094.
Finally, some insights about the uncertainty aspects of wind speed dependency
were addressed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ehsan_M/0/1/0/all/0/1"&gt;Md Amimul Ehsan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Low-Rank Neural Network Compression. (arXiv:2107.05787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05787</id>
        <link href="http://arxiv.org/abs/2107.05787"/>
        <updated>2021-07-14T01:41:51.293Z</updated>
        <summary type="html"><![CDATA[Despite many modern applications of Deep Neural Networks (DNNs), the large
number of parameters in the hidden layers makes them unattractive for
deployment on devices with storage capacity constraints. In this paper we
propose a Data-Driven Low-rank (DDLR) method to reduce the number of parameters
of pretrained DNNs and expedite inference by imposing low-rank structure on the
fully connected layers, while controlling for the overall accuracy and without
requiring any retraining. We pose the problem as finding the lowest rank
approximation of each fully connected layer with given performance guarantees
and relax it to a tractable convex optimization problem. We show that it is
possible to significantly reduce the number of parameters in common DNN
architectures with only a small reduction in classification accuracy. We
compare DDLR with Net-Trim, which is another data-driven DNN compression
technique based on sparsity and show that DDLR consistently produces more
compressed neural networks while maintaining higher accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_D/0/1/0/all/0/1"&gt;Dimitris Papadimitriou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Swayambhoo Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Based Behavioral Representation Learning Enables Transfer Learning for Mobile Sensing in Small Datasets. (arXiv:2107.06097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06097</id>
        <link href="http://arxiv.org/abs/2107.06097"/>
        <updated>2021-07-14T01:41:51.286Z</updated>
        <summary type="html"><![CDATA[While deep learning has revolutionized research and applications in NLP and
computer vision, this has not yet been the case for behavioral modeling and
behavioral health applications. This is because the domain's datasets are
smaller, have heterogeneous datatypes, and typically exhibit a large degree of
missingness. Therefore, off-the-shelf deep learning models require significant,
often prohibitive, adaptation. Accordingly, many research applications still
rely on manually coded features with boosted tree models, sometimes with
task-specific features handcrafted by experts. Here, we address these
challenges by providing a neural architecture framework for mobile sensing data
that can learn generalizable feature representations from time series and
demonstrates the feasibility of transfer learning on small data domains through
finetuning. This architecture combines benefits from CNN and Trans-former
architectures to (1) enable better prediction performance by learning directly
from raw minute-level sensor data without the need for handcrafted features by
up to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models
and boosted decision trees with data from as few a dozen participants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_M/0/1/0/all/0/1"&gt;Mike A. Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1"&gt;Tim Althoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest Feature Importance. (arXiv:2007.11117v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11117</id>
        <link href="http://arxiv.org/abs/2007.11117"/>
        <updated>2021-07-14T01:41:51.280Z</updated>
        <summary type="html"><![CDATA[Anomaly Detection is an unsupervised learning task aimed at detecting
anomalous behaviours with respect to historical data. In particular,
multivariate Anomaly Detection has an important role in many applications
thanks to the capability of summarizing the status of a complex system or
observed phenomenon with a single indicator (typically called `Anomaly Score')
and thanks to the unsupervised nature of the task that does not require human
tagging. The Isolation Forest is one of the most commonly adopted algorithms in
the field of Anomaly Detection, due to its proven effectiveness and low
computational complexity. A major problem affecting Isolation Forest is
represented by the lack of interpretability, an effect of the inherent
randomness governing the splits performed by the Isolation Trees, the building
blocks of the Isolation Forest. In this paper we propose effective, yet
computationally inexpensive, methods to define feature importance scores at
both global and local level for the Isolation Forest. Moreover, we define a
procedure to perform unsupervised feature selection for Anomaly Detection
problems based on our interpretability method; such procedure also serves the
purpose of tackling the challenging task of feature importance evaluation in
unsupervised anomaly detection. We assess the performance on several synthetic
and real-world datasets, including comparisons against state-of-the-art
interpretability techniques, and make the code publicly available to enhance
reproducibility and foster research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1"&gt;Mattia Carletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1"&gt;Matteo Terzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1"&gt;Gian Antonio Susto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convex and Nonconvex Optimization Are Both Minimax-Optimal for Noisy Blind Deconvolution under Random Designs. (arXiv:2008.01724v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01724</id>
        <link href="http://arxiv.org/abs/2008.01724"/>
        <updated>2021-07-14T01:41:51.273Z</updated>
        <summary type="html"><![CDATA[We investigate the effectiveness of convex relaxation and nonconvex
optimization in solving bilinear systems of equations under two different
designs (i.e.$~$a sort of random Fourier design and Gaussian design). Despite
the wide applicability, the theoretical understanding about these two paradigms
remains largely inadequate in the presence of random noise. The current paper
makes two contributions by demonstrating that: (1) a two-stage nonconvex
algorithm attains minimax-optimal accuracy within a logarithmic number of
iterations. (2) convex relaxation also achieves minimax-optimal statistical
accuracy vis-\`a-vis random noise. Both results significantly improve upon the
state-of-the-art theoretical guarantees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jianqing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bingyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuling Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Representation Identical Privacy-Preserving Graph Neural Network via Split Learning. (arXiv:2107.05917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05917</id>
        <link href="http://arxiv.org/abs/2107.05917"/>
        <updated>2021-07-14T01:41:51.266Z</updated>
        <summary type="html"><![CDATA[In recent years, the fast rise in number of studies on graph neural network
(GNN) has put it from the theories research to reality application stage.
Despite the encouraging performance achieved by GNN, less attention has been
paid to the privacy-preserving training and inference over distributed graph
data in the related literature. Due to the particularity of graph structure, it
is challenging to extend the existing private learning framework to GNN.
Motivated by the idea of split learning, we propose a \textbf{S}erver
\textbf{A}ided \textbf{P}rivacy-preserving \textbf{GNN} (SAPGNN) for the node
level task on horizontally partitioned cross-silo scenario. It offers a natural
extension of centralized GNN to isolated graph with max/min pooling
aggregation, while guaranteeing that all the private data involved in
computation still stays at local data holders. To further enhancing the data
privacy, a secure pooling aggregation mechanism is proposed. Theoretical and
experimental results show that the proposed model achieves the same accuracy as
the one learned over the combined data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1"&gt;Chuanqiang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_H/0/1/0/all/0/1"&gt;Huiyun Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pessimistic Model-based Offline RL: PAC Bounds and Posterior Sampling under Partial Coverage. (arXiv:2107.06226v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06226</id>
        <link href="http://arxiv.org/abs/2107.06226"/>
        <updated>2021-07-14T01:41:51.239Z</updated>
        <summary type="html"><![CDATA[We study model-based offline Reinforcement Learning with general function
approximation. We present an algorithm named Constrained Pessimistic Policy
Optimization (CPPO) which leverages a general function class and uses a
constraint to encode pessimism. Under the assumption that the ground truth
model belongs to our function class, CPPO can learn with the offline data only
providing partial coverage, i.e., it can learn a policy that completes against
any policy that is covered by the offline data, in polynomial sample complexity
with respect to the statistical complexity of the function class. We then
demonstrate that this algorithmic framework can be applied to many specialized
Markov Decision Processes where the additional structural assumptions can
further refine the concept of partial coverage. One notable example is low-rank
MDP with representation learning where the partial coverage is defined using
the concept of relative condition number measured by the underlying unknown
ground truth feature representation. Finally, we introduce and study the
Bayesian setting in offline RL. The key benefit of Bayesian offline RL is that
algorithmically, we do not need to explicitly construct pessimism or reward
penalty which could be hard beyond models with linear structures. We present a
posterior sampling-based incremental policy optimization algorithm (PS-PO)
which proceeds by iteratively sampling a model from the posterior distribution
and performing one-step incremental policy optimization inside the sampled
model. Theoretically, in expectation with respect to the prior distribution,
PS-PO can learn a near optimal policy under partial coverage with polynomial
sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1"&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical and Configurable Network Traffic Classification Using Probabilistic Machine Learning. (arXiv:2107.06080v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06080</id>
        <link href="http://arxiv.org/abs/2107.06080"/>
        <updated>2021-07-14T01:41:51.227Z</updated>
        <summary type="html"><![CDATA[Network traffic classification that is widely applicable and highly accurate
is valuable for many network security and management tasks. A flexible and
easily configurable classification framework is ideal, as it can be customized
for use in a wide variety of networks. In this paper, we propose a highly
configurable and flexible machine learning traffic classification method that
relies only on statistics of sequences of packets to distinguish known, or
approved, traffic from unknown traffic. Our method is based on likelihood
estimation, provides a measure of certainty for classification decisions, and
can classify traffic at adjustable certainty levels. Our classification method
can also be applied in different classification scenarios, each prioritizing a
different classification goal. We demonstrate how our classification scheme and
all its configurations perform well on real-world traffic from a high
performance computing network environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiahui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breen_J/0/1/0/all/0/1"&gt;Joe Breen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1"&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merwe_J/0/1/0/all/0/1"&gt;Jacobus Van der Merwe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein GAN: Deep Generation applied on Bitcoins financial time series. (arXiv:2107.06008v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.06008</id>
        <link href="http://arxiv.org/abs/2107.06008"/>
        <updated>2021-07-14T01:41:51.219Z</updated>
        <summary type="html"><![CDATA[Modeling financial time series is challenging due to their high volatility
and unexpected happenings on the market. Most financial models and algorithms
trying to fill the lack of historical financial time series struggle to perform
and are highly vulnerable to overfitting. As an alternative, we introduce in
this paper a deep neural network called the WGAN-GP, a data-driven model that
focuses on sample generation. The WGAN-GP consists of a generator and
discriminator function which utilize an LSTM architecture. The WGAN-GP is
supposed to learn the underlying structure of the input data, which in our
case, is the Bitcoin. Bitcoin is unique in its behavior; the prices fluctuate
what makes guessing the price trend hardly impossible. Through adversarial
training, the WGAN-GP should learn the underlying structure of the bitcoin and
generate very similar samples of the bitcoin distribution. The generated
synthetic time series are visually indistinguishable from the real data. But
the numerical results show that the generated data were close to the real data
distribution but distinguishable. The model mainly shows a stable learning
behavior. However, the model has space for optimization, which could be
achieved by adjusting the hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Samuel_R/0/1/0/all/0/1"&gt;Rikli Samuel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nico_B/0/1/0/all/0/1"&gt;Bigler Daniel Nico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moritz_P/0/1/0/all/0/1"&gt;Pfenninger Moritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Joerg_O/0/1/0/all/0/1"&gt;Osterrieder Joerg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05775</id>
        <link href="http://arxiv.org/abs/2107.05775"/>
        <updated>2021-07-14T01:41:51.190Z</updated>
        <summary type="html"><![CDATA[We study the problem of novel view synthesis of a scene comprised of 3D
objects. We propose a simple yet effective approach that is neither continuous
nor implicit, challenging recent trends on view synthesis. We demonstrate that
although continuous radiance field representations have gained a lot of
attention due to their expressive power, our simple approach obtains comparable
or even better novel view reconstruction quality comparing with
state-of-the-art baselines while increasing rendering speed by over 400x. Our
model is trained in a category-agnostic manner and does not require
scene-specific optimization. Therefore, it is able to generalize novel view
synthesis to object categories not seen during training. In addition, we show
that with our simple formulation, we can use view synthesis as a
self-supervision signal for efficient learning of 3D geometry without explicit
3D supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengsheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1"&gt;Miguel Angel Bautista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1"&gt;Alex Colburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1"&gt;Daniel Ulbricht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1"&gt;Joshua M. Susskind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1"&gt;Qi Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Domain Generalization. (arXiv:2102.11436v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11436</id>
        <link href="http://arxiv.org/abs/2102.11436"/>
        <updated>2021-07-14T01:41:51.174Z</updated>
        <summary type="html"><![CDATA[Despite remarkable success in a variety of applications, it is well-known
that deep learning can fail catastrophically when presented with
out-of-distribution data. Toward addressing this challenge, we consider the
domain generalization problem, wherein predictors are trained using data drawn
from a family of related training domains and then evaluated on a distinct and
unseen test domain. We show that under a natural model of data generation and a
concomitant invariance condition, the domain generalization problem is
equivalent to an infinite-dimensional constrained statistical learning problem;
this problem forms the basis of our approach, which we call Model-Based Domain
Generalization. Due to the inherent challenges in solving constrained
optimization problems in deep learning, we exploit nonconvex duality theory to
develop unconstrained relaxations of this statistical problem with tight bounds
on the duality gap. Based on this theoretical motivation, we propose a novel
domain generalization algorithm with convergence guarantees. In our
experiments, we report improvements of up to 30 percentage points over
state-of-the-art domain generalization baselines on several benchmarks
including ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George J. Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1"&gt;Hamed Hassani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online nonparametric regression with Sobolev kernels. (arXiv:2102.03594v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03594</id>
        <link href="http://arxiv.org/abs/2102.03594"/>
        <updated>2021-07-14T01:41:51.159Z</updated>
        <summary type="html"><![CDATA[In this work we investigate the variation of the online kernelized ridge
regression algorithm in the setting of $d-$dimensional adversarial
nonparametric regression. We derive the regret upper bounds on the classes of
Sobolev spaces $W_{p}^{\beta}(\mathcal{X})$, $p\geq 2, \beta>\frac{d}{p}$. The
upper bounds are supported by the minimax regret analysis, which reveals that
in the cases $\beta> \frac{d}{2}$ or $p=\infty$ these rates are (essentially)
optimal. Finally, we compare the performance of the kernelized ridge regression
forecaster to the known non-parametric forecasters in terms of the regret rates
and their computational complexity as well as to the excess risk rates in the
setting of statistical (i.i.d.) nonparametric regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zadorozhnyi_O/0/1/0/all/0/1"&gt;Oleksandr Zadorozhnyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gaillard_P/0/1/0/all/0/1"&gt;Pierre Gaillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gerschinovitz_S/0/1/0/all/0/1"&gt;Sebastien Gerschinovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Bernstein Online Aggregation for Day-Ahead Electricity Demand Forecasting. (arXiv:2107.06268v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06268</id>
        <link href="http://arxiv.org/abs/2107.06268"/>
        <updated>2021-07-14T01:41:51.151Z</updated>
        <summary type="html"><![CDATA[We present a winning method of the IEEE DataPort Competition on Day-Ahead
Electricity Demand Forecasting: Post-COVID Paradigm. The day-ahead load
forecasting approach is based on online forecast combination of multiple point
prediction models. It contains four steps: i) data cleaning and preprocessing,
ii) a holiday adjustment procedure, iii) training of individual forecasting
models, iv) forecast combination by smoothed Bernstein Online Aggregation
(BOA). The approach is flexible and can quickly adopt to new energy system
situations as they occurred during and after COVID-19 shutdowns. The pool of
individual prediction models ranges from rather simple time series models to
sophisticated models like generalized additive models (GAMs) and
high-dimensional linear models estimated by lasso. They incorporate
autoregressive, calendar and weather effects efficiently. All steps contain
novel concepts that contribute to the excellent forecasting performance of the
proposed method. This holds particularly for the holiday adjustment procedure
and the fully adaptive smoothed BOA approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziel_F/0/1/0/all/0/1"&gt;Florian Ziel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06256</id>
        <link href="http://arxiv.org/abs/2107.06256"/>
        <updated>2021-07-14T01:41:51.144Z</updated>
        <summary type="html"><![CDATA[We present Retrieve in Style (RIS), an unsupervised framework for
fine-grained facial feature transfer and retrieval on real images. Recent work
shows that it is possible to learn a catalog that allows local semantic
transfers of facial features on generated images by capitalizing on the
disentanglement property of the StyleGAN latent space. RIS improves existing
art on: 1) feature disentanglement and allows for challenging transfers (i.e.,
hair and pose) that were not shown possible in SoTA methods. 2) eliminating the
need for per-image hyperparameter tuning, and for computing a catalog over a
large batch of images. 3) enabling face retrieval using the proposed facial
features (e.g., eyes), and to our best knowledge, is the first work to retrieve
face images at the fine-grained level. 4) robustness and natural application to
real images. Our qualitative and quantitative analyses show RIS achieves both
high-fidelity feature transfers and accurate fine-grained retrievals on real
images. We discuss the responsible application of RIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min Jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wen-Sheng Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhishek Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto IV: Counterfactual Prediction via Automatic Instrumental Variable Decomposition. (arXiv:2107.05884v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05884</id>
        <link href="http://arxiv.org/abs/2107.05884"/>
        <updated>2021-07-14T01:41:51.115Z</updated>
        <summary type="html"><![CDATA[Instrumental variables (IVs), sources of treatment randomization that are
conditionally independent of the outcome, play an important role in causal
inference with unobserved confounders. However, the existing IV-based
counterfactual prediction methods need well-predefined IVs, while it's an art
rather than science to find valid IVs in many real-world scenes. Moreover, the
predefined hand-made IVs could be weak or erroneous by violating the conditions
of valid IVs. These thorny facts hinder the application of the IV-based
counterfactual prediction methods. In this paper, we propose a novel Automatic
Instrumental Variable decomposition (AutoIV) algorithm to automatically
generate representations serving the role of IVs from observed variables (IV
candidates). Specifically, we let the learned IV representations satisfy the
relevance condition with the treatment and exclusion condition with the outcome
via mutual information maximization and minimization constraints, respectively.
We also learn confounder representations by encouraging them to be relevant to
both the treatment and the outcome. The IV and confounder representations
compete for the information with their constraints in an adversarial game,
which allows us to get valid IV representations for IV-based counterfactual
prediction. Extensive experiments demonstrate that our method generates valid
IV representations for accurate IV-based counterfactual prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junkun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Anpeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Runze Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lanfen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Optimal Control with Affine Constraints. (arXiv:2010.04891v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04891</id>
        <link href="http://arxiv.org/abs/2010.04891"/>
        <updated>2021-07-14T01:41:51.104Z</updated>
        <summary type="html"><![CDATA[This paper considers online optimal control with affine constraints on the
states and actions under linear dynamics with bounded random disturbances. The
system dynamics and constraints are assumed to be known and time-invariant but
the convex stage cost functions change adversarially. To solve this problem, we
propose Online Gradient Descent with Buffer Zones (OGD-BZ). Theoretically, we
show that OGD-BZ with proper parameters can guarantee the system to satisfy all
the constraints despite any admissible disturbances. Further, we investigate
the policy regret of OGD-BZ, which compares OGD-BZ's performance with the
performance of the optimal linear policy in hindsight. We show that OGD-BZ can
achieve a policy regret upper bound that is the square root of the horizon
length multiplied by some logarithmic terms of the horizon length under proper
algorithm parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1"&gt;Subhro Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ML-Quest: A Game for Introducing Machine Learning Concepts to K-12 Students. (arXiv:2107.06206v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.06206</id>
        <link href="http://arxiv.org/abs/2107.06206"/>
        <updated>2021-07-14T01:41:51.097Z</updated>
        <summary type="html"><![CDATA[Today, Machine Learning (ML) is of a great importance to society due to the
availability of huge data and high computational resources. This ultimately led
to the introduction of ML concepts at multiple levels of education including
K-12 students to promote computational thinking. However, teaching these
concepts to K-12 through traditional methodologies such as video lectures and
books is challenging. Many studies in the literature have reported that using
interactive environments such as games to teach computational thinking and
programming improves retention capacity and motivation among students.
Therefore, introducing ML concepts using a game might enhance students'
understanding of the subject and motivate them to learn further. However, we
are not aware of any existing game which explicitly focuses on introducing ML
concepts to students using game play. Hence, in this paper, we propose
ML-Quest, a 3D video game to provide conceptual overview of three ML concepts:
Supervised Learning, Gradient Descent and K-Nearest Neighbor (KNN)
Classification. The crux of the game is to introduce the definition and working
of these concepts, which we call conceptual overview, in a simulated scenario
without overwhelming students with the intricacies of ML. The game has been
predominantly evaluated for its usefulness and player experience using the
Technology Acceptance Model (TAM) model with the help of 23 higher-secondary
school students. The survey result shows that around 70% of the participants
either agree or strongly agree that the ML-Quest is quite interactive and
useful in introducing them to ML concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Priya_S/0/1/0/all/0/1"&gt;Shruti Priya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhadra_S/0/1/0/all/0/1"&gt;Shubhankar Bhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chimalakonda_S/0/1/0/all/0/1"&gt;Sridhar Chimalakonda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Doubly Reparameterized Gradient Estimators. (arXiv:2101.11046v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11046</id>
        <link href="http://arxiv.org/abs/2101.11046"/>
        <updated>2021-07-14T01:41:51.090Z</updated>
        <summary type="html"><![CDATA[Efficient low-variance gradient estimation enabled by the reparameterization
trick (RT) has been essential to the success of variational autoencoders.
Doubly-reparameterized gradients (DReGs) improve on the RT for multi-sample
variational bounds by applying reparameterization a second time for an
additional reduction in variance. Here, we develop two generalizations of the
DReGs estimator and show that they can be used to train conditional and
hierarchical VAEs on image modelling tasks more effectively. First, we extend
the estimator to hierarchical models with several stochastic layers by showing
how to treat additional score function terms due to the hierarchical
variational posterior. We then generalize DReGs to score functions of arbitrary
distributions instead of just those of the sampling distribution, which makes
the estimator applicable to the parameters of the prior in addition to those of
the posterior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Matthias Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mnih_A/0/1/0/all/0/1"&gt;Andriy Mnih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[National-scale electricity peak load forecasting: Traditional, machine learning, or hybrid model?. (arXiv:2107.06174v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06174</id>
        <link href="http://arxiv.org/abs/2107.06174"/>
        <updated>2021-07-14T01:41:51.082Z</updated>
        <summary type="html"><![CDATA[As the volatility of electricity demand increases owing to climate change and
electrification, the importance of accurate peak load forecasting is
increasing. Traditional peak load forecasting has been conducted through time
series-based models; however, recently, new models based on machine or deep
learning are being introduced. This study performs a comparative analysis to
determine the most accurate peak load-forecasting model for Korea, by comparing
the performance of time series, machine learning, and hybrid models. Seasonal
autoregressive integrated moving average with exogenous variables (SARIMAX) is
used for the time series model. Artificial neural network (ANN), support vector
regression (SVR), and long short-term memory (LSTM) are used for the machine
learning models. SARIMAX-ANN, SARIMAX-SVR, and SARIMAX-LSTM are used for the
hybrid models. The results indicate that the hybrid models exhibit significant
improvement over the SARIMAX model. The LSTM-based models outperformed the
others; the single and hybrid LSTM models did not exhibit a significant
performance difference. In the case of Korea's highest peak load in 2019, the
predictive power of the LSTM model proved to be greater than that of the
SARIMAX-LSTM model. The LSTM, SARIMAX-SVR, and SARIMAX-LSTM models outperformed
the current time series-based forecasting model used in Korea. Thus, Korea's
peak load-forecasting performance can be improved by including machine learning
or hybrid models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Juyong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Youngsang Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning in Multi-Agent Reinforcement Learning with Double Q-Networks for Distributed Resource Sharing in V2X Communication. (arXiv:2107.06195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06195</id>
        <link href="http://arxiv.org/abs/2107.06195"/>
        <updated>2021-07-14T01:41:51.075Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of decentralized spectrum sharing in
vehicle-to-everything (V2X) communication networks. The aim is to provide
resource-efficient coexistence of vehicle-to-infrastructure(V2I) and
vehicle-to-vehicle(V2V) links. A recent work on the topic proposes a
multi-agent reinforcement learning (MARL) approach based on deep Q-learning,
which leverages a fingerprint-based deep Q-network (DQN) architecture. This
work considers an extension of this framework by combining Double Q-learning
(via Double DQN) and transfer learning. The motivation behind is that Double
Q-learning can alleviate the problem of overestimation of the action values
present in conventional Q-learning, while transfer learning can leverage
knowledge acquired by an expert model to accelerate learning in the MARL
setting. The proposed algorithm is evaluated in a realistic V2X setting, with
synthetic data generated based on a geometry-based propagation model that
incorporates location-specific geographical descriptors of the simulated
environment(outlines of buildings, foliage, and vehicles). The advantages of
the proposed approach are demonstrated via numerical simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_H/0/1/0/all/0/1"&gt;Hammad Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Utkovski_Z/0/1/0/all/0/1"&gt;Zoran Utkovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasparick_M/0/1/0/all/0/1"&gt;Martin Kasparick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanczak_S/0/1/0/all/0/1"&gt;Slawomir Stanczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06219</id>
        <link href="http://arxiv.org/abs/2107.06219"/>
        <updated>2021-07-14T01:41:51.056Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) aims to help models trained on a set of source
domains generalize better on unseen target domains. The performances of current
DG methods largely rely on sufficient labeled data, which however are usually
costly or unavailable. While unlabeled data are far more accessible, we seek to
explore how unsupervised learning can help deep models generalizes across
domains. Specifically, we study a novel generalization problem called
unsupervised domain generalization, which aims to learn generalizable models
with unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised
Learning (DIUL) method to cope with the significant and misleading
heterogeneity within unlabeled data and severe distribution shifts between
source and target data. Surprisingly we observe that DIUL can not only
counterbalance the scarcity of labeled data but also further strengthen the
generalization ability of models when the labeled data are sufficient. As a
pretraining approach, DIUL shows superior to ImageNet pretraining protocol even
when the available data are unlabeled and of a greatly smaller amount compared
to ImageNet. Extensive experiments clearly demonstrate the effectiveness of our
method compared with state-of-the-art unsupervised learning counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linjun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haoxin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06187</id>
        <link href="http://arxiv.org/abs/2107.06187"/>
        <updated>2021-07-14T01:41:51.047Z</updated>
        <summary type="html"><![CDATA[We propose a simple modification from a fixed margin triplet loss to an
adaptive margin triplet loss. While the original triplet loss is used widely in
classification problems such as face recognition, face re-identification and
fine-grained similarity, our proposed loss is well suited for rating datasets
in which the ratings are continuous values. In contrast to original triplet
loss where we have to sample data carefully, in out method, we can generate
triplets using the whole dataset, and the optimization can still converge
without frequently running into a model collapsing issue. The adaptive margins
only need to be computed once before the training, which is much less expensive
than generating triplets after every epoch as in the fixed margin case. Besides
substantially improved training stability (the proposed model never collapsed
in our experiments compared to a couple of times that the training collapsed on
existing triplet loss), we achieved slightly better performance than the
original triplet loss on various rating datasets and network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1"&gt;Mai Lan Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1"&gt;Volker Blanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06050</id>
        <link href="http://arxiv.org/abs/2107.06050"/>
        <updated>2021-07-14T01:41:51.041Z</updated>
        <summary type="html"><![CDATA[Empirical works suggest that various semantics emerge in the latent space of
Generative Adversarial Networks (GANs) when being trained to generate images.
To perform real image editing, it requires an accurate mapping from the real
image to the latent space to leveraging these learned semantics, which is
important yet difficult. An in-domain GAN inversion approach is recently
proposed to constraint the inverted code within the latent space by forcing the
reconstructed image obtained from the inverted code within the real image
space. Empirically, we find that the inverted code by the in-domain GAN can
deviate from the latent space significantly. To solve this problem, we propose
a force-in-domain GAN based on the in-domain GAN, which utilizes a
discriminator to force the inverted code within the latent space. The
force-in-domain GAN can also be interpreted by a cycle-GAN with slight
modification. Extensive experiments show that our force-in-domain GAN not only
reconstructs the target image at the pixel level, but also align the inverted
code with the latent space well for semantic editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1"&gt;Guangjie Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yeku Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Carle's Game: An Open-Ended Challenge in Exploratory Machine Creativity. (arXiv:2107.05786v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.05786</id>
        <link href="http://arxiv.org/abs/2107.05786"/>
        <updated>2021-07-14T01:41:51.034Z</updated>
        <summary type="html"><![CDATA[This paper is both an introduction and an invitation. It is an introduction
to CARLE, a Life-like cellular automata simulator and reinforcement learning
environment. It is also an invitation to Carle's Game, a challenge in
open-ended machine exploration and creativity. Inducing machine agents to excel
at creating interesting patterns across multiple cellular automata universes is
a substantial challenge, and approaching this challenge is likely to require
contributions from the fields of artificial life, AI, machine learning, and
complexity, at multiple levels of interest. Carle's Game is based on machine
agent interaction with CARLE, a Cellular Automata Reinforcement Learning
Environment. CARLE is flexible, capable of simulating any of the 262,144
different rules defining Life-like cellular automaton universes. CARLE is also
fast and can simulate automata universes at a rate of tens of thousands of
steps per second through a combination of vectorization and GPU acceleration.
Finally, CARLE is simple. Compared to high-fidelity physics simulators and
video games designed for human players, CARLE's two-dimensional grid world
offers a discrete, deterministic, and atomic universal playground, despite its
complexity. In combination with CARLE, Carle's Game offers an initial set of
agent policies, learning and meta-learning algorithms, and reward wrappers that
can be tailored to encourage exploration or specific tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davis_Q/0/1/0/all/0/1"&gt;Q. Tyrell Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Timbre Classification of Musical Instruments with a Deep Learning Multi-Head Attention-Based Model. (arXiv:2107.06231v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.06231</id>
        <link href="http://arxiv.org/abs/2107.06231"/>
        <updated>2021-07-14T01:41:51.027Z</updated>
        <summary type="html"><![CDATA[The aim of this work is to define a model based on deep learning that is able
to identify different instrument timbres with as few parameters as possible.
For this purpose, we have worked with classical orchestral instruments played
with different dynamics, which are part of a few instrument families and which
play notes in the same pitch range. It has been possible to assess the ability
to classify instruments by timbre even if the instruments are playing the same
note with the same intensity. The network employed uses a multi-head attention
mechanism, with 8 heads and a dense network at the output taking as input the
log-mel magnitude spectrograms of the sound samples. This network allows the
identification of 20 instrument classes of the classical orchestra, achieving
an overall F$_1$ value of 0.62. An analysis of the weights of the attention
layer has been performed and the confusion matrix of the model is presented,
allowing us to assess the ability of the proposed architecture to distinguish
timbre and to establish the aspects on which future work should focus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_C/0/1/0/all/0/1"&gt;Carlos Hernandez-Olivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1"&gt;Jose R. Beltran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What classifiers know what they don't?. (arXiv:2107.06217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06217</id>
        <link href="http://arxiv.org/abs/2107.06217"/>
        <updated>2021-07-14T01:41:51.008Z</updated>
        <summary type="html"><![CDATA[Being uncertain when facing the unknown is key to intelligent decision
making. However, machine learning algorithms lack reliable estimates about
their predictive uncertainty. This leads to wrong and overly-confident
decisions when encountering classes unseen during training. Despite the
importance of equipping classifiers with uncertainty estimates ready for the
real world, prior work has focused on small datasets and little or no class
discrepancy between training and testing data. To close this gap, we introduce
UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty
estimates for deep image classifiers. Our benchmark provides implementations of
eight state-of-the-art algorithms, six uncertainty measures, four in-domain
metrics, three out-domain metrics, and a fully automated pipeline to train,
calibrate, ensemble, select, and evaluate models. Our test-bed is open-source
and all of our results are reproducible from a fixed commit in our repository.
Adding new datasets, algorithms, measures, or metrics is a matter of a few
lines of code-in so hoping that UIMNET becomes a stepping stone towards
realistic, rigorous, and reproducible research in uncertainty estimation. Our
results show that ensembles of ERM classifiers as well as single MIMO
classifiers are the two best alternatives currently available to measure
uncertainty about both in-domain and out-domain classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belghazi_M/0/1/0/all/0/1"&gt;Mohamed Ishmael Belghazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1"&gt;David Lopez-Paz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast-Slow Streamflow Model Using Mass-Conserving LSTM. (arXiv:2107.06057v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06057</id>
        <link href="http://arxiv.org/abs/2107.06057"/>
        <updated>2021-07-14T01:41:50.959Z</updated>
        <summary type="html"><![CDATA[Streamflow forecasting is key to effectively managing water resources and
preparing for the occurrence of natural calamities being exacerbated by climate
change. Here we use the concept of fast and slow flow components to create a
new mass-conserving Long Short-Term Memory (LSTM) neural network model. It uses
hydrometeorological time series and catchment attributes to predict daily river
discharges. Preliminary results evidence improvement in skills for different
scores compared to the recent literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quinones_M/0/1/0/all/0/1"&gt;Miguel Paredes Qui&amp;#xf1;ones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zortea_M/0/1/0/all/0/1"&gt;Maciel Zortea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_L/0/1/0/all/0/1"&gt;Leonardo S. A. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Reinforcement Learning Approach for Traffic Signal Control Optimization. (arXiv:2107.06115v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06115</id>
        <link href="http://arxiv.org/abs/2107.06115"/>
        <updated>2021-07-14T01:41:50.951Z</updated>
        <summary type="html"><![CDATA[Inefficient traffic signal control methods may cause numerous problems, such
as traffic congestion and waste of energy. Reinforcement learning (RL) is a
trending data-driven approach for adaptive traffic signal control in complex
urban traffic networks. Although the development of deep neural networks (DNN)
further enhances its learning capability, there are still some challenges in
applying deep RLs to transportation networks with multiple signalized
intersections, including non-stationarity environment, exploration-exploitation
dilemma, multi-agent training schemes, continuous action spaces, etc. In order
to address these issues, this paper first proposes a multi-agent deep
deterministic policy gradient (MADDPG) method by extending the actor-critic
policy gradient algorithms. MADDPG has a centralized learning and decentralized
execution paradigm in which critics use additional information to streamline
the training process, while actors act on their own local observations. The
model is evaluated via simulation on the Simulation of Urban MObility (SUMO)
platform. Model comparison results show the efficiency of the proposed
algorithm in controlling traffic lights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengzhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guohui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drug-Target Interaction Prediction with Graph Attention networks. (arXiv:2107.06099v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.06099</id>
        <link href="http://arxiv.org/abs/2107.06099"/>
        <updated>2021-07-14T01:41:50.945Z</updated>
        <summary type="html"><![CDATA[Motivation: Predicting Drug-Target Interaction (DTI) is a well-studied topic
in bioinformatics due to its relevance in the fields of proteomics and
pharmaceutical research. Although many machine learning methods have been
successfully applied in this task, few of them aim at leveraging the inherent
heterogeneous graph structure in the DTI network to address the challenge. For
better learning and interpreting the DTI topological structure and the
similarity, it is desirable to have methods specifically for predicting
interactions from the graph structure.

Results: We present an end-to-end framework, DTI-GAT (Drug-Target Interaction
prediction with Graph Attention networks) for DTI predictions. DTI-GAT
incorporates a deep neural network architecture that operates on
graph-structured data with the attention mechanism, which leverages both the
interaction patterns and the features of drug and protein sequences. DTI-GAT
facilitates the interpretation of the DTI topological structure by assigning
different attention weights to each node with the self-attention mechanism.
Experimental evaluations show that DTI-GAT outperforms various state-of-the-art
systems on the binary DTI prediction problem. Moreover, the independent study
results further demonstrate that our model can be generalized better than other
conventional methods.

Availability: The source code and all datasets are available at
https://github.com/Haiyang-W/DTI-GRAPH]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guangyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jyun-Yu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Choice of Hyper-parameter in Extreme Value Theory based on Machine Learning Techniques. (arXiv:2107.06074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06074</id>
        <link href="http://arxiv.org/abs/2107.06074"/>
        <updated>2021-07-14T01:41:50.938Z</updated>
        <summary type="html"><![CDATA[Extreme value theory (EVT) is a statistical tool for analysis of extreme
events. It has a strong theoretical background, however, we need to choose
hyper-parameters

to apply EVT. In recent studies of machine learning, techniques of choosing
hyper-parameters have been well-studied. In this paper, we propose a new method
of choosing hyper-parameters in EVT based on machine learning techniques. We
also experiment our method to real-world data and show good usability of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_C/0/1/0/all/0/1"&gt;Chikara Nakamura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05541</id>
        <link href="http://arxiv.org/abs/2107.05541"/>
        <updated>2021-07-14T01:41:50.921Z</updated>
        <summary type="html"><![CDATA[Chatbots are intelligent software built to be used as a replacement for human
interaction. However, existing studies typically do not provide enough support
for low-resource languages like Bangla. Moreover, due to the increasing
popularity of social media, we can also see the rise of interactions in Bangla
transliteration (mostly in English) among the native Bangla speakers. In this
paper, we propose a novel approach to build a Bangla chatbot aimed to be used
as a business assistant which can communicate in Bangla and Bangla
Transliteration in English with high confidence consistently. Since annotated
data was not available for this purpose, we had to work on the whole machine
learning life cycle (data preparation, machine learning modeling, and model
deployment) using Rasa Open Source Framework, fastText embeddings, Polyglot
embeddings, Flask, and other systems as building blocks. While working with the
skewed annotated dataset, we try out different setups and pipelines to evaluate
which works best and provide possible reasoning behind the observed results.
Finally, we present a pipeline for intent classification and entity extraction
which achieves reasonable performance (accuracy: 83.02\%, precision: 80.82\%,
recall: 83.02\%, F1-score: 80\%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahim Shahriar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1"&gt;Mueeze Al Mushabbir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1"&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;MD Abdullah Al Nasim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges. (arXiv:2107.05847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05847</id>
        <link href="http://arxiv.org/abs/2107.05847"/>
        <updated>2021-07-14T01:41:50.914Z</updated>
        <summary type="html"><![CDATA[Most machine learning algorithms are configured by one or several
hyperparameters that must be carefully chosen and often considerably impact
performance. To avoid a time consuming and unreproducible manual
trial-and-error process to find well-performing hyperparameter configurations,
various automatic hyperparameter optimization (HPO) methods, e.g., based on
resampling error estimation for supervised machine learning, can be employed.
After introducing HPO from a general perspective, this paper reviews important
HPO methods such as grid or random search, evolutionary algorithms, Bayesian
optimization, Hyperband and racing. It gives practical recommendations
regarding important choices to be made when conducting HPO, including the HPO
algorithms themselves, performance evaluation, how to combine HPO with ML
pipelines, runtime improvements, and parallelization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Binder_M/0/1/0/all/0/1"&gt;Martin Binder&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Lang_M/0/1/0/all/0/1"&gt;Michel Lang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Pielok_T/0/1/0/all/0/1"&gt;Tobias Pielok&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Richter_J/0/1/0/all/0/1"&gt;Jakob Richter&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1"&gt;Stefan Coors&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Thomas_J/0/1/0/all/0/1"&gt;Janek Thomas&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Ullmann_T/0/1/0/all/0/1"&gt;Theresa Ullmann&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/stat/1/au:+Becker_M/0/1/0/all/0/1"&gt;Marc Becker&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/stat/1/au:+Boulesteix_A/0/1/0/all/0/1"&gt;Anne-Laure Boulesteix&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/stat/1/au:+Deng_D/0/1/0/all/0/1"&gt;Difan Deng&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/stat/1/au:+Lindauer_M/0/1/0/all/0/1"&gt;Marius Lindauer&lt;/a&gt; (3) ((1) Department of Statistics, Ludwig Maximilian University Munich, (2) Institute for Medical Information Processing, Biometry and Epidemiology, Ludwig Maximilian University Munich, (3) Institute for Information Processing, Leibniz University Hannover)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoScore-Imbalance: An interpretable machine learning tool for development of clinical scores with rare events data. (arXiv:2107.06039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06039</id>
        <link href="http://arxiv.org/abs/2107.06039"/>
        <updated>2021-07-14T01:41:50.902Z</updated>
        <summary type="html"><![CDATA[Background: Medical decision-making impacts both individual and public
health. Clinical scores are commonly used among a wide variety of
decision-making models for determining the degree of disease deterioration at
the bedside. AutoScore was proposed as a useful clinical score generator based
on machine learning and a generalized linear model. Its current framework,
however, still leaves room for improvement when addressing unbalanced data of
rare events. Methods: Using machine intelligence approaches, we developed
AutoScore-Imbalance, which comprises three components: training dataset
optimization, sample weight optimization, and adjusted AutoScore. All scoring
models were evaluated on the basis of their area under the curve (AUC) in the
receiver operating characteristic analysis and balanced accuracy (i.e., mean
value of sensitivity and specificity). By utilizing a publicly accessible
dataset from Beth Israel Deaconess Medical Center, we assessed the proposed
model and baseline approaches in the prediction of inpatient mortality.
Results: AutoScore-Imbalance outperformed baselines in terms of AUC and
balanced accuracy. The nine-variable AutoScore-Imbalance sub-model achieved the
highest AUC of 0.786 (0.732-0.839) while the eleven-variable original AutoScore
obtained an AUC of 0.723 (0.663-0.783), and the logistic regression with 21
variables obtained an AUC of 0.743 (0.685-0.800). The AutoScore-Imbalance
sub-model (using down-sampling algorithm) yielded an AUC of 0. 0.771
(0.718-0.823) with only five variables, demonstrating a good balance between
performance and variable sparsity. Conclusions: The AutoScore-Imbalance tool
has the potential to be applied to highly unbalanced datasets to gain further
insight into rare medical events and to facilitate real-world clinical
decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Han Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1"&gt;Feng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_M/0/1/0/all/0/1"&gt;Marcus Eng Hock Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yilin Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chee_M/0/1/0/all/0/1"&gt;Marcel Lucas Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saffari_S/0/1/0/all/0/1"&gt;Seyed Ehsan Saffari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1"&gt;Hairil Rizal Abdullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Alan Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_B/0/1/0/all/0/1"&gt;Bibhas Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1"&gt;Nan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Continual Learning. (arXiv:2107.05757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05757</id>
        <link href="http://arxiv.org/abs/2107.05757"/>
        <updated>2021-07-14T01:41:50.895Z</updated>
        <summary type="html"><![CDATA[This paper introduces kernel continual learning, a simple but effective
variant of continual learning that leverages the non-parametric nature of
kernel methods to tackle catastrophic forgetting. We deploy an episodic memory
unit that stores a subset of samples for each task to learn task-specific
classifiers based on kernel ridge regression. This does not require memory
replay and systematically avoids task interference in the classifiers. We
further introduce variational random features to learn a data-driven kernel for
each task. To do so, we formulate kernel continual learning as a variational
inference problem, where a random Fourier basis is incorporated as the latent
variable. The variational posterior distribution over the random Fourier basis
is inferred from the coreset of each task. In this way, we are able to generate
more informative kernels specific to each task, and, more importantly, the
coreset size can be reduced to achieve more compact memory, resulting in more
efficient continual learning based on episodic memory. Extensive evaluation on
four benchmarks demonstrates the effectiveness and promise of kernels for
continual learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1"&gt;Mohammad Mahdi Derakhshani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G. M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Discovery and Validation Using Scientific Research Methods. (arXiv:2107.06065v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06065</id>
        <link href="http://arxiv.org/abs/2107.06065"/>
        <updated>2021-07-14T01:41:50.883Z</updated>
        <summary type="html"><![CDATA[Pattern discovery, the process of discovering previously unrecognized
patterns, is often performed as an ad-hoc process with little resulting
certainty in the quality of the proposed patterns. Pattern validation, the
process of validating the accuracy of proposed patterns, remains dominated by
the simple heuristic of "the rule of three". This article shows how to use
established scientific research methods for the purpose of pattern discovery
and validation. We present a specific approach, called the handbook method,
that uses the qualitative survey, action research, and case study research for
pattern discovery and evaluation, and we discuss the underlying principle of
using scientific methods in general. We evaluate the handbook method using
three exploratory studies and demonstrate its usefulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riehle_D/0/1/0/all/0/1"&gt;Dirk Riehle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harutyunyan_N/0/1/0/all/0/1"&gt;Nikolay Harutyunyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barcomb_A/0/1/0/all/0/1"&gt;Ann Barcomb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Selection with Near Optimal Rates for Reinforcement Learning with General Model Classes. (arXiv:2107.05849v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05849</id>
        <link href="http://arxiv.org/abs/2107.05849"/>
        <updated>2021-07-14T01:41:50.861Z</updated>
        <summary type="html"><![CDATA[We address the problem of model selection for the finite horizon episodic
Reinforcement Learning (RL) problem where the transition kernel $P^*$ belongs
to a family of models $\mathcal{P}^*$ with finite metric entropy. In the model
selection framework, instead of $\mathcal{P}^*$, we are given $M$ nested
families of transition kernels $\cP_1 \subset \cP_2 \subset \ldots \subset
\cP_M$. We propose and analyze a novel algorithm, namely \emph{Adaptive
Reinforcement Learning (General)} (\texttt{ARL-GEN}) that adapts to the
smallest such family where the true transition kernel $P^*$ lies.
\texttt{ARL-GEN} uses the Upper Confidence Reinforcement Learning
(\texttt{UCRL}) algorithm with value targeted regression as a blackbox and puts
a model selection module at the beginning of each epoch. Under a mild
separability assumption on the model classes, we show that \texttt{ARL-GEN}
obtains a regret of
$\Tilde{\mathcal{O}}(d_{\mathcal{E}}^*H^2+\sqrt{d_{\mathcal{E}}^* \mathbb{M}^*
H^2 T})$, with high probability, where $H$ is the horizon length, $T$ is the
total number of steps, $d_{\mathcal{E}}^*$ is the Eluder dimension and
$\mathbb{M}^*$ is the metric entropy corresponding to $\mathcal{P}^*$. Note
that this regret scaling matches that of an oracle that knows $\mathcal{P}^*$
in advance. We show that the cost of model selection for \texttt{ARL-GEN} is an
additive term in the regret having a weak dependence on $T$. Subsequently, we
remove the separability assumption and consider the setup of linear mixture
MDPs, where the transition kernel $P^*$ has a linear function approximation.
With this low rank structure, we propose novel adaptive algorithms for model
selection, and obtain (order-wise) regret identical to that of an oracle with
knowledge of the true model class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chowdhury_S/0/1/0/all/0/1"&gt;Sayak Ray Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ramchandran_K/0/1/0/all/0/1"&gt;Kannan Ramchandran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05754</id>
        <link href="http://arxiv.org/abs/2107.05754"/>
        <updated>2021-07-14T01:41:50.854Z</updated>
        <summary type="html"><![CDATA[Recent work has shown how easily white-box adversarial attacks can be applied
to state-of-the-art image classifiers. However, real-life scenarios resemble
more the black-box adversarial conditions, lacking transparency and usually
imposing natural, hard constraints on the query budget.

We propose $\textbf{EvoBA}$, a black-box adversarial attack based on a
surprisingly simple evolutionary search strategy. $\textbf{EvoBA}$ is
query-efficient, minimizes $L_0$ adversarial perturbations, and does not
require any form of training.

$\textbf{EvoBA}$ shows efficiency and efficacy through results that are in
line with much more complex state-of-the-art black-box attacks such as
$\textbf{AutoZOOM}$. It is more query-efficient than $\textbf{SimBA}$, a simple
and powerful baseline black-box attack, and has a similar level of complexity.
Therefore, we propose it both as a new strong baseline for black-box
adversarial attacks and as a fast and general tool for gaining empirical
insight into how robust image classifiers are with respect to $L_0$ adversarial
perturbations.

There exist fast and reliable $L_2$ black-box attacks, such as
$\textbf{SimBA}$, and $L_{\infty}$ black-box attacks, such as
$\textbf{DeepSearch}$. We propose $\textbf{EvoBA}$ as a query-efficient $L_0$
black-box adversarial attack which, together with the aforementioned methods,
can serve as a generic tool to assess the empirical robustness of image
classifiers. The main advantages of such methods are that they run fast, are
query-efficient, and can easily be integrated in image classifiers development
pipelines.

While our attack minimises the $L_0$ adversarial perturbation, we also report
$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$
black-box attack, $\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,
$\textbf{SimBA}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1"&gt;Andrei Ilie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1"&gt;Marius Popescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1"&gt;Alin Stefanescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hierarchical Bayesian model for Inverse RL in Partially-Controlled Environments. (arXiv:2107.05818v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05818</id>
        <link href="http://arxiv.org/abs/2107.05818"/>
        <updated>2021-07-14T01:41:50.847Z</updated>
        <summary type="html"><![CDATA[Robots learning from observations in the real world using inverse
reinforcement learning (IRL) may encounter objects or agents in the
environment, other than the expert, that cause nuisance observations during the
demonstration. These confounding elements are typically removed in
fully-controlled environments such as virtual simulations or lab settings. When
complete removal is impossible the nuisance observations must be filtered out.
However, identifying the source of observations when large amounts of
observations are made is difficult. To address this, we present a hierarchical
Bayesian model that incorporates both the expert's and the confounding
elements' observations thereby explicitly modeling the diverse observations a
robot may receive. We extend an existing IRL algorithm originally designed to
work under partial occlusion of the expert to consider the diverse
observations. In a simulated robotic sorting domain containing both occlusion
and confounding elements, we demonstrate the model's effectiveness. In
particular, our technique outperforms several other comparative methods, second
only to having perfect knowledge of the subject's trajectory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bogert_K/0/1/0/all/0/1"&gt;Kenneth Bogert&lt;/a&gt; (University of North Carolina Asheville), &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1"&gt;Prashant Doshi&lt;/a&gt; (University of Georgia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradual Domain Adaptation in the Wild:When Intermediate Distributions are Absent. (arXiv:2106.06080v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06080</id>
        <link href="http://arxiv.org/abs/2106.06080"/>
        <updated>2021-07-14T01:41:50.840Z</updated>
        <summary type="html"><![CDATA[We focus on the problem of domain adaptation when the goal is shifting the
model towards the target distribution, rather than learning domain invariant
representations. It has been shown that under the following two assumptions:
(a) access to samples from intermediate distributions, and (b) samples being
annotated with the amount of change from the source distribution, self-training
can be successfully applied on gradually shifted samples to adapt the model
toward the target distribution. We hypothesize having (a) is enough to enable
iterative self-training to slowly adapt the model to the target distribution,
by making use of an implicit curriculum. In the case where (a) does not hold,
we observe that iterative self-training falls short. We propose GIFT, a method
that creates virtual samples from intermediate distributions by interpolating
representations of examples from source and target domains. We evaluate an
iterative-self-training method on datasets with natural distribution shifts,
and show that when applied on top of other domain adaptation methods, it
improves the performance of the model on the target dataset. We run an analysis
on a synthetic dataset to show that in the presence of (a)
iterative-self-training naturally forms a curriculum of samples. Furthermore,
we show that when (a) does not hold, GIFT performs better than iterative
self-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1"&gt;Samira Abnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_R/0/1/0/all/0/1"&gt;Rianne van den Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1"&gt;Golnaz Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalchbrenner_N/0/1/0/all/0/1"&gt;Nal Kalchbrenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1"&gt;Hanie Sedghi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey. (arXiv:2107.05989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05989</id>
        <link href="http://arxiv.org/abs/2107.05989"/>
        <updated>2021-07-14T01:41:50.832Z</updated>
        <summary type="html"><![CDATA[Recognizing the patient's emotions using deep learning techniques has
attracted significant attention recently due to technological advancements.
Automatically identifying the emotions can help build smart healthcare centers
that can detect depression and stress among the patients in order to start the
medication early. Using advanced technology to identify emotions is one of the
most exciting topics as it defines the relationships between humans and
machines. Machines learned how to predict emotions by adopting various methods.
In this survey, we present recent research in the field of using neural
networks to recognize emotions. We focus on studying emotions' recognition from
speech, facial expressions, and audio-visual input and show the different
techniques of deploying these algorithms in the real world. These three emotion
recognition techniques can be used as a surveillance system in healthcare
centers to monitor patients. We conclude the survey with a presentation of the
challenges and the related future work to provide an insight into the
applications of using emotion recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1"&gt;Marwan Dhuheir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1"&gt;Abdullatif Albaseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1"&gt;Mohamed Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02561</id>
        <link href="http://arxiv.org/abs/2107.02561"/>
        <updated>2021-07-14T01:41:50.813Z</updated>
        <summary type="html"><![CDATA[It is well noted that coordinate based MLPs benefit greatly -- in terms of
preserving high-frequency information -- through the encoding of coordinate
positions as an array of Fourier features. Hitherto, the rationale for the
effectiveness of these positional encodings has been solely studied through a
Fourier lens. In this paper, we strive to broaden this understanding by showing
that alternative non-Fourier embedding functions can indeed be used for
positional encoding. Moreover, we show that their performance is entirely
determined by a trade-off between the stable rank of the embedded matrix and
the distance preservation between embedded coordinates. We further establish
that the now ubiquitous Fourier feature mapping of position is a special case
that fulfills these conditions. Consequently, we present a more general theory
to analyze positional encoding in terms of shifted basis functions. To this
end, we develop the necessary theoretical formulae and empirically verify that
our theoretical claims hold in practice. Codes available at
https://github.com/osiriszjq/Rethinking-positional-encoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jianqiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1"&gt;Sameera Ramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1"&gt;Simon Lucey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Inverse QSAR Method Based on Linear Regression and Integer Programming. (arXiv:2107.02381v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02381</id>
        <link href="http://arxiv.org/abs/2107.02381"/>
        <updated>2021-07-14T01:41:50.806Z</updated>
        <summary type="html"><![CDATA[Recently a novel framework has been proposed for designing the molecular
structure of chemical compounds using both artificial neural networks (ANNs)
and mixed integer linear programming (MILP). In the framework, we first define
a feature vector $f(C)$ of a chemical graph $C$ and construct an ANN that maps
$x=f(C)$ to a predicted value $\eta(x)$ of a chemical property $\pi$ to $C$.
After this, we formulate an MILP that simulates the computation process of
$f(C)$ from $C$ and that of $\eta(x)$ from $x$. Given a target value $y^*$ of
the chemical property $\pi$, we infer a chemical graph $C^\dagger$ such that
$\eta(f(C^\dagger))=y^*$ by solving the MILP. In this paper, we use linear
regression to construct a prediction function $\eta$ instead of ANNs. For this,
we derive an MILP formulation that simulates the computation process of a
prediction function by linear regression. The results of computational
experiments suggest our method can infer chemical graphs with around up to 50
non-hydrogen atoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianshen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azam_N/0/1/0/all/0/1"&gt;Naveed Ahmed Azam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haraguchi_K/0/1/0/all/0/1"&gt;Kazuya Haraguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagamochi_H/0/1/0/all/0/1"&gt;Hiroshi Nagamochi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akutsu_T/0/1/0/all/0/1"&gt;Tatsuya Akutsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GDI: Rethinking What Makes Reinforcement Learning Different From Supervised Learning. (arXiv:2106.06232v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06232</id>
        <link href="http://arxiv.org/abs/2106.06232"/>
        <updated>2021-07-14T01:41:50.795Z</updated>
        <summary type="html"><![CDATA[Deep Q Network (DQN) firstly kicked the door of deep reinforcement learning
(DRL) via combining deep learning (DL) with reinforcement learning (RL), which
has noticed that the distribution of the acquired data would change during the
training process. DQN found this property might cause instability for training,
so it proposed effective methods to handle the downside of the property.
Instead of focusing on the unfavourable aspects, we find it critical for RL to
ease the gap between the estimated data distribution and the ground truth data
distribution while supervised learning (SL) fails to do so. From this new
perspective, we extend the basic paradigm of RL called the Generalized Policy
Iteration (GPI) into a more generalized version, which is called the
Generalized Data Distribution Iteration (GDI). We see massive RL algorithms and
techniques can be unified into the GDI paradigm, which can be considered as one
of the special cases of GDI. We provide theoretical proof of why GDI is better
than GPI and how it works. Several practical algorithms based on GDI have been
proposed to verify the effectiveness and extensiveness of it. Empirical
experiments prove our state-of-the-art (SOTA) performance on Arcade Learning
Environment (ALE), wherein our algorithm has achieved 9620.98% mean human
normalized score (HNS), 1146.39% median HNS and 22 human world record
breakthroughs (HWRB) using only 200 training frames. Our work aims to lead the
RL research to step into the journey of conquering the human world records and
seek real superhuman agents on both performance and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiajun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05384</id>
        <link href="http://arxiv.org/abs/2107.05384"/>
        <updated>2021-07-14T01:41:50.787Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a commonly used approach to improving the generalization
of deep learning models. Recent works show that learned data augmentation
policies can achieve better generalization than hand-crafted ones. However,
most of these works use unified augmentation policies for all samples in a
dataset, which is observed not necessarily beneficial for all labels in
multi-label classification tasks, i.e., some policies may have negative impacts
on some labels while benefitting the others. To tackle this problem, we propose
a novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,
where augmentation policies are generated with respect to labels by an
augmentation-policy network. The policies are learned via reinforcement
learning using policy gradient methods, providing a mapping from instance
labels to their optimal augmentation policies. Numerical experiments show that
our LB-Aug outperforms previous state-of-the-art augmentation methods by large
margins in multiple benchmarks on image and video classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ya Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hesen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Diffusion Models. (arXiv:2107.00630v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00630</id>
        <link href="http://arxiv.org/abs/2107.00630"/>
        <updated>2021-07-14T01:41:50.779Z</updated>
        <summary type="html"><![CDATA[Diffusion-based generative models have demonstrated a capacity for
perceptually impressive synthesis, but can they also be great likelihood-based
models? We answer this in the affirmative, and introduce a family of
diffusion-based generative models that obtain state-of-the-art likelihoods on
standard image density estimation benchmarks. Unlike other diffusion-based
models, our method allows for efficient optimization of the noise schedule
jointly with the rest of the model. We show that the variational lower bound
(VLB) simplifies to a remarkably short expression in terms of the
signal-to-noise ratio of the diffused data, thereby improving our theoretical
understanding of this model class. Using this insight, we prove an equivalence
between several models proposed in the literature. In addition, we show that
the continuous-time VLB is invariant to the noise schedule, except for the
signal-to-noise ratio at its endpoints. This enables us to learn a noise
schedule that minimizes the variance of the resulting VLB estimator, leading to
faster optimization. Combining these advances with architectural improvements,
we obtain state-of-the-art likelihoods on image density estimation benchmarks,
outperforming autoregressive models that have dominated these benchmarks for
many years, with often significantly faster optimization. In addition, we show
how to turn the model into a bits-back compression scheme, and demonstrate
lossless compression rates close to the theoretical optimum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kingma_D/0/1/0/all/0/1"&gt;Diederik P. Kingma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1"&gt;Tim Salimans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1"&gt;Ben Poole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1"&gt;Jonathan Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06011</id>
        <link href="http://arxiv.org/abs/2107.06011"/>
        <updated>2021-07-14T01:41:50.760Z</updated>
        <summary type="html"><![CDATA[In the context of visual navigation, the capacity to map a novel environment
is necessary for an agent to exploit its observation history in the considered
place and efficiently reach known goals. This ability can be associated with
spatial reasoning, where an agent is able to perceive spatial relationships and
regularities, and discover object affordances. In classical Reinforcement
Learning (RL) setups, this capacity is learned from reward alone. We introduce
supplementary supervision in the form of auxiliary tasks designed to favor the
emergence of spatial perception capabilities in agents trained for a
goal-reaching downstream objective. We show that learning to estimate metrics
quantifying the spatial relationships between an agent at a given location and
a goal to reach has a high positive impact in Multi-Object Navigation settings.
Our method significantly improves the performance of different baseline agents,
that either build an explicit or implicit representation of the environment,
even matching the performance of incomparable oracle agents taking ground-truth
maps as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1"&gt;Pierre Marza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1"&gt;Laetitia Matignon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1"&gt;Olivier Simonin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Label Relation Learning for Multi-Label Classification Using 1-Dimensional Convolutional Neural Networks. (arXiv:2107.05941v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05941</id>
        <link href="http://arxiv.org/abs/2107.05941"/>
        <updated>2021-07-14T01:41:50.754Z</updated>
        <summary type="html"><![CDATA[We present Multi-Scale Label Dependence Relation Networks (MSDN), a novel
approach to multi-label classification (MLC) using 1-dimensional convolution
kernels to learn label dependencies at multi-scale. Modern multi-label
classifiers have been adopting recurrent neural networks (RNNs) as a memory
structure to capture and exploit label dependency relations. The RNN-based MLC
models however tend to introduce a very large number of parameters that may
cause under-/over-fitting problems. The proposed method uses the 1-dimensional
convolutional neural network (1D-CNN) to serve the same purpose in a more
efficient manner. By training a model with multiple kernel sizes, the method is
able to learn the dependency relations among labels at multiple scales, while
it uses a drastically smaller number of parameters. With public benchmark
datasets, we demonstrate that our model can achieve better accuracies with much
smaller number of model parameters compared to RNN-based MLC models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junhyung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1"&gt;Byungyoon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1"&gt;Charmgil Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05768</id>
        <link href="http://arxiv.org/abs/2107.05768"/>
        <updated>2021-07-14T01:41:50.747Z</updated>
        <summary type="html"><![CDATA[Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hanjun Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mengjiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05885</id>
        <link href="http://arxiv.org/abs/2107.05885"/>
        <updated>2021-07-14T01:41:50.741Z</updated>
        <summary type="html"><![CDATA[This paper presents the participation of the MiniTrue team in the FinSim-3
shared task on learning semantic similarities for the financial domain in
English language. Our approach combines contextual embeddings learned by
transformer-based language models with network structures embeddings extracted
on external knowledge sources, to create more meaningful representations of
financial domain entities and terms. For this, two BERT based language models
and a knowledge graph embedding model are used. Besides, we propose a voting
function to joint three basic models for the final inference. Experimental
results show that the model with the knowledge graph embeddings has achieved a
superior result than these models with only contextual embeddings.
Nevertheless, we also observe that our voting function brings an extra benefit
to the final system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1"&gt;Shi-jie We&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06098</id>
        <link href="http://arxiv.org/abs/2107.06098"/>
        <updated>2021-07-14T01:41:50.734Z</updated>
        <summary type="html"><![CDATA[Model explainability is essential for the creation of trustworthy Machine
Learning models in healthcare. An ideal explanation resembles the
decision-making process of a domain expert and is expressed using concepts or
terminology that is meaningful to the clinicians. To provide such an
explanation, we first associate the hidden units of the classifier to
clinically relevant concepts. We take advantage of radiology reports
accompanying the chest X-ray images to define concepts. We discover sparse
associations between concepts and hidden units using a linear sparse logistic
regression. To ensure that the identified units truly influence the
classifier's outcome, we adopt tools from Causal Inference literature and, more
specifically, mediation analysis through counterfactual interventions. Finally,
we construct a low-depth decision tree to translate all the discovered concepts
into a straightforward decision rule, expressed to the radiologist. We
evaluated our approach on a large chest x-ray dataset, where our model produces
a global explanation consistent with clinical knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sumedha Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1"&gt;Stephen Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1"&gt;Sofia Triantafillou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05975</id>
        <link href="http://arxiv.org/abs/2107.05975"/>
        <updated>2021-07-14T01:41:50.727Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of lung lesions in computer tomography has the
potential to ease the burden of clinicians during the Covid-19 pandemic. Yet
predictive deep learning models are not trusted in the clinical routine due to
failing silently in out-of-distribution (OOD) data. We propose a lightweight
OOD detection method that exploits the Mahalanobis distance in the feature
space. The proposed approach can be seamlessly integrated into state-of-the-art
segmentation pipelines without requiring changes in model architecture or
training procedure, and can therefore be used to assess the suitability of
pre-trained models to new data. We validate our method with a patch-based
nnU-Net architecture trained with a multi-institutional dataset and find that
it effectively detects samples that the model segments incorrectly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1"&gt;Karol Gotkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1"&gt;Andreas Bucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1"&gt;Ricarda Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1"&gt;Isabel Kaltenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Less be More? When Increasing-to-Balancing Label Noise Rates Considered Beneficial. (arXiv:2107.05913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05913</id>
        <link href="http://arxiv.org/abs/2107.05913"/>
        <updated>2021-07-14T01:41:50.708Z</updated>
        <summary type="html"><![CDATA[In this paper, we answer the question when inserting label noise (less
informative labels) can instead return us more accurate and fair models. We are
primarily inspired by two observations that 1) increasing a certain class of
instances' label noise to balance the noise rates (increasing-to-balancing)
results in an easier learning problem; 2) Increasing-to-balancing improves
fairness guarantees against label bias. In this paper, we will first quantify
the trade-offs introduced by increasing a certain group of instances' label
noise rate w.r.t. the learning difficulties and performance guarantees. We
analytically demonstrate when such an increase proves to be beneficial, in
terms of either improved generalization errors or the fairness guarantees. Then
we present a method to leverage our idea of inserting label noise for the task
of learning with noisy labels, either without or with a fairness constraint.
The primary technical challenge we face is due to the fact that we would not
know which data instances are suffering from higher noise, and we would not
have the ground truth labels to verify any possible hypothesis. We propose a
detection method that informs us which group of labels might suffer from higher
noise, without using ground truth information. We formally establish the
effectiveness of the proposed solution and demonstrate it with extensive
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jialu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Least-Squares Linear Dilation-Erosion Regressor Trained using Stochastic Descent Gradient or the Difference of Convex Methods. (arXiv:2107.05682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05682</id>
        <link href="http://arxiv.org/abs/2107.05682"/>
        <updated>2021-07-14T01:41:50.701Z</updated>
        <summary type="html"><![CDATA[This paper presents a hybrid morphological neural network for regression
tasks called linear dilation-erosion regression ($\ell$-DER). In few words, an
$\ell$-DER model is given by a convex combination of the composition of linear
and elementary morphological operators. As a result, they yield continuous
piecewise linear functions and, thus, are universal approximators. Apart from
introducing the $\ell$-DER models, we present three approaches for training
these models: one based on stochastic descent gradient and two based on the
difference of convex programming problems. Finally, we evaluate the performance
of the $\ell$-DER model using 14 regression tasks. Although the approach based
on SDG revealed faster than the other two, the $\ell$-DER trained using a
disciplined convex-concave programming problem outperformed the others in terms
of the least mean absolute error score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Angelica Louren&amp;#xe7;o Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valle_M/0/1/0/all/0/1"&gt;Marcos Eduardo Valle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting to Misspecification in Contextual Bandits. (arXiv:2107.05745v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05745</id>
        <link href="http://arxiv.org/abs/2107.05745"/>
        <updated>2021-07-14T01:41:50.694Z</updated>
        <summary type="html"><![CDATA[A major research direction in contextual bandits is to develop algorithms
that are computationally efficient, yet support flexible, general-purpose
function approximation. Algorithms based on modeling rewards have shown strong
empirical performance, but typically require a well-specified model, and can
fail when this assumption does not hold. Can we design algorithms that are
efficient and flexible, yet degrade gracefully in the face of model
misspecification? We introduce a new family of oracle-efficient algorithms for
$\varepsilon$-misspecified contextual bandits that adapt to unknown model
misspecification -- both for finite and infinite action settings. Given access
to an online oracle for square loss regression, our algorithm attains optimal
regret and -- in particular -- optimal dependence on the misspecification
level, with no prior knowledge. Specializing to linear contextual bandits with
infinite actions in $d$ dimensions, we obtain the first algorithm that achieves
the optimal $O(d\sqrt{T} + \varepsilon\sqrt{d}T)$ regret bound for unknown
misspecification level $\varepsilon$.

On a conceptual level, our results are enabled by a new optimization-based
perspective on the regression oracle reduction framework of Foster and Rakhlin,
which we anticipate will find broader use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1"&gt;Dylan J. Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1"&gt;Claudio Gentile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmert_J/0/1/0/all/0/1"&gt;Julian Zimmert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Generative Artificial Intelligence system to decipher species coexistence patterns. (arXiv:2107.06020v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06020</id>
        <link href="http://arxiv.org/abs/2107.06020"/>
        <updated>2021-07-14T01:41:50.687Z</updated>
        <summary type="html"><![CDATA[1. Deciphering coexistence patterns is a current challenge to understanding
diversity maintenance, especially in rich communities where the complexity of
these patterns is magnified through indirect interactions that prevent their
approximation with classical experimental approaches. 2. We explore
cutting-edge Machine Learning techniques called Generative Artificial
Intelligence (GenAI) to decipher species coexistence patterns in vegetation
patches, training generative adversarial networks (GAN) and variational
AutoEncoders (VAE) that are then used to unravel some of the mechanisms behind
community assemblage. 3. The GAN accurately reproduces the species composition
of real patches as well as the affinity of plant species to different soil
types, and the VAE also reaches a high level of accuracy, above 99%. Using the
artificially generated patches, we found that high order interactions tend to
suppress the positive effects of low order interactions. Finally, by
reconstructing successional trajectories we could identify the pioneer species
with larger potential to generate a high diversity of distinct patches in terms
of species composition. 4. Understanding the complexity of species coexistence
patterns in diverse ecological communities requires new approaches beyond
heuristic rules. Generative Artificial Intelligence can be a powerful tool to
this end as it allows to overcome the inherent dimensionality of this
challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1"&gt;J. Hirn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;J. E. Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesinos_Navarro_A/0/1/0/all/0/1"&gt;A. Montesinos-Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Martin_R/0/1/0/all/0/1"&gt;R. Sanchez-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1"&gt;V. Sanz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verdu_M/0/1/0/all/0/1"&gt;M. Verd&amp;#xfa;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Analysis between the Robustness of Sparse Neural Networks and their Random Hidden Structural Priors. (arXiv:2107.06158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06158</id>
        <link href="http://arxiv.org/abs/2107.06158"/>
        <updated>2021-07-14T01:41:50.669Z</updated>
        <summary type="html"><![CDATA[Deep learning models have been shown to be vulnerable to adversarial attacks.
This perception led to analyzing deep learning models not only from the
perspective of their performance measures but also their robustness to certain
types of adversarial attacks. We take another step forward in relating the
architectural structure of neural networks from a graph theoretic perspective
to their robustness. We aim to investigate any existing correlations between
graph theoretic properties and the robustness of Sparse Neural Networks. Our
hypothesis is, that graph theoretic properties as a prior of neural network
structures are related to their robustness. To answer to this hypothesis, we
designed an empirical study with neural network models obtained through random
graphs used as sparse structural priors for the networks. We additionally
investigated the evaluation of a randomly pruned fully connected network as a
point of reference.

We found that robustness measures are independent of initialization methods
but show weak correlations with graph properties: higher graph densities
correlate with lower robustness, but higher average path lengths and average
node eccentricities show negative correlations with robustness measures. We
hope to motivate further empirical and analytical research to tightening an
answer to our hypothesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amor_M/0/1/0/all/0/1"&gt;M. Ben Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stier_J/0/1/0/all/0/1"&gt;J. Stier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granitzer_M/0/1/0/all/0/1"&gt;M. Granitzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experience Report: Deep Learning-based System Log Analysis for Anomaly Detection. (arXiv:2107.05908v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.05908</id>
        <link href="http://arxiv.org/abs/2107.05908"/>
        <updated>2021-07-14T01:41:50.663Z</updated>
        <summary type="html"><![CDATA[Logs have been an imperative resource to ensure the reliability and
continuity of many software systems, especially large-scale distributed
systems. They faithfully record runtime information to facilitate system
troubleshooting and behavior understanding. Due to the large scale and
complexity of modern software systems, the volume of logs has reached an
unprecedented level. Consequently, for log-based anomaly detection,
conventional methods of manual inspection or even traditional machine
learning-based methods become impractical, which serve as a catalyst for the
rapid development of deep learning-based solutions. However, there is currently
a lack of rigorous comparison among the representative log-based anomaly
detectors which resort to neural network models. Moreover, the
re-implementation process demands non-trivial efforts and bias can be easily
introduced. To better understand the characteristics of different anomaly
detectors, in this paper, we provide a comprehensive review and evaluation on
five popular models used by six state-of-the-art methods. Particularly, four of
the selected methods are unsupervised and the remaining two are supervised.
These methods are evaluated with two publicly-available log datasets, which
contain nearly 16 millions log messages and 0.4 million anomaly instances in
total. We believe our work can serve as a basis in this field and contribute to
the future academic researches and industrial applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuangbin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jinyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1"&gt;Wenwei Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yuxin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael R. Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal input representation in neural systems at the edge of chaos. (arXiv:2107.05709v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2107.05709</id>
        <link href="http://arxiv.org/abs/2107.05709"/>
        <updated>2021-07-14T01:41:50.656Z</updated>
        <summary type="html"><![CDATA[Shedding light onto how biological systems represent, process and store
information in noisy environments is a key and challenging goal. A stimulating,
though controversial, hypothesis poses that operating in dynamical regimes near
the edge of a phase transition, i.e. at criticality or the "edge of chaos", can
provide information-processing living systems with important operational
advantages, creating, e.g., an optimal trade-off between robustness and
flexibility. Here, we elaborate on a recent theoretical result, which
establishes that the spectrum of covariance matrices of neural networks
representing complex inputs in a robust way needs to decay as a power-law of
the rank, with an exponent close to unity, a result that has been indeed
experimentally verified in neurons of the mouse visual cortex. Aimed at
understanding and mimicking these results, we construct an artificial neural
network and train it to classify images. Remarkably, we find that the best
performance in such a task is obtained when the network operates near the
critical point, at which the eigenspectrum of the covariance matrix follows the
very same statistics as actual neurons do. Thus, we conclude that operating
near criticality can also have -- besides the usually alleged virtues -- the
advantage of allowing for flexible, robust and efficient input representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Morales_G/0/1/0/all/0/1"&gt;Guillermo B. Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Munoz_M/0/1/0/all/0/1"&gt;Miguel A. Mu&amp;#xf1;oz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Learning for Out-Of-Distribution Generalization in Reinforcement Learning. (arXiv:2107.05686v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05686</id>
        <link href="http://arxiv.org/abs/2107.05686"/>
        <updated>2021-07-14T01:41:50.650Z</updated>
        <summary type="html"><![CDATA[Learning data representations that are useful for various downstream tasks is
a cornerstone of artificial intelligence. While existing methods are typically
evaluated on downstream tasks such as classification or generative image
quality, we propose to assess representations through their usefulness in
downstream control tasks, such as reaching or pushing objects. By training over
10,000 reinforcement learning policies, we extensively evaluate to what extent
different representation properties affect out-of-distribution (OOD)
generalization. Finally, we demonstrate zero-shot transfer of these policies
from simulation to the real world, without any domain randomization or
fine-tuning. This paper aims to establish the first systematic characterization
of the usefulness of learned representations for real-world OOD downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1"&gt;Andrea Dittadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1"&gt;Frederik Tr&amp;#xe4;uble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wuthrich_M/0/1/0/all/0/1"&gt;Manuel W&amp;#xfc;thrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widmaier_F/0/1/0/all/0/1"&gt;Felix Widmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1"&gt;Peter Gehler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1"&gt;Ole Winther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1"&gt;Francesco Locatello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1"&gt;Stefan Bauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05990</id>
        <link href="http://arxiv.org/abs/2107.05990"/>
        <updated>2021-07-14T01:41:50.643Z</updated>
        <summary type="html"><![CDATA[Prior work on diagnosing Alzheimer's disease from magnetic resonance images
of the brain established that convolutional neural networks (CNNs) can leverage
the high-dimensional image information for classifying patients. However,
little research focused on how these models can utilize the usually
low-dimensional tabular information, such as patient demographics or laboratory
measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a
general-purpose module for CNNs that dynamically rescales and shifts the
feature maps of a convolutional layer, conditional on a patient's tabular
clinical information. We show that DAFT is highly effective in combining 3D
image and tabular information for diagnosis and time-to-dementia prediction,
where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and
mean c-index of 0.748, respectively. Our extensive ablation study provides
valuable insights into the architectural properties of DAFT. Our implementation
is available at https://github.com/ai-med/DAFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1"&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1"&gt;Tom Nuno Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1"&gt;Christian Wachinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model of the Weak Reset Process in HfOx Resistive Memory for Deep Learning Frameworks. (arXiv:2107.06064v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06064</id>
        <link href="http://arxiv.org/abs/2107.06064"/>
        <updated>2021-07-14T01:41:50.637Z</updated>
        <summary type="html"><![CDATA[The implementation of current deep learning training algorithms is
power-hungry, owing to data transfer between memory and logic units.
Oxide-based RRAMs are outstanding candidates to implement in-memory computing,
which is less power-intensive. Their weak RESET regime, is particularly
attractive for learning, as it allows tuning the resistance of the devices with
remarkable endurance. However, the resistive change behavior in this regime
suffers many fluctuations and is particularly challenging to model, especially
in a way compatible with tools used for simulating deep learning. In this work,
we present a model of the weak RESET process in hafnium oxide RRAM and
integrate this model within the PyTorch deep learning framework. Validated on
experiments on a hybrid CMOS/RRAM technology, our model reproduces both the
noisy progressive behavior and the device-to-device (D2D) variability. We use
this tool to train Binarized Neural Networks for the MNIST handwritten digit
recognition task and the CIFAR-10 object classification task. We simulate our
model with and without various aspects of device imperfections to understand
their impact on the training process and identify that the D2D variability is
the most detrimental aspect. The framework can be used in the same manner for
other types of memories to identify the device imperfections that cause the
most degradation, which can, in turn, be used to optimize the devices to reduce
the impact of these imperfections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1"&gt;Atreya Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquet_M/0/1/0/all/0/1"&gt;Marc Bocquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirtzlin_T/0/1/0/all/0/1"&gt;Tifenn Hirtzlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laborieux_A/0/1/0/all/0/1"&gt;Axel Laborieux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1"&gt;Jacques-Olivier Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowak_E/0/1/0/all/0/1"&gt;Etienne Nowak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vianello_E/0/1/0/all/0/1"&gt;Elisa Vianello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portal_J/0/1/0/all/0/1"&gt;Jean-Michel Portal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1"&gt;Damien Querlioz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks. (arXiv:2107.05825v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.05825</id>
        <link href="http://arxiv.org/abs/2107.05825"/>
        <updated>2021-07-14T01:41:50.621Z</updated>
        <summary type="html"><![CDATA[A longstanding goal of artificial intelligence is to create artificial agents
capable of learning to perform tasks that require sequential decision making.
Importantly, while it is the artificial agent that learns and acts, it is still
up to humans to specify the particular task to be performed. Classical
task-specification approaches typically involve humans providing stationary
reward functions or explicit demonstrations of the desired tasks. However,
there has recently been a great deal of research energy invested in exploring
alternative ways in which humans may guide learning agents that may, e.g., be
more suitable for certain tasks or require less human effort. This survey
provides a high-level overview of five recent machine learning frameworks that
primarily rely on human guidance apart from pre-specified reward functions or
conventional, step-by-step action demonstrations. We review the motivation,
assumptions, and implementation of each framework, and we discuss possible
future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1"&gt;Faraz Torabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1"&gt;Garrett Warnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Autoregressive Models with Spectral Attention. (arXiv:2107.05984v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05984</id>
        <link href="http://arxiv.org/abs/2107.05984"/>
        <updated>2021-07-14T01:41:50.615Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is an important problem across many domains, playing
a crucial role in multiple real-world applications. In this paper, we propose a
forecasting architecture that combines deep autoregressive models with a
Spectral Attention (SA) module, which merges global and local frequency domain
information in the model's embedded space. By characterizing in the spectral
domain the embedding of the time series as occurrences of a random process, our
method can identify global trends and seasonality patterns. Two spectral
attention models, global and local to the time series, integrate this
information within the forecast and perform spectral filtering to remove time
series's noise. The proposed architecture has a number of useful properties: it
can be effectively incorporated into well-know forecast architectures,
requiring a low number of parameters and producing interpretable results that
improve forecasting accuracy. We test the Spectral Attention Autoregressive
Model (SAAM) on several well-know forecast datasets, consistently demonstrating
that our model compares favorably to state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Moreno_Pino_F/0/1/0/all/0/1"&gt;Fernando Moreno-Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Olmos_P/0/1/0/all/0/1"&gt;Pablo M. Olmos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Artes_Rodriguez_A/0/1/0/all/0/1"&gt;Antonio Art&amp;#xe9;s-Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How many degrees of freedom do we need to train deep networks: a loss landscape perspective. (arXiv:2107.05802v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05802</id>
        <link href="http://arxiv.org/abs/2107.05802"/>
        <updated>2021-07-14T01:41:50.608Z</updated>
        <summary type="html"><![CDATA[A variety of recent works, spanning pruning, lottery tickets, and training
within random subspaces, have shown that deep neural networks can be trained
using far fewer degrees of freedom than the total number of parameters. We
explain this phenomenon by first examining the success probability of hitting a
training loss sub-level set when training within a random subspace of a given
training dimensionality. We find a sharp phase transition in the success
probability from $0$ to $1$ as the training dimension surpasses a threshold.
This threshold training dimension increases as the desired final loss
decreases, but decreases as the initial loss decreases. We then theoretically
explain the origin of this phase transition, and its dependence on
initialization and final desired loss, in terms of precise properties of the
high dimensional geometry of the loss landscape. In particular, we show via
Gordon's escape theorem, that the training dimension plus the Gaussian width of
the desired loss sub-level set, projected onto a unit sphere surrounding the
initialization, must exceed the total number of parameters for the success
probability to be large. In several architectures and datasets, we measure the
threshold training dimension as a function of initialization and demonstrate
that it is a small fraction of the total number of parameters, thereby
implying, by our theory, that successful training with so few dimensions is
possible precisely because the Gaussian width of low loss sub-level sets is
very large. Moreover, this threshold training dimension provides a strong null
model for assessing the efficacy of more sophisticated ways to reduce training
degrees of freedom, including lottery tickets as well a more optimal method we
introduce: lottery subspaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larsen_B/0/1/0/all/0/1"&gt;Brett W. Larsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_N/0/1/0/all/0/1"&gt;Nic Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Surya Ganguli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intraoperative Liver Surface Completion with Graph Convolutional VAE. (arXiv:2009.03871v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03871</id>
        <link href="http://arxiv.org/abs/2009.03871"/>
        <updated>2021-07-14T01:41:50.599Z</updated>
        <summary type="html"><![CDATA[In this work we propose a method based on geometric deep learning to predict
the complete surface of the liver, given a partial point cloud of the organ
obtained during the surgical laparoscopic procedure. We introduce a new data
augmentation technique that randomly perturbs shapes in their frequency domain
to compensate the limited size of our dataset. The core of our method is a
variational autoencoder (VAE) that is trained to learn a latent space for
complete shapes of the liver. At inference time, the generative part of the
model is embedded in an optimisation procedure where the latent representation
is iteratively updated to generate a model that matches the intraoperative
partial point cloud. The effect of this optimisation is a progressive non-rigid
deformation of the initially generated shape. Our method is qualitatively
evaluated on real data and quantitatively evaluated on synthetic data. We
compared with a state-of-the-art rigid registration algorithm, that our method
outperformed in visible areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1"&gt;Simone Foti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1"&gt;Bongjin Koo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowrick_T/0/1/0/all/0/1"&gt;Thomas Dowrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramalhinho_J/0/1/0/all/0/1"&gt;Joao Ramalhinho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allam_M/0/1/0/all/0/1"&gt;Moustafa Allam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davidson_B/0/1/0/all/0/1"&gt;Brian Davidson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1"&gt;Matthew J. Clarkson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph Data Augmentation Strategy with Entropy Preserving. (arXiv:2107.06048v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06048</id>
        <link href="http://arxiv.org/abs/2107.06048"/>
        <updated>2021-07-14T01:41:50.581Z</updated>
        <summary type="html"><![CDATA[The Graph Convolutional Networks (GCNs) proposed by Kipf and Welling are
effective models for semi-supervised learning, but facing the obstacle of
over-smoothing, which will weaken the representation ability of GCNs. Recently
some works are proposed to tackle with above limitation by randomly perturbing
graph topology or feature matrix to generate data augmentations as input for
training. However, these operations have to pay the price of information
structure integrity breaking, and inevitably sacrifice information
stochastically from original graph. In this paper, we introduce a novel graph
entropy definition as an quantitative index to evaluate feature information
diffusion among a graph. Under considerations of preserving graph entropy, we
propose an effective strategy to generate perturbed training data using a
stochastic mechanism but guaranteeing graph topology integrity and with only a
small amount of graph entropy decaying. Extensive experiments have been
conducted on real-world datasets and the results verify the effectiveness of
our proposed method in improving semi-supervised node classification accuracy
compared with a surge of baselines. Beyond that, our proposed approach
significantly enhances the robustness and generalization ability of GCNs during
the training process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Dan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at the Adversarial Robustness of Information Bottleneck Models. (arXiv:2107.05712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05712</id>
        <link href="http://arxiv.org/abs/2107.05712"/>
        <updated>2021-07-14T01:41:50.574Z</updated>
        <summary type="html"><![CDATA[We study the adversarial robustness of information bottleneck models for
classification. Previous works showed that the robustness of models trained
with information bottlenecks can improve upon adversarial training. Our
evaluation under a diverse range of white-box $l_{\infty}$ attacks suggests
that information bottlenecks alone are not a strong defense strategy, and that
previous results were likely influenced by gradient obfuscation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korshunova_I/0/1/0/all/0/1"&gt;Iryna Korshunova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1"&gt;David Stutz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1"&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiles_O/0/1/0/all/0/1"&gt;Olivia Wiles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cautious Policy Programming: Exploiting KL Regularization in Monotonic Policy Improvement for Reinforcement Learning. (arXiv:2107.05798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05798</id>
        <link href="http://arxiv.org/abs/2107.05798"/>
        <updated>2021-07-14T01:41:50.568Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose cautious policy programming (CPP), a novel
value-based reinforcement learning (RL) algorithm that can ensure monotonic
policy improvement during learning. Based on the nature of entropy-regularized
RL, we derive a new entropy regularization-aware lower bound of policy
improvement that only requires estimating the expected policy advantage
function. CPP leverages this lower bound as a criterion for adjusting the
degree of a policy update for alleviating policy oscillation. Different from
similar algorithms that are mostly theory-oriented, we also propose a novel
interpolation scheme that makes CPP better scale in high dimensional control
problems. We demonstrate that the proposed algorithm can trade o? performance
and stability in both didactic classic control problems and challenging
high-dimensional Atari games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1"&gt;Toshinori Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takamitsu Matsubara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration. (arXiv:2107.05719v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05719</id>
        <link href="http://arxiv.org/abs/2107.05719"/>
        <updated>2021-07-14T01:41:50.562Z</updated>
        <summary type="html"><![CDATA[When facing uncertainty, decision-makers want predictions they can trust. A
machine learning provider can convey confidence to decision-makers by
guaranteeing their predictions are distribution calibrated -- amongst the
inputs that receive a predicted class probabilities vector $q$, the actual
distribution over classes is $q$. For multi-class prediction problems, however,
achieving distribution calibration tends to be infeasible, requiring sample
complexity exponential in the number of classes $C$. In this work, we introduce
a new notion -- \emph{decision calibration} -- that requires the predicted
distribution and true distribution to be ``indistinguishable'' to a set of
downstream decision-makers. When all possible decision makers are under
consideration, decision calibration is the same as distribution calibration.
However, when we only consider decision makers choosing between a bounded
number of actions (e.g. polynomial in $C$), our main result shows that
decisions calibration becomes feasible -- we design a recalibration algorithm
that requires sample complexity polynomial in the number of actions and the
number of classes. We validate our recalibration algorithm empirically:
compared to existing methods, decision calibration improves decision-making on
skin lesion and ImageNet classification with modern neural network predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shengjia Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_M/0/1/0/all/0/1"&gt;Michael P. Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sahoo_R/0/1/0/all/0/1"&gt;Roshni Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05804</id>
        <link href="http://arxiv.org/abs/2107.05804"/>
        <updated>2021-07-14T01:41:50.556Z</updated>
        <summary type="html"><![CDATA[Deep neural networks suffer from catastrophic forgetting when learning
multiple knowledge sequentially, and a growing number of approaches have been
proposed to mitigate this problem. Some of these methods achieved considerable
performance by associating the flat local minima with forgetting mitigation in
continual learning. However, they inevitably need (1) tedious hyperparameters
tuning, and (2) additional computational cost. To alleviate these problems, in
this paper, we propose a simple yet effective optimization method, called
AlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we
conduct gradient descent and ascent alternatively when the network tends to
converge at each session of learning new knowledge. Moreover, we theoretically
prove that such a strategy can encourage the optimization to converge to a flat
minima. We verify AlterSGD on continual learning benchmark for semantic
segmentation and the empirical results show that we can significantly mitigate
the forgetting and outperform the state-of-the-art methods with a large margin
under challenging continual learning protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning based E2E Energy Efficient in Joint Radio and NFV Resource Allocation for 5G and Beyond Networks. (arXiv:2107.05991v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.05991</id>
        <link href="http://arxiv.org/abs/2107.05991"/>
        <updated>2021-07-14T01:41:50.549Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a joint radio and core resource allocation
framework for NFV-enabled networks. In the proposed system model, the goal is
to maximize energy efficiency (EE), by guaranteeing end-to-end (E2E) quality of
service (QoS) for different service types. To this end, we formulate an
optimization problem in which power and spectrum resources are allocated in the
radio part. In the core part, the chaining, placement, and scheduling of
functions are performed to ensure the QoS of all users. This joint optimization
problem is modeled as a Markov decision process (MDP), considering time-varying
characteristics of the available resources and wireless channels. A soft
actor-critic deep reinforcement learning (SAC-DRL) algorithm based on the
maximum entropy framework is subsequently utilized to solve the above MDP.
Numerical results reveal that the proposed joint approach based on the SAC-DRL
algorithm could significantly reduce energy consumption compared to the case in
which R-RA and NFV-RA problems are optimized separately.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gholipoor_N/0/1/0/all/0/1"&gt;Narges Gholipoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nouruzi_A/0/1/0/all/0/1"&gt;Ali Nouruzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salarhosseini_S/0/1/0/all/0/1"&gt;Shima Salarhosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1"&gt;Mohammad Reza Javan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1"&gt;Nader Mokari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1"&gt;Eduard A. Jorswieck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIVINE: Diverse Influential Training Points for Data Visualization and Model Refinement. (arXiv:2107.05978v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05978</id>
        <link href="http://arxiv.org/abs/2107.05978"/>
        <updated>2021-07-14T01:41:50.531Z</updated>
        <summary type="html"><![CDATA[As the complexity of machine learning (ML) models increases, resulting in a
lack of prediction explainability, several methods have been developed to
explain a model's behavior in terms of the training data points that most
influence the model. However, these methods tend to mark outliers as highly
influential points, limiting the insights that practitioners can draw from
points that are not representative of the training data. In this work, we take
a step towards finding influential training points that also represent the
training data well. We first review methods for assigning importance scores to
training points. Given importance scores, we propose a method to select a set
of DIVerse INfluEntial (DIVINE) training points as a useful explanation of
model behavior. As practitioners might not only be interested in finding data
points influential with respect to model accuracy, but also with respect to
other important metrics, we show how to evaluate training data points on the
basis of group fairness. Our method can identify unfairness-inducing training
points, which can be removed to improve fairness outcomes. Our quantitative
experiments and user studies show that visualizing DIVINE points helps
practitioners understand and explain model behavior better than earlier
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_U/0/1/0/all/0/1"&gt;Umang Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_I/0/1/0/all/0/1"&gt;Isabel Chien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05687</id>
        <link href="http://arxiv.org/abs/2107.05687"/>
        <updated>2021-07-14T01:41:50.525Z</updated>
        <summary type="html"><![CDATA[Active learning is the iterative construction of a classification model
through targeted labeling, enabling significant labeling cost savings. As most
research on active learning has been carried out before transformer-based
language models ("transformers") became popular, despite its practical
importance, comparably few papers have investigated how transformers can be
combined with active learning to date. This can be attributed to the fact that
using state-of-the-art query strategies for transformers induces a prohibitive
runtime overhead, which effectively cancels out, or even outweighs
aforementioned cost savings. In this paper, we revisit uncertainty-based query
strategies, which had been largely outperformed before, but are particularly
suited in the context of fine-tuning transformers. In an extensive evaluation
on five widely used text classification benchmarks, we show that considerable
improvements of up to 14.4 percentage points in area under the learning curve
are achieved, as well as a final accuracy close to the state of the art for all
but one benchmark, using only between 0.4% and 15% of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1"&gt;Christopher Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1"&gt;Andreas Niekler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization of graph network inferences in higher-order probabilistic graphical models. (arXiv:2107.05729v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.05729</id>
        <link href="http://arxiv.org/abs/2107.05729"/>
        <updated>2021-07-14T01:41:50.518Z</updated>
        <summary type="html"><![CDATA[Probabilistic graphical models provide a powerful tool to describe complex
statistical structure, with many real-world applications in science and
engineering from controlling robotic arms to understanding neuronal
computations. A major challenge for these graphical models is that inferences
such as marginalization are intractable for general graphs. These inferences
are often approximated by a distributed message-passing algorithm such as
Belief Propagation, which does not always perform well on graphs with cycles,
nor can it always be easily specified for complex continuous probability
distributions. Such difficulties arise frequently in expressive graphical
models that include intractable higher-order interactions. In this paper we
construct iterative message-passing algorithms using Graph Neural Networks
defined on factor graphs to achieve fast approximate inference on graphical
models that involve many-variable interactions. Experimental results on several
families of graphical models demonstrate the out-of-distribution generalization
capability of our method to different sized graphs, and indicate the domain in
which our method gains advantage over Belief Propagation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1"&gt;Yicheng Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pitkow_X/0/1/0/all/0/1"&gt;Xaq Pitkow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.05901</id>
        <link href="http://arxiv.org/abs/2107.05901"/>
        <updated>2021-07-14T01:41:50.501Z</updated>
        <summary type="html"><![CDATA[The Jeffreys divergence is a renown symmetrization of the statistical
Kullback-Leibler divergence which is often used in machine learning, signal
processing, and information sciences. Since the Jeffreys divergence between the
ubiquitous Gaussian Mixture Models are not available in closed-form, many
techniques with various pros and cons have been proposed in the literature to
either (i) estimate, (ii) approximate, or (iii) lower and upper bound this
divergence. In this work, we propose a simple yet fast heuristic to approximate
the Jeffreys divergence between two GMMs of arbitrary number of components. The
heuristic relies on converting GMMs into pairs of dually parameterized
probability densities belonging to exponential families. In particular, we
consider Polynomial Exponential Densities, and design a goodness-of-fit
criterion to measure the dissimilarity between a GMM and a PED which is a
generalization of the Hyv\"arinen divergence. This criterion allows one to
select the orders of the PEDs to approximate the GMMs. We demonstrate
experimentally that the computational time of our heuristic improves over the
stochastic Monte Carlo estimation baseline by several orders of magnitude while
approximating reasonably well the Jeffreys divergence, specially when the
univariate mixtures have a small number of modes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motion Planning by Learning the Solution Manifold in Trajectory Optimization. (arXiv:2107.05842v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.05842</id>
        <link href="http://arxiv.org/abs/2107.05842"/>
        <updated>2021-07-14T01:41:50.495Z</updated>
        <summary type="html"><![CDATA[The objective function used in trajectory optimization is often non-convex
and can have an infinite set of local optima. In such cases, there are diverse
solutions to perform a given task. Although there are a few methods to find
multiple solutions for motion planning, they are limited to generating a finite
set of solutions. To address this issue, we presents an optimization method
that learns an infinite set of solutions in trajectory optimization. In our
framework, diverse solutions are obtained by learning latent representations of
solutions. Our approach can be interpreted as training a deep generative model
of collision-free trajectories for motion planning. The experimental results
indicate that the trained model represents an infinite set of homotopic
solutions for motion planning problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osa_T/0/1/0/all/0/1"&gt;Takayuki Osa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Learning Rate Scheduler for Large-batch Training. (arXiv:2107.05855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05855</id>
        <link href="http://arxiv.org/abs/2107.05855"/>
        <updated>2021-07-14T01:41:50.489Z</updated>
        <summary type="html"><![CDATA[Large-batch training has been essential in leveraging large-scale datasets
and models in deep learning. While it is computationally beneficial to use
large batch sizes, it often requires a specially designed learning rate (LR)
schedule to achieve a comparable level of performance as in smaller batch
training. Especially, when the number of training epochs is constrained, the
use of a large LR and a warmup strategy is critical in the final performance of
large-batch training due to the reduced number of updating steps. In this work,
we propose an automated LR scheduling algorithm which is effective for neural
network training with a large batch size under the given epoch budget. In
specific, the whole schedule consists of two phases: adaptive warmup and
predefined decay, where the LR is increased until the training loss no longer
decreases and decreased to zero until the end of training. Here, whether the
training loss has reached the minimum value is robustly checked with Gaussian
process smoothing in an online manner with a low computational burden. Coupled
with adaptive stochastic optimizers such as AdamP and LAMB, the proposed
scheduler successfully adjusts the LRs without cumbersome hyperparameter tuning
and achieves comparable or better performances than tuned baselines on various
image classification benchmarks and architectures with a wide range of batch
sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chiheon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Saehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jongmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Donghoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungwoong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Efficient Transfer Learning in 6G. (arXiv:2107.05728v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.05728</id>
        <link href="http://arxiv.org/abs/2107.05728"/>
        <updated>2021-07-14T01:41:50.482Z</updated>
        <summary type="html"><![CDATA[6G networks will greatly expand the support for data-oriented, autonomous
applications for over the top (OTT) and networking use cases. The success of
these use cases will depend on the availability of big data sets which is not
practical in many real scenarios due to the highly dynamic behavior of systems
and the cost of data collection procedures. Transfer learning (TL) is a
promising approach to deal with these challenges through the sharing of
knowledge among diverse learning algorithms. with TL, the learning rate and
learning accuracy can be considerably improved. However, there are
implementation challenges to efficiently deploy and utilize TL in 6G. In this
paper, we initiate this discussion by providing some performance metrics to
measure the TL success. Then, we show how infrastructure, application,
management, and training planes of 6G can be adapted to handle TL. We provide
examples of TL in 6G and highlight the spatio-temporal features of data in 6G
that can lead to efficient TL. By simulation results, we demonstrate how
transferring the quantized neural network weights between two use cases can
make a trade-off between overheads and performance and attain more efficient TL
in 6G. We also provide a list of future research directions in TL for 6G.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1"&gt;Saeedeh Parsaeefard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1"&gt;Alberto Leon-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational modelling and data-driven homogenisation of knitted membranes. (arXiv:2107.05707v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2107.05707</id>
        <link href="http://arxiv.org/abs/2107.05707"/>
        <updated>2021-07-14T01:41:50.476Z</updated>
        <summary type="html"><![CDATA[Knitting is an effective technique for producing complex three-dimensional
surfaces owing to the inherent flexibility of interlooped yarns and recent
advances in manufacturing providing better control of local stitch patterns.
Fully yarn-level modelling of large-scale knitted membranes is not feasible.
Therefore, we consider a two-scale homogenisation approach and model the
membrane as a Kirchhoff-Love shell on the macroscale and as Euler-Bernoulli
rods on the microscale. The governing equations for both the shell and the rod
are discretised with cubic B-spline basis functions. The solution of the
nonlinear microscale problem requires a significant amount of time due to the
large deformations and the enforcement of contact constraints, rendering
conventional online computational homogenisation approaches infeasible. To
sidestep this problem, we use a pre-trained statistical Gaussian Process
Regression (GPR) model to map the macroscale deformations to macroscale
stresses. During the offline learning phase, the GPR model is trained by
solving the microscale problem for a sufficiently rich set of deformation
states obtained by either uniform or Sobol sampling. The trained GPR model
encodes the nonlinearities and anisotropies present in the microscale and
serves as a material model for the macroscale Kirchhoff-Love shell. After
verifying and validating the different components of the proposed approach, we
introduce several examples involving membranes subjected to tension and shear
to demonstrate its versatility and good performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Herath_S/0/1/0/all/0/1"&gt;Sumudu Herath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xiao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cirak_F/0/1/0/all/0/1"&gt;Fehmi Cirak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Metric to Measure them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks. (arXiv:2011.10772v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10772</id>
        <link href="http://arxiv.org/abs/2011.10772"/>
        <updated>2021-07-14T01:41:50.468Z</updated>
        <summary type="html"><![CDATA[Despite being widely used as a performance measure for visual detection
tasks, Average Precision (AP) is limited in reflecting localisation quality,
(ii) interpretability and (iii) robustness to the design choices regarding its
computation, and its applicability to outputs without confidence scores.
Panoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation
(Kirillov et al., 2019), does not suffer from these limitations but is limited
to panoptic segmentation. In this paper, we propose Localisation Recall
Precision (LRP) Error as the performance measure for all visual detection
tasks. LRP Error, initially proposed only for object detection by Oksuz et al.
(2018), does not suffer from the aforementioned limitations and is applicable
to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as
the minimum LRP error obtained over confidence scores to evaluate visual
detectors and obtain optimal thresholds for deployment. We provide a detailed
comparative analysis of LRP with AP and PQ, and use nearly 100 state-of-the-art
visual detectors from seven visual detection tasks (i.e. object detection,
keypoint detection, instance segmentation, panoptic segmentation, visual
relationship detection, zero-shot detection and generalised zero-shot
detection) using ten datasets (i.e. different COCO variants, LVIS, Open Images,
Pascal, ILSVRC) to empirically show that LRP provides richer and more
discriminative information than its counterparts. Code available at:
https://github.com/kemaloksuz/LRP-Error]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oksuz_K/0/1/0/all/0/1"&gt;Kemal Oksuz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cam_B/0/1/0/all/0/1"&gt;Baris Can Cam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalkan_S/0/1/0/all/0/1"&gt;Sinan Kalkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05677</id>
        <link href="http://arxiv.org/abs/2107.05677"/>
        <updated>2021-07-14T01:41:50.461Z</updated>
        <summary type="html"><![CDATA[We demonstrate that language models pre-trained on codified
(discretely-encoded) music audio learn representations that are useful for
downstream MIR tasks. Specifically, we explore representations from Jukebox
(Dhariwal et al. 2020): a music generation system containing a language model
trained on codified audio from 1M songs. To determine if Jukebox's
representations contain useful information for MIR, we use them as input
features to train shallow models on several MIR tasks. Relative to
representations from conventional MIR models which are pre-trained on tagging,
we find that using representations from Jukebox as input features yields 30%
stronger performance on average across four MIR tasks: tagging, genre
classification, emotion recognition, and key detection. For key detection, we
observe that representations from Jukebox are considerably stronger than those
from models pre-trained on tagging, suggesting that pre-training via codified
audio language modeling may address blind spots in conventional approaches. We
interpret the strength of Jukebox's representations as evidence that modeling
audio instead of tags provides richer representations for MIR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1"&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05634</id>
        <link href="http://arxiv.org/abs/2107.05634"/>
        <updated>2021-07-14T01:41:50.421Z</updated>
        <summary type="html"><![CDATA[Dense optical flow estimation is challenging when there are large
displacements in a scene with heterogeneous motion dynamics, occlusion, and
scene homogeneity. Traditional approaches to handle these challenges include
hierarchical and multiresolution processing methods. Learning-based optical
flow methods typically use a multiresolution approach with image warping when a
broad range of flow velocities and heterogeneous motion is present. Accuracy of
such coarse-to-fine methods is affected by the ghosting artifacts when images
are warped across multiple resolutions and by the vanishing problem in smaller
scene extents with higher motion contrast. Previously, we devised strategies
for building compact dense prediction networks guided by the effective
receptive field (ERF) characteristics of the network (DDCNet). The DDCNet
design was intentionally simple and compact allowing it to be used as a
building block for designing more complex yet compact networks. In this work,
we extend the DDCNet strategies to handle heterogeneous motion dynamics by
cascading DDCNet based sub-nets with decreasing extents of their ERF. Our
DDCNet with multiresolution capability (DDCNet-Multires) is compact without any
specialized network layers. We evaluate the performance of the DDCNet-Multires
network using standard optical flow benchmark datasets. Our experiments
demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and
provides optical flow estimates with accuracy comparable to similar lightweight
learning-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1"&gt;Ali Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1"&gt;Madhusudhanan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05747</id>
        <link href="http://arxiv.org/abs/2107.05747"/>
        <updated>2021-07-14T01:41:50.403Z</updated>
        <summary type="html"><![CDATA[State-of-the-art artificial neural networks (ANNs) require labelled data or
feedback between layers, are often biologically implausible, and are vulnerable
to adversarial attacks that humans are not susceptible to. On the other hand,
Hebbian learning in winner-take-all (WTA) networks, is unsupervised,
feed-forward, and biologically plausible. However, an objective optimization
theory for WTA networks has been missing, except under very limiting
assumptions. Here we derive formally such a theory, based on biologically
plausible but generic ANN elements. Through Hebbian learning, network
parameters maintain a Bayesian generative model of the data. There is no
supervisory loss function, but the network does minimize cross-entropy between
its activations and the input distribution. The key is a "soft" WTA where there
is no absolute "hard" winner neuron, and a specific type of Hebbian-like
plasticity of weights and biases. We confirm our theory in practice, where, in
handwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,
minimizes cross-entropy without having access to it, and outperforms the more
frequently used, hard-WTA-based method. Strikingly, it even outperforms
supervised end-to-end backpropagation, under certain conditions. Specifically,
in a two-layered network, SoftHebb outperforms backpropagation when the
training dataset is only presented once, when the testing data is noisy, and
under gradient-based adversarial attacks. Adversarial attacks that confuse
SoftHebb are also confusing to the human eye. Finally, the model can generate
interpolations of objects from its input distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1"&gt;Dmitry Toichkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1"&gt;Yansong Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qinghai Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Momentum Uncertainty Hashing. (arXiv:2009.08012v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08012</id>
        <link href="http://arxiv.org/abs/2009.08012"/>
        <updated>2021-07-14T01:41:50.382Z</updated>
        <summary type="html"><![CDATA[Combinatorial optimization (CO) has been a hot research topic because of its
theoretic and practical importance. As a classic CO problem, deep hashing aims
to find an optimal code for each data from finite discrete possibilities, while
the discrete nature brings a big challenge to the optimization process.
Previous methods usually mitigate this challenge by binary approximation,
substituting binary codes for real-values via activation functions or
regularizations. However, such approximation leads to uncertainty between
real-values and binary ones, degrading retrieval performance. In this paper, we
propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly
estimates the uncertainty during training and leverages the uncertainty
information to guide the approximation process. Specifically, we model
bit-level uncertainty via measuring the discrepancy between the output of a
hashing network and that of a momentum-updated network. The discrepancy of each
bit indicates the uncertainty of the hashing network to the approximate output
of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can
be regarded as image-level uncertainty. It embodies the uncertainty of the
hashing network to the corresponding input image. The hashing bit and image
with higher uncertainty are paid more attention during optimization. To the
best of our knowledge, this is the first work to study the uncertainty in
hashing bits. Extensive experiments are conducted on four datasets to verify
the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a
million-scale dataset Clothing1M. Our method achieves the best performance on
all of the datasets and surpasses existing state-of-the-art methods by a large
margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Chaoyou Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guoli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Positional Encoding. (arXiv:2107.02561v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02561</id>
        <link href="http://arxiv.org/abs/2107.02561"/>
        <updated>2021-07-14T01:41:50.364Z</updated>
        <summary type="html"><![CDATA[It is well noted that coordinate based MLPs benefit greatly -- in terms of
preserving high-frequency information -- through the encoding of coordinate
positions as an array of Fourier features. Hitherto, the rationale for the
effectiveness of these positional encodings has been solely studied through a
Fourier lens. In this paper, we strive to broaden this understanding by showing
that alternative non-Fourier embedding functions can indeed be used for
positional encoding. Moreover, we show that their performance is entirely
determined by a trade-off between the stable rank of the embedded matrix and
the distance preservation between embedded coordinates. We further establish
that the now ubiquitous Fourier feature mapping of position is a special case
that fulfills these conditions. Consequently, we present a more general theory
to analyze positional encoding in terms of shifted basis functions. To this
end, we develop the necessary theoretical formulae and empirically verify that
our theoretical claims hold in practice. Codes available at
https://github.com/osiriszjq/Rethinking-positional-encoding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jianqiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1"&gt;Sameera Ramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1"&gt;Simon Lucey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closed-Form Solution to Local Non-Rigid Structure-from-Motion. (arXiv:2011.11567v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11567</id>
        <link href="http://arxiv.org/abs/2011.11567"/>
        <updated>2021-07-14T01:41:50.356Z</updated>
        <summary type="html"><![CDATA[A recent trend in Non-Rigid Structure-from-Motion (NRSfM) is to express
local, differential constraints between pairs of images, from which the surface
normal at any point can be obtained by solving a system of polynomial
equations. The systems of equations derived in previous work, however, are of
high degree, having up to five real solutions, thus requiring a computationally
expensive strategy to select a unique solution. Furthermore, they suffer from
degeneracies that make the resulting estimates unreliable, without any
mechanism to identify this situation.

In this paper, we show that, under widely applicable assumptions, we can
derive a new system of equation in terms of the surface normals whose two
solutions can be obtained in closed-form and can easily be disambiguated
locally. Our formalism further allows us to assess how reliable the estimated
local normals are and, hence, to discard them if they are not. Our experiments
show that our reconstructions, obtained from two or more views, are
significantly more accurate than those of state-of-the-art methods, while also
being faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1"&gt;Shaifali Parashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yuxuan Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality of Service Guarantees for Physical Unclonable Functions. (arXiv:2107.05675v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.05675</id>
        <link href="http://arxiv.org/abs/2107.05675"/>
        <updated>2021-07-14T01:41:50.350Z</updated>
        <summary type="html"><![CDATA[We consider a secret key agreement problem in which noisy physical unclonable
function (PUF) outputs facilitate reliable, secure, and private key agreement
with the help of public, noiseless, and authenticated storage. PUF outputs are
highly correlated, so transform coding methods have been combined with scalar
quantizers to extract uncorrelated bit sequences with reliability guarantees.
For PUF circuits with continuous-valued outputs, the models for transformed
outputs are made more realistic by replacing the fitted distributions with
corresponding truncated ones. The state-of-the-art PUF methods that provide
reliability guarantees to each extracted bit are shown to be inadequate to
guarantee the same reliability level for all PUF outputs. Thus, a quality of
service parameter is introduced to control the percentage of PUF outputs for
which a target reliability level can be guaranteed. A public ring oscillator
(RO) output dataset is used to illustrate that a truncated Gaussian
distribution can be fitted to transformed RO outputs that are inputs to uniform
scalar quantizers such that reliability guarantees can be provided for each bit
extracted from any PUF device under additive Gaussian noise components by
eliminating a small subset of PUF outputs. Furthermore, we conversely show that
it is not possible to provide such reliability guarantees without eliminating
any PUF output if no extra secrecy and privacy leakage is allowed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gunlu_O/0/1/0/all/0/1"&gt;Onur G&amp;#xfc;nl&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schaefer_R/0/1/0/all/0/1"&gt;Rafael F. Schaefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05680</id>
        <link href="http://arxiv.org/abs/2107.05680"/>
        <updated>2021-07-14T01:41:50.330Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) are commonly used for modeling complex
distributions of data. Both the generators and discriminators of GANs are often
modeled by neural networks, posing a non-transparent optimization problem which
is non-convex and non-concave over the generator and discriminator,
respectively. Such networks are often heuristically optimized with gradient
descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in
practice. In this work, we analyze the training of Wasserstein GANs with
two-layer neural network discriminators through the lens of convex duality, and
for a variety of generators expose the conditions under which Wasserstein GANs
can be solved exactly with convex optimization approaches, or can be
represented as convex-concave games. Using this convex duality interpretation,
we further demonstrate the impact of different activation functions of the
discriminator. Our observations are verified with numerical results
demonstrating the power of the convex interpretation, with applications in
progressive training of convex architectures corresponding to linear generators
and quadratic-activation discriminators for CelebA image generation. The code
for our experiments is available at https://github.com/ardasahiner/ProCoGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1"&gt;Arda Sahiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1"&gt;Tolga Ergen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1"&gt;Batu Ozturkler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1"&gt;Burak Bartan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1"&gt;John Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1"&gt;Morteza Mardani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05693</id>
        <link href="http://arxiv.org/abs/2107.05693"/>
        <updated>2021-07-14T01:41:50.323Z</updated>
        <summary type="html"><![CDATA[The healthcare domain is one of the most exciting application areas for
machine learning, but a lack of model transparency contributes to a lag in
adoption within the industry. In this work, we explore the current art of
explainability and interpretability within a case study in clinical text
classification, using a task of mortality prediction within MIMIC-III clinical
notes. We demonstrate various visualization techniques for fully interpretable
methods as well as model-agnostic post hoc attributions, and we provide a
generalized method for evaluating the quality of explanations using infidelity
and local Lipschitz across model types from logistic regression to BERT
variants. With these metrics, we introduce a framework through which
practitioners and researchers can assess the frontier between a model's
predictive performance and the quality of its available explanations. We make
our code available to encourage continued refinement of these methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1"&gt;Mitchell Naylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1"&gt;Christi French&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1"&gt;Samantha Terker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1"&gt;Uday Kamath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StreamSoNG: A Soft Streaming Classification Approach. (arXiv:2010.00635v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00635</id>
        <link href="http://arxiv.org/abs/2010.00635"/>
        <updated>2021-07-14T01:41:50.317Z</updated>
        <summary type="html"><![CDATA[Examining most streaming clustering algorithms leads to the understanding
that they are actually incremental classification models. They model existing
and newly discovered structures via summary information that we call
footprints. Incoming data is normally assigned a crisp label (into one of the
structures) and that structure's footprint is incrementally updated. There is
no reason that these assignments need to be crisp. In this paper, we propose a
new streaming classification algorithm that uses Neural Gas prototypes as
footprints and produces a possibilistic label vector (of typicalities) for each
incoming vector. These typicalities are generated by a modified possibilistic
k-nearest neighbor algorithm. The approach is tested on synthetic and real
image datasets. We compare our approach to three other streaming classifiers
based on the Adaptive Random Forest, Very Fast Decision Rules, and the
DenStream algorithm with excellent results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenlong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_J/0/1/0/all/0/1"&gt;James M. Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dale_J/0/1/0/all/0/1"&gt;Jeffrey Dale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bezdek_J/0/1/0/all/0/1"&gt;James C. Bezdek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges for machine learning in clinical translation of big data imaging studies. (arXiv:2107.05630v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05630</id>
        <link href="http://arxiv.org/abs/2107.05630"/>
        <updated>2021-07-14T01:41:50.309Z</updated>
        <summary type="html"><![CDATA[The combination of deep learning image analysis methods and large-scale
imaging datasets offers many opportunities to imaging neuroscience and
epidemiology. However, despite the success of deep learning when applied to
many neuroimaging tasks, there remain barriers to the clinical translation of
large-scale datasets and processing tools. Here, we explore the main challenges
and the approaches that have been explored to overcome them. We focus on issues
relating to data availability, interpretability, evaluation and logistical
challenges, and discuss the challenges we believe are still to be overcome to
enable the full success of big data deep learning approaches to be experienced
outside of the research field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dinsdale_N/0/1/0/all/0/1"&gt;Nicola K Dinsdale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bluemke_E/0/1/0/all/0/1"&gt;Emma Bluemke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundaresan_V/0/1/0/all/0/1"&gt;Vaanathi Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1"&gt;Mark Jenkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smith_S/0/1/0/all/0/1"&gt;Stephen Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana IL Namburete&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-07-14T01:41:50.302Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Md Mamunur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiquan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware virtual adversarial training for anatomically-plausible segmentation. (arXiv:2107.05532v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05532</id>
        <link href="http://arxiv.org/abs/2107.05532"/>
        <updated>2021-07-14T01:41:50.280Z</updated>
        <summary type="html"><![CDATA[Despite their outstanding accuracy, semi-supervised segmentation methods
based on deep neural networks can still yield predictions that are considered
anatomically impossible by clinicians, for instance, containing holes or
disconnected regions. To solve this problem, we present a Context-aware Virtual
Adversarial Training (CaVAT) method for generating anatomically plausible
segmentation. Unlike approaches focusing solely on accuracy, our method also
considers complex topological constraints like connectivity which cannot be
easily modeled in a differentiable loss function. We use adversarial training
to generate examples violating the constraints, so the network can learn to
avoid making such incorrect predictions on new examples, and employ the
Reinforce algorithm to handle non-differentiable segmentation constraints. The
proposed method offers a generic and efficient way to add any constraint on top
of any segmentation network. Experiments on two clinically-relevant datasets
show our method to produce segmentations that are both accurate and
anatomically-plausible in terms of region connectivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jizong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1"&gt;Marco Pedersoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuanfeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Caiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1"&gt;Christian Desrosiers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16139</id>
        <link href="http://arxiv.org/abs/2106.16139"/>
        <updated>2021-07-14T01:41:50.272Z</updated>
        <summary type="html"><![CDATA[Clinical dermatology, still relies heavily on manual introspection of fungi
within a Potassium Hydroxide (KOH) solution using a brightfield microscope.
However, this method takes a long time, is based on the experience of the
clinician, and has a low accuracy. With the increase of neural network
applications in the field of clinical microscopy it is now possible to automate
such manual processes increasing both efficiency and accuracy. This study
presents a deep neural network structure that enables the rapid solutions for
these problems and can perform automatic fungi detection in grayscale images
without colorants. Microscopic images of 81 fungi and 235 ceratine were
collected. Then, smaller patches were extracted containing 2062 fungi and 2142
ceratine. In order to detect fungus and ceratine, two models were created one
of which was a custom neural network and the other was based on the VGG16
architecture. The developed custom model had 99.84% accuracy, and an area under
the curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an
AUC value of 0.99. However, average accuracy and AUC value of clinicians is
72.8% and 0.87 respectively. This deep learning model allows the development of
an automated system that can detect fungi within microscopic images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Abdurrahim Yilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1"&gt;Rahmetullah Varol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1"&gt;Fatih Goktay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1"&gt;Gulsum Gencoglan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1"&gt;Ali Anil Demircali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1"&gt;Berk Dilsizoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1"&gt;Huseyin Uvet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Denoising For Scientific Discovery: A Case Study In Electron Microscopy. (arXiv:2010.12970v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12970</id>
        <link href="http://arxiv.org/abs/2010.12970"/>
        <updated>2021-07-14T01:41:50.265Z</updated>
        <summary type="html"><![CDATA[Denoising is a fundamental challenge in scientific imaging. Deep
convolutional neural networks (CNNs) provide the current state of the art in
denoising natural images, where they produce impressive results. However, their
potential has barely been explored in the context of scientific imaging.
Denoising CNNs are typically trained on real natural images artificially
corrupted with simulated noise. In contrast, in scientific applications,
noiseless ground-truth images are usually not available. To address this issue,
we propose a simulation-based denoising (SBD) framework, in which CNNs are
trained on simulated images. We test the framework on data obtained from
transmission electron microscopy (TEM), an imaging technique with widespread
applications in material science, biology, and medicine. SBD outperforms
existing techniques by a wide margin on a simulated benchmark dataset, as well
as on real data. Apart from the denoised images, SBD generates likelihood maps
to visualize the agreement between the structure of the denoised image and the
observed data. Our results reveal shortcomings of state-of-the-art denoising
architectures, such as their small field-of-view: substantially increasing the
field-of-view of the CNNs allows them to exploit non-local periodic patterns in
the data, which is crucial at high noise levels. In addition, we analyze the
generalization capability of SBD, demonstrating that the trained networks are
robust to variations of imaging parameters and of the underlying signal
structure. Finally, we release the first publicly available benchmark dataset
of TEM images, containing 18,000 examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1"&gt;Sreyas Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzorro_R/0/1/0/all/0/1"&gt;Ramon Manzorro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_J/0/1/0/all/0/1"&gt;Joshua L. Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Binh Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1"&gt;Dev Yashpal Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simoncelli_E/0/1/0/all/0/1"&gt;Eero P. Simoncelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteson_D/0/1/0/all/0/1"&gt;David S. Matteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crozier_P/0/1/0/all/0/1"&gt;Peter A. Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1"&gt;Carlos Fernandez-Granda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Label Correction for the Accurate Edge Detection of Overlapping Cervical Cells. (arXiv:2010.01919v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01919</id>
        <link href="http://arxiv.org/abs/2010.01919"/>
        <updated>2021-07-14T01:41:50.257Z</updated>
        <summary type="html"><![CDATA[Accurate labeling is essential for supervised deep learning methods. In this
paper, to accurately segment images of multiple overlapping cervical cells with
deep learning models, we propose an automatic label correction algorithm to
improve the edge positioning accuracy of overlapping cervical cells in manual
labeling. Our algorithm is designed based on gradient guidance, and can
automatically correct edge positions for overlapping cervical cells and
differences among manual labeling with different annotators. Using the proposed
algorithm, we constructed an open cervical cell edge detection dataset (CCEDD)
with high labeling accuracy. The experiments on the dataset for training show
that our automatic label correction algorithm can improve the accuracy of
manual labels and further improve the positioning accuracy of overlapping cells
with deep learning models. We have released the dataset and code at
https://github.com/nachifur/automatic-label-correction-CCEDD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Huijie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wentao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yandong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Danbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUM: A Benchmark Dataset of Semantic Urban Meshes. (arXiv:2103.00355v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00355</id>
        <link href="http://arxiv.org/abs/2103.00355"/>
        <updated>2021-07-14T01:41:50.250Z</updated>
        <summary type="html"><![CDATA[Recent developments in data acquisition technology allow us to collect 3D
texture meshes quickly. Those can help us understand and analyse the urban
environment, and as a consequence are useful for several applications like
spatial analysis and urban planning. Semantic segmentation of texture meshes
through deep learning methods can enhance this understanding, but it requires a
lot of labelled data. The contributions of this work are threefold: (1) a new
benchmark dataset of semantic urban meshes, (2) a novel semi-automatic
annotation framework, and (3) an annotation tool for 3D meshes. In particular,
our dataset covers about 4 km2 in Helsinki (Finland), with six classes, and we
estimate that we save about 600 hours of labelling work using our annotation
framework, which includes initial segmentation and interactive refinement. We
also compare the performance of several state-of-theart 3D semantic
segmentation methods on the new benchmark dataset. Other researchers can use
our results to train their networks: the dataset is publicly available, and the
annotation tool is released as open-source.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Weixiao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1"&gt;Liangliang Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boom_B/0/1/0/all/0/1"&gt;Bas Boom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1"&gt;Hugo Ledoux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03097</id>
        <link href="http://arxiv.org/abs/2103.03097"/>
        <updated>2021-07-14T01:41:50.230Z</updated>
        <summary type="html"><![CDATA[Machine learning systems generally assume that the training and testing
distributions are the same. To this end, a key requirement is to develop models
that can generalize to unseen distributions. Domain generalization (DG), i.e.,
out-of-distribution generalization, has attracted increasing interests in
recent years. Domain generalization deals with a challenging setting where one
or several different but related domain(s) are given, and the goal is to learn
a model that can generalize to an unseen test domain. Great progress has been
made in the area of domain generalization for years. This paper presents the
first review of recent advances in this area. First, we provide a formal
definition of domain generalization and discuss several related fields. We then
thoroughly review the theories related to domain generalization and carefully
analyze the theory behind generalization. We categorize recent algorithms into
three classes: data manipulation, representation learning, and learning
strategy, and present several popular algorithms in detail for each category.
Third, we introduce the commonly used datasets and applications. Finally, we
summarize existing literature and present some potential research topics for
the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1"&gt;Yidong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations. (arXiv:2103.04167v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04167</id>
        <link href="http://arxiv.org/abs/2103.04167"/>
        <updated>2021-07-14T01:41:50.223Z</updated>
        <summary type="html"><![CDATA[Radiomic representations can quantify properties of regions of interest in
medical image data. Classically, they account for pre-defined statistics of
shape, texture, and other low-level image features. Alternatively, deep
learning-based representations are derived from supervised learning but require
expensive annotations from experts and often suffer from overfitting and data
imbalance issues. In this work, we address the challenge of learning
representations of 3D medical images for an effective quantification under data
imbalance. We propose a \emph{self-supervised} representation learning
framework to learn high-level features of 3D volumes as a complement to
existing radiomics features. Specifically, we demonstrate how to learn image
representations in a self-supervised fashion using a 3D Siamese network. More
importantly, we deal with data imbalance by exploiting two unsupervised
strategies: a) sample re-weighting, and b) balancing the composition of
training batches. When combining our learned self-supervised feature with
traditional radiomics, we show significant improvement in brain tumor
classification and lung cancer staging tasks covering MRI and CT imaging
modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fei-Fei Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaitanya_K/0/1/0/all/0/1"&gt;Krishna Chaitanya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengda Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1"&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1"&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1"&gt;Bjoern Menze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-Basis: a Compact Representation for Neural Networks. (arXiv:2007.06631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06631</id>
        <link href="http://arxiv.org/abs/2007.06631"/>
        <updated>2021-07-14T01:41:50.214Z</updated>
        <summary type="html"><![CDATA[We introduce T-Basis, a novel concept for a compact representation of a set
of tensors, each of an arbitrary shape, which is often seen in Neural Networks.
Each of the tensors in the set is modeled using Tensor Rings, though the
concept applies to other Tensor Networks. Owing its name to the T-shape of
nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally
shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such
representation allows us to parameterize the tensor set with a small number of
parameters (coefficients of the T-Basis tensors), scaling logarithmically with
each tensor's size in the set and linearly with the dimensionality of T-Basis.
We evaluate the proposed approach on the task of neural network compression and
demonstrate that it reaches high compression rates at acceptable performance
drops. Finally, we analyze memory and operation requirements of the compressed
networks and conclude that T-Basis networks are equally well suited for
training and inference in resource-constrained environments and usage on the
edge devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Obukhov_A/0/1/0/all/0/1"&gt;Anton Obukhov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakhuba_M/0/1/0/all/0/1"&gt;Maxim Rakhuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1"&gt;Menelaos Kanakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Dengxin Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained AutoAugmentation for Multi-Label Classification. (arXiv:2107.05384v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05384</id>
        <link href="http://arxiv.org/abs/2107.05384"/>
        <updated>2021-07-14T01:41:50.206Z</updated>
        <summary type="html"><![CDATA[Data augmentation is a commonly used approach to improving the generalization
of deep learning models. Recent works show that learned data augmentation
policies can achieve better generalization than hand-crafted ones. However,
most of these works use unified augmentation policies for all samples in a
dataset, which is observed not necessarily beneficial for all labels in
multi-label classification tasks, i.e., some policies may have negative impacts
on some labels while benefitting the others. To tackle this problem, we propose
a novel Label-Based AutoAugmentation (LB-Aug) method for multi-label scenarios,
where augmentation policies are generated with respect to labels by an
augmentation-policy network. The policies are learned via reinforcement
learning using policy gradient methods, providing a mapping from instance
labels to their optimal augmentation policies. Numerical experiments show that
our LB-Aug outperforms previous state-of-the-art augmentation methods by large
margins in multiple benchmarks on image and video classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Ya Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hesen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fangyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaohua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiuyu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional LSTM networks. (arXiv:2104.00138v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00138</id>
        <link href="http://arxiv.org/abs/2104.00138"/>
        <updated>2021-07-14T01:41:50.199Z</updated>
        <summary type="html"><![CDATA[Quantitative lung measures derived from computed tomography (CT) have been
demonstrated to improve prognostication in coronavirus disease (COVID-19)
patients, but are not part of the clinical routine since required manual
segmentation of lung lesions is prohibitively time-consuming. We propose a new
fully automated deep learning framework for rapid quantification and
differentiation between lung lesions in COVID-19 pneumonia from both contrast
and non-contrast CT images using convolutional Long Short-Term Memory
(ConvLSTM) networks. Utilizing the expert annotations, model training was
performed 5 times with separate hold-out sets using 5-fold cross-validation to
segment ground-glass opacity and high opacity (including consolidation and
pleural effusion). The performance of the method was evaluated on CT data sets
from 197 patients with positive reverse transcription polymerase chain reaction
test result for SARS-CoV-2. Strong agreement between expert manual and
automatic segmentation was obtained for lung lesions with a Dice score
coefficient of 0.876 $\pm$ 0.005; excellent correlations of 0.978 and 0.981 for
ground-glass opacity and high opacity volumes. In the external validation set
of 67 patients, there was dice score coefficient of 0.767 $\pm$ 0.009 as well
as excellent correlations of 0.989 and 0.996 for ground-glass opacity and high
opacity volumes. Computations for a CT scan comprising 120 slices were
performed under 2 seconds on a personal computer equipped with NVIDIA Titan RTX
graphics processing unit. Therefore, our deep learning-based method allows
rapid fully-automated quantitative measurement of pneumonia burden from CT and
may generate results with an accuracy similar to the expert readers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grodecki_K/0/1/0/all/0/1"&gt;Kajetan Grodecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Killekar_A/0/1/0/all/0/1"&gt;Aditya Killekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_A/0/1/0/all/0/1"&gt;Andrew Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cadet_S/0/1/0/all/0/1"&gt;Sebastien Cadet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McElhinney_P/0/1/0/all/0/1"&gt;Priscilla McElhinney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Razipour_A/0/1/0/all/0/1"&gt;Aryabod Razipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1"&gt;Cato Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pressman_B/0/1/0/all/0/1"&gt;Barry D. Pressman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Julien_P/0/1/0/all/0/1"&gt;Peter Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Simon_J/0/1/0/all/0/1"&gt;Judit Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maurovich_Horvat_P/0/1/0/all/0/1"&gt;Pal Maurovich-Horvat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gaibazzi_N/0/1/0/all/0/1"&gt;Nicola Gaibazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thakur_U/0/1/0/all/0/1"&gt;Udit Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mancini_E/0/1/0/all/0/1"&gt;Elisabetta Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agalbato_C/0/1/0/all/0/1"&gt;Cecilia Agalbato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munechika_J/0/1/0/all/0/1"&gt;Jiro Munechika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Matsumoto_H/0/1/0/all/0/1"&gt;Hidenari Matsumoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mene_R/0/1/0/all/0/1"&gt;Roberto Men&amp;#xe8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Parati_G/0/1/0/all/0/1"&gt;Gianfranco Parati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cernigliaro_F/0/1/0/all/0/1"&gt;Franco Cernigliaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nerlekar_N/0/1/0/all/0/1"&gt;Nitesh Nerlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Torlasco_C/0/1/0/all/0/1"&gt;Camilla Torlasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pontone_G/0/1/0/all/0/1"&gt;Gianluca Pontone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dey_D/0/1/0/all/0/1"&gt;Damini Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slomka_P/0/1/0/all/0/1"&gt;Piotr J. Slomka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework. (arXiv:1911.07936v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07936</id>
        <link href="http://arxiv.org/abs/1911.07936"/>
        <updated>2021-07-14T01:41:50.178Z</updated>
        <summary type="html"><![CDATA[Eye tracking is handled as one of the key technologies for applications that
assess and evaluate human attention, behavior, and biometrics, especially using
gaze, pupillary, and blink behaviors. One of the challenges with regard to the
social acceptance of eye tracking technology is however the preserving of
sensitive and personal information. To tackle this challenge, we employ a
privacy-preserving framework based on randomized encoding to train a Support
Vector Regression model using synthetic eye images privately to estimate the
human gaze. During the computation, none of the parties learn about the data or
the result that any other party has. Furthermore, the party that trains the
model cannot reconstruct pupil, blinks or visual scanpath. The experimental
results show that our privacy-preserving framework is capable of working in
real-time, with the same accuracy as compared to non-private version and could
be extended to other eye tracking related problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bozkir_E/0/1/0/all/0/1"&gt;Efe Bozkir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1"&gt;Ali Burak &amp;#xdc;nal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1"&gt;Mete Akg&amp;#xfc;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1"&gt;Enkelejda Kasneci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1"&gt;Nico Pfeifer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Learn Once: Universal Anatomical Landmark Detection. (arXiv:2103.04657v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04657</id>
        <link href="http://arxiv.org/abs/2103.04657"/>
        <updated>2021-07-14T01:41:50.170Z</updated>
        <summary type="html"><![CDATA[Detecting anatomical landmarks in medical images plays an essential role in
understanding the anatomy and planning automated processing. In recent years, a
variety of deep neural network methods have been developed to detect landmarks
automatically. However, all of those methods are unary in the sense that a
highly specialized network is trained for a single task say associated with a
particular anatomical region. In this work, for the first time, we investigate
the idea of ``You Only Learn Once (YOLO)'' and develop a universal anatomical
landmark detection model to realize multiple landmark detection tasks with
end-to-end training based on mixed datasets. The model consists of a local
network and a global network: The local network is built upon the idea of
universal U-Net to learn multi-domain local features and the global network is
a parallelly-duplicated sequential of dilated convolutions that extract global
features to further disambiguate the landmark locations. It is worth mentioning
that the new model design requires much fewer parameters than models with
standard convolutions to train. We evaluate our YOLO model on three X-ray
datasets of 1,588 images on the head, hand, and chest, collectively
contributing 62 landmarks. The experimental results show that our proposed
universal model behaves largely better than any previous models trained on
multiple datasets. It even beats the performance of the model that is trained
separately for every single dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Heqin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+QingsongYao/0/1/0/all/0/1"&gt;QingsongYao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Li Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;S.kevin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The efficacy of Neural Planning Metrics: A meta-analysis of PKL on nuScenes. (arXiv:2010.09350v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09350</id>
        <link href="http://arxiv.org/abs/2010.09350"/>
        <updated>2021-07-14T01:41:50.160Z</updated>
        <summary type="html"><![CDATA[A high-performing object detection system plays a crucial role in autonomous
driving (AD). The performance, typically evaluated in terms of mean Average
Precision, does not take into account orientation and distance of the actors in
the scene, which are important for the safe AD. It also ignores environmental
context. Recently, Philion et al. proposed a neural planning metric (PKL),
based on the KL divergence of a planner's trajectory and the groundtruth route,
to accommodate these requirements. In this paper, we use this neural planning
metric to score all submissions of the nuScenes detection challenge and analyze
the results. We find that while somewhat correlated with mAP, the PKL metric
shows different behavior to increased traffic density, ego velocity, road
curvature and intersections. Finally, we propose ideas to extend the neural
planning metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yiluan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1"&gt;Holger Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1"&gt;Jonah Philion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration. (arXiv:2107.06211v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06211</id>
        <link href="http://arxiv.org/abs/2107.06211"/>
        <updated>2021-07-14T01:41:50.153Z</updated>
        <summary type="html"><![CDATA[High Dynamic Range (HDR) imaging via multi-exposure fusion is an important
task for most modern imaging platforms. In spite of recent developments in both
hardware and algorithm innovations, challenges remain over content association
ambiguities caused by saturation, motion, and various artifacts introduced
during multi-exposure fusion such as ghosting, noise, and blur. In this work,
we propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion)
HDR restoration model which aims to address these issues within one framework.
An efficient two-stream structure is proposed which separately focuses on
texture feature transfer over saturated regions and multi-exposure tonal and
texture feature fusion. A neural feature transfer mechanism is proposed which
establishes spatial correspondence between different exposures based on
multi-scale VGG features in the masked saturated HDR domain for discriminative
contextual clues over the ambiguous image areas. A progressive texture blending
module is designed to blend the encoded two-stream features in a multi-scale
and progressive manner. In addition, we introduce several novel attention
mechanisms, i.e., the motion attention module detects and suppresses the
content discrepancies among the reference images; the saturation attention
module facilitates differentiating the misalignment caused by saturation from
those caused by motion; and the scale attention module ensures texture blending
consistency between different coder/decoder scales. We carry out comprehensive
qualitative and quantitative evaluations and ablation studies, which validate
that these novel modules work coherently under the same framework and
outperform state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zaifeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_T/0/1/0/all/0/1"&gt;Tsz Nam Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dividing and Conquering Cross-Modal Recipe Retrieval: from Nearest Neighbours Baselines to SoTA. (arXiv:1911.12763v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.12763</id>
        <link href="http://arxiv.org/abs/1911.12763"/>
        <updated>2021-07-14T01:41:50.146Z</updated>
        <summary type="html"><![CDATA[We propose a novel non-parametric method for cross-modal recipe retrieval
which is applied on top of precomputed image and text embeddings. By combining
our method with standard approaches for building image and text encoders,
trained independently with a self-supervised classification objective, we
create a baseline model which outperforms most existing methods on a
challenging image-to-recipe task. We also use our method for comparing image
and text encoders trained using different modern approaches, thus addressing
the issues hindering the development of novel methods for cross-modal recipe
retrieval. We demonstrate how to use the insights from model comparison and
extend our baseline model with standard triplet loss that improves
state-of-the-art on the Recipe1M dataset by a large margin, while using only
precomputed features and with much less complexity than existing methods.
Further, our approach readily generalizes beyond recipe retrieval to other
challenging domains, achieving state-of-the-art performance on Politics and
GoodNews cross-modal retrieval tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fain_M/0/1/0/all/0/1"&gt;Mikhail Fain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1"&gt;Niall Twomey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponikar_A/0/1/0/all/0/1"&gt;Andrey Ponikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_R/0/1/0/all/0/1"&gt;Ryan Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1"&gt;Danushka Bollegala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Per-Pixel Classification is Not All You Need for Semantic Segmentation. (arXiv:2107.06278v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06278</id>
        <link href="http://arxiv.org/abs/2107.06278"/>
        <updated>2021-07-14T01:41:50.128Z</updated>
        <summary type="html"><![CDATA[Modern approaches typically formulate semantic segmentation as a per-pixel
classification task, while instance-level segmentation is handled with an
alternative mask classification. Our key insight: mask classification is
sufficiently general to solve both semantic- and instance-level segmentation
tasks in a unified manner using the exact same model, loss, and training
procedure. Following this observation, we propose MaskFormer, a simple mask
classification model which predicts a set of binary masks, each associated with
a single global class label prediction. Overall, the proposed mask
classification-based method simplifies the landscape of effective approaches to
semantic and panoptic segmentation tasks and shows excellent empirical results.
In particular, we observe that MaskFormer outperforms per-pixel classification
baselines when the number of classes is large. Our mask classification-based
method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)
and panoptic segmentation (52.7 PQ on COCO) models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Bowen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander G. Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1"&gt;Alexander Kirillov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning approaches to Earth Observation change detection. (arXiv:2107.06132v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06132</id>
        <link href="http://arxiv.org/abs/2107.06132"/>
        <updated>2021-07-14T01:41:50.121Z</updated>
        <summary type="html"><![CDATA[The interest for change detection in the field of remote sensing has
increased in the last few years. Searching for changes in satellite images has
many useful applications, ranging from land cover and land use analysis to
anomaly detection. In particular, urban change detection provides an efficient
tool to study urban spread and growth through several years of observation. At
the same time, change detection is often a computationally challenging and
time-consuming task, which requires innovative methods to guarantee optimal
results with unquestionable value and within reasonable time. In this paper we
present two different approaches to change detection (semantic segmentation and
classification) that both exploit convolutional neural networks to achieve good
results, which can be further refined and used in a post-processing workflow
for a large variety of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pilato_A/0/1/0/all/0/1"&gt;Antonio Di Pilato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taggio_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Taggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pompili_A/0/1/0/all/0/1"&gt;Alexis Pompili&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iacobellis_M/0/1/0/all/0/1"&gt;Michele Iacobellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florio_A/0/1/0/all/0/1"&gt;Adriano Di Florio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passarelli_D/0/1/0/all/0/1"&gt;Davide Passarelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samarelli_S/0/1/0/all/0/1"&gt;Sergio Samarelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation. (arXiv:2107.06235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06235</id>
        <link href="http://arxiv.org/abs/2107.06235"/>
        <updated>2021-07-14T01:41:50.113Z</updated>
        <summary type="html"><![CDATA[We introduce an unsupervised domain adaption (UDA) strategy that combines
multiple image translations, ensemble learning and self-supervised learning in
one coherent approach. We focus on one of the standard tasks of UDA in which a
semantic segmentation model is trained on labeled synthetic data together with
unlabeled real-world data, aiming to perform well on the latter. To exploit the
advantage of using multiple image translations, we propose an ensemble learning
approach, where three classifiers calculate their prediction by taking as input
features of different image translations, making each classifier learn
independently, with the purpose of combining their outputs by sparse
Multinomial Logistic Regression. This regression layer known as meta-learner
helps to reduce the bias during pseudo label generation when performing
self-supervised learning and improves the generalizability of the model by
taking into consideration the contribution of each classifier. We evaluate our
method on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to
Cityscapes, and achieve state-of-the-art results in the mean intersection over
union metric. Extensive ablation experiments are reported to highlight the
advantageous properties of our proposed UDA strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piva_F/0/1/0/all/0/1"&gt;Fabrizio J. Piva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubbelman_G/0/1/0/all/0/1"&gt;Gijs Dubbelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMT: Convolutional Neural Networks Meet Vision Transformers. (arXiv:2107.06263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06263</id>
        <link href="http://arxiv.org/abs/2107.06263"/>
        <updated>2021-07-14T01:41:50.107Z</updated>
        <summary type="html"><![CDATA[Vision transformers have been successfully applied to image recognition tasks
due to their ability to capture long-range dependencies within an image.
However, there are still gaps in both performance and computational cost
between transformers and existing convolutional neural networks (CNNs). In this
paper, we aim to address this issue and develop a network that can outperform
not only the canonical transformers, but also the high-performance
convolutional models. We propose a new transformer based hybrid network by
taking advantage of transformers to capture long-range dependencies, and of
CNNs to model local features. Furthermore, we scale it to obtain a family of
models, called CMTs, obtaining much better accuracy and efficiency than
previous convolution and transformer based models. In particular, our CMT-S
achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on
FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S
also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),
and other challenging vision datasets such as COCO (44.3% mAP), with
considerably less computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Han Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06219</id>
        <link href="http://arxiv.org/abs/2107.06219"/>
        <updated>2021-07-14T01:41:50.100Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) aims to help models trained on a set of source
domains generalize better on unseen target domains. The performances of current
DG methods largely rely on sufficient labeled data, which however are usually
costly or unavailable. While unlabeled data are far more accessible, we seek to
explore how unsupervised learning can help deep models generalizes across
domains. Specifically, we study a novel generalization problem called
unsupervised domain generalization, which aims to learn generalizable models
with unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised
Learning (DIUL) method to cope with the significant and misleading
heterogeneity within unlabeled data and severe distribution shifts between
source and target data. Surprisingly we observe that DIUL can not only
counterbalance the scarcity of labeled data but also further strengthen the
generalization ability of models when the labeled data are sufficient. As a
pretraining approach, DIUL shows superior to ImageNet pretraining protocol even
when the available data are unlabeled and of a greatly smaller amount compared
to ImageNet. Extensive experiments clearly demonstrate the effectiveness of our
method compared with state-of-the-art unsupervised learning counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linjun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haoxin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plug-and-Play Image Restoration with Deep Denoiser Prior. (arXiv:2008.13751v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.13751</id>
        <link href="http://arxiv.org/abs/2008.13751"/>
        <updated>2021-07-14T01:41:50.080Z</updated>
        <summary type="html"><![CDATA[Recent works on plug-and-play image restoration have shown that a denoiser
can implicitly serve as the image prior for model-based methods to solve many
inverse problems. Such a property induces considerable advantages for
plug-and-play image restoration (e.g., integrating the flexibility of
model-based method and effectiveness of learning-based methods) when the
denoiser is discriminatively learned via deep convolutional neural network
(CNN) with large modeling capacity. However, while deeper and larger CNN models
are rapidly gaining popularity, existing plug-and-play image restoration
hinders its performance due to the lack of suitable denoiser prior. In order to
push the limits of plug-and-play image restoration, we set up a benchmark deep
denoiser prior by training a highly flexible and effective CNN denoiser. We
then plug the deep denoiser prior as a modular part into a half quadratic
splitting based iterative algorithm to solve various image restoration
problems. We, meanwhile, provide a thorough analysis of parameter setting,
intermediate results and empirical convergence to better understand the working
mechanism. Experimental results on three representative image restoration
tasks, including deblurring, super-resolution and demosaicing, demonstrate that
the proposed plug-and-play image restoration with deep denoiser prior not only
significantly outperforms other state-of-the-art model-based methods but also
achieves competitive or even superior performance against state-of-the-art
learning-based methods. The source code is available at
https://github.com/cszn/DPIR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RelativeNAS: Relative Neural Architecture Search via Slow-Fast Learning. (arXiv:2009.06193v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06193</id>
        <link href="http://arxiv.org/abs/2009.06193"/>
        <updated>2021-07-14T01:41:50.073Z</updated>
        <summary type="html"><![CDATA[Despite the remarkable successes of Convolutional Neural Networks (CNNs) in
computer vision, it is time-consuming and error-prone to manually design a CNN.
Among various Neural Architecture Search (NAS) methods that are motivated to
automate designs of high-performance CNNs, the differentiable NAS and
population-based NAS are attracting increasing interests due to their unique
characters. To benefit from the merits while overcoming the deficiencies of
both, this work proposes a novel NAS method, RelativeNAS. As the key to
efficient search, RelativeNAS performs joint learning between fast-learners
(i.e. networks with relatively higher accuracy) and slow-learners in a pairwise
manner. Moreover, since RelativeNAS only requires low-fidelity performance
estimation to distinguish each pair of fast-learner and slow-learner, it saves
certain computation costs for training the candidate architectures. The
proposed RelativeNAS brings several unique advantages: (1) it achieves
state-of-the-art performance on ImageNet with top-1 error rate of 24.88%, i.e.
outperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively; (2) it
spends only nine hours with a single 1080Ti GPU to obtain the discovered cells,
i.e. 3.75x and 7875x faster than DARTS and AmoebaNet respectively; (3) it
provides that the discovered cells obtained on CIFAR-10 can be directly
transferred to object detection, semantic segmentation, and keypoint detection,
yielding competitive results of 73.1% mAP on PASCAL VOC, 78.7% mIoU on
Cityscapes, and 68.5% AP on MSCOCO, respectively. The implementation of
RelativeNAS is available at https://github.com/EMI-Group/RelativeNAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Hao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1"&gt;Ran Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shihua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Cheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1"&gt;Changxiao Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06154</id>
        <link href="http://arxiv.org/abs/2107.06154"/>
        <updated>2021-07-14T01:41:50.066Z</updated>
        <summary type="html"><![CDATA[Due to the domain discrepancy in visual domain adaptation, the performance of
source model degrades when bumping into the high data density near decision
boundary in target domain. A common solution is to minimize the Shannon Entropy
to push the decision boundary away from the high density area. However, entropy
minimization also leads to severe reduction of prediction diversity, and
unfortunately brings harm to the domain adaptation. In this paper, we
investigate the prediction discriminability and diversity by studying the
structure of the classification output matrix of a randomly selected data
batch. We find by theoretical analysis that the prediction discriminability and
diversity could be separately measured by the Frobenius-norm and rank of the
batch output matrix. The nuclear-norm is an upperbound of the former, and a
convex approximation of the latter. Accordingly, we propose Batch Nuclear-norm
Maximization and Minimization, which performs nuclear-norm maximization on the
target output matrix to enhance the target prediction ability, and nuclear-norm
minimization on the source batch output matrix to increase applicability of the
source domain knowledge. We further approximate the nuclear-norm by
L_{1,2}-norm, and design multi-batch optimization for stable solution on large
number of categories. The fast approximation method achieves O(n^2)
computational complexity and better convergence property. Experiments show that
our method could boost the adaptation accuracy and robustness under three
typical domain adaptation scenarios. The code is available at
https://github.com/cuishuhao/BNM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Ranking with Adaptive Margin Triplet Loss. (arXiv:2107.06187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06187</id>
        <link href="http://arxiv.org/abs/2107.06187"/>
        <updated>2021-07-14T01:41:50.059Z</updated>
        <summary type="html"><![CDATA[We propose a simple modification from a fixed margin triplet loss to an
adaptive margin triplet loss. While the original triplet loss is used widely in
classification problems such as face recognition, face re-identification and
fine-grained similarity, our proposed loss is well suited for rating datasets
in which the ratings are continuous values. In contrast to original triplet
loss where we have to sample data carefully, in out method, we can generate
triplets using the whole dataset, and the optimization can still converge
without frequently running into a model collapsing issue. The adaptive margins
only need to be computed once before the training, which is much less expensive
than generating triplets after every epoch as in the fixed margin case. Besides
substantially improved training stability (the proposed model never collapsed
in our experiments compared to a couple of times that the training collapsed on
existing triplet loss), we achieved slightly better performance than the
original triplet loss on various rating datasets and network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1"&gt;Mai Lan Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1"&gt;Volker Blanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Review of Deep Learning-based Single Image Super-resolution. (arXiv:2102.09351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09351</id>
        <link href="http://arxiv.org/abs/2102.09351"/>
        <updated>2021-07-14T01:41:50.052Z</updated>
        <summary type="html"><![CDATA[Image super-resolution (SR) is one of the vital image processing methods that
improve the resolution of an image in the field of computer vision. In the last
two decades, significant progress has been made in the field of
super-resolution, especially by utilizing deep learning methods. This survey is
an effort to provide a detailed survey of recent progress in single-image
super-resolution in the perspective of deep learning while also informing about
the initial classical methods used for image super-resolution. The survey
classifies the image SR methods into four categories, i.e., classical methods,
supervised learning-based methods, unsupervised learning-based methods, and
domain-specific SR methods. We also introduce the problem of SR to provide
intuition about image quality metrics, available reference datasets, and SR
challenges. Deep learning-based approaches of SR are evaluated using a
reference dataset. Some of the reviewed state-of-the-art image SR methods
include the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN),
multiscale residual network (MSRN), meta residual dense network (Meta-RDN),
recurrent back-projection network (RBPN), second-order attention network (SAN),
SR feedback network (SRFBN) and the wavelet-based residual attention network
(WRAN). Finally, this survey is concluded with future directions and trends in
SR and open problems in SR to be addressed by the researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1"&gt;Syed Muhammad Arsalan Bashir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mahrukh Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1"&gt;Yilong Niu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Applications of Artificial Intelligence for Myocardial Infarction Disease Diagnosis. (arXiv:2107.06179v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.06179</id>
        <link href="http://arxiv.org/abs/2107.06179"/>
        <updated>2021-07-14T01:41:50.033Z</updated>
        <summary type="html"><![CDATA[Myocardial infarction disease (MID) is caused to the rapid progress of
undiagnosed coronary artery disease (CAD) that indicates the injury of a heart
cell by decreasing the blood flow to the cardiac muscles. MID is the leading
cause of death in middle-aged and elderly subjects all over the world. In
general, raw Electrocardiogram (ECG) signals are tested for MID identification
by clinicians that is exhausting, time-consuming, and expensive. Artificial
intelligence-based methods are proposed to handle the problems to diagnose MID
on the ECG signals automatically. Hence, in this survey paper, artificial
intelligence-based methods, including machine learning and deep learning, are
review for MID diagnosis on the ECG signals. Using the methods demonstrate that
the feature extraction and selection of ECG signals required to be handcrafted
in the ML methods. In contrast, these tasks are explored automatically in the
DL methods. Based on our best knowledge, Deep Convolutional Neural Network
(DCNN) methods are highly required methods developed for the early diagnosis of
MID on the ECG signals. Most researchers have tended to use DCNN methods, and
no studies have surveyed using artificial intelligence methods for MID
diagnosis on the ECG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1"&gt;Javad Hassannataj Joloudari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mojrian_S/0/1/0/all/0/1"&gt;Sanaz Mojrian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nodehi_I/0/1/0/all/0/1"&gt;Issa Nodehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mashmool_A/0/1/0/all/0/1"&gt;Amir Mashmool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zadegan_Z/0/1/0/all/0/1"&gt;Zeynab Kiani Zadegan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shirkharkolaie_S/0/1/0/all/0/1"&gt;Sahar Khanjani Shirkharkolaie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tamadon_T/0/1/0/all/0/1"&gt;Tahereh Tamadon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khosravi_S/0/1/0/all/0/1"&gt;Samiyeh Khosravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akbari_M/0/1/0/all/0/1"&gt;Mitra Akbari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hassannataj_E/0/1/0/all/0/1"&gt;Edris Hassannataj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1"&gt;Roohallah Alizadehsani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1"&gt;Danial Sharifrazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mosavi_A/0/1/0/all/0/1"&gt;Amir Mosavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. (arXiv:2012.00759v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00759</id>
        <link href="http://arxiv.org/abs/2012.00759"/>
        <updated>2021-07-14T01:41:50.026Z</updated>
        <summary type="html"><![CDATA[We present MaX-DeepLab, the first end-to-end model for panoptic segmentation.
Our approach simplifies the current pipeline that depends heavily on surrogate
sub-tasks and hand-designed components, such as box detection, non-maximum
suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by
area experts, they fail to comprehensively solve the target task. By contrast,
our MaX-DeepLab directly predicts class-labeled masks with a mask transformer,
and is trained with a panoptic quality inspired loss via bipartite matching.
Our mask transformer employs a dual-path architecture that introduces a global
memory path in addition to a CNN path, allowing direct communication with any
CNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the
box-free regime on the challenging COCO dataset, closing the gap between
box-based and box-free methods for the first time. A small variant of
MaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds.
Furthermore, MaX-DeepLab, without test time augmentation, achieves new
state-of-the-art 51.3% PQ on COCO test-dev set. Code is available at
https://github.com/google-research/deeplab2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huiyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yukun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1"&gt;Hartwig Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang-Chieh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining 3D Image and Tabular Data via the Dynamic Affine Feature Map Transform. (arXiv:2107.05990v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05990</id>
        <link href="http://arxiv.org/abs/2107.05990"/>
        <updated>2021-07-14T01:41:50.018Z</updated>
        <summary type="html"><![CDATA[Prior work on diagnosing Alzheimer's disease from magnetic resonance images
of the brain established that convolutional neural networks (CNNs) can leverage
the high-dimensional image information for classifying patients. However,
little research focused on how these models can utilize the usually
low-dimensional tabular information, such as patient demographics or laboratory
measurements. We introduce the Dynamic Affine Feature Map Transform (DAFT), a
general-purpose module for CNNs that dynamically rescales and shifts the
feature maps of a convolutional layer, conditional on a patient's tabular
clinical information. We show that DAFT is highly effective in combining 3D
image and tabular information for diagnosis and time-to-dementia prediction,
where it outperforms competing CNNs with a mean balanced accuracy of 0.622 and
mean c-index of 0.748, respectively. Our extensive ablation study provides
valuable insights into the architectural properties of DAFT. Our implementation
is available at https://github.com/ai-med/DAFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polsterl_S/0/1/0/all/0/1"&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wolf_T/0/1/0/all/0/1"&gt;Tom Nuno Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wachinger_C/0/1/0/all/0/1"&gt;Christian Wachinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06011</id>
        <link href="http://arxiv.org/abs/2107.06011"/>
        <updated>2021-07-14T01:41:50.011Z</updated>
        <summary type="html"><![CDATA[In the context of visual navigation, the capacity to map a novel environment
is necessary for an agent to exploit its observation history in the considered
place and efficiently reach known goals. This ability can be associated with
spatial reasoning, where an agent is able to perceive spatial relationships and
regularities, and discover object affordances. In classical Reinforcement
Learning (RL) setups, this capacity is learned from reward alone. We introduce
supplementary supervision in the form of auxiliary tasks designed to favor the
emergence of spatial perception capabilities in agents trained for a
goal-reaching downstream objective. We show that learning to estimate metrics
quantifying the spatial relationships between an agent at a given location and
a goal to reach has a high positive impact in Multi-Object Navigation settings.
Our method significantly improves the performance of different baseline agents,
that either build an explicit or implicit representation of the environment,
even matching the performance of incomparable oracle agents taking ground-truth
maps as input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1"&gt;Pierre Marza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1"&gt;Laetitia Matignon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1"&gt;Olivier Simonin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAT: Hierarchical Aggregation Transformers for Person Re-identification. (arXiv:2107.05946v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05946</id>
        <link href="http://arxiv.org/abs/2107.05946"/>
        <updated>2021-07-14T01:41:50.004Z</updated>
        <summary type="html"><![CDATA[Recently, with the advance of deep Convolutional Neural Networks (CNNs),
person Re-Identification (Re-ID) has witnessed great success in various
applications. However, with limited receptive fields of CNNs, it is still
challenging to extract discriminative representations in a global view for
persons under non-overlapped cameras. Meanwhile, Transformers demonstrate
strong abilities of modeling long-range dependencies for spatial and sequential
data. In this work, we take advantages of both CNNs and Transformers, and
propose a novel learning framework named Hierarchical Aggregation Transformer
(HAT) for image-based person Re-ID with high performance. To achieve this goal,
we first propose a Deeply Supervised Aggregation (DSA) to recurrently aggregate
hierarchical features from CNN backbones. With multi-granularity supervisions,
the DSA can enhance multi-scale features for person retrieval, which is very
different from previous methods. Then, we introduce a Transformer-based Feature
Calibration (TFC) to integrate low-level detail information as the global prior
for high-level semantic information. The proposed TFC is inserted to each level
of hierarchical features, resulting in great performance improvements. To our
best knowledge, this work is the first to take advantages of both CNNs and
Transformers for image-based person Re-ID. Comprehensive experiments on four
large-scale Re-ID benchmarks demonstrate that our method shows better results
than several state-of-the-art methods. The code is released at
https://github.com/AI-Zhpp/HAT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guowen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jinqing Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional Regression for Arbitrary-Shaped Text Detection. (arXiv:2107.06129v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06129</id>
        <link href="http://arxiv.org/abs/2107.06129"/>
        <updated>2021-07-14T01:41:49.985Z</updated>
        <summary type="html"><![CDATA[Arbitrary-shaped text detection has recently attracted increasing interests
and witnessed rapid development with the popularity of deep learning
algorithms. Nevertheless, existing approaches often obtain inaccurate detection
results, mainly due to the relatively weak ability to utilize context
information and the inappropriate choice of offset references. This paper
presents a novel text instance expression which integrates both foreground and
background information into the pipeline, and naturally uses the pixels near
text boundaries as the offset starts. Besides, a corresponding post-processing
algorithm is also designed to sequentially combine the four prediction results
and reconstruct the text instance accurately. We evaluate our method on several
challenging scene text benchmarks, including both curved and multi-oriented
text datasets. Experimental results demonstrate that the proposed approach
obtains superior or competitive performance compared to other state-of-the-art
methods, e.g., 83.4% F-score for Total-Text, 82.4% F-score for MSRA-TD500, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1"&gt;Tao Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1"&gt;Zhouhui Lian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Deep Learning Method for Thermal to Annotated Thermal-Optical Fused Images. (arXiv:2107.05942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05942</id>
        <link href="http://arxiv.org/abs/2107.05942"/>
        <updated>2021-07-14T01:41:49.977Z</updated>
        <summary type="html"><![CDATA[Thermal Images profile the passive radiation of objects and capture them in
grayscale images. Such images have a very different distribution of data
compared to optical colored images. We present here a work that produces a
grayscale thermo-optical fused mask given a thermal input. This is a deep
learning based pioneering work since to the best of our knowledge, there exists
no other work on thermal-optical grayscale fusion. Our method is also unique in
the sense that the deep learning method we are proposing here works on the
Discrete Wavelet Transform (DWT) domain instead of the gray level domain. As a
part of this work, we also present a new and unique database for obtaining the
region of interest in thermal images based on an existing thermal visual paired
database, containing the Region of Interest on 5 different classes of data.
Finally, we are proposing a simple low cost overhead statistical measure for
identifying the region of interest in the fused images, which we call as the
Region of Fusion (RoF). Experiments on the database show encouraging results in
identifying the region of interest in the fused images. We also show that they
can be processed better in the mixed form rather than with only thermal images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1"&gt;Suranjan Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Member_I/0/1/0/all/0/1"&gt;IEEE Student Member&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Member_S/0/1/0/all/0/1"&gt;Senior Member&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1"&gt;IEEE&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_B/0/1/0/all/0/1"&gt;Bidyut B. Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fellow_L/0/1/0/all/0/1"&gt;Life Fellow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+IEEE/0/1/0/all/0/1"&gt;IEEE&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval. (arXiv:2107.06256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06256</id>
        <link href="http://arxiv.org/abs/2107.06256"/>
        <updated>2021-07-14T01:41:49.969Z</updated>
        <summary type="html"><![CDATA[We present Retrieve in Style (RIS), an unsupervised framework for
fine-grained facial feature transfer and retrieval on real images. Recent work
shows that it is possible to learn a catalog that allows local semantic
transfers of facial features on generated images by capitalizing on the
disentanglement property of the StyleGAN latent space. RIS improves existing
art on: 1) feature disentanglement and allows for challenging transfers (i.e.,
hair and pose) that were not shown possible in SoTA methods. 2) eliminating the
need for per-image hyperparameter tuning, and for computing a catalog over a
large batch of images. 3) enabling face retrieval using the proposed facial
features (e.g., eyes), and to our best knowledge, is the first work to retrieve
face images at the fine-grained level. 4) robustness and natural application to
real images. Our qualitative and quantitative analyses show RIS achieves both
high-fidelity feature transfers and accurate fine-grained retrievals on real
images. We discuss the responsible application of RIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1"&gt;Min Jin Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wen-Sheng Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Abhishek Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention based CNN-LSTM Network for Pulmonary Embolism Prediction on Chest Computed Tomography Pulmonary Angiograms. (arXiv:2107.06276v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.06276</id>
        <link href="http://arxiv.org/abs/2107.06276"/>
        <updated>2021-07-14T01:41:49.962Z</updated>
        <summary type="html"><![CDATA[With more than 60,000 deaths annually in the United States, Pulmonary
Embolism (PE) is among the most fatal cardiovascular diseases. It is caused by
an artery blockage in the lung; confirming its presence is time-consuming and
is prone to over-diagnosis. The utilization of automated PE detection systems
is critical for diagnostic accuracy and efficiency. In this study we propose a
two-stage attention-based CNN-LSTM network for predicting PE, its associated
type (chronic, acute) and corresponding location (leftsided, rightsided or
central) on computed tomography (CT) examinations. We trained our model on the
largest available public Computed Tomography Pulmonary Angiogram PE dataset
(RSNA-STR Pulmonary Embolism CT (RSPECT) Dataset, N=7279 CT studies) and tested
it on an in-house curated dataset of N=106 studies. Our framework mirrors the
radiologic diagnostic process via a multi-slice approach so that the accuracy
and pathologic sequela of true pulmonary emboli may be meticulously assessed,
enabling physicians to better appraise the morbidity of a PE when present. Our
proposed method outperformed a baseline CNN classifier and a single-stage
CNN-LSTM network, achieving an AUC of 0.95 on the test set for detecting the
presence of PE in the study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Suman_S/0/1/0/all/0/1"&gt;Sudhir Suman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sakla_N/0/1/0/all/0/1"&gt;Nicole Sakla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gattu_R/0/1/0/all/0/1"&gt;Rishabh Gattu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Green_J/0/1/0/all/0/1"&gt;Jeremy Green&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Phatak_T/0/1/0/all/0/1"&gt;Tej Phatak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1"&gt;Dimitris Samaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1"&gt;Prateek Prasanna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06262</id>
        <link href="http://arxiv.org/abs/2107.06262"/>
        <updated>2021-07-14T01:41:49.954Z</updated>
        <summary type="html"><![CDATA[We explore computational approaches for visual guidance to aid in creating
aesthetically pleasing art and graphic design. Our work complements and builds
on previous work that developed models for how humans look at images. Our
approach comprises three steps. First, we collected a dataset of art
masterpieces and labeled the visual fixations with state-of-art vision models.
Second, we clustered the visual guidance templates of the art masterpieces with
unsupervised learning. Third, we developed a pipeline using generative
adversarial networks to learn the principles of visual guidance and that can
produce aesthetically pleasing layouts. We show that the aesthetic visual
guidance principles can be learned and integrated into a high-dimensional model
and can be queried by the features of graphic elements. We evaluate our
approach by generating layouts on various drawings and graphic designs.
Moreover, our model considers the color and structure of graphic elements when
generating layouts. Consequently, we believe our tool, which generates multiple
aesthetic layout options in seconds, can help artists create beautiful art and
graphic designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qingyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuoru Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1"&gt;Adam Bargteil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA['CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval with Deep Neural Networks. (arXiv:2107.06212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06212</id>
        <link href="http://arxiv.org/abs/2107.06212"/>
        <updated>2021-07-14T01:41:49.934Z</updated>
        <summary type="html"><![CDATA[Ongoing advancements in the fields of 3D modelling and digital archiving have
led to an outburst in the amount of data stored digitally. Consequently,
several retrieval systems have been developed depending on the type of data
stored in these databases. However, unlike text data or images, performing a
search for 3D models is non-trivial. Among 3D models, retrieving 3D
Engineering/CAD models or mechanical components is even more challenging due to
the presence of holes, volumetric features, presence of sharp edges etc., which
make CAD a domain unto itself. The research work presented in this paper aims
at developing a dataset suitable for building a retrieval system for 3D CAD
models based on deep learning. 3D CAD models from the available CAD databases
are collected, and a dataset of computer-generated sketch data, termed
'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the
components are also added to CADSketchNet. Using the sketch images from this
dataset, the paper also aims at evaluating the performance of various retrieval
system or a search engine for 3D CAD models that accepts a sketch image as the
input query. Many experimental models are constructed and tested on
CADSketchNet. These experiments, along with the model architecture, choice of
similarity metrics are reported along with the search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1"&gt;Bharadwaj Manda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhayarkar_S/0/1/0/all/0/1"&gt;Shubham Dhayarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1"&gt;Sai Mitheran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viekash_V/0/1/0/all/0/1"&gt;V.K. Viekash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1"&gt;Ramanathan Muthuganapathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields. (arXiv:2107.06028v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.06028</id>
        <link href="http://arxiv.org/abs/2107.06028"/>
        <updated>2021-07-14T01:41:49.925Z</updated>
        <summary type="html"><![CDATA[Dual decomposition approaches in nonconvex optimization may suffer from a
duality gap. This poses a challenge when applying them directly to nonconvex
problems such as MAP-inference in a Markov random field (MRF) with continuous
state spaces. To eliminate such gaps, this paper considers a reformulation of
the original nonconvex task in the space of measures. This infinite-dimensional
reformulation is then approximated by a semi-infinite one, which is obtained
via a piecewise polynomial discretization in the dual. We provide a geometric
intuition behind the primal problem induced by the dual discretization and draw
connections to optimization over moment spaces. In contrast to existing
discretizations which suffer from a grid bias, we show that a piecewise
polynomial discretization better preserves the continuous nature of our
problem. Invoking results from optimal transport theory and convex algebraic
geometry we reduce the semi-infinite program to a finite one and provide a
practical implementation based on semidefinite programming. We show,
experimentally and in theory, that the approach successfully reduces the
duality gap. To showcase the scalability of our approach, we apply it to the
stereo matching problem between two images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bauermeister_H/0/1/0/all/0/1"&gt;Hartmut Bauermeister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Laude_E/0/1/0/all/0/1"&gt;Emanuel Laude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1"&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Surface Reconstruction with Delaunay-Graph Neural Networks. (arXiv:2107.06130v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06130</id>
        <link href="http://arxiv.org/abs/2107.06130"/>
        <updated>2021-07-14T01:41:49.918Z</updated>
        <summary type="html"><![CDATA[We introduce a novel learning-based, visibility-aware, surface reconstruction
method for large-scale, defect-laden point clouds. Our approach can cope with
the scale and variety of point cloud defects encountered in real-life
Multi-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay
tetrahedralization whose cells are classified as inside or outside the surface
by a graph neural network and an energy model solvable with a graph cut. Our
model, making use of both local geometric attributes and line-of-sight
visibility information, is able to learn a visibility model from a small amount
of synthetic training data and generalizes to real-life acquisitions. Combining
the efficiency of deep learning methods and the scalability of energy based
models, our approach outperforms both learning and non learning-based
reconstruction algorithms on two publicly available reconstruction benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1"&gt;Raphael Sulzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1"&gt;Loic Landrieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1"&gt;Renaud Marlet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1"&gt;Bruno Vallet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting when pre-trained nnU-Net models fail silently for Covid-19. (arXiv:2107.05975v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05975</id>
        <link href="http://arxiv.org/abs/2107.05975"/>
        <updated>2021-07-14T01:41:49.911Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of lung lesions in computer tomography has the
potential to ease the burden of clinicians during the Covid-19 pandemic. Yet
predictive deep learning models are not trusted in the clinical routine due to
failing silently in out-of-distribution (OOD) data. We propose a lightweight
OOD detection method that exploits the Mahalanobis distance in the feature
space. The proposed approach can be seamlessly integrated into state-of-the-art
segmentation pipelines without requiring changes in model architecture or
training procedure, and can therefore be used to assess the suitability of
pre-trained models to new data. We validate our method with a patch-based
nnU-Net architecture trained with a multi-institutional dataset and find that
it effectively detects samples that the model segments incorrectly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1"&gt;Camila Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gotkowski_K/0/1/0/all/0/1"&gt;Karol Gotkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bucher_A/0/1/0/all/0/1"&gt;Andreas Bucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fischbach_R/0/1/0/all/0/1"&gt;Ricarda Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaltenborn_I/0/1/0/all/0/1"&gt;Isabel Kaltenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1"&gt;Anirban Mukhopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Tracking and Geo-localization from Street Images. (arXiv:2107.06257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06257</id>
        <link href="http://arxiv.org/abs/2107.06257"/>
        <updated>2021-07-14T01:41:49.904Z</updated>
        <summary type="html"><![CDATA[Geo-localizing static objects from street images is challenging but also very
important for road asset mapping and autonomous driving. In this paper we
present a two-stage framework that detects and geolocalizes traffic signs from
low frame rate street videos. Our proposed system uses a modified version of
RetinaNet (GPS-RetinaNet), which predicts a positional offset for each sign
relative to the camera, in addition to performing the standard classification
and bounding box regression. Candidate sign detections from GPS-RetinaNet are
condensed into geolocalized signs by our custom tracker, which consists of a
learned metric network and a variant of the Hungarian Algorithm. Our metric
network estimates the similarity between pairs of detections, then the
Hungarian Algorithm matches detections across images using the similarity
scores provided by the metric network. Our models were trained using an updated
version of the ARTS dataset, which contains 25,544 images and 47.589 sign
annotations ~\cite{arts}. The proposed dataset covers a diverse set of
environments gathered from a broad selection of roads. Each annotaiton contains
a sign class label, its geospatial location, an assembly label, a side of road
indicator, and unique identifiers that aid in the evaluation. This dataset will
support future progress in the field, and the proposed system demonstrates how
to take advantage of some of the unique characteristics of a realistic
geolocalization dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1"&gt;Thayer Alshaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1"&gt;Colin Van Oort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaohan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_J/0/1/0/all/0/1"&gt;Jonathan Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1"&gt;Safwan Wshah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05997</id>
        <link href="http://arxiv.org/abs/2107.05997"/>
        <updated>2021-07-14T01:41:49.897Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have an enormous potential to learn from complex
biomedical data. In particular, DNNs have been used to seamlessly fuse
heterogeneous information from neuroanatomy, genetics, biomarkers, and
neuropsychological tests for highly accurate Alzheimer's disease diagnosis. On
the other hand, their black-box nature is still a barrier for the adoption of
such a system in the clinic, where interpretability is absolutely essential. We
propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for
explaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of
the neuroanatomy and tabular biomarkers. Our explanations are based on the
Shapley value, which is the unique method that satisfies all fundamental axioms
for local explanations previously established in the literature. Thus, SVEHNN
has many desirable characteristics that previous work on interpretability for
medical decision making is lacking. To avoid the exponential time complexity of
the Shapley value, we propose to transform a given DNN into a Lightweight
Probabilistic Deep Network without re-training, thus achieving a complexity
only quadratic in the number of features. In our experiments on synthetic and
real data, we show that we can closely approximate the exact Shapley value with
a dramatically reduced runtime and can reveal the hidden knowledge the network
has learned from the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1"&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1"&gt;Christina Aigner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1"&gt;Christian Wachinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis. (arXiv:2107.06149v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06149</id>
        <link href="http://arxiv.org/abs/2107.06149"/>
        <updated>2021-07-14T01:41:49.878Z</updated>
        <summary type="html"><![CDATA[With the rapid development of data-driven techniques, data has played an
essential role in various computer vision tasks. Many realistic and synthetic
datasets have been proposed to address different problems. However, there are
lots of unresolved challenges: (1) the creation of dataset is usually a tedious
process with manual annotations, (2) most datasets are only designed for a
single specific task, (3) the modification or randomization of the 3D scene is
difficult, and (4) the release of commercial 3D data may encounter copyright
issue.

This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl
Synthesis system, to facilitate the 3D scene modification and the 2D image
synthesis for various vision tasks. In particular, we design a programmable
pipeline with Domain-Specific Language, allowing users to (1) select scenes
from the commercial indoor scene database, (2) synthesize scenes for different
tasks with customized rules, and (3) render various imagery data, such as
visual color, geometric structures, semantic label. Our system eases the
difficulty of customizing massive numbers of scenes for different tasks and
relieves users from manipulating fine-grained scene configurations by providing
user-controllable randomness using multi-level samplers. Most importantly, it
empowers users to access commercial scene databases with millions of indoor
scenes and protects the copyright of core data assets, e.g., 3D CAD models. We
demonstrate the validity and flexibility of our system by using our synthesized
data to improve the performance on different kinds of computer vision tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Haocheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jiaxiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Rui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1"&gt;Hujun Bao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Learning via Kernel Density Discrimination. (arXiv:2107.06197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06197</id>
        <link href="http://arxiv.org/abs/2107.06197"/>
        <updated>2021-07-14T01:41:49.871Z</updated>
        <summary type="html"><![CDATA[We introduce Kernel Density Discrimination GAN (KDD GAN), a novel method for
generative adversarial learning. KDD GAN formulates the training as a
likelihood ratio optimization problem where the data distributions are written
explicitly via (local) Kernel Density Estimates (KDE). This is inspired by the
recent progress in contrastive learning and its relation to KDE. We define the
KDEs directly in feature space and forgo the requirement of invertibility of
the kernel feature mappings. In our approach, features are no longer optimized
for linear separability, as in the original GAN formulation, but for the more
general discrimination of distributions in the feature space. We analyze the
gradient of our loss with respect to the feature representation and show that
it is better behaved than that of the original hinge loss. We perform
experiments with the proposed KDE-based loss, used either as a training loss or
a regularization term, on both CIFAR10 and scaled versions of ImageNet. We use
BigGAN/SA-GAN as a backbone and baseline, since our focus is not to design the
architecture of the networks. We show a boost in the quality of generated
samples with respect to FID from 10% to 40% compared to the baseline. Code will
be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1"&gt;Abdelhak Lemkhenter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielski_A/0/1/0/all/0/1"&gt;Adam Bielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sari_A/0/1/0/all/0/1"&gt;Alp Eren Sari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cats, not CAT scans: a study of dataset similarity in transfer learning for 2D medical image classification. (arXiv:2107.05940v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05940</id>
        <link href="http://arxiv.org/abs/2107.05940"/>
        <updated>2021-07-14T01:41:49.864Z</updated>
        <summary type="html"><![CDATA[Transfer learning is a commonly used strategy for medical image
classification, especially via pretraining on source data and fine-tuning on
target data. There is currently no consensus on how to choose appropriate
source data, and in the literature we can find both evidence of favoring large
natural image datasets such as ImageNet, and evidence of favoring more
specialized medical datasets. In this paper we perform a systematic study with
nine source datasets with natural or medical images, and three target medical
datasets, all with 2D images. We find that ImageNet is the source leading to
the highest performances, but also that larger datasets are not necessarily
better. We also study different definitions of data similarity. We show that
common intuitions about similarity may be inaccurate, and therefore not
sufficient to predict an appropriate source a priori. Finally, we discuss
several steps needed for further research in this field, especially with regard
to other types (for example 3D) medical images. Our experiments and pretrained
models are available via \url{https://www.github.com/vcheplygina/cats-scans}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_I/0/1/0/all/0/1"&gt;Irma van den Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fok_F/0/1/0/all/0/1"&gt;Floris Fok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulders_B/0/1/0/all/0/1"&gt;Bas Mulders&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1"&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1"&gt;Veronika Cheplygina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image Enhancement. (arXiv:2107.05830v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05830</id>
        <link href="http://arxiv.org/abs/2107.05830"/>
        <updated>2021-07-14T01:41:49.857Z</updated>
        <summary type="html"><![CDATA[Low-light image enhancement (LLIE) is a pervasive yet challenging problem,
since: 1) low-light measurements may vary due to different imaging conditions
in practice; 2) images can be enlightened subjectively according to diverse
preferences by each individual. To tackle these two challenges, this paper
presents a novel deep reinforcement learning based method, dubbed ReLLIE, for
customized low-light enhancement. ReLLIE models LLIE as a markov decision
process, i.e., estimating the pixel-wise image-specific curves sequentially and
recurrently. Given the reward computed from a set of carefully crafted
non-reference loss functions, a lightweight network is proposed to estimate the
curves for enlightening of a low-light image input. As ReLLIE learns a policy
instead of one-one image translation, it can handle various low-light
measurements and provide customized enhanced outputs by flexibly applying the
policy different times. Furthermore, ReLLIE can enhance real-world images with
hybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily.
Extensive experiments on various benchmarks demonstrate the advantages of
ReLLIE, comparing to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rongkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Lanqing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Everybody Is Unique: Towards Unbiased Human Mesh Recovery. (arXiv:2107.06239v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06239</id>
        <link href="http://arxiv.org/abs/2107.06239"/>
        <updated>2021-07-14T01:41:49.839Z</updated>
        <summary type="html"><![CDATA[We consider the problem of obese human mesh recovery, i.e., fitting a
parametric human mesh to images of obese people. Despite obese person mesh
fitting being an important problem with numerous applications (e.g.,
healthcare), much recent progress in mesh recovery has been restricted to
images of non-obese people. In this work, we identify this crucial gap in the
current literature by presenting and discussing limitations of existing
algorithms. Next, we present a simple baseline to address this problem that is
scalable and can be easily used in conjunction with existing algorithms to
improve their performance. Finally, we present a generalized human mesh
optimization algorithm that substantially improves the performance of existing
methods on both obese person images as well as community-standard benchmark
datasets. A key innovation of this technique is that it does not rely on
supervision from expensive-to-create mesh parameters. Instead, starting from
widely and cheaply available 2D keypoints annotations, our method automatically
generates mesh parameters that can in turn be used to re-train and fine-tune
any existing mesh estimation algorithm. This way, we show our method acts as a
drop-in to improve the performance of a wide variety of contemporary mesh
estimation methods. We conduct extensive experiments on multiple datasets
comprising both standard and obese person images and demonstrate the efficacy
of our proposed techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ren Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Meng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1"&gt;Srikrishna Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Terrence Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Discriminant Latent Space with Neural Discriminant Analysis. (arXiv:2107.06209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06209</id>
        <link href="http://arxiv.org/abs/2107.06209"/>
        <updated>2021-07-14T01:41:49.831Z</updated>
        <summary type="html"><![CDATA[Discriminative features play an important role in image and object
classification and also in other fields of research such as semi-supervised
learning, fine-grained classification, out of distribution detection. Inspired
by Linear Discriminant Analysis (LDA), we propose an optimization called Neural
Discriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA
transforms deep features to become more discriminative and, therefore, improves
the performances in various tasks. Our proposed optimization has two primary
goals for inter- and intra-class variances. The first one is to minimize
variances within each individual class. The second goal is to maximize pairwise
distances between features coming from different classes. We evaluate our NDA
optimization in different research fields: general supervised classification,
fine-grained classification, semi-supervised learning, and out of distribution
detection. We achieve performance improvements in all the fields compared to
baseline methods that do not use NDA. Besides, using NDA, we also surpass the
state of the art on the four tasks on various testing datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ha_M/0/1/0/all/0/1"&gt;Mai Lan Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1"&gt;Gianni Franchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1"&gt;Emanuel Aldea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanz_V/0/1/0/all/0/1"&gt;Volker Blanz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Parametric Wireframe Extraction Based on Distance Fields. (arXiv:2107.06165v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06165</id>
        <link href="http://arxiv.org/abs/2107.06165"/>
        <updated>2021-07-14T01:41:49.824Z</updated>
        <summary type="html"><![CDATA[We present a pipeline for parametric wireframe extraction from densely
sampled point clouds. Our approach processes a scalar distance field that
represents proximity to the nearest sharp feature curve. In intermediate
stages, it detects corners, constructs curve segmentation, and builds a
topological graph fitted to the wireframe. As an output, we produce parametric
spline curves that can be edited and sampled arbitrarily. We evaluate our
method on 50 complex 3D shapes and compare it to the novel deep learning-based
technique, demonstrating superior quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1"&gt;Albert Matveev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1"&gt;Alexey Artemov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1"&gt;Denis Zorin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05945</id>
        <link href="http://arxiv.org/abs/2107.05945"/>
        <updated>2021-07-14T01:41:49.818Z</updated>
        <summary type="html"><![CDATA[Scene text detection remains a grand challenge due to the variation in text
curvatures, orientations, and aspect ratios. One of the most intractable
problems is how to represent text instances of arbitrary shapes. Although many
state-of-the-art methods have been proposed to model irregular texts in a
flexible manner, most of them lose simplicity and robustness. Their complicated
post-processings and the regression under Dirac delta distribution undermine
the detection performance and the generalization ability. In this paper, we
propose an efficient text instance representation named CentripetalText (CT),
which decomposes text instances into the combination of text kernels and
centripetal shifts. Specifically, we utilize the centripetal shifts to
implement the pixel aggregation, which guide the external text pixels to the
internal text kernels. The relaxation operation is integrated into the dense
regression for centripetal shifts, allowing the correct prediction in a range,
not a specific value. The convenient reconstruction of the text contours and
the tolerance of the prediction errors in our method guarantee the high
detection accuracy and the fast inference speed respectively. Besides, we
shrink our text detector into a proposal generation module, namely
CentripetalText Proposal Network (CPN), replacing SPN in Mask TextSpotter v3
and producing more accurate proposals. To validate the effectiveness of our
designs, we conduct experiments on several commonly used scene text benchmarks,
including both curved and multi-oriented text datasets. For the task of scene
text detection, our approach achieves superior or competitive performance
compared to other existing methods, e.g., F-measure of 86.3% at 40.0 FPS on
Total-Text, F-measure of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of
end-to-end scene text recognition, we outperform Mask TextSpotter v3 by 1.1% on
Total-Text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1"&gt;Tao Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1"&gt;Zhouhui Lian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Partially Overlapping Labels: Image Segmentation under Annotation Shift. (arXiv:2107.05938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05938</id>
        <link href="http://arxiv.org/abs/2107.05938"/>
        <updated>2021-07-14T01:41:49.811Z</updated>
        <summary type="html"><![CDATA[Scarcity of high quality annotated images remains a limiting factor for
training accurate image segmentation models. While more and more annotated
datasets become publicly available, the number of samples in each individual
database is often small. Combining different databases to create larger amounts
of training data is appealing yet challenging due to the heterogeneity as a
result of differences in data acquisition and annotation processes, often
yielding incompatible or even conflicting information. In this paper, we
investigate and propose several strategies for learning from partially
overlapping labels in the context of abdominal organ segmentation. We find that
combining a semi-supervised approach with an adaptive cross entropy loss can
successfully exploit heterogeneously annotated data and substantially improve
segmentation accuracy compared to baseline and alternative approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filbrandt_G/0/1/0/all/0/1"&gt;Gregory Filbrandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1"&gt;Konstantinos Kamnitsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernstein_D/0/1/0/all/0/1"&gt;David Bernstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1"&gt;Alexandra Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale. (arXiv:2107.05840v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05840</id>
        <link href="http://arxiv.org/abs/2107.05840"/>
        <updated>2021-07-14T01:41:49.804Z</updated>
        <summary type="html"><![CDATA[Segmenting 3D cell nuclei from microscopy image volumes is critical for
biological and clinical analysis, enabling the study of cellular expression
patterns and cell lineages. However, current datasets for neuronal nuclei
usually contain volumes smaller than $10^{\text{-}3}\ mm^3$ with fewer than 500
instances per volume, unable to reveal the complexity in large brain regions
and restrict the investigation of neuronal structures. In this paper, we have
pushed the task forward to the sub-cubic millimeter scale and curated the NucMM
dataset with two fully annotated volumes: one $0.1\ mm^3$ electron microscopy
(EM) volume containing nearly the entire zebrafish brain with around 170,000
nuclei; and one $0.25\ mm^3$ micro-CT (uCT) volume containing part of a mouse
visual cortex with about 7,000 nuclei. With two imaging modalities and
significantly increased volume size and instance numbers, we discover a great
diversity of neuronal nuclei in appearance and density, introducing new
challenges to the field. We also perform a statistical analysis to illustrate
those challenges quantitatively. To tackle the challenges, we propose a novel
hybrid-representation learning model that combines the merits of foreground
mask, contour map, and signed distance transform to produce high-quality 3D
masks. The benchmark comparisons on the NucMM dataset show that our proposed
method significantly outperforms state-of-the-art nuclei segmentation
approaches. Code and data are available at
https://connectomics-bazaar.github.io/proj/nucMM/index.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zudi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Donglai Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petkova_M/0/1/0/all/0/1"&gt;Mariela D. Petkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuelong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zergham Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1"&gt;Krishna Swaroop K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1"&gt;Silin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wendt_N/0/1/0/all/0/1"&gt;Nils Wendt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boulanger_Weill_J/0/1/0/all/0/1"&gt;Jonathan Boulanger-Weill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xueying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhanyasi_N/0/1/0/all/0/1"&gt;Nagaraju Dhanyasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arganda_Carreras_I/0/1/0/all/0/1"&gt;Ignacio Arganda-Carreras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engert_F/0/1/0/all/0/1"&gt;Florian Engert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lichtman_J/0/1/0/all/0/1"&gt;Jeff Lichtman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Hanspeter Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Seizure Detection Using the Pulse Transit Time. (arXiv:2107.05894v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05894</id>
        <link href="http://arxiv.org/abs/2107.05894"/>
        <updated>2021-07-14T01:41:49.785Z</updated>
        <summary type="html"><![CDATA[Documentation of epileptic seizures plays an essential role in planning
medical therapy. Solutions for automated epileptic seizure detection can help
improve the current problem of incomplete and erroneous manual documentation of
epileptic seizures. In recent years, a number of wearable sensors have been
tested for this purpose. However, detecting seizures with subtle symptoms
remains difficult and current solutions tend to have a high false alarm rate.
Seizures can also affect the patient's arterial blood pressure, which has not
yet been studied for detection with sensors. The pulse transit time (PTT)
provides a noninvasive estimate of arterial blood pressure. It can be obtained
by using to two sensors, which are measuring the time differences between
arrivals of the pulse waves. Due to separated time chips a clock drift emerges,
which is strongly influencing the PTT. In this work, we present an algorithm
which responds to alterations in the PTT, considering the clock drift and
enabling the noninvasive monitoring of blood pressure alterations using
separated sensors. Furthermore we investigated whether seizures can be detected
using the PTT. Our results indicate that using the algorithm, it is possible to
detect seizures with a Random Forest. Using the PTT along with other signals in
a multimodal approach, the detection of seizures with subtle symptoms could
thereby be improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiege_E/0/1/0/all/0/1"&gt;Eric Fiege&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houta_S/0/1/0/all/0/1"&gt;Salima Houta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisgin_P/0/1/0/all/0/1"&gt;Pinar Bisgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surges_R/0/1/0/all/0/1"&gt;Rainer Surges&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howar_F/0/1/0/all/0/1"&gt;Falk Howar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSR-Net: Multi-Scale Relighting Network for One-to-One Relighting. (arXiv:2107.06125v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06125</id>
        <link href="http://arxiv.org/abs/2107.06125"/>
        <updated>2021-07-14T01:41:49.778Z</updated>
        <summary type="html"><![CDATA[Deep image relighting allows photo enhancement by illumination-specific
retouching without human effort and so it is getting much interest lately. Most
of the existing popular methods available for relighting are run-time intensive
and memory inefficient. Keeping these issues in mind, we propose the use of
Stacked Deep Multi-Scale Hierarchical Network, which aggregates features from
each image at different scales. Our solution is differentiable and robust for
translating image illumination setting from input image to target image.
Additionally, we have also shown that using a multi-step training approach to
this problem with two different loss functions can significantly boost
performance and can achieve a high quality reconstruction of a relighted image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Sourya Dipta Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Nisarg A. Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Saikat Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Person (Probably) Exists. Identity Membership Attacks Against GAN Generated Faces. (arXiv:2107.06018v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06018</id>
        <link href="http://arxiv.org/abs/2107.06018"/>
        <updated>2021-07-14T01:41:49.771Z</updated>
        <summary type="html"><![CDATA[Recently, generative adversarial networks (GANs) have achieved stunning
realism, fooling even human observers. Indeed, the popular tongue-in-cheek
website {\small \url{ this http URL}}, taunts users with
GAN generated images that seem too real to believe. On the other hand, GANs do
leak information about their training data, as evidenced by membership attacks
recently demonstrated in the literature. In this work, we challenge the
assumption that GAN faces really are novel creations, by constructing a
successful membership attack of a new kind. Unlike previous works, our attack
can accurately discern samples sharing the same identity as training samples
without being the same samples. We demonstrate the interest of our attack
across several popular face datasets and GAN training procedures. Notably, we
show that even in the presence of significant dataset diversity, an over
represented person can pose a privacy concern.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Webster_R/0/1/0/all/0/1"&gt;Ryan Webster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabin_J/0/1/0/all/0/1"&gt;Julien Rabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1"&gt;Loic Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1"&gt;Frederic Jurie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Causal Analysis for Conceptual Deep Learning Explanation. (arXiv:2107.06098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.06098</id>
        <link href="http://arxiv.org/abs/2107.06098"/>
        <updated>2021-07-14T01:41:49.763Z</updated>
        <summary type="html"><![CDATA[Model explainability is essential for the creation of trustworthy Machine
Learning models in healthcare. An ideal explanation resembles the
decision-making process of a domain expert and is expressed using concepts or
terminology that is meaningful to the clinicians. To provide such an
explanation, we first associate the hidden units of the classifier to
clinically relevant concepts. We take advantage of radiology reports
accompanying the chest X-ray images to define concepts. We discover sparse
associations between concepts and hidden units using a linear sparse logistic
regression. To ensure that the identified units truly influence the
classifier's outcome, we adopt tools from Causal Inference literature and, more
specifically, mediation analysis through counterfactual interventions. Finally,
we construct a low-depth decision tree to translate all the discovered concepts
into a straightforward decision rule, expressed to the radiologist. We
evaluated our approach on a large chest x-ray dataset, where our model produces
a global explanation consistent with clinical knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1"&gt;Sumedha Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallace_S/0/1/0/all/0/1"&gt;Stephen Wallace&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triantafillou_S/0/1/0/all/0/1"&gt;Sofia Triantafillou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1"&gt;Kayhan Batmanghelich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eProduct: A Million-Scale Visual Search Benchmark to Address Product Recognition Challenges. (arXiv:2107.05856v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05856</id>
        <link href="http://arxiv.org/abs/2107.05856"/>
        <updated>2021-07-14T01:41:49.746Z</updated>
        <summary type="html"><![CDATA[Large-scale product recognition is one of the major applications of computer
vision and machine learning in the e-commerce domain. Since the number of
products is typically much larger than the number of categories of products,
image-based product recognition is often cast as a visual search rather than a
classification problem. It is also one of the instances of super fine-grained
recognition, where there are many products with slight or subtle visual
differences. It has always been a challenge to create a benchmark dataset for
training and evaluation on various visual search solutions in a real-world
setting. This motivated creation of eProduct, a dataset consisting of 2.5
million product images towards accelerating development in the areas of
self-supervised learning, weakly-supervised learning, and multimodal learning,
for fine-grained recognition. We present eProduct as a training set and an
evaluation set, where the training set contains 1.3M+ listing images with
titles and hierarchical category labels, for model development, and the
evaluation set includes 10,000 query and 1.1 million index images for visual
search evaluation. We will present eProduct's construction steps, provide
analysis about its diversity and cover the performance of baseline models
trained on it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiangbo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_A/0/1/0/all/0/1"&gt;An-Ti Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haro_A/0/1/0/all/0/1"&gt;Antonio Haro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Force-in-domain GAN inversion. (arXiv:2107.06050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06050</id>
        <link href="http://arxiv.org/abs/2107.06050"/>
        <updated>2021-07-14T01:41:49.733Z</updated>
        <summary type="html"><![CDATA[Empirical works suggest that various semantics emerge in the latent space of
Generative Adversarial Networks (GANs) when being trained to generate images.
To perform real image editing, it requires an accurate mapping from the real
image to the latent space to leveraging these learned semantics, which is
important yet difficult. An in-domain GAN inversion approach is recently
proposed to constraint the inverted code within the latent space by forcing the
reconstructed image obtained from the inverted code within the real image
space. Empirically, we find that the inverted code by the in-domain GAN can
deviate from the latent space significantly. To solve this problem, we propose
a force-in-domain GAN based on the in-domain GAN, which utilizes a
discriminator to force the inverted code within the latent space. The
force-in-domain GAN can also be interpreted by a cycle-GAN with slight
modification. Extensive experiments show that our force-in-domain GAN not only
reconstructs the target image at the pixel level, but also align the inverted
code with the latent space well for semantic editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leng_G/0/1/0/all/0/1"&gt;Guangjie Leng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yeku Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhi-Qin John Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-07-14T01:41:49.726Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PU-Flow: a Point Cloud Upsampling Networkwith Normalizing Flows. (arXiv:2107.05893v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05893</id>
        <link href="http://arxiv.org/abs/2107.05893"/>
        <updated>2021-07-14T01:41:49.718Z</updated>
        <summary type="html"><![CDATA[Point cloud upsampling aims to generate dense point clouds from given sparse
ones, which is a challenging task due to the irregular and unordered nature of
point sets. To address this issue, we present a novel deep learning-based
model, called PU-Flow,which incorporates normalizing flows and feature
interpolation techniques to produce dense points uniformly distributed on the
underlying surface. Specifically, we formulate the upsampling process as point
interpolation in a latent space, where the interpolation weights are adaptively
learned from local geometric context, and exploit the invertible
characteristics of normalizing flows to transform points between Euclidean and
latent spaces. We evaluate PU-Flow on a wide range of 3D models with sharp
features and high-frequency details. Qualitative and quantitative results show
that our method outperforms state-of-the-art deep learning-based approaches in
terms of reconstruction quality, proximity-to-surface accuracy, and computation
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1"&gt;Aihua Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zihui Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1"&gt;Yaqi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong-jin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Ying He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region attention and graph embedding network for occlusion objective class-based micro-expression recognition. (arXiv:2107.05904v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05904</id>
        <link href="http://arxiv.org/abs/2107.05904"/>
        <updated>2021-07-14T01:41:49.705Z</updated>
        <summary type="html"><![CDATA[Micro-expression recognition (\textbf{MER}) has attracted lots of
researchers' attention in a decade. However, occlusion will occur for MER in
real-world scenarios. This paper deeply investigates an interesting but
unexplored challenging issue in MER, \ie, occlusion MER. First, to research MER
under real-world occlusion, synthetic occluded micro-expression databases are
created by using various mask for the community. Second, to suppress the
influence of occlusion, a \underline{R}egion-inspired \underline{R}elation
\underline{R}easoning \underline{N}etwork (\textbf{RRRN}) is proposed to model
relations between various facial regions. RRRN consists of a backbone network,
the Region-Inspired (\textbf{RI}) module and Relation Reasoning (\textbf{RR})
module. More specifically, the backbone network aims at extracting feature
representations from different facial regions, RI module computing an adaptive
weight from the region itself based on attention mechanism with respect to the
unobstructedness and importance for suppressing the influence of occlusion, and
RR module exploiting the progressive interactions among these regions by
performing graph convolutions. Experiments are conducted on handout-database
evaluation and composite database evaluation tasks of MEGC 2018 protocol.
Experimental results show that RRRN can significantly explore the importance of
facial regions and capture the cooperative complementary relationship of facial
regions for MER. The results also demonstrate RRRN outperforms the
state-of-the-art approaches, especially on occlusion, and RRRN acts more robust
to occlusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1"&gt;Qirong Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Ling Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenming Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1"&gt;Xiuyan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaohua Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer. (arXiv:2107.05887v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05887</id>
        <link href="http://arxiv.org/abs/2107.05887"/>
        <updated>2021-07-14T01:41:49.692Z</updated>
        <summary type="html"><![CDATA[We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for
object detection from a sequence of temporal frames. We treat the temporal
frames as sequences in both space and time and employ the full attention
mechanisms to take advantage of the features correlations over both dimensions.
This treatment enables us to deal with frames sequence as temporal object
features traces over every location in the space. We explore two possible
approaches; the early spatial features aggregation over the temporal dimension,
and the late temporal aggregation of object query spatial features. Moreover,
we propose a novel Temporal Positional Embedding technique to encode the time
sequence information. To evaluate our approach, we choose the Moving Object
Detection (MOD)task, since it is a perfect candidate to showcase the importance
of the temporal dimension. Results show a significant 5% mAP improvement on the
KITTI MOD dataset over the 1-step spatial baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Building a Food Knowledge Graph for Internet of Food. (arXiv:2107.05869v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05869</id>
        <link href="http://arxiv.org/abs/2107.05869"/>
        <updated>2021-07-14T01:41:49.683Z</updated>
        <summary type="html"><![CDATA[Background: The deployment of various networks (e.g., Internet of Things
(IoT) and mobile networks) and databases (e.g., nutrition tables and food
compositional databases) in the food system generates massive information silos
due to the well-known data harmonization problem. The food knowledge graph
provides a unified and standardized conceptual terminology and their
relationships in a structured form and thus can transform these information
silos across the whole food system to a more reusable globally digitally
connected Internet of Food, enabling every stage of the food system from
farm-to-fork.

Scope and approach: We review the evolution of food knowledge organization,
from food classification, food ontology to food knowledge graphs. We then
discuss the progress in food knowledge graphs from several representative
applications. We finally discuss the main challenges and future directions.

Key findings and conclusions: Our comprehensive summary of current research
on food knowledge graphs shows that food knowledge graphs play an important
role in food-oriented applications, including food search and Question
Answering (QA), personalized dietary recommendation, food analysis and
visualization, food traceability, and food machinery intelligent manufacturing.
Future directions for food knowledge graphs cover several fields such as
multimodal food knowledge graphs and food intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1"&gt;Weiqing Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chunlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuqiang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and Defense Against Adversarial Examples in Deep Learning using Natural Scene Statistics and Adaptive Denoising. (arXiv:2107.05780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05780</id>
        <link href="http://arxiv.org/abs/2107.05780"/>
        <updated>2021-07-14T01:41:49.666Z</updated>
        <summary type="html"><![CDATA[Despite the enormous performance of deepneural networks (DNNs), recent
studies have shown theirvulnerability to adversarial examples (AEs), i.e.,
care-fully perturbed inputs designed to fool the targetedDNN. Currently, the
literature is rich with many ef-fective attacks to craft such AEs. Meanwhile,
many de-fenses strategies have been developed to mitigate thisvulnerability.
However, these latter showed their effec-tiveness against specific attacks and
does not general-ize well to different attacks. In this paper, we proposea
framework for defending DNN classifier against ad-versarial samples. The
proposed method is based on atwo-stage framework involving a separate detector
anda denoising block. The detector aims to detect AEs bycharacterizing them
through the use of natural scenestatistic (NSS), where we demonstrate that
these statis-tical features are altered by the presence of
adversarialperturbations. The denoiser is based on block matching3D (BM3D)
filter fed by an optimum threshold valueestimated by a convolutional neural
network (CNN) toproject back the samples detected as AEs into theirdata
manifold. We conducted a complete evaluation onthree standard datasets namely
MNIST, CIFAR-10 andTiny-ImageNet. The experimental results show that
theproposed defense method outperforms the state-of-the-art defense techniques
by improving the robustnessagainst a set of attacks under black-box, gray-box
and white-box settings. The source code is available at:
https://github.com/kherchouche-anouar/2DAE]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kherchouche_A/0/1/0/all/0/1"&gt;Anouar Kherchouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fezza_S/0/1/0/all/0/1"&gt;Sid Ahmed Fezza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Identity-Aware Image Steganography via Minimax Optimization. (arXiv:2107.05819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05819</id>
        <link href="http://arxiv.org/abs/2107.05819"/>
        <updated>2021-07-14T01:41:49.617Z</updated>
        <summary type="html"><![CDATA[High-capacity image steganography, aimed at concealing a secret image in a
cover image, is a technique to preserve sensitive data, e.g., faces and
fingerprints. Previous methods focus on the security during transmission and
subsequently run a risk of privacy leakage after the restoration of secret
images at the receiving end. To address this issue, we propose a framework,
called Multitask Identity-Aware Image Steganography (MIAIS), to achieve direct
recognition on container images without restoring secret images. The key issue
of the direct recognition is to preserve identity information of secret images
into container images and make container images look similar to cover images at
the same time. Thus, we introduce a simple content loss to preserve the
identity information, and design a minimax optimization to deal with the
contradictory aspects. We demonstrate that the robustness results can be
transferred across different cover datasets. In order to be flexible for the
secret image restoration in some cases, we incorporate an optional restoration
network into our method, providing a multitask framework. The experiments under
the multitask scenario show the effectiveness of our framework compared with
other visual information hiding methods and state-of-the-art high-capacity
image steganography methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jiabao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Liangli Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1"&gt;Cuizhu Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1"&gt;Jupeng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms. (arXiv:2107.05637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05637</id>
        <link href="http://arxiv.org/abs/2107.05637"/>
        <updated>2021-07-14T01:41:49.577Z</updated>
        <summary type="html"><![CDATA[Self-Attention has become prevalent in computer vision models. Inspired by
fully connected Conditional Random Fields (CRFs), we decompose it into local
and context terms. They correspond to the unary and binary terms in CRF and are
implemented by attention mechanisms with projection matrices. We observe that
the unary terms only make small contributions to the outputs, and meanwhile
standard CNNs that rely solely on the unary terms achieve great performances on
a variety of tasks. Therefore, we propose Locally Enhanced Self-Attention
(LESA), which enhances the unary term by incorporating it with convolutions,
and utilizes a fusion module to dynamically couple the unary and binary
operations. In our experiments, we replace the self-attention modules with
LESA. The results on ImageNet and COCO show the superiority of LESA over
convolution and self-attention baselines for the tasks of image recognition,
object detection, and instance segmentation. The code is made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenglin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1"&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Explicit Neural View Synthesis. (arXiv:2107.05775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05775</id>
        <link href="http://arxiv.org/abs/2107.05775"/>
        <updated>2021-07-14T01:41:49.558Z</updated>
        <summary type="html"><![CDATA[We study the problem of novel view synthesis of a scene comprised of 3D
objects. We propose a simple yet effective approach that is neither continuous
nor implicit, challenging recent trends on view synthesis. We demonstrate that
although continuous radiance field representations have gained a lot of
attention due to their expressive power, our simple approach obtains comparable
or even better novel view reconstruction quality comparing with
state-of-the-art baselines while increasing rendering speed by over 400x. Our
model is trained in a category-agnostic manner and does not require
scene-specific optimization. Therefore, it is able to generalize novel view
synthesis to object categories not seen during training. In addition, we show
that with our simple formulation, we can use view synthesis as a
self-supervision signal for efficient learning of 3D geometry without explicit
3D supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1"&gt;Pengsheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bautista_M/0/1/0/all/0/1"&gt;Miguel Angel Bautista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colburn_A/0/1/0/all/0/1"&gt;Alex Colburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulbricht_D/0/1/0/all/0/1"&gt;Daniel Ulbricht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1"&gt;Joshua M. Susskind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1"&gt;Qi Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Parser: Representing Part-whole Hierarchies with Transformers. (arXiv:2107.05790v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05790</id>
        <link href="http://arxiv.org/abs/2107.05790"/>
        <updated>2021-07-14T01:41:49.550Z</updated>
        <summary type="html"><![CDATA[Human vision is able to capture the part-whole hierarchical information from
the entire scene. This paper presents the Visual Parser (ViP) that explicitly
constructs such a hierarchy with transformers. ViP divides visual
representations into two levels, the part level and the whole level.
Information of each part represents a combination of several independent
vectors within the whole. To model the representations of the two levels, we
first encode the information from the whole into part vectors through an
attention mechanism, then decode the global information within the part vectors
back into the whole representation. By iteratively parsing the two levels with
the proposed encoder-decoder interaction, the model can gradually refine the
features on both levels. Experimental results demonstrate that ViP can achieve
very competitive performance on three major tasks e.g. classification,
detection and instance segmentation. In particular, it can surpass the previous
state-of-the-art CNN backbones by a large margin on object detection. The tiny
model of the ViP family with $7.2\times$ fewer parameters and $10.9\times$
fewer FLOPS can perform comparably with the largest model
ResNeXt-101-64$\times$4d of ResNe(X)t family. Visualization results also
demonstrate that the learnt parts are highly informative of the predicting
class, making ViP more explainable than previous fundamental architectures.
Code is available at https://github.com/kevin-ssy/ViP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun%2A_S/0/1/0/all/0/1"&gt;Shuyang Sun*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue%2A_X/0/1/0/all/0/1"&gt;Xiaoyu Yue*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect and Locate: A Face Anti-Manipulation Approach with Semantic and Noise-level Supervision. (arXiv:2107.05821v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05821</id>
        <link href="http://arxiv.org/abs/2107.05821"/>
        <updated>2021-07-14T01:41:49.543Z</updated>
        <summary type="html"><![CDATA[The technological advancements of deep learning have enabled sophisticated
face manipulation schemes, raising severe trust issues and security concerns in
modern society. Generally speaking, detecting manipulated faces and locating
the potentially altered regions are challenging tasks. Herein, we propose a
conceptually simple but effective method to efficiently detect forged faces in
an image while simultaneously locating the manipulated regions. The proposed
scheme relies on a segmentation map that delivers meaningful high-level
semantic information clues about the image. Furthermore, a noise map is
estimated, playing a complementary role in capturing low-level clues and
subsequently empowering decision-making. Finally, the features from these two
modules are combined to distinguish fake faces. Extensive experiments show that
the proposed model achieves state-of-the-art detection accuracy and remarkable
localization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1"&gt;Chenqi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1"&gt;Anderson Rocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1"&gt;Sam Kwong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AlterSGD: Finding Flat Minima for Continual Learning by Alternative Training. (arXiv:2107.05804v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05804</id>
        <link href="http://arxiv.org/abs/2107.05804"/>
        <updated>2021-07-14T01:41:49.519Z</updated>
        <summary type="html"><![CDATA[Deep neural networks suffer from catastrophic forgetting when learning
multiple knowledge sequentially, and a growing number of approaches have been
proposed to mitigate this problem. Some of these methods achieved considerable
performance by associating the flat local minima with forgetting mitigation in
continual learning. However, they inevitably need (1) tedious hyperparameters
tuning, and (2) additional computational cost. To alleviate these problems, in
this paper, we propose a simple yet effective optimization method, called
AlterSGD, to search for a flat minima in the loss landscape. In AlterSGD, we
conduct gradient descent and ascent alternatively when the network tends to
converge at each session of learning new knowledge. Moreover, we theoretically
prove that such a strategy can encourage the optimization to converge to a flat
minima. We verify AlterSGD on continual learning benchmark for semantic
segmentation and the empirical results show that we can significantly mitigate
the forgetting and outperform the state-of-the-art methods with a large margin
under challenging continual learning protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kit-Net: Self-Supervised Learning to Kit Novel 3D Objects into Novel 3D Cavities. (arXiv:2107.05789v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.05789</id>
        <link href="http://arxiv.org/abs/2107.05789"/>
        <updated>2021-07-14T01:41:49.511Z</updated>
        <summary type="html"><![CDATA[In industrial part kitting, 3D objects are inserted into cavities for
transportation or subsequent assembly. Kitting is a critical step as it can
decrease downstream processing and handling times and enable lower storage and
shipping costs. We present Kit-Net, a framework for kitting previously unseen
3D objects into cavities given depth images of both the target cavity and an
object held by a gripper in an unknown initial orientation. Kit-Net uses
self-supervised deep learning and data augmentation to train a convolutional
neural network (CNN) to robustly estimate 3D rotations between objects and
matching concave or convex cavities using a large training dataset of simulated
depth images pairs. Kit-Net then uses the trained CNN to implement a controller
to orient and position novel objects for insertion into novel prismatic and
conformal 3D cavities. Experiments in simulation suggest that Kit-Net can
orient objects to have a 98.9% average intersection volume between the object
mesh and that of the target cavity. Physical experiments with industrial
objects succeed in 18% of trials using a baseline method and in 63% of trials
with Kit-Net. Video, code, and data are available at
https://github.com/BerkeleyAutomation/Kit-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devgon_S/0/1/0/all/0/1"&gt;Shivin Devgon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danielczuk_M/0/1/0/all/0/1"&gt;Michael Danielczuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shirin Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocha_E/0/1/0/all/0/1"&gt;Eduardo M. C. Rocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solowjow_E/0/1/0/all/0/1"&gt;Eugen Solowjow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Distribution of Edge Intelligence at the Node Level for Internet of Things. (arXiv:2107.05828v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05828</id>
        <link href="http://arxiv.org/abs/2107.05828"/>
        <updated>2021-07-14T01:41:49.503Z</updated>
        <summary type="html"><![CDATA[In this paper, dynamic deployment of Convolutional Neural Network (CNN)
architecture is proposed utilizing only IoT-level devices. By partitioning and
pipelining the CNN, it horizontally distributes the computation load among
resource-constrained devices (called horizontal collaboration), which in turn
increases the throughput. Through partitioning, we can decrease the computation
and energy consumption on individual IoT devices and increase the throughput
without sacrificing accuracy. Also, by processing the data at the generation
point, data privacy can be achieved. The results show that throughput can be
increased by 1.55x to 1.75x for sharing the CNN into two and three
resource-constrained devices, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1"&gt;Hawzhin Mohammed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1"&gt;Tolulope A. Odetola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Nan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1"&gt;Syed Rafay Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11431</id>
        <link href="http://arxiv.org/abs/2101.11431"/>
        <updated>2021-07-14T01:41:49.489Z</updated>
        <summary type="html"><![CDATA[In today's digital world, there is an increasing focus on soft skills. On the
one hand, they facilitate innovation at companies, but on the other, they are
unlikely to be automated soon. Researchers struggle with accurately approaching
quantitatively the study of soft skills due to the lack of data-driven methods
to retrieve them. This limits the possibility for psychologists and HR managers
to understand the relation between humans and digitalisation. This paper
presents SkillNER, a novel data-driven method for automatically extracting soft
skills from text. It is a named entity recognition (NER) system trained with a
support vector machine (SVM) on a corpus of more than 5000 scientific papers.
We developed this system by measuring the performance of our approach against
different training models and validating the results together with a team of
psychologists. Finally, SkillNER was tested in a real-world case study using
the job descriptions of ESCO (European Skill/Competence Qualification and
Occupation) as textual source. The system enabled the detection of communities
of job profiles based on their shared soft skills and communities of soft
skills based on their shared job profiles. This case study demonstrates that
the tool can automatically retrieve soft skills from a large corpus in an
efficient way, proving useful for firms, institutions, and workers. The tool is
open and available online to foster quantitative methods for the study of soft
skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1"&gt;Silvia Fareri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1"&gt;Nicola Melluso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1"&gt;Filippo Chiarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1"&gt;Gualtiero Fantoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FastSeq: Make Sequence Generation Faster. (arXiv:2106.04718v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04718</id>
        <link href="http://arxiv.org/abs/2106.04718"/>
        <updated>2021-07-14T01:41:49.481Z</updated>
        <summary type="html"><![CDATA[Transformer-based models have made tremendous impacts in natural language
generation. However the inference speed is a bottleneck due to large model size
and intensive computing involved in auto-regressive decoding process. We
develop FastSeq framework to accelerate sequence generation without accuracy
loss. The proposed optimization techniques include an attention cache
optimization, an efficient algorithm for detecting repeated n-grams, and an
asynchronous generation pipeline with parallel I/O. These optimizations are
general enough to be applicable to Transformer-based models (e.g., T5, GPT2,
and UniLM). Our benchmark results on a set of widely used and diverse models
demonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use
with a simple one-line code change. The source code is available at
https://github.com/microsoft/fastseq.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1"&gt;Fei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhendawade_N/0/1/0/all/0/1"&gt;Nikhil Bhendawade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Ting Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_D/0/1/0/all/0/1"&gt;Desheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_B/0/1/0/all/0/1"&gt;Bingyu Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design and Challenges of Cloze-Style Reading Comprehension Tasks on Multiparty Dialogue. (arXiv:1911.00773v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00773</id>
        <link href="http://arxiv.org/abs/1911.00773"/>
        <updated>2021-07-14T01:41:49.461Z</updated>
        <summary type="html"><![CDATA[This paper analyzes challenges in cloze-style reading comprehension on
multiparty dialogue and suggests two new tasks for more comprehensive
predictions of personal entities in daily conversations. We first demonstrate
that there are substantial limitations to the evaluation methods of previous
work, namely that randomized assignment of samples to training and test data
substantially decreases the complexity of cloze-style reading comprehension.
According to our analysis, replacing the random data split with a chronological
data split reduces test accuracy on previous single-variable passage completion
task from 72\% to 34\%, that leaves much more room to improve. Our proposed
tasks extend the previous single-variable passage completion task by replacing
more character mentions with variables. Several deep learning models are
developed to validate these three tasks. A thorough error analysis is provided
to understand the challenges and guide the future direction of this research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Changmao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all networks. (arXiv:2107.05747v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05747</id>
        <link href="http://arxiv.org/abs/2107.05747"/>
        <updated>2021-07-14T01:41:49.454Z</updated>
        <summary type="html"><![CDATA[State-of-the-art artificial neural networks (ANNs) require labelled data or
feedback between layers, are often biologically implausible, and are vulnerable
to adversarial attacks that humans are not susceptible to. On the other hand,
Hebbian learning in winner-take-all (WTA) networks, is unsupervised,
feed-forward, and biologically plausible. However, an objective optimization
theory for WTA networks has been missing, except under very limiting
assumptions. Here we derive formally such a theory, based on biologically
plausible but generic ANN elements. Through Hebbian learning, network
parameters maintain a Bayesian generative model of the data. There is no
supervisory loss function, but the network does minimize cross-entropy between
its activations and the input distribution. The key is a "soft" WTA where there
is no absolute "hard" winner neuron, and a specific type of Hebbian-like
plasticity of weights and biases. We confirm our theory in practice, where, in
handwritten digit (MNIST) recognition, our Hebbian algorithm, SoftHebb,
minimizes cross-entropy without having access to it, and outperforms the more
frequently used, hard-WTA-based method. Strikingly, it even outperforms
supervised end-to-end backpropagation, under certain conditions. Specifically,
in a two-layered network, SoftHebb outperforms backpropagation when the
training dataset is only presented once, when the testing data is noisy, and
under gradient-based adversarial attacks. Adversarial attacks that confuse
SoftHebb are also confusing to the human eye. Finally, the model can generate
interpolations of objects from its input distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moraitis_T/0/1/0/all/0/1"&gt;Timoleon Moraitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toichkin_D/0/1/0/all/0/1"&gt;Dmitry Toichkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_Y/0/1/0/all/0/1"&gt;Yansong Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qinghai Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Neural Coreference Resolution Revisited: A Simple yet Effective Baseline. (arXiv:2107.01700v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01700</id>
        <link href="http://arxiv.org/abs/2107.01700"/>
        <updated>2021-07-14T01:41:49.447Z</updated>
        <summary type="html"><![CDATA[Since the first end-to-end neural coreference resolution model was
introduced, many extensions to the model have been proposed, ranging from using
higher-order inference to directly optimizing evaluation metrics using
reinforcement learning. Despite improving the coreference resolution
performance by a large margin, these extensions add a lot of extra complexity
to the original model. Motivated by this observation and the recent advances in
pre-trained Transformer language models, we propose a simple yet effective
baseline for coreference resolution. Our model is a simplified version of the
original neural coreference resolution model, however, it achieves impressive
performance, outperforming all recent extended works on the public English
OntoNotes benchmark. Our work provides evidence for the necessity of carefully
justifying the complexity of existing or newly proposed models, as introducing
a conceptual or practical simplification to an existing model can still yield
competitive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1"&gt;Tuan Manh Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doo Soon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effect of Domain and Diacritics in Yor\`ub\'a-English Neural Machine Translation. (arXiv:2103.08647v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08647</id>
        <link href="http://arxiv.org/abs/2103.08647"/>
        <updated>2021-07-14T01:41:49.440Z</updated>
        <summary type="html"><![CDATA[Massively multilingual machine translation (MT) has shown impressive
capabilities, including zero and few-shot translation between low-resource
language pairs. However, these models are often evaluated on high-resource
languages with the assumption that they generalize to low-resource ones. The
difficulty of evaluating MT models on low-resource pairs is often due to lack
of standardized evaluation datasets. In this paper, we present MENYO-20k, the
first multi-domain parallel corpus with a special focus on clean orthography
for Yor\`ub\'a--English with standardized train-test splits for benchmarking.
We provide several neural MT benchmarks and compare them to the performance of
popular pre-trained (massively multilingual) MT models both for the
heterogeneous test set and its subdomains. Since these pre-trained models use
huge amounts of data with uncertain quality, we also analyze the effect of
diacritics, a major characteristic of Yor\`ub\'a, in the training data. We
investigate how and when this training condition affects the final quality and
intelligibility of a translation. Our models outperform massively multilingual
models such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when
translating to Yor\`ub\'a, setting a high quality benchmark for future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1"&gt;David I. Adelani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1"&gt;Dana Ruiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1"&gt;Jesujoba O. Alabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adebonojo_D/0/1/0/all/0/1"&gt;Damilola Adebonojo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayeni_A/0/1/0/all/0/1"&gt;Adesina Ayeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeyemi_M/0/1/0/all/0/1"&gt;Mofe Adeyemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awokoya_A/0/1/0/all/0/1"&gt;Ayodele Awokoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1"&gt;Cristina Espa&amp;#xf1;a-Bonet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParsiNLU: A Suite of Language Understanding Challenges for Persian. (arXiv:2012.06154v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06154</id>
        <link href="http://arxiv.org/abs/2012.06154"/>
        <updated>2021-07-14T01:41:49.433Z</updated>
        <summary type="html"><![CDATA[Despite the progress made in recent years in addressing natural language
understanding (NLU) challenges, the majority of this progress remains to be
concentrated on resource-rich languages like English. This work focuses on
Persian language, one of the widely spoken languages in the world, and yet
there are few NLU datasets available for this rich language. The availability
of high-quality evaluation datasets is a necessity for reliable assessment of
the progress on different NLU tasks and domains. We introduce ParsiNLU, the
first benchmark in Persian language that includes a range of high-level tasks
-- Reading Comprehension, Textual Entailment, etc. These datasets are collected
in a multitude of ways, often involving manual annotations by native speakers.
This results in over 14.5$k$ new instances across 6 distinct NLU tasks.
Besides, we present the first results on state-of-the-art monolingual and
multi-lingual pre-trained language-models on this benchmark and compare them
with human performance, which provides valuable insights into our ability to
tackle natural language understanding challenges in Persian. We hope ParsiNLU
fosters further research and advances in Persian language understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1"&gt;Siamak Shakeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1"&gt;Pedram Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1"&gt;Pouya Pezeshkpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1"&gt;Malihe Alikhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aminnaseri_M/0/1/0/all/0/1"&gt;Moin Aminnaseri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bitaab_M/0/1/0/all/0/1"&gt;Marzieh Bitaab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1"&gt;Faeze Brahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1"&gt;Sarik Ghazarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1"&gt;Mozhdeh Gheini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabiri_A/0/1/0/all/0/1"&gt;Arman Kabiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1"&gt;Rabeeh Karimi Mahabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Memarrast_O/0/1/0/all/0/1"&gt;Omid Memarrast&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mosallanezhad_A/0/1/0/all/0/1"&gt;Ahmadreza Mosallanezhad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noury_E/0/1/0/all/0/1"&gt;Erfan Noury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raji_S/0/1/0/all/0/1"&gt;Shahab Raji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1"&gt;Mohammad Sadegh Rasooli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghi_S/0/1/0/all/0/1"&gt;Sepideh Sadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azer_E/0/1/0/all/0/1"&gt;Erfan Sadeqi Azer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samghabadi_N/0/1/0/all/0/1"&gt;Niloofar Safi Samghabadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1"&gt;Mahsa Shafaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheybani_S/0/1/0/all/0/1"&gt;Saber Sheybani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1"&gt;Ali Tazarv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1"&gt;Yadollah Yaghoobzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDCNet-Multires: Effective Receptive Field Guided Multiresolution CNN for Dense Prediction. (arXiv:2107.05634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05634</id>
        <link href="http://arxiv.org/abs/2107.05634"/>
        <updated>2021-07-14T01:41:49.412Z</updated>
        <summary type="html"><![CDATA[Dense optical flow estimation is challenging when there are large
displacements in a scene with heterogeneous motion dynamics, occlusion, and
scene homogeneity. Traditional approaches to handle these challenges include
hierarchical and multiresolution processing methods. Learning-based optical
flow methods typically use a multiresolution approach with image warping when a
broad range of flow velocities and heterogeneous motion is present. Accuracy of
such coarse-to-fine methods is affected by the ghosting artifacts when images
are warped across multiple resolutions and by the vanishing problem in smaller
scene extents with higher motion contrast. Previously, we devised strategies
for building compact dense prediction networks guided by the effective
receptive field (ERF) characteristics of the network (DDCNet). The DDCNet
design was intentionally simple and compact allowing it to be used as a
building block for designing more complex yet compact networks. In this work,
we extend the DDCNet strategies to handle heterogeneous motion dynamics by
cascading DDCNet based sub-nets with decreasing extents of their ERF. Our
DDCNet with multiresolution capability (DDCNet-Multires) is compact without any
specialized network layers. We evaluate the performance of the DDCNet-Multires
network using standard optical flow benchmark datasets. Our experiments
demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and
provides optical flow estimates with accuracy comparable to similar lightweight
learning-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1"&gt;Ali Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1"&gt;Madhusudhanan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05680</id>
        <link href="http://arxiv.org/abs/2107.05680"/>
        <updated>2021-07-14T01:41:49.405Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) are commonly used for modeling complex
distributions of data. Both the generators and discriminators of GANs are often
modeled by neural networks, posing a non-transparent optimization problem which
is non-convex and non-concave over the generator and discriminator,
respectively. Such networks are often heuristically optimized with gradient
descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in
practice. In this work, we analyze the training of Wasserstein GANs with
two-layer neural network discriminators through the lens of convex duality, and
for a variety of generators expose the conditions under which Wasserstein GANs
can be solved exactly with convex optimization approaches, or can be
represented as convex-concave games. Using this convex duality interpretation,
we further demonstrate the impact of different activation functions of the
discriminator. Our observations are verified with numerical results
demonstrating the power of the convex interpretation, with applications in
progressive training of convex architectures corresponding to linear generators
and quadratic-activation discriminators for CelebA image generation. The code
for our experiments is available at https://github.com/ardasahiner/ProCoGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1"&gt;Arda Sahiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1"&gt;Tolga Ergen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1"&gt;Batu Ozturkler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartan_B/0/1/0/all/0/1"&gt;Burak Bartan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1"&gt;John Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1"&gt;Morteza Mardani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure. (arXiv:2101.00387v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00387</id>
        <link href="http://arxiv.org/abs/2101.00387"/>
        <updated>2021-07-14T01:41:49.397Z</updated>
        <summary type="html"><![CDATA[In recent times, BERT based transformer models have become an inseparable
part of the 'tech stack' of text processing models. Similar progress is being
observed in the speech domain with a multitude of models observing
state-of-the-art results by using audio transformer models to encode speech.
This begs the question of what are these audio transformer models learning.
Moreover, although the standard methodology is to choose the last layer
embedding for any downstream task, but is it the optimal choice? We try to
answer these questions for the two recent audio transformer models, Mockingjay
and wave2vec2.0. We compare them on a comprehensive set of language delivery
and structure features including audio, fluency and pronunciation features.
Additionally, we probe the audio models' understanding of textual surface,
syntax, and semantic features and compare them to BERT. We do this over
exhaustive settings for native, non-native, synthetic, read and spontaneous
speech datasets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Jui Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1"&gt;Yaman Kumar Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Changyou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1"&gt;Rajiv Ratn Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05541</id>
        <link href="http://arxiv.org/abs/2107.05541"/>
        <updated>2021-07-14T01:41:49.390Z</updated>
        <summary type="html"><![CDATA[Chatbots are intelligent software built to be used as a replacement for human
interaction. However, existing studies typically do not provide enough support
for low-resource languages like Bangla. Moreover, due to the increasing
popularity of social media, we can also see the rise of interactions in Bangla
transliteration (mostly in English) among the native Bangla speakers. In this
paper, we propose a novel approach to build a Bangla chatbot aimed to be used
as a business assistant which can communicate in Bangla and Bangla
Transliteration in English with high confidence consistently. Since annotated
data was not available for this purpose, we had to work on the whole machine
learning life cycle (data preparation, machine learning modeling, and model
deployment) using Rasa Open Source Framework, fastText embeddings, Polyglot
embeddings, Flask, and other systems as building blocks. While working with the
skewed annotated dataset, we try out different setups and pipelines to evaluate
which works best and provide possible reasoning behind the observed results.
Finally, we present a pipeline for intent classification and entity extraction
which achieves reasonable performance (accuracy: 83.02\%, precision: 80.82\%,
recall: 83.02\%, F1-score: 80\%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1"&gt;Fahim Shahriar Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1"&gt;Mueeze Al Mushabbir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1"&gt;Mohammad Sabik Irbaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1"&gt;MD Abdullah Al Nasim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dialogue-based Information Extraction System for Medical Insurance Assessment. (arXiv:2107.05866v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05866</id>
        <link href="http://arxiv.org/abs/2107.05866"/>
        <updated>2021-07-14T01:41:49.383Z</updated>
        <summary type="html"><![CDATA[In the Chinese medical insurance industry, the assessor's role is essential
and requires significant efforts to converse with the claimant. This is a
highly professional job that involves many parts, such as identifying personal
information, collecting related evidence, and making a final insurance report.
Due to the coronavirus (COVID-19) pandemic, the previous offline insurance
assessment has to be conducted online. However, for the junior assessor often
lacking practical experience, it is not easy to quickly handle such a complex
online procedure, yet this is important as the insurance company needs to
decide how much compensation the claimant should receive based on the
assessor's feedback. In order to promote assessors' work efficiency and speed
up the overall procedure, in this paper, we propose a dialogue-based
information extraction system that integrates advanced NLP technologies for
medical insurance assessment. With the assistance of our system, the average
time cost of the procedure is reduced from 55 minutes to 35 minutes, and the
total human resources cost is saved 30% compared with the previous offline
procedure. Until now, the system has already served thousands of online claim
cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shuang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengdi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Minghui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1"&gt;Haitao Mi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shaosheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1"&gt;Zujie Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Teng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongbin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Affect Expression Behaviour Analysis in the Wild using Consensual Collaborative Training. (arXiv:2107.05736v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05736</id>
        <link href="http://arxiv.org/abs/2107.05736"/>
        <updated>2021-07-14T01:41:49.362Z</updated>
        <summary type="html"><![CDATA[Facial expression recognition (FER) in the wild is crucial for building
reliable human-computer interactive systems. However, annotations of large
scale datasets in FER has been a key challenge as these datasets suffer from
noise due to various factors like crowd sourcing, subjectivity of annotators,
poor quality of images, automatic labelling based on key word search etc. Such
noisy annotations impede the performance of FER due to the memorization ability
of deep networks. During early learning stage, deep networks fit on clean data.
Then, eventually, they start overfitting on noisy labels due to their
memorization ability, which limits FER performance. This report presents
Consensual Collaborative Training (CCT) framework used in our submission to
expression recognition track of the Affective Behaviour Analysis in-the-wild
(ABAW) 2021 competition. CCT co-trains three networks jointly using a convex
combination of supervision loss and consistency loss, without making any
assumption about the noise distribution. A dynamic transition mechanism is used
to move from supervision loss in early learning to consistency loss for
consensus of predictions among networks in the later stage. Co-training reduces
overall error, and consistency loss prevents overfitting to noisy samples. The
performance of the model is validated on challenging Aff-Wild2 dataset for
categorical expression classification. Our code is made publicly available at
https://github.com/1980x/ABAW2021DMACS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1"&gt;Darshan Gera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1"&gt;S Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indian Legal NLP Benchmarks : A Survey. (arXiv:2107.06056v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06056</id>
        <link href="http://arxiv.org/abs/2107.06056"/>
        <updated>2021-07-14T01:41:49.355Z</updated>
        <summary type="html"><![CDATA[Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalamkar_P/0/1/0/all/0/1"&gt;Prathamesh Kalamkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+D%2E_J/0/1/0/all/0/1"&gt;Janani Venugopalan Ph.D.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+D_V/0/1/0/all/0/1"&gt;Vivek Raghavan Ph.D&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Atlas Building with Hierarchical Priors for Subject-specific Regularization. (arXiv:2107.05698v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05698</id>
        <link href="http://arxiv.org/abs/2107.05698"/>
        <updated>2021-07-14T01:41:49.345Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel hierarchical Bayesian model for unbiased atlas
building with subject-specific regularizations of image registration. We
develop an atlas construction process that automatically selects parameters to
control the smoothness of diffeomorphic transformation according to individual
image data. To achieve this, we introduce a hierarchical prior distribution on
regularization parameters that allows multiple penalties on images with various
degrees of geometric transformations. We then treat the regularization
parameters as latent variables and integrate them out from the model by using
the Monte Carlo Expectation Maximization (MCEM) algorithm. Another advantage of
our algorithm is that it eliminates the need for manual parameter tuning, which
can be tedious and infeasible. We demonstrate the effectiveness of our model on
3D brain MR images. Experimental results show that our model provides a sharper
atlas compared to the current atlas building algorithms with single-penalty
regularizations. Our code is publicly available at
https://github.com/jw4hv/HierarchicalBayesianAtlasBuild.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miaomiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextualized Representations Using Textual Encyclopedic Knowledge. (arXiv:2004.12006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12006</id>
        <link href="http://arxiv.org/abs/2004.12006"/>
        <updated>2021-07-14T01:41:49.338Z</updated>
        <summary type="html"><![CDATA[We present a method to represent input texts by contextualizing them jointly
with dynamically retrieved textual encyclopedic background knowledge from
multiple documents. We apply our method to reading comprehension tasks by
encoding questions and passages together with background sentences about the
entities they mention. We show that integrating background knowledge from text
is effective for tasks focusing on factual reasoning and allows direct reuse of
powerful pretrained BERT-style encoders. Moreover, knowledge integration can be
further improved with suitable pretraining via a self-supervised masked
language model objective over words in background-augmented input text. On
TriviaQA, our approach obtains improvements of 1.6 to 3.1 F1 over comparable
RoBERTa models which do not integrate background knowledge dynamically. On
MRQA, a large collection of diverse QA datasets, we see consistent gains
in-domain along with large improvements out-of-domain on BioASQ (2.1 to 4.2
F1), TextbookQA (1.6 to 2.0 F1), and DuoRC (1.1 to 2.0 F1).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kenton Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1"&gt;Yi Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1"&gt;Kristina Toutanova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08793</id>
        <link href="http://arxiv.org/abs/2104.08793"/>
        <updated>2021-07-14T01:41:49.331Z</updated>
        <summary type="html"><![CDATA[Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, we
propose SalKG, a framework for KG-augmented models to learn from coarse and/or
fine saliency explanations. Given saliency explanations created from a task's
training set, SalKG jointly trains the model to predict the explanations, then
solve the task by attending to KG features highlighted by the predicted
explanations. On two popular commonsense QA benchmarks (CSQA, OBQA), we show
that \textsc{SalKG} models can yield large performance gains -- up to 3.27% on
CSQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1"&gt;Boyuan Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiashu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1"&gt;Soumya Sanyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1"&gt;Tanishq Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05768</id>
        <link href="http://arxiv.org/abs/2107.05768"/>
        <updated>2021-07-14T01:41:49.311Z</updated>
        <summary type="html"><![CDATA[Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hanjun Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mengjiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Between Flexibility and Consistency: Joint Generation of Captions and Subtitles. (arXiv:2107.06246v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06246</id>
        <link href="http://arxiv.org/abs/2107.06246"/>
        <updated>2021-07-14T01:41:49.303Z</updated>
        <summary type="html"><![CDATA[Speech translation (ST) has lately received growing interest for the
generation of subtitles without the need for an intermediate source language
transcription and timing (i.e. captions). However, the joint generation of
source captions and target subtitles does not only bring potential output
quality advantages when the two decoding processes inform each other, but it is
also often required in multilingual scenarios. In this work, we focus on ST
models which generate consistent captions-subtitles in terms of structure and
lexical content. We further introduce new metrics for evaluating subtitling
consistency. Our findings show that joint decoding leads to increased
performance and consistency between the generated captions and subtitles while
still allowing for sufficient flexibility to produce subtitles conforming to
language-specific needs and norms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1"&gt;Alina Karakanta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1"&gt;Marco Gaido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing the Benefit of Synthetic Training Data for Various Automatic Speech Recognition Architectures. (arXiv:2104.05379v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05379</id>
        <link href="http://arxiv.org/abs/2104.05379"/>
        <updated>2021-07-14T01:41:49.292Z</updated>
        <summary type="html"><![CDATA[Recent publications on automatic-speech-recognition (ASR) have a strong focus
on attention encoder-decoder (AED) architectures which tend to suffer from
over-fitting in low resource scenarios. One solution to tackle this issue is to
generate synthetic data with a trained text-to-speech system (TTS) if
additional text is available. This was successfully applied in many
publications with AED systems, but only very limited in the context of other
ASR architectures. We investigate the effect of varying pre-processing, the
speaker embedding and input encoding of the TTS system w.r.t. the effectiveness
of the synthesized data for AED-ASR training. Additionally, we also consider
internal language model subtraction for the first time, resulting in up to 38%
relative improvement. We compare the AED results to a state-of-the-art hybrid
ASR system, a monophone based system using
connectionist-temporal-classification (CTC) and a monotonic transducer based
system. We show that for the later systems the addition of synthetic data has
no relevant effect, but they still outperform the AED systems on
LibriSpeech-100h. We achieve a final word-error-rate of 3.3%/10.0% with a
hybrid system on the clean/noisy test-sets, surpassing any previous
state-of-the-art systems on Librispeech-100h that do not include unlabeled
audio data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossenbach_N/0/1/0/all/0/1"&gt;Nick Rossenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1"&gt;Mohammad Zeineldeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilmes_B/0/1/0/all/0/1"&gt;Benedikt Hilmes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rating Facts under Coarse-to-fine Regimes. (arXiv:2107.06051v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06051</id>
        <link href="http://arxiv.org/abs/2107.06051"/>
        <updated>2021-07-14T01:41:49.285Z</updated>
        <summary type="html"><![CDATA[The rise of manipulating fake news as a political weapon has become a global
concern and highlighted the incapability of manually fact checking against
rapidly produced fake news. Thus, statistical approaches are required if we are
to address this problem efficiently. The shortage of publicly available
datasets is one major bottleneck of automated fact checking. To remedy this, we
collected 24K manually rated statements from PolitiFact. The class values
exhibit a natural order with respect to truthfulness as shown in Table 1. Thus,
our task represents a twist from standard classification, due to the various
degrees of similarity between classes. To investigate this, we defined
coarse-to-fine classification regimes, which presents new challenge for
classification. To address this, we propose BERT-based models. After training,
class similarity is sensible over the multi-class datasets, especially in the
fine-grained one. Under all the regimes, BERT achieves state of the art, while
the additional layers provide insignificant improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Guojun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EvoBA: An Evolution Strategy as a Strong Baseline forBlack-Box Adversarial Attacks. (arXiv:2107.05754v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05754</id>
        <link href="http://arxiv.org/abs/2107.05754"/>
        <updated>2021-07-14T01:41:49.277Z</updated>
        <summary type="html"><![CDATA[Recent work has shown how easily white-box adversarial attacks can be applied
to state-of-the-art image classifiers. However, real-life scenarios resemble
more the black-box adversarial conditions, lacking transparency and usually
imposing natural, hard constraints on the query budget.

We propose $\textbf{EvoBA}$, a black-box adversarial attack based on a
surprisingly simple evolutionary search strategy. $\textbf{EvoBA}$ is
query-efficient, minimizes $L_0$ adversarial perturbations, and does not
require any form of training.

$\textbf{EvoBA}$ shows efficiency and efficacy through results that are in
line with much more complex state-of-the-art black-box attacks such as
$\textbf{AutoZOOM}$. It is more query-efficient than $\textbf{SimBA}$, a simple
and powerful baseline black-box attack, and has a similar level of complexity.
Therefore, we propose it both as a new strong baseline for black-box
adversarial attacks and as a fast and general tool for gaining empirical
insight into how robust image classifiers are with respect to $L_0$ adversarial
perturbations.

There exist fast and reliable $L_2$ black-box attacks, such as
$\textbf{SimBA}$, and $L_{\infty}$ black-box attacks, such as
$\textbf{DeepSearch}$. We propose $\textbf{EvoBA}$ as a query-efficient $L_0$
black-box adversarial attack which, together with the aforementioned methods,
can serve as a generic tool to assess the empirical robustness of image
classifiers. The main advantages of such methods are that they run fast, are
query-efficient, and can easily be integrated in image classifiers development
pipelines.

While our attack minimises the $L_0$ adversarial perturbation, we also report
$L_2$, and notice that we compare favorably to the state-of-the-art $L_2$
black-box attack, $\textbf{AutoZOOM}$, and of the $L_2$ strong baseline,
$\textbf{SimBA}$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilie_A/0/1/0/all/0/1"&gt;Andrei Ilie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1"&gt;Marius Popescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanescu_A/0/1/0/all/0/1"&gt;Alin Stefanescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Adapter Tuning for Multilingual Speech Translation. (arXiv:2106.01463v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01463</id>
        <link href="http://arxiv.org/abs/2106.01463"/>
        <updated>2021-07-14T01:41:49.258Z</updated>
        <summary type="html"><![CDATA[Adapter modules were recently introduced as an efficient alternative to
fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters
of a model and injecting lightweight modules between layers, resulting in the
addition of only a small number of task-specific trainable parameters. While
adapter tuning was investigated for multilingual neural machine translation,
this paper proposes a comprehensive analysis of adapters for multilingual
speech translation (ST). Starting from different pre-trained models (a
multilingual ST trained on parallel data or a multilingual BART (mBART) trained
on non-parallel multilingual data), we show that adapters can be used to: (a)
efficiently specialize ST to specific language pairs with a low extra cost in
terms of parameters, and (b) transfer from an automatic speech recognition
(ASR) task and an mBART pre-trained model to a multilingual ST task.
Experiments show that adapter tuning offer competitive results to full
fine-tuning, while being much more parameter-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hang Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1"&gt;Didier Schwab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The IWSLT 2021 BUT Speech Translation Systems. (arXiv:2107.06155v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06155</id>
        <link href="http://arxiv.org/abs/2107.06155"/>
        <updated>2021-07-14T01:41:49.250Z</updated>
        <summary type="html"><![CDATA[The paper describes BUT's English to German offline speech translation(ST)
systems developed for IWSLT2021. They are based on jointly trained Automatic
Speech Recognition-Machine Translation models. Their performances is evaluated
on MustC-Common test set. In this work, we study their efficiency from the
perspective of having a large amount of separate ASR training data and MT
training data, and a smaller amount of speech-translation training data. Large
amounts of ASR and MT training data are utilized for pre-training the ASR and
MT models. Speech-translation data is used to jointly optimize ASR-MT models by
defining an end-to-end differentiable path from speech to translations. For
this purpose, we use the internal continuous representations from the
ASR-decoder as the input to MT module. We show that speech translation can be
further improved by training the ASR-decoder jointly with the MT-module using
large amount of text-only MT training data. We also show significant
improvements by training an ASR module capable of generating punctuated text,
rather than leaving the punctuation task to the MT module.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vydana_H/0/1/0/all/0/1"&gt;Hari Krishna Vydana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karafiat_M/0/1/0/all/0/1"&gt;Martin Karafi&amp;#x27;at&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burget_L/0/1/0/all/0/1"&gt;Luk&amp;#x27;as Burget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cernocky_%22/0/1/0/all/0/1"&gt;&amp;quot;Honza&amp;quot; Cernock&amp;#x27;y&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoding EEG Brain Activity for Multi-Modal Natural Language Processing. (arXiv:2102.08655v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08655</id>
        <link href="http://arxiv.org/abs/2102.08655"/>
        <updated>2021-07-14T01:41:49.242Z</updated>
        <summary type="html"><![CDATA[Until recently, human behavioral data from reading has mainly been of
interest to researchers to understand human cognition. However, these human
language processing signals can also be beneficial in machine learning-based
natural language processing tasks. Using EEG brain activity to this purpose is
largely unexplored as of yet. In this paper, we present the first large-scale
study of systematically analyzing the potential of EEG brain activity data for
improving natural language processing tasks, with a special focus on which
features of the signal are most beneficial. We present a multi-modal machine
learning architecture that learns jointly from textual input as well as from
EEG features. We find that filtering the EEG signals into frequency bands is
more beneficial than using the broadband signal. Moreover, for a range of word
embedding types, EEG data improves binary and ternary sentiment classification
and outperforms multiple baselines. For more complex tasks such as relation
detection, further research is needed. Finally, EEG data shows to be
particularly promising when limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hollenstein_N/0/1/0/all/0/1"&gt;Nora Hollenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renggli_C/0/1/0/all/0/1"&gt;Cedric Renggli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaus_B/0/1/0/all/0/1"&gt;Benjamin Glaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barrett_M/0/1/0/all/0/1"&gt;Maria Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troendle_M/0/1/0/all/0/1"&gt;Marius Troendle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Langer_N/0/1/0/all/0/1"&gt;Nicolas Langer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Network Structures to Improve Semantic Representation for the Financial Domain. (arXiv:2107.05885v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05885</id>
        <link href="http://arxiv.org/abs/2107.05885"/>
        <updated>2021-07-14T01:41:49.233Z</updated>
        <summary type="html"><![CDATA[This paper presents the participation of the MiniTrue team in the FinSim-3
shared task on learning semantic similarities for the financial domain in
English language. Our approach combines contextual embeddings learned by
transformer-based language models with network structures embeddings extracted
on external knowledge sources, to create more meaningful representations of
financial domain entities and terms. For this, two BERT based language models
and a knowledge graph embedding model are used. Besides, we propose a voting
function to joint three basic models for the final inference. Experimental
results show that the model with the knowledge graph embeddings has achieved a
superior result than these models with only contextual embeddings.
Nevertheless, we also observe that our voting function brings an extra benefit
to the final system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1"&gt;Chao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+We_S/0/1/0/all/0/1"&gt;Shi-jie We&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks Evolve Human-like Attention Distribution during Reading Comprehension. (arXiv:2107.05799v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05799</id>
        <link href="http://arxiv.org/abs/2107.05799"/>
        <updated>2021-07-14T01:41:49.225Z</updated>
        <summary type="html"><![CDATA[Attention is a key mechanism for information selection in both biological
brains and many state-of-the-art deep neural networks (DNNs). Here, we
investigate whether humans and DNNs allocate attention in comparable ways when
reading a text passage to subsequently answer a specific question. We analyze 3
transformer-based DNNs that reach human-level performance when trained to
perform the reading comprehension task. We find that the DNN attention
distribution quantitatively resembles human attention distribution measured by
fixation times. Human readers fixate longer on words that are more relevant to
the question-answering task, demonstrating that attention is modulated by
top-down reading goals, on top of lower-level visual and text features of the
stimulus. Further analyses reveal that the attention weights in DNNs are also
influenced by both top-down reading goals and lower-level stimulus features,
with the shallow layers more strongly influenced by lower-level text features
and the deep layers attending more to task-relevant words. Additionally, deep
layers' attention to task-relevant words gradually emerges when pre-trained DNN
models are fine-tuned to perform the reading comprehension task, which
coincides with the improvement in task performance. These results demonstrate
that DNNs can evolve human-like attention distribution through task
optimization, which suggests that human attention during goal-directed reading
comprehension is a consequence of task optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Jiajie Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Nai Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conformer-based End-to-end Speech Recognition With Rotary Position Embedding. (arXiv:2107.05907v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05907</id>
        <link href="http://arxiv.org/abs/2107.05907"/>
        <updated>2021-07-14T01:41:49.217Z</updated>
        <summary type="html"><![CDATA[Transformer-based end-to-end speech recognition models have received
considerable attention in recent years due to their high training speed and
ability to model a long-range global context. Position embedding in the
transformer architecture is indispensable because it provides supervision for
dependency modeling between elements at different positions in the input
sequence. To make use of the time order of the input sequence, many works
inject some information about the relative or absolute position of the element
into the input sequence. In this work, we investigate various position
embedding methods in the convolution-augmented transformer (conformer) and
adopt a novel implementation named rotary position embedding (RoPE). RoPE
encodes absolute positional information into the input sequence by a rotation
matrix, and then naturally incorporates explicit relative position information
into a self-attention module. To evaluate the effectiveness of the RoPE method,
we conducted experiments on AISHELL-1 and LibriSpeech corpora. Results show
that the conformer enhanced with RoPE achieves superior performance in the
speech recognition task. Specifically, our model achieves a relative word error
rate reduction of 8.70% and 7.27% over the conformer on test-clean and
test-other sets of the LibriSpeech corpus respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shengqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Menglong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao-Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Gender Augmented Data for NLP. (arXiv:2107.05987v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05987</id>
        <link href="http://arxiv.org/abs/2107.05987"/>
        <updated>2021-07-14T01:41:49.168Z</updated>
        <summary type="html"><![CDATA[Gender bias is a frequent occurrence in NLP-based applications, especially
pronounced in gender-inflected languages. Bias can appear through associations
of certain adjectives and animate nouns with the natural gender of referents,
but also due to unbalanced grammatical gender frequencies of inflected words.
This type of bias becomes more evident in generating conversational utterances
where gender is not specified within the sentence, because most current NLP
applications still work on a sentence-level context. As a step towards more
inclusive NLP, this paper proposes an automatic and generalisable rewriting
approach for short conversational sentences. The rewriting method can be
applied to sentences that, without extra-sentential context, have multiple
equivalent alternatives in terms of gender. The method can be applied both for
creating gender balanced outputs as well as for creating gender balanced
training data. The proposed approach is based on a neural machine translation
(NMT) system trained to 'translate' from one gender alternative to another.
Both the automatic and manual analysis of the approach show promising results
for automatic generation of gender alternatives for conversational sentences in
Spanish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Nishtha Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovic_M/0/1/0/all/0/1"&gt;Maja Popovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Groves_D/0/1/0/all/0/1"&gt;Declan Groves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1"&gt;Eva Vanmassenhove&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Difficulty of Translating Free-Order Case-Marking Languages. (arXiv:2107.06055v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06055</id>
        <link href="http://arxiv.org/abs/2107.06055"/>
        <updated>2021-07-14T01:41:49.071Z</updated>
        <summary type="html"><![CDATA[Identifying factors that make certain languages harder to model than others
is essential to reach language equality in future Natural Language Processing
technologies. Free-order case-marking languages, such as Russian, Latin or
Tamil, have proved more challenging than fixed-order languages for the tasks of
syntactic parsing and subject-verb agreement prediction. In this work, we
investigate whether this class of languages is also more difficult to translate
by state-of-the-art Neural Machine Translation models (NMT). Using a variety of
synthetic languages and a newly introduced translation challenge set, we find
that word order flexibility in the source language only leads to a very small
loss of NMT quality, even though the core verb arguments become impossible to
disambiguate in sentences without semantic cues. The latter issue is indeed
solved by the addition of case marking. However, in medium- and low-resource
settings, the overall NMT quality of fixed-order languages remains unmatched.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1"&gt;Arianna Bisazza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1"&gt;Ahmet &amp;#xdc;st&amp;#xfc;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sportel_S/0/1/0/all/0/1"&gt;Stephan Sportel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition. (arXiv:2105.06804v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06804</id>
        <link href="http://arxiv.org/abs/2105.06804"/>
        <updated>2021-07-14T01:41:49.059Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) is a well-studied task in natural language
processing. Traditional NER research only deals with flat entities and ignores
nested entities. The span-based methods treat entity recognition as a span
classification task. Although these methods have the innate ability to handle
nested NER, they suffer from high computational cost, ignorance of boundary
information, under-utilization of the spans that partially match with entities,
and difficulties in long entity recognition. To tackle these issues, we propose
a two-stage entity identifier. First we generate span proposals by filtering
and boundary regression on the seed spans to locate the entities, and then
label the boundary-adjusted span proposals with the corresponding categories.
Our method effectively utilizes the boundary information of entities and
partially matched spans during training. Through boundary regression, entities
of any length can be covered theoretically, which improves the ability to
recognize long entities. In addition, many low-quality seed spans are filtered
out in the first stage, which reduces the time complexity of inference.
Experiments on nested NER datasets demonstrate that our proposed method
outperforms previous state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yongliang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinyin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zeqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weiming Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Speech Translation. (arXiv:2107.06010v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.06010</id>
        <link href="http://arxiv.org/abs/2107.06010"/>
        <updated>2021-07-14T01:41:49.035Z</updated>
        <summary type="html"><![CDATA[Speech Translation (ST) is the task of translating speech in one language
into text in another language. Traditional cascaded approaches for ST, using
Automatic Speech Recognition (ASR) and Machine Translation (MT) systems, are
prone to error propagation. End-to-end approaches use only one system to avoid
propagating error, yet are difficult to employ due to data scarcity. We explore
zero-shot translation, which enables translating a pair of languages that is
unseen during training, thus avoid the use of end-to-end ST data. Zero-shot
translation has been shown to work for multilingual machine translation, yet
has not been studied for speech translation. We attempt to build zero-shot ST
models that are trained only on ASR and MT tasks but can do ST task during
inference. The challenge is that the representation of text and audio is
significantly different, thus the models learn ASR and MT tasks in different
ways, making it non-trivial to perform zero-shot. These models tend to output
the wrong language when performing zero-shot ST. We tackle the issues by
including additional training data and an auxiliary loss function that
minimizes the text-audio difference. Our experiment results and analysis show
that the methods are promising for zero-shot ST. Moreover, our methods are
particularly useful in the few-shot settings where a limited amount of ST data
is available, with improvements of up to +11.8 BLEU points compared to direct
end-to-end ST models and +3.9 BLEU points compared to ST models fine-tuned from
pre-trained ASR model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1"&gt;Tu Anh Dinh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enforcing Consistency in Weakly Supervised Semantic Parsing. (arXiv:2107.05833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05833</id>
        <link href="http://arxiv.org/abs/2107.05833"/>
        <updated>2021-07-14T01:41:48.992Z</updated>
        <summary type="html"><![CDATA[The predominant challenge in weakly supervised semantic parsing is that of
spurious programs that evaluate to correct answers for the wrong reasons. Prior
work uses elaborate search strategies to mitigate the prevalence of spurious
programs; however, they typically consider only one input at a time. In this
work we explore the use of consistency between the output programs for related
inputs to reduce the impact of spurious programs. We bias the program search
(and thus the model's training signal) towards programs that map the same
phrase in related inputs to the same sub-parts in their respective programs.
Additionally, we study the importance of designing logical formalisms that
facilitate this kind of consAistency-based training. We find that a more
consistent formalism leads to improved model performance even without
consistency-based training. When combined together, these two insights lead to
a 10% absolute improvement over the best prior result on the Natural Language
Visual Reasoning dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nitish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05684</id>
        <link href="http://arxiv.org/abs/2107.05684"/>
        <updated>2021-07-14T01:41:48.984Z</updated>
        <summary type="html"><![CDATA[This paper discusses the approach used by the Accenture Team for CLEF2021
CheckThat! Lab, Task 1, to identify whether a claim made in social media would
be interesting to a wide audience and should be fact-checked. Twitter training
and test data were provided in English, Arabic, Spanish, Turkish, and
Bulgarian. Claims were to be classified (check-worthy/not check-worthy) and
ranked in priority order for the fact-checker. Our method used deep neural
network transformer models with contextually sensitive lexical augmentation
applied on the supplied training datasets to create additional training
samples. This augmentation approach improved the performance for all languages.
Overall, our architecture and data augmentation pipeline produced the best
submitted system for Arabic, and performance scales according to the quantity
of provided training data for English, Spanish, Turkish, and Bulgarian. This
paper investigates the deep neural network architectures for each language as
well as the provided data to examine why the approach worked so effectively for
Arabic, and discusses additional data augmentation measures that should could
be useful to this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1"&gt;Evan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1"&gt;Paul Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1"&gt;Sieu Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Explainability in NLP and Analyzing Algorithms for Performance-Explainability Tradeoff. (arXiv:2107.05693v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05693</id>
        <link href="http://arxiv.org/abs/2107.05693"/>
        <updated>2021-07-14T01:41:48.976Z</updated>
        <summary type="html"><![CDATA[The healthcare domain is one of the most exciting application areas for
machine learning, but a lack of model transparency contributes to a lag in
adoption within the industry. In this work, we explore the current art of
explainability and interpretability within a case study in clinical text
classification, using a task of mortality prediction within MIMIC-III clinical
notes. We demonstrate various visualization techniques for fully interpretable
methods as well as model-agnostic post hoc attributions, and we provide a
generalized method for evaluating the quality of explanations using infidelity
and local Lipschitz across model types from logistic regression to BERT
variants. With these metrics, we introduce a framework through which
practitioners and researchers can assess the frontier between a model's
predictive performance and the quality of its available explanations. We make
our code available to encourage continued refinement of these methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naylor_M/0/1/0/all/0/1"&gt;Mitchell Naylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+French_C/0/1/0/all/0/1"&gt;Christi French&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terker_S/0/1/0/all/0/1"&gt;Samantha Terker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_U/0/1/0/all/0/1"&gt;Uday Kamath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Language Coordination by Modeling Theory of Mind. (arXiv:2107.05697v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05697</id>
        <link href="http://arxiv.org/abs/2107.05697"/>
        <updated>2021-07-14T01:41:48.952Z</updated>
        <summary type="html"><![CDATA[$\textit{No man is an island.}$ Humans communicate with a large community by
coordinating with different interlocutors within short conversations. This
ability has been understudied by the research on building neural communicative
agents. We study the task of few-shot $\textit{language coordination}$: agents
quickly adapting to their conversational partners' language abilities.
Different from current communicative agents trained with self-play, we require
the lead agent to coordinate with a $\textit{population}$ of agents with
different linguistic abilities, quickly adapting to communicate with unseen
agents in the population. This requires the ability to model the partner's
beliefs, a vital component of human communication. Drawing inspiration from
theory-of-mind (ToM; Premack& Woodruff (1978)), we study the effect of the
speaker explicitly modeling the listeners' mental states. The speakers, as
shown in our experiments, acquire the ability to predict the reactions of their
partner, which helps it generate instructions that concisely express its
communicative goal. We examine our hypothesis that the instructions generated
with ToM modeling yield better communication performance in both a referential
game and a language navigation task. Positive results from our experiments hint
at the importance of explicitly modeling communication as a socio-pragmatic
progress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05687</id>
        <link href="http://arxiv.org/abs/2107.05687"/>
        <updated>2021-07-14T01:41:48.940Z</updated>
        <summary type="html"><![CDATA[Active learning is the iterative construction of a classification model
through targeted labeling, enabling significant labeling cost savings. As most
research on active learning has been carried out before transformer-based
language models ("transformers") became popular, despite its practical
importance, comparably few papers have investigated how transformers can be
combined with active learning to date. This can be attributed to the fact that
using state-of-the-art query strategies for transformers induces a prohibitive
runtime overhead, which effectively cancels out, or even outweighs
aforementioned cost savings. In this paper, we revisit uncertainty-based query
strategies, which had been largely outperformed before, but are particularly
suited in the context of fine-tuning transformers. In an extensive evaluation
on five widely used text classification benchmarks, we show that considerable
improvements of up to 14.4 percentage points in area under the learning curve
are achieved, as well as a final accuracy close to the state of the art for all
but one benchmark, using only between 0.4% and 15% of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_C/0/1/0/all/0/1"&gt;Christopher Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1"&gt;Andreas Niekler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Configurable Multilingual Model is All You Need to Recognize All Languages. (arXiv:2107.05876v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.05876</id>
        <link href="http://arxiv.org/abs/2107.05876"/>
        <updated>2021-07-14T01:41:48.929Z</updated>
        <summary type="html"><![CDATA[Multilingual automatic speech recognition (ASR) models have shown great
promise in recent years because of the simplified model training and deployment
process. Conventional methods either train a universal multilingual model
without taking any language information or with a 1-hot language ID (LID)
vector to guide the recognition of the target language. In practice, the user
can be prompted to pre-select several languages he/she can speak. The
multilingual model without LID cannot well utilize the language information set
by the user while the multilingual model with LID can only handle one
pre-selected language. In this paper, we propose a novel configurable
multilingual model (CMM) which is trained only once but can be configured as
different models based on users' choices by extracting language-specific
modules together with a universal model from the trained CMM. Particularly, a
single CMM can be deployed to any user scenario where the users can pre-select
any combination of languages. Trained with 75K hours of transcribed anonymized
Microsoft multilingual data and evaluated with 10-language test sets, the
proposed CMM improves from the universal multilingual model by 26.0%, 16.9%,
and 10.4% relative word error reduction when the user selects 1, 2, or 3
languages, respectively. CMM also performs significantly better on
code-switching test sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Long Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shujie Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combiner: Full Attention Transformer with Sparse Computation Cost. (arXiv:2107.05768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05768</id>
        <link href="http://arxiv.org/abs/2107.05768"/>
        <updated>2021-07-14T01:41:48.921Z</updated>
        <summary type="html"><![CDATA[Transformers provide a class of expressive architectures that are extremely
effective for sequence modeling. However, the key limitation of transformers is
their quadratic memory and time complexity $\mathcal{O}(L^2)$ with respect to
the sequence length in attention layers, which restricts application in
extremely long sequences. Most existing approaches leverage sparsity or
low-rank assumptions in the attention matrix to reduce cost, but sacrifice
expressiveness. Instead, we propose Combiner, which provides full attention
capability in each attention head while maintaining low computation and memory
complexity. The key idea is to treat the self-attention mechanism as a
conditional expectation over embeddings at each location, and approximate the
conditional distribution with a structured factorization. Each location can
attend to all other locations, either via direct attention, or through indirect
attention to abstractions, which are again conditional expectations of
embeddings from corresponding local regions. We show that most sparse attention
patterns used in existing sparse transformers are able to inspire the design of
such factorization for full attention, resulting in the same sub-quadratic cost
($\mathcal{O}(L\log(L))$ or $\mathcal{O}(L\sqrt{L})$). Combiner is a drop-in
replacement for attention layers in existing transformers and can be easily
implemented in common frameworks. An experimental evaluation on both
autoregressive and bidirectional sequence tasks demonstrates the effectiveness
of this approach, yielding state-of-the-art results on several image and text
modeling tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hanjun Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mengjiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-aware Summarization for Justified Decision-Making. (arXiv:2107.06243v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.06243</id>
        <link href="http://arxiv.org/abs/2107.06243"/>
        <updated>2021-07-14T01:41:48.911Z</updated>
        <summary type="html"><![CDATA[In many applications such as recidivism prediction, facility inspection, and
benefit assignment, it's important for individuals to know the
decision-relevant information for the model's prediction. In addition, the
model's predictions should be fairly justified. Essentially, decision-relevant
features should provide sufficient information for the predicted outcome and
should be independent of the membership of individuals in protected groups such
as race and gender. In this work, we focus on the problem of (un)fairness in
the justification of the text-based neural models. We tie the explanatory power
of the model to fairness in the outcome and propose a fairness-aware
summarization mechanism to detect and counteract the bias in such models. Given
a potentially biased natural language explanation for a decision, we use a
multi-task neural model and an attribution mechanism based on integrated
gradients to extract the high-utility and discrimination-free justifications in
the form of a summary. The extracted summary is then used for training a model
to make decisions for individuals. Results on several real-world datasets
suggests that our method: (i) assists users to understand what information is
used for the model's decision and (ii) enhances the fairness in outcomes while
significantly reducing the demographic leakage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1"&gt;Moniba Keymanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1"&gt;Tanya Berger-Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1"&gt;Micha Elsner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1"&gt;Srinivasan Parthasarathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task. (arXiv:2107.05782v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05782</id>
        <link href="http://arxiv.org/abs/2107.05782"/>
        <updated>2021-07-14T01:41:48.902Z</updated>
        <summary type="html"><![CDATA[Pretraining and multitask learning are widely used to improve the speech to
text translation performance. In this study, we are interested in training a
speech to text translation model along with an auxiliary text to text
translation task. We conduct a detailed analysis to understand the impact of
the auxiliary task on the primary task within the multitask learning framework.
Our analysis confirms that multitask learning tends to generate similar decoder
representations from different modalities and preserve more information from
the pretrained text translation modules. We observe minimal negative transfer
effect between the two tasks and sharing more parameters is helpful to transfer
knowledge from the text task to the speech task. The analysis also reveals that
the modality representation difference at the top decoder layers is still not
negligible, and those layers are critical for the translation quality. Inspired
by these findings, we propose three methods to improve translation quality.
First, a parameter sharing and initialization strategy is proposed to enhance
information sharing between the tasks. Second, a novel attention-based
regularization is proposed for the encoders and pulls the representations from
different modalities closer. Third, an online knowledge distillation is
proposed to enhance the knowledge transfer from the text to the speech task.
Our experiments show that the proposed approach improves translation
performance by more than 2 BLEU over a strong baseline and achieves
state-of-the-art results on the \textsc{MuST-C} English-German, English-French
and English-Spanish language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genzel_D/0/1/0/all/0/1"&gt;Dmitriy Genzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalRec: Causal Inference for Visual Debiasing in Visually-Aware Recommendation. (arXiv:2107.02390v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02390</id>
        <link href="http://arxiv.org/abs/2107.02390"/>
        <updated>2021-07-14T01:41:48.864Z</updated>
        <summary type="html"><![CDATA[Visually-aware recommendation on E-commerce platforms aims to leverage visual
information of items to predict a user's preference. It is commonly observed
that user's attention to visual features does not always reflect the real
preference. Although a user may click and view an item in light of a visual
satisfaction of their expectations, a real purchase does not always occur due
to the unsatisfaction of other essential features (e.g., brand, material,
price). We refer to the reason for such a visually related interaction
deviating from the real preference as a visual bias. Existing visually-aware
models make use of the visual features as a separate collaborative signal
similarly to other features to directly predict the user's preference without
considering a potential bias, which gives rise to a visually biased
recommendation. In this paper, we derive a causal graph to identify and analyze
the visual bias of these existing methods. In this causal graph, the visual
feature of an item acts as a mediator, which could introduce a spurious
relationship between the user and the item. To eliminate this spurious
relationship that misleads the prediction of the user's real preference, an
intervention and a counterfactual inference are developed over the mediator.
Particularly, the Total Indirect Effect is applied for a debiased prediction
during the testing phase of the model. This causal inference framework is model
agnostic such that it can be integrated into the existing methods. Furthermore,
we propose a debiased visually-aware recommender system, denoted as CausalRec
to effectively retain the supportive significance of the visual information and
remove the visual bias. Extensive experiments are conducted on eight benchmark
datasets, which shows the state-of-the-art performance of CausalRec and the
efficacy of debiasing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Approach for Semantic Web Matching. (arXiv:2107.06083v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.06083</id>
        <link href="http://arxiv.org/abs/2107.06083"/>
        <updated>2021-07-14T01:41:48.852Z</updated>
        <summary type="html"><![CDATA[In this work we propose a new approach for semantic web matching to improve
the performance of Web Service replacement. Because in automatic systems we
should ensure the self-healing, self-configuration, self-optimization and
self-management, all services should be always available and if one of them
crashes, it should be replaced with the most similar one. Candidate services
are advertised in Universal Description, Discovery and Integration (UDDI) all
in Web Ontology Language (OWL). By the help of bipartite graph, we did the
matching between the crashed service and a Candidate one. Then we chose the
best service, which had the maximum rate of matching. In fact we compare two
services` functionalities and capabilities to see how much they match. We found
that the best way for matching two web services, is comparing the
functionalities of them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1"&gt;Kamran Zamanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1"&gt;Golsa Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nematbakhsh_N/0/1/0/all/0/1"&gt;Naser Nematbakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mardookhi_F/0/1/0/all/0/1"&gt;Farhad Mardookhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. (arXiv:2107.05720v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05720</id>
        <link href="http://arxiv.org/abs/2107.05720"/>
        <updated>2021-07-14T01:41:48.840Z</updated>
        <summary type="html"><![CDATA[In neural Information Retrieval, ongoing research is directed towards
improving the first retriever in ranking pipelines. Learning dense embeddings
to conduct retrieval using efficient approximate nearest neighbors methods has
proven to work well. Meanwhile, there has been a growing interest in learning
sparse representations for documents and queries, that could inherit from the
desirable properties of bag-of-words models such as the exact matching of terms
and the efficiency of inverted indexes. In this work, we present a new
first-stage ranker based on explicit sparsity regularization and a
log-saturation effect on term weights, leading to highly sparse representations
and competitive results with respect to state-of-the-art dense and sparse
methods. Our approach is simple, trained end-to-end in a single stage. We also
explore the trade-off between effectiveness and efficiency, by controlling the
contribution of the sparsity regularization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1"&gt;Thibault Formal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1"&gt;Benjamin Piwowarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Clinchant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SkillNER: Mining and Mapping Soft Skills from any Text. (arXiv:2101.11431v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11431</id>
        <link href="http://arxiv.org/abs/2101.11431"/>
        <updated>2021-07-14T01:41:48.823Z</updated>
        <summary type="html"><![CDATA[In today's digital world, there is an increasing focus on soft skills. On the
one hand, they facilitate innovation at companies, but on the other, they are
unlikely to be automated soon. Researchers struggle with accurately approaching
quantitatively the study of soft skills due to the lack of data-driven methods
to retrieve them. This limits the possibility for psychologists and HR managers
to understand the relation between humans and digitalisation. This paper
presents SkillNER, a novel data-driven method for automatically extracting soft
skills from text. It is a named entity recognition (NER) system trained with a
support vector machine (SVM) on a corpus of more than 5000 scientific papers.
We developed this system by measuring the performance of our approach against
different training models and validating the results together with a team of
psychologists. Finally, SkillNER was tested in a real-world case study using
the job descriptions of ESCO (European Skill/Competence Qualification and
Occupation) as textual source. The system enabled the detection of communities
of job profiles based on their shared soft skills and communities of soft
skills based on their shared job profiles. This case study demonstrates that
the tool can automatically retrieve soft skills from a large corpus in an
efficient way, proving useful for firms, institutions, and workers. The tool is
open and available online to foster quantitative methods for the study of soft
skills.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fareri_S/0/1/0/all/0/1"&gt;Silvia Fareri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melluso_N/0/1/0/all/0/1"&gt;Nicola Melluso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiarello_F/0/1/0/all/0/1"&gt;Filippo Chiarello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fantoni_G/0/1/0/all/0/1"&gt;Gualtiero Fantoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asking Clarifying Questions Based on Negative Feedback in Conversational Search. (arXiv:2107.05760v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05760</id>
        <link href="http://arxiv.org/abs/2107.05760"/>
        <updated>2021-07-14T01:41:48.791Z</updated>
        <summary type="html"><![CDATA[Users often need to look through multiple search result pages or reformulate
queries when they have complex information-seeking needs. Conversational search
systems make it possible to improve user satisfaction by asking questions to
clarify users' search intents. This, however, can take significant effort to
answer a series of questions starting with "what/why/how". To quickly identify
user intent and reduce effort during interactions, we propose an intent
clarification task based on yes/no questions where the system needs to ask the
correct question about intents within the fewest conversation turns. In this
task, it is essential to use negative feedback about the previous questions in
the conversation history. To this end, we propose a Maximum-Marginal-Relevance
(MMR) based BERT model (MMR-BERT) to leverage negative feedback based on the
MMR principle for the next clarifying question selection. Experiments on the
Qulac dataset show that MMR-BERT outperforms state-of-the-art baselines
significantly on the intent identification task and the selected questions also
achieve significantly better performance in the associated document retrieval
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Keping Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1"&gt;W. Bruce Croft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COPER a query-adaptable Semantics-based Search Engine for Persian COVID-19 Articles. (arXiv:2107.05722v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05722</id>
        <link href="http://arxiv.org/abs/2107.05722"/>
        <updated>2021-07-14T01:41:48.775Z</updated>
        <summary type="html"><![CDATA[With the surge of pretrained language models, a new pathway has been opened
to incorporate Persian text contextual information. Meanwhile, as many other
countries, including Iran, are fighting against COVID-19, a plethora of
COVID-19 related articles has been published in Iranian Healthcare magazines to
better inform the public of the situation. However, finding answers in this
sheer volume of information is an extremely difficult task. In this paper, we
collected a large dataset of these articles, leveraged different BERT
variations as well as other keyword models such as BM25 and TF-IDF, and created
a search engine to sift through these documents and rank them, given a user's
query. Our final search engine consists of a ranker and a re-ranker, which
adapts itself to the query. We fine-tune our models using Semantic Textual
Similarity and evaluate them with standard task metrics. Our final method
outperforms the rest by a considerable margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1"&gt;Reza Khanmohammadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1"&gt;Mitra Sadat Mirshafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allahyari_M/0/1/0/all/0/1"&gt;Mehdi Allahyari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05677</id>
        <link href="http://arxiv.org/abs/2107.05677"/>
        <updated>2021-07-14T01:41:48.737Z</updated>
        <summary type="html"><![CDATA[We demonstrate that language models pre-trained on codified
(discretely-encoded) music audio learn representations that are useful for
downstream MIR tasks. Specifically, we explore representations from Jukebox
(Dhariwal et al. 2020): a music generation system containing a language model
trained on codified audio from 1M songs. To determine if Jukebox's
representations contain useful information for MIR, we use them as input
features to train shallow models on several MIR tasks. Relative to
representations from conventional MIR models which are pre-trained on tagging,
we find that using representations from Jukebox as input features yields 30%
stronger performance on average across four MIR tasks: tagging, genre
classification, emotion recognition, and key detection. For key detection, we
observe that representations from Jukebox are considerably stronger than those
from models pre-trained on tagging, suggesting that pre-training via codified
audio language modeling may address blind spots in conventional approaches. We
interpret the strength of Jukebox's representations as evidence that modeling
audio instead of tags provides richer representations for MIR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1"&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Aesthetic Layouts via Visual Guidance. (arXiv:2107.06262v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06262</id>
        <link href="http://arxiv.org/abs/2107.06262"/>
        <updated>2021-07-14T01:41:48.550Z</updated>
        <summary type="html"><![CDATA[We explore computational approaches for visual guidance to aid in creating
aesthetically pleasing art and graphic design. Our work complements and builds
on previous work that developed models for how humans look at images. Our
approach comprises three steps. First, we collected a dataset of art
masterpieces and labeled the visual fixations with state-of-art vision models.
Second, we clustered the visual guidance templates of the art masterpieces with
unsupervised learning. Third, we developed a pipeline using generative
adversarial networks to learn the principles of visual guidance and that can
produce aesthetically pleasing layouts. We show that the aesthetic visual
guidance principles can be learned and integrated into a high-dimensional model
and can be queried by the features of graphic elements. We evaluate our
approach by generating layouts on various drawings and graphic designs.
Moreover, our model considers the color and structure of graphic elements when
generating layouts. Consequently, we believe our tool, which generates multiple
aesthetic layout options in seconds, can help artists create beautiful art and
graphic designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qingyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhuoru Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargteil_A/0/1/0/all/0/1"&gt;Adam Bargteil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Irrelevant Representation Learning for Unsupervised Domain Generalization. (arXiv:2107.06219v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.06219</id>
        <link href="http://arxiv.org/abs/2107.06219"/>
        <updated>2021-07-14T01:41:48.534Z</updated>
        <summary type="html"><![CDATA[Domain generalization (DG) aims to help models trained on a set of source
domains generalize better on unseen target domains. The performances of current
DG methods largely rely on sufficient labeled data, which however are usually
costly or unavailable. While unlabeled data are far more accessible, we seek to
explore how unsupervised learning can help deep models generalizes across
domains. Specifically, we study a novel generalization problem called
unsupervised domain generalization, which aims to learn generalizable models
with unlabeled data. Furthermore, we propose a Domain-Irrelevant Unsupervised
Learning (DIUL) method to cope with the significant and misleading
heterogeneity within unlabeled data and severe distribution shifts between
source and target data. Surprisingly we observe that DIUL can not only
counterbalance the scarcity of labeled data but also further strengthen the
generalization ability of models when the labeled data are sufficient. As a
pretraining approach, DIUL shows superior to ImageNet pretraining protocol even
when the available data are unlabeled and of a greatly smaller amount compared
to ImageNet. Extensive experiments clearly demonstrate the effectiveness of our
method compared with state-of-the-art unsupervised learning counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Linjun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1"&gt;Peng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zheyan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haoxin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dance2Music: Automatic Dance-driven Music Generation. (arXiv:2107.06252v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.06252</id>
        <link href="http://arxiv.org/abs/2107.06252"/>
        <updated>2021-07-14T01:41:48.495Z</updated>
        <summary type="html"><![CDATA[Dance and music typically go hand in hand. The complexities in dance, music,
and their synchronisation make them fascinating to study from a computational
creativity perspective. While several works have looked at generating dance for
a given music, automatically generating music for a given dance remains
under-explored. This capability could have several creative expression and
entertainment applications. We present some early explorations in this
direction. We present a search-based offline approach that generates music
after processing the entire dance video and an online approach that uses a deep
neural network to generate music on-the-fly as the video proceeds. We compare
these approaches to a strong heuristic baseline via human studies and present
our findings. We have integrated our online approach in a live demo! A video of
the demo can be found here:
https://sites.google.com/view/dance2music/live-demo.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1"&gt;Gunjan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accenture at CheckThat! 2021: Interesting claim identification and ranking with contextually sensitive lexical training data augmentation. (arXiv:2107.05684v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05684</id>
        <link href="http://arxiv.org/abs/2107.05684"/>
        <updated>2021-07-14T01:41:48.465Z</updated>
        <summary type="html"><![CDATA[This paper discusses the approach used by the Accenture Team for CLEF2021
CheckThat! Lab, Task 1, to identify whether a claim made in social media would
be interesting to a wide audience and should be fact-checked. Twitter training
and test data were provided in English, Arabic, Spanish, Turkish, and
Bulgarian. Claims were to be classified (check-worthy/not check-worthy) and
ranked in priority order for the fact-checker. Our method used deep neural
network transformer models with contextually sensitive lexical augmentation
applied on the supplied training datasets to create additional training
samples. This augmentation approach improved the performance for all languages.
Overall, our architecture and data augmentation pipeline produced the best
submitted system for Arabic, and performance scales according to the quantity
of provided training data for English, Spanish, Turkish, and Bulgarian. This
paper investigates the deep neural network architectures for each language as
well as the provided data to examine why the approach worked so effectively for
Arabic, and discusses additional data augmentation measures that should could
be useful to this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Williams_E/0/1/0/all/0/1"&gt;Evan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_P/0/1/0/all/0/1"&gt;Paul Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1"&gt;Sieu Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Codified audio language modeling learns useful representations for music information retrieval. (arXiv:2107.05677v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05677</id>
        <link href="http://arxiv.org/abs/2107.05677"/>
        <updated>2021-07-14T01:41:48.398Z</updated>
        <summary type="html"><![CDATA[We demonstrate that language models pre-trained on codified
(discretely-encoded) music audio learn representations that are useful for
downstream MIR tasks. Specifically, we explore representations from Jukebox
(Dhariwal et al. 2020): a music generation system containing a language model
trained on codified audio from 1M songs. To determine if Jukebox's
representations contain useful information for MIR, we use them as input
features to train shallow models on several MIR tasks. Relative to
representations from conventional MIR models which are pre-trained on tagging,
we find that using representations from Jukebox as input features yields 30%
stronger performance on average across four MIR tasks: tagging, genre
classification, emotion recognition, and key detection. For key detection, we
observe that representations from Jukebox are considerably stronger than those
from models pre-trained on tagging, suggesting that pre-training via codified
audio language modeling may address blind spots in conventional approaches. We
interpret the strength of Jukebox's representations as evidence that modeling
audio instead of tags provides richer representations for MIR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castellon_R/0/1/0/all/0/1"&gt;Rodrigo Castellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donahue_C/0/1/0/all/0/1"&gt;Chris Donahue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified framework based on graph consensus term for multi-view learning. (arXiv:2105.11781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11781</id>
        <link href="http://arxiv.org/abs/2105.11781"/>
        <updated>2021-07-13T01:59:37.467Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view learning technologies for various applications
have attracted a surge of interest. Due to more compatible and complementary
information from multiple views, existing multi-view methods could achieve more
promising performance than conventional single-view methods in most situations.
However, there are still no sufficient researches on the unified framework in
existing multi-view works. Meanwhile, how to efficiently integrate multi-view
information is still full of challenges. In this paper, we propose a novel
multi-view learning framework, which aims to leverage most existing graph
embedding works into a unified formula via introducing the graph consensus
term. In particular, our method explores the graph structure in each view
independently to preserve the diversity property of graph embedding methods.
Meanwhile, we choose heterogeneous graphs to construct the graph consensus term
to explore the correlations among multiple views jointly. To this end, the
diversity and complementary information among different views could be
simultaneously considered. Furthermore, the proposed framework is utilized to
implement the multi-view extension of Locality Linear Embedding, named
Multi-view Locality Linear Embedding (MvLLE), which could be efficiently solved
by applying the alternating optimization strategy. Empirical validations
conducted on six benchmark datasets can show the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangzhu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chonghui Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coupled VAE: Improved Accuracy and Robustness of a Variational Autoencoder. (arXiv:1906.00536v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.00536</id>
        <link href="http://arxiv.org/abs/1906.00536"/>
        <updated>2021-07-13T01:59:37.461Z</updated>
        <summary type="html"><![CDATA[We present a coupled Variational Auto-Encoder (VAE) method that improves the
accuracy and robustness of the probabilistic inferences on represented data.
The new method models the dependency between input feature vectors (images) and
weighs the outliers with a higher penalty by generalizing the original loss
function to the coupled entropy function, using the principles of nonlinear
statistical coupling. We evaluate the performance of the coupled VAE model
using the MNIST dataset. Compared with the traditional VAE algorithm, the
output images generated by the coupled VAE method are clearer and less blurry.
The visualization of the input images embedded in 2D latent variable space
provides a deeper insight into the structure of new model with coupled loss
function: the latent variable has a smaller deviation and a more compact latent
space generates the output values. We analyze the histogram of the likelihoods
of the input images using the generalized mean, which measures the model's
accuracy as a function of the relative risk. The neutral accuracy, which is the
geometric mean and is consistent with a measure of the Shannon cross-entropy,
is improved. The robust accuracy, measured by the -2/3 generalized mean, is
also improved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shichen Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nelson_K/0/1/0/all/0/1"&gt;Kenric P. Nelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kon_M/0/1/0/all/0/1"&gt;Mark A. Kon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00857</id>
        <link href="http://arxiv.org/abs/2012.00857"/>
        <updated>2021-07-13T01:59:37.448Z</updated>
        <summary type="html"><![CDATA[There are two major classes of natural language grammar -- the dependency
grammar that models one-to-one correspondences between words and the
constituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only
inducing one class of grammars, we introduce a novel model, StructFormer, that
can simultaneously induce dependency and constituency structure. To achieve
this, we propose a new parsing framework that can jointly generate a
constituency tree and dependency graph. Then we integrate the induced
dependency relations into the transformer, in a differentiable manner, through
a novel dependency-constrained self-attention mechanism. Experimental results
show that our model can achieve strong results on unsupervised constituency
parsing, unsupervised dependency parsing, and masked language modeling at the
same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yikang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v6 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-07-13T01:59:37.442Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to difficult Fisher-matrix
computations. We address this issue by using \emph{local-parameter coordinates}
to obtain a flexible and efficient NGD method that works well for a
wide-variety of structured parameterizations. We show four applications where
our method (1) generalizes the exponential natural evolutionary strategy, (2)
recovers existing Newton-like algorithms, (3) yields new structured
second-order algorithms, and (4) gives new algorithms to learn covariances of
Gaussian and Wishart-based distributions. We show results on a range of
problems from deep learning, variational inference, and evolution strategies.
Our work opens a new direction for scalable structured geometric methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Graph Neural Networks: Methods and Applications. (arXiv:2103.00137v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00137</id>
        <link href="http://arxiv.org/abs/2103.00137"/>
        <updated>2021-07-13T01:59:37.436Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs), a generalization of deep neural networks on
graph data have been widely used in various domains, ranging from drug
discovery to recommender systems. However, GNNs on such applications are
limited when there are few available samples. Meta-learning has been an
important framework to address the lack of samples in machine learning, and in
recent years, researchers have started to apply meta-learning to GNNs. In this
work, we provide a comprehensive survey of different meta-learning approaches
involving GNNs on various graph problems showing the power of using these two
approaches together. We categorize the literature based on proposed
architectures, shared representations, and applications. Finally, we discuss
several exciting future research directions and open problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1"&gt;Debmalya Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1"&gt;Sourav Medya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uzzi_B/0/1/0/all/0/1"&gt;Brian Uzzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05204</id>
        <link href="http://arxiv.org/abs/2107.05204"/>
        <updated>2021-07-13T01:59:37.430Z</updated>
        <summary type="html"><![CDATA[Content feed, a type of product that recommends a sequence of items for users
to browse and engage with, has gained tremendous popularity among social media
platforms. In this paper, we propose to study the diversity problem in such a
scenario from an item sequence perspective using time series analysis
techniques. We derive a method called sliding spectrum decomposition (SSD) that
captures users' perception of diversity in browsing a long item sequence. We
also share our experiences in designing and implementing a suitable item
embedding method for accurate similarity measurement under long tail effect.
Combined together, they are now fully implemented and deployed in Xiaohongshu
App's production recommender system that serves the main Explore Feed product
for tens of millions of users every day. We demonstrate the effectiveness and
efficiency of the method through theoretical analysis, offline experiments and
online A/B tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanhua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiwen Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05241</id>
        <link href="http://arxiv.org/abs/2107.05241"/>
        <updated>2021-07-13T01:59:37.424Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are very popular to generate realistic
images, but they often suffer from the training instability issues and the
phenomenon of mode loss. In order to attain greater diversity in GAN
synthesized data, it is critical to solving the problem of mode loss. Our work
explores probabilistic approaches to GAN modelling that could allow us to
tackle these issues. We present Prb-GANs, a new variation that uses dropout to
create a distribution over the network parameters with the posterior learnt
using variational inference. We describe theoretically and validate
experimentally using simple and complex datasets the benefits of such an
approach. We look into further improvements using the concept of uncertainty
measures. Through a set of further modifications to the loss functions for each
network of the GAN, we are able to get results that show the improvement of GAN
performance. Our methods are extremely simple and require very little
modification to existing GAN architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1"&gt;Blessen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K. Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data. (arXiv:2107.01863v2 [gr-qc] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01863</id>
        <link href="http://arxiv.org/abs/2107.01863"/>
        <updated>2021-07-13T01:59:37.417Z</updated>
        <summary type="html"><![CDATA[LIGO is considered the most sensitive and complicated gravitational
experiment ever built. Its main objective is to detect the gravitational wave
from the strongest events in the universe by observing if the length of its
4-kilometer arms change by a distance 10,000 times smaller than the diameter of
a proton. Due to its sensitivity, LIGO is prone to the disturbance of external
noises which affects the data being collected to detect the gravitational wave.
These noises are commonly called by the LIGO community as glitches. The
objective of this study is to evaluate the effeciency of various deep trasnfer
learning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch
waveform in gravitational wave data. The accuracy achieved by the said models
are 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models
achieved fairly high accuracy, it is observed that all of the model suffered
from the lack of data for certain classes which is the main concern found in
the experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/gr-qc/1/au:+Mesuga_R/0/1/0/all/0/1"&gt;Reymond Mesuga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/gr-qc/1/au:+Bayanay_B/0/1/0/all/0/1"&gt;Brian James Bayanay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06671</id>
        <link href="http://arxiv.org/abs/2103.06671"/>
        <updated>2021-07-13T01:59:37.410Z</updated>
        <summary type="html"><![CDATA[We study the statistical theory of offline reinforcement learning (RL) with
deep ReLU network function approximation. We analyze a variant of fitted-Q
iteration (FQI) algorithm under a new dynamic condition that we call Besov
dynamic closure, which encompasses the conditions from prior analyses for deep
neural network function approximation. Under Besov dynamic closure, we prove
that the FQI-type algorithm enjoys the sample complexity of
$\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 -
2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the
dimensionality of the state-action space, $\alpha$ is the (possibly fractional)
smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified
precision. This is an improvement over the sample complexity of
$\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 -
d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an
algorithmic iteration number which is arbitrarily large in practice.
Importantly, our sample complexity is obtained under the new general dynamic
condition and a data-dependent structure where the latter is either ignored in
prior algorithms or improperly handled by prior analyses. This is the first
comprehensive analysis for offline RL with deep ReLU network function
approximation under a general setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1"&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attentive Survey of Attention Models. (arXiv:1904.02874v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.02874</id>
        <link href="http://arxiv.org/abs/1904.02874"/>
        <updated>2021-07-13T01:59:37.392Z</updated>
        <summary type="html"><![CDATA[Attention Model has now become an important concept in neural networks that
has been researched within diverse application domains. This survey provides a
structured and comprehensive overview of the developments in modeling
attention. In particular, we propose a taxonomy which groups existing
techniques into coherent categories. We review salient neural architectures in
which attention has been incorporated, and discuss applications in which
modeling attention has shown a significant impact. We also describe how
attention has been used to improve the interpretability of neural networks.
Finally, we discuss some future research directions in attention. We hope this
survey will provide a succinct introduction to attention models and guide
practitioners while developing approaches for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1"&gt;Sneha Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mithal_V/0/1/0/all/0/1"&gt;Varun Mithal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1"&gt;Gungor Polatkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1"&gt;Rohan Ramanath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08464</id>
        <link href="http://arxiv.org/abs/2011.08464"/>
        <updated>2021-07-13T01:59:37.383Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based framework to recover vehicle pose in SO(3)
from a single RGB image. In contrast to previous works that map from local
appearance to observation angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) to
estimate egocentric vehicle orientation. This approach features a deep model
that transforms perceived intensities to IGRs, which are mapped to a 3D
representation encoding object orientation in the camera coordinate system.
Core problems are what IGRs to use and how to learn them more effectively. We
answer the former question by designing IGRs based on an interpolated cuboid
that derives from primitive 3D annotation readily. The latter question
motivates us to incorporate geometry knowledge with a new loss function based
on a projective invariant. This loss function allows unlabeled data to be used
in the training stage to improve representation learning. Without additional
labels, our system outperforms previous monocular RGB-based methods for joint
vehicle detection and pose estimation on the KITTI benchmark, achieving
performance even comparable to stereo methods. Code and pre-trained models are
available at this https URL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prescient teleoperation of humanoid robots. (arXiv:2107.01281v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01281</id>
        <link href="http://arxiv.org/abs/2107.01281"/>
        <updated>2021-07-13T01:59:37.375Z</updated>
        <summary type="html"><![CDATA[Humanoid robots could be versatile and intuitive human avatars that operate
remotely in inaccessible places: the robot could reproduce in the remote
location the movements of an operator equipped with a wearable motion capture
device while sending visual feedback to the operator. While substantial
progress has been made on transferring ("retargeting") human motions to
humanoid robots, a major problem preventing the deployment of such systems in
real applications is the presence of communication delays between the human
input and the feedback from the robot: even a few hundred milliseconds of delay
can irreversibly disturb the operator, let alone a few seconds. To overcome
these delays, we introduce a system in which a humanoid robot executes commands
before it actually receives them, so that the visual feedback appears to be
synchronized to the operator, whereas the robot executed the commands in the
past. To do so, the robot continuously predicts future commands by querying a
machine learning model that is trained on past trajectories and conditioned on
the last received commands. In our experiments, an operator was able to
successfully control a humanoid robot (32 degrees of freedom) with stochastic
delays up to 2 seconds in several whole-body manipulation tasks, including
reaching different targets, picking up, and placing a box at distinct
locations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Penco_L/0/1/0/all/0/1"&gt;Luigi Penco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouret_J/0/1/0/all/0/1"&gt;Jean-Baptiste Mouret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ivaldi_S/0/1/0/all/0/1"&gt;Serena Ivaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04813</id>
        <link href="http://arxiv.org/abs/2107.04813"/>
        <updated>2021-07-13T01:59:37.352Z</updated>
        <summary type="html"><![CDATA[Plant leaf diseases pose a significant danger to food security and they cause
depletion in quality and volume of production. Therefore accurate and timely
detection of leaf disease is very important to check the loss of the crops and
meet the growing food demand of the people. Conventional techniques depend on
lab investigation and human skills which are generally costly and inaccessible.
Recently, Deep Neural Networks have been exceptionally fruitful in image
classification. In this research paper, plant leaf disease detection employing
transfer learning is explored in the JPEG compressed domain. Here, the JPEG
compressed stream consisting of DCT coefficients is, directly fed into the
Neural Network to improve the efficiency of classification. The experimental
results on JPEG compressed leaf dataset demonstrate the efficacy of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Atul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1"&gt;Bulla Rajesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability. (arXiv:2003.12699v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12699</id>
        <link href="http://arxiv.org/abs/2003.12699"/>
        <updated>2021-07-13T01:59:37.345Z</updated>
        <summary type="html"><![CDATA[We consider the general (stochastic) contextual bandit problem under the
realizability assumption, i.e., the expected reward, as a function of contexts
and actions, belongs to a general function class $\mathcal{F}$. We design a
fast and simple algorithm that achieves the statistically optimal regret with
only ${O}(\log T)$ calls to an offline regression oracle across all $T$ rounds.
The number of oracle calls can be further reduced to $O(\log\log T)$ if $T$ is
known in advance. Our results provide the first universal and optimal reduction
from contextual bandits to offline regression, solving an important open
problem in the contextual bandit literature. A direct consequence of our
results is that any advances in offline regression immediately translate to
contextual bandits, statistically and computationally. This leads to faster
algorithms and improved regret guarantees for broader classes of contextual
bandit problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yunzong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05235</id>
        <link href="http://arxiv.org/abs/2107.05235"/>
        <updated>2021-07-13T01:59:37.330Z</updated>
        <summary type="html"><![CDATA[Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yutao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies. (arXiv:2006.11645v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11645</id>
        <link href="http://arxiv.org/abs/2006.11645"/>
        <updated>2021-07-13T01:59:37.315Z</updated>
        <summary type="html"><![CDATA[We consider the problem of reinforcement learning when provided with (1) a
baseline control policy and (2) a set of constraints that the learner must
satisfy. The baseline policy can arise from demonstration data or a teacher
agent and may provide useful cues for learning, but it might also be
sub-optimal for the task at hand, and is not guaranteed to satisfy the
specified constraints, which might encode safety, fairness or other
application-specific requirements. In order to safely learn from baseline
policies, we propose an iterative policy optimization algorithm that alternates
between maximizing expected return on the task, minimizing distance to the
baseline policy, and projecting the policy onto the constraint-satisfying set.
We analyze our algorithm theoretically and provide a finite-time convergence
guarantee. In our experiments on five different control tasks, our algorithm
consistently outperforms several state-of-the-art baselines, achieving 10 times
fewer constraint violations and 40% higher reward on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosca_J/0/1/0/all/0/1"&gt;Justinian Rosca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05188</id>
        <link href="http://arxiv.org/abs/2107.05188"/>
        <updated>2021-07-13T01:59:37.309Z</updated>
        <summary type="html"><![CDATA[In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1"&gt;Hu Menghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1"&gt;Zhai Guangtao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1"&gt;Zhang Xiao-Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04721</id>
        <link href="http://arxiv.org/abs/2107.04721"/>
        <updated>2021-07-13T01:59:37.292Z</updated>
        <summary type="html"><![CDATA[Fundus photography has routinely been used to document the presence and
severity of retinal degenerative diseases such as age-related macular
degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical
practice, for which the fovea and optic disc (OD) are important retinal
landmarks. However, the occurrence of lesions, drusen, and other retinal
abnormalities during retinal degeneration severely complicates automatic
landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net
backbone enriched with hierarchical bottleneck attention. The network consists
of a novel bottleneck attention block that combines and refines self-attention,
channel attention, and relative-position attention to highlight retinal
abnormalities that may be important for fovea and OD segmentation in the
degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea
detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of
25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for
AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:
ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for
landmark detection in the presence of a variety of retinal degenerative
diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shuyun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Ziming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1"&gt;Jacob Granley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1"&gt;Michael Beyeler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14473</id>
        <link href="http://arxiv.org/abs/2106.14473"/>
        <updated>2021-07-13T01:59:37.287Z</updated>
        <summary type="html"><![CDATA[Physics informed neural networks approximate solutions of PDEs by minimizing
pointwise residuals. We derive rigorous bounds on the error, incurred by PINNs
in approximating the solutions of a large class of linear parabolic PDEs,
namely Kolmogorov equations that include the heat equation and Black-Scholes
equation of option pricing, as examples. We construct neural networks, whose
PINN residual (generalization error) can be made as small as desired. We also
prove that the total $L^2$-error can be bounded by the generalization error,
which in turn is bounded in terms of the training error, provided that a
sufficient number of randomly chosen training (collocation) points is used.
Moreover, we prove that the size of the PINNs and the number of training
samples only grow polynomially with the underlying dimension, enabling PINNs to
overcome the curse of dimensionality in this context. These results enable us
to provide a comprehensive error analysis for PINNs in approximating Kolmogorov
PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1"&gt;Tim De Ryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Siddhartha Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation. (arXiv:2107.05045v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05045</id>
        <link href="http://arxiv.org/abs/2107.05045"/>
        <updated>2021-07-13T01:59:37.281Z</updated>
        <summary type="html"><![CDATA[Learning from positive and unlabeled (PU) data is an important problem in
various applications. Most of the recent approaches for PU classification
assume that the class-prior (the ratio of positive samples) in the training
unlabeled dataset is identical to that of the test data, which does not hold in
many practical cases. In addition, we usually do not know the class-priors of
the training and test data, thus we have no clue on how to train a classifier
without them. To address these problems, we propose a novel PU classification
method based on density ratio estimation. A notable advantage of our proposed
method is that it does not require the class-priors in the training phase;
class-prior shift is incorporated only in the test phase. We theoretically
justify our proposed method and experimentally demonstrate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shota Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Longitudinal Correlation Analysis for Decoding Multi-Modal Brain Development. (arXiv:2107.04724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04724</id>
        <link href="http://arxiv.org/abs/2107.04724"/>
        <updated>2021-07-13T01:59:37.276Z</updated>
        <summary type="html"><![CDATA[Starting from childhood, the human brain restructures and rewires throughout
life. Characterizing such complex brain development requires effective analysis
of longitudinal and multi-modal neuroimaging data. Here, we propose such an
analysis approach named Longitudinal Correlation Analysis (LCA). LCA couples
the data of two modalities by first reducing the input from each modality to a
latent representation based on autoencoders. A self-supervised strategy then
relates the two latent spaces by jointly disentangling two directions, one in
each space, such that the longitudinal changes in latent representations along
those directions are maximally correlated between modalities. We applied LCA to
analyze the longitudinal T1-weighted and diffusion-weighted MRIs of 679 youths
from the National Consortium on Alcohol and Neurodevelopment in Adolescence.
Unlike existing approaches that focus on either cross-sectional or single-modal
modeling, LCA successfully unraveled coupled macrostructural and
microstructural brain development from morphological and diffusivity features
extracted from the data. A retesting of LCA on raw 3D image volumes of those
subjects successfully replicated the findings from the feature-based analysis.
Lastly, the developmental effects revealed by LCA were inline with the current
understanding of maturational patterns of the adolescent brain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M. Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05223</id>
        <link href="http://arxiv.org/abs/2107.05223"/>
        <updated>2021-07-13T01:59:37.270Z</updated>
        <summary type="html"><![CDATA[This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yi-Hui Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1"&gt;I-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chin-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cautious Actor-Critic. (arXiv:2107.05217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05217</id>
        <link href="http://arxiv.org/abs/2107.05217"/>
        <updated>2021-07-13T01:59:37.253Z</updated>
        <summary type="html"><![CDATA[The oscillating performance of off-policy learning and persisting errors in
the actor-critic (AC) setting call for algorithms that can conservatively learn
to suit the stability-critical applications better. In this paper, we propose a
novel off-policy AC algorithm cautious actor-critic (CAC). The name cautious
comes from the doubly conservative nature that we exploit the classic policy
interpolation from conservative policy iteration for the actor and the
entropy-regularization of conservative value iteration for the critic. Our key
observation is the entropy-regularized critic facilitates and simplifies the
unwieldy interpolated actor update while still ensuring robust policy
improvement. We compare CAC to state-of-the-art AC methods on a set of
challenging continuous control problems and demonstrate that CAC achieves
comparable performance while significantly stabilizes learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamura_T/0/1/0/all/0/1"&gt;Toshinori Kitamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1"&gt;Takamitsu Matsubara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polynomial Time Reinforcement Learning in Correlated FMDPs with Linear Value Functions. (arXiv:2107.05187v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05187</id>
        <link href="http://arxiv.org/abs/2107.05187"/>
        <updated>2021-07-13T01:59:37.247Z</updated>
        <summary type="html"><![CDATA[Many reinforcement learning (RL) environments in practice feature enormous
state spaces that may be described compactly by a "factored" structure, that
may be modeled by Factored Markov Decision Processes (FMDPs). We present the
first polynomial-time algorithm for RL with FMDPs that does not rely on an
oracle planner, and instead of requiring a linear transition model, only
requires a linear value function with a suitable local basis with respect to
the factorization. With this assumption, we can solve FMDPs in polynomial time
by constructing an efficient separation oracle for convex optimization.
Importantly, and in contrast to prior work, we do not assume that the
transitions on various factors are independent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devic_S/0/1/0/all/0/1"&gt;Siddartha Devic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zihao Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juba_B/0/1/0/all/0/1"&gt;Brendan Juba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Re-understanding Finite-State Representations of Recurrent Policy Networks. (arXiv:2006.03745v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03745</id>
        <link href="http://arxiv.org/abs/2006.03745"/>
        <updated>2021-07-13T01:59:37.241Z</updated>
        <summary type="html"><![CDATA[We introduce an approach for understanding control policies represented as
recurrent neural networks. Recent work has approached this problem by
transforming such recurrent policy networks into finite-state machines (FSM)
and then analyzing the equivalent minimized FSM. While this led to interesting
insights, the minimization process can obscure a deeper understanding of a
machine's operation by merging states that are semantically distinct. To
address this issue, we introduce an analysis approach that starts with an
unminimized FSM and applies more-interpretable reductions that preserve the key
decision points of the policy. We also contribute an attention tool to attain a
deeper understanding of the role of observations in the decisions. Our case
studies on 7 Atari games and 3 control benchmarks demonstrate that the approach
can reveal insights that have not been previously noticed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1"&gt;Mohamad H. Danesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anurag Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1"&gt;Alan Fern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1"&gt;Saeed Khorram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors to Improve Covariance Matrix Estimation. (arXiv:2107.05201v1 [q-fin.RM])]]></title>
        <id>http://arxiv.org/abs/2107.05201</id>
        <link href="http://arxiv.org/abs/2107.05201"/>
        <updated>2021-07-13T01:59:37.235Z</updated>
        <summary type="html"><![CDATA[Modeling and managing portfolio risk is perhaps the most important step to
achieve growing and preserving investment performance. Within the modern
portfolio construction framework that built on Markowitz's theory, the
covariance matrix of stock returns is required to model the portfolio risk.
Traditional approaches to estimate the covariance matrix are based on human
designed risk factors, which often requires tremendous time and effort to
design better risk factors to improve the covariance estimation. In this work,
we formulate the quest of mining risk factors as a learning problem and propose
a deep learning solution to effectively "design" risk factors with neural
networks. The learning objective is carefully set to ensure the learned risk
factors are effective in explaining stock returns as well as have desired
orthogonality and stability. Our experiments on the stock market data
demonstrate the effectiveness of the proposed method: our method can obtain
$1.9\%$ higher explained variance measured by $R^2$ and also reduce the risk of
a global minimum variance portfolio. Incremental analysis further supports our
design of both the architecture and the learning objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hengxu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Over-parameterized Models with Non-decomposable Objectives. (arXiv:2107.04641v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04641</id>
        <link href="http://arxiv.org/abs/2107.04641"/>
        <updated>2021-07-13T01:59:37.229Z</updated>
        <summary type="html"><![CDATA[Many modern machine learning applications come with complex and nuanced
design goals such as minimizing the worst-case error, satisfying a given
precision or recall target, or enforcing group-fairness constraints. Popular
techniques for optimizing such non-decomposable objectives reduce the problem
into a sequence of cost-sensitive learning tasks, each of which is then solved
by re-weighting the training loss with example-specific costs. We point out
that the standard approach of re-weighting the loss to incorporate label costs
can produce unsatisfactory results when used to train over-parameterized
models. As a remedy, we propose new cost-sensitive losses that extend the
classical idea of logit adjustment to handle more general cost matrices. Our
losses are calibrated, and can be further improved with distilled labels from a
teacher model. Through experiments on benchmark image datasets, we showcase the
effectiveness of our approach in training ResNet models with common robust and
constrained optimization objectives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective. (arXiv:2107.05101v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05101</id>
        <link href="http://arxiv.org/abs/2107.05101"/>
        <updated>2021-07-13T01:59:37.223Z</updated>
        <summary type="html"><![CDATA[The improvement of computers' capacities, advancements in algorithmic
techniques, and the significant increase of available data have enabled the
recent developments of Artificial Intelligence (AI) technology. One of its
branches, called Machine Learning (ML), has shown strong capacities in
mimicking characteristics attributed to human intelligence, such as vision,
speech, and problem-solving. However, as previous technological revolutions
suggest, their most significant impacts could be mostly expected on other
sectors that were not traditional users of that technology. The agricultural
sector is vital for African economies; improving yields, mitigating losses, and
effective management of natural resources are crucial in a climate change era.
Machine Learning is a technology with an added value in making predictions,
hence the potential to reduce uncertainties and risk across sectors, in this
case, the agricultural sector. The purpose of this paper is to contextualize
and discuss barriers to ML-based solutions for African agriculture. In the
second section, we provided an overview of ML technology from a historical and
technical perspective and its main driving force. In the third section, we
provided a brief review of the current use of ML in agriculture. Finally, in
section 4, we discuss ML growing interest in Africa and the potential barriers
to creating and using ML-based solutions in the agricultural sector.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ly_R/0/1/0/all/0/1"&gt;Racine Ly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07019</id>
        <link href="http://arxiv.org/abs/2005.07019"/>
        <updated>2021-07-13T01:59:37.204Z</updated>
        <summary type="html"><![CDATA[Social media has become an essential channel for posting disaster-related
information, which provide governments and relief agencies real-time data for
better disaster management. However, research in this field has not received
sufficient attention and extracting useful information is still challenging.
This paper aims to improve disaster relief efficiency via mining and analyzing
social media data like public attitudes towards disaster response and public
demands for targeted relief supplies during different types of disasters. We
focus on different natural disasters based on properties such as types,
durations, and damages, which contains a total of 41,993 tweets. In this paper,
public perception is assessed qualitatively by manually classified tweets,
which contain information like the demand for targeted relief supplies,
satisfactions of disaster response, and public fear. Public attitudes to
natural disasters are studied via a quantitative analysis using eight machine
learning models. To better provide decision-makers with the appropriate model,
the comparison of machine learning models based on computational time and
prediction accuracy is conducted. The change of public opinion during different
natural disasters and the evolution of people's behavior of using social media
for disaster relief in the face of the identical type of natural disasters as
Twitter continues to evolve are studied. The results in this paper demonstrate
the feasibility and validation of the proposed research approach and provide
relief agencies with insights into better disaster management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhijie Sasha Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1"&gt;Lauren Christenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1"&gt;Lawrence Fulton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05342</id>
        <link href="http://arxiv.org/abs/2107.05342"/>
        <updated>2021-07-13T01:59:37.198Z</updated>
        <summary type="html"><![CDATA[Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1"&gt;Numan Celik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Soumya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1"&gt;Barbara Braden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning. (arXiv:2107.05127v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05127</id>
        <link href="http://arxiv.org/abs/2107.05127"/>
        <updated>2021-07-13T01:59:37.192Z</updated>
        <summary type="html"><![CDATA[Adversarial learning is used to test the robustness of machine learning
algorithms under attack and create attacks that deceive the anomaly detection
methods in Industrial Control System (ICS). Given that security assessment of
an ICS demands that an exhaustive set of possible attack patterns is studied,
in this work, we propose an association rule mining-based attack generation
technique. The technique has been implemented using data from a secure Water
Treatment plant. The proposed technique was able to generate more than 300,000
attack patterns constituting a vast majority of new attack vectors which were
not seen before. Automatically generated attacks improve our understanding of
the potential attacks and enable the design of robust attack detection
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Umer_M/0/1/0/all/0/1"&gt;Muhammad Azmi Umer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_C/0/1/0/all/0/1"&gt;Chuadhry Mujeeb Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jilani_M/0/1/0/all/0/1"&gt;Muhammad Taha Jilani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1"&gt;Aditya P. Mathur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preference-based Online Learning with Dueling Bandits: A Survey. (arXiv:1807.11398v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1807.11398</id>
        <link href="http://arxiv.org/abs/1807.11398"/>
        <updated>2021-07-13T01:59:37.185Z</updated>
        <summary type="html"><![CDATA[In machine learning, the notion of multi-armed bandits refers to a class of
online learning problems, in which an agent is supposed to simultaneously
explore and exploit a given set of choice alternatives in the course of a
sequential decision process. In the standard setting, the agent learns from
stochastic feedback in the form of real-valued rewards. In many applications,
however, numerical reward signals are not readily available -- instead, only
weaker information is provided, in particular relative preferences in the form
of qualitative comparisons between pairs of alternatives. This observation has
motivated the study of variants of the multi-armed bandit problem, in which
more general representations are used both for the type of feedback to learn
from and the target of prediction. The aim of this paper is to provide a survey
of the state of the art in this field, referred to as preference-based
multi-armed bandits or dueling bandits. To this end, we provide an overview of
problems that have been considered in the literature as well as methods for
tackling them. Our taxonomy is mainly based on the assumptions made by these
methods about the data-generating process and, related to this, the properties
of the preference-based feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1"&gt;Viktor Bengs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Busa_Fekete_R/0/1/0/all/0/1"&gt;Robert Busa-Fekete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mesaoudi_Paul_A/0/1/0/all/0/1"&gt;Adil El Mesaoudi-Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Langevin Algorithms for Sampling From Log-concave Densities. (arXiv:1903.12322v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12322</id>
        <link href="http://arxiv.org/abs/1903.12322"/>
        <updated>2021-07-13T01:59:37.168Z</updated>
        <summary type="html"><![CDATA[For sampling from a log-concave density, we study implicit integrators
resulting from $\theta$-method discretization of the overdamped Langevin
diffusion stochastic differential equation. Theoretical and algorithmic
properties of the resulting sampling methods for $ \theta \in [0,1] $ and a
range of step sizes are established. Our results generalize and extend prior
works in several directions. In particular, for $\theta\ge1/2$, we prove
geometric ergodicity and stability of the resulting methods for all step sizes.
We show that obtaining subsequent samples amounts to solving a strongly-convex
optimization problem, which is readily achievable using one of numerous
existing methods. Numerical examples supporting our theoretical analysis are
also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1"&gt;Robert Salomone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roosta_F/0/1/0/all/0/1"&gt;Fred Roosta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05382</id>
        <link href="http://arxiv.org/abs/2107.05382"/>
        <updated>2021-07-13T01:59:37.161Z</updated>
        <summary type="html"><![CDATA[We propose a semi-supervised learning method for building end-to-end rich
transcription-style automatic speech recognition (RT-ASR) systems from
small-scale rich transcription-style and large-scale common transcription-style
datasets. In spontaneous speech tasks, various speech phenomena such as
fillers, word fragments, laughter and coughs, etc. are often included. While
common transcriptions do not give special awareness to these phenomena, rich
transcriptions explicitly convert them into special phenomenon tokens as well
as textual tokens. In previous studies, the textual and phenomenon tokens were
simultaneously estimated in an end-to-end manner. However, it is difficult to
build accurate RT-ASR systems because large-scale rich transcription-style
datasets are often unavailable. To solve this problem, our training method uses
a limited rich transcription-style dataset and common transcription-style
dataset simultaneously. The Key process in our semi-supervised learning is to
convert the common transcription-style dataset into a pseudo-rich
transcription-style dataset. To this end, we introduce style tokens which
control phenomenon tokens are generated or not into transformer-based
autoregressive modeling. We use this modeling for generating the pseudo-rich
transcription-style datasets and for building RT-ASR system from the pseudo and
original datasets. Our experiments on spontaneous ASR tasks showed the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05047</id>
        <link href="http://arxiv.org/abs/2107.05047"/>
        <updated>2021-07-13T01:59:37.155Z</updated>
        <summary type="html"><![CDATA[Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of AI models for clinical decision support. For medical
images, saliency maps are the most common form of explanation. The maps
highlight important features for AI model's prediction. Although many saliency
map methods have been proposed, it is unknown how well they perform on
explaining decisions on multi-modal medical images, where each modality/channel
carries distinct clinical meanings of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the MSFI
(Modality-Specific Feature Importance) metric to examine whether saliency maps
can highlight modality-specific important features. MSFI encodes the clinical
requirements on modality prioritization and modality-specific feature
localization. Our evaluations on 16 commonly used saliency map methods,
including a clinician user study, show that although most saliency map methods
captured modality importance information in general, most of them failed to
highlight modality-specific important features consistently and precisely. The
evaluation results guide the choices of saliency map methods and provide
insights to propose new ones targeting clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Weina Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1"&gt;Ghassan Hamarneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Traffic Prediction as a Matrix Completion Problem with Ensemble Learning. (arXiv:2001.02492v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02492</id>
        <link href="http://arxiv.org/abs/2001.02492"/>
        <updated>2021-07-13T01:59:37.149Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of short-term traffic prediction for
signalized traffic operations management. Specifically, we focus on predicting
sensor states in high-resolution (second-by-second). This contrasts with
traditional traffic forecasting problems, which have focused on predicting
aggregated traffic variables, typically over intervals that are no shorter than
5 minutes. Our contributions can be summarized as offering three insights:
first, we show how the prediction problem can be modeled as a matrix completion
problem. Second, we employ a block-coordinate descent algorithm and demonstrate
that the algorithm converges in sub-linear time to a block coordinate-wise
optimizer. This allows us to capitalize on the "bigness" of high-resolution
data in a computationally feasible way. Third, we develop an ensemble learning
(or adaptive boosting) approach to reduce the training error to within any
arbitrary error threshold. The latter utilizes past days so that the boosting
can be interpreted as capturing periodic patterns in the data. The performance
of the proposed method is analyzed theoretically and tested empirically using
both simulated data and a real-world high-resolution traffic dataset from Abu
Dhabi, UAE. Our experimental results show that the proposed method outperforms
other state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chuhan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jabari_S/0/1/0/all/0/1"&gt;Saif Eddin Jabari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning via Maximizing Correlation with Sparse and Hierarchical Extensions. (arXiv:2107.05330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05330</id>
        <link href="http://arxiv.org/abs/2107.05330"/>
        <updated>2021-07-13T01:59:37.143Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a collaborative machine learning technique to
train a global model without obtaining clients' private data. The main
challenges in FL are statistical diversity among clients, limited computing
capability among client equipments and the excessive communication overhead and
long latency between server and clients. To address these problems,

we propose a novel personalized federated learning via maximizing correlation
pFedMac), and further extend it to sparse and hierarchical models. By
minimizing loss functions including the properties of an approximated L1-norm
and the hierarchical correlation, the performance on statistical diversity data
is improved and the communicational and computational loads required in the
network are reduced. Theoretical proofs show that pFedMac performs better than
the L2-norm distance based personalization methods. Experimentally, we
demonstrate the benefits of this sparse hierarchical personalization
architecture compared with the state-of-the-art personalization methods and
their extensions (e.g. pFedMac achieves 99.75% accuracy on MNIST and 87.27%
accuracy on Synthetic under heterogeneous and non-i.i.d data distributions)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1"&gt;YinchuanLi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1"&gt;XiaofengLiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XuZhang/0/1/0/all/0/1"&gt;XuZhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1"&gt;YunfengShao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1"&gt;QingWang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1"&gt;YanhuiGeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-13T01:59:37.138Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05377</id>
        <link href="http://arxiv.org/abs/2107.05377"/>
        <updated>2021-07-13T01:59:37.103Z</updated>
        <summary type="html"><![CDATA[In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianwen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianwei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shenghuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping. (arXiv:2107.05341v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05341</id>
        <link href="http://arxiv.org/abs/2107.05341"/>
        <updated>2021-07-13T01:59:37.083Z</updated>
        <summary type="html"><![CDATA[We explore the ability of overparameterized shallow neural networks to learn
Lipschitz regression functions with and without label noise when trained by
Gradient Descent (GD). To avoid the problem that in the presence of noisy
labels, neural networks trained to nearly zero training error are inconsistent
on this class, we propose an early stopping rule that allows us to show optimal
rates. This provides an alternative to the result of Hu et al. (2021) who
studied the performance of $\ell 2$ -regularized GD for training shallow
networks in nonparametric regression which fully relied on the infinite-width
network (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler
analysis which is based on a partitioning argument of the input space (as in
the case of 1-nearest-neighbor rule) coupled with the fact that trained neural
networks are smooth with respect to their inputs when trained by GD. In the
noise-free case the proof does not rely on any kernelization and can be
regarded as a finite-width result. In the case of label noise, by slightly
modifying the proof, the noise is controlled using a technique of Yao, Rosasco,
and Caponnetto (2007).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuzborskij_I/0/1/0/all/0/1"&gt;Ilja Kuzborskij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05380</id>
        <link href="http://arxiv.org/abs/2107.05380"/>
        <updated>2021-07-13T01:59:37.077Z</updated>
        <summary type="html"><![CDATA[In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rudrajit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards explainable meta-learning. (arXiv:2002.04276v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04276</id>
        <link href="http://arxiv.org/abs/2002.04276"/>
        <updated>2021-07-13T01:59:37.071Z</updated>
        <summary type="html"><![CDATA[Meta-learning is a field that aims at discovering how different machine
learning algorithms perform on a wide range of predictive tasks. Such knowledge
speeds up the hyperparameter tuning or feature engineering. With the use of
surrogate models various aspects of the predictive task such as meta-features,
landmarker models e.t.c. are used to predict the expected performance. State of
the art approaches are focused on searching for the best meta-model but do not
explain how these different aspects contribute to its performance. However, to
build a new generation of meta-models we need a deeper understanding of the
importance and effect of meta-features on the model tunability. In this paper,
we propose techniques developed for eXplainable Artificial Intelligence (XAI)
to examine and extract knowledge from black-box surrogate models. To our
knowledge, this is the first paper that shows how post-hoc explainability can
be used to improve the meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Woznica_K/0/1/0/all/0/1"&gt;Katarzyna Wo&amp;#x17a;nica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Biecek_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05295</id>
        <link href="http://arxiv.org/abs/2107.05295"/>
        <updated>2021-07-13T01:59:37.054Z</updated>
        <summary type="html"><![CDATA[Danish natural language processing (NLP) has in recent years obtained
considerable improvements with the addition of multiple new datasets and
models. However, at present, there is no coherent framework for applying
state-of-the-art models for Danish. We present DaCy: a unified framework for
Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain
state-of-the-art performance on named entity recognition, part-of-speech
tagging, and dependency parsing. DaCy contains tools for easy integration of
existing models such as for polarity, emotion, or subjectivity detection. In
addition, we conduct a series of tests for biases and robustness of Danish NLP
pipelines through augmentation of the test set of DaNE. DaCy large compares
favorably and is especially robust to long input lengths and spelling
variations and errors. All models except DaCy large display significant biases
related to ethnicity while only Polyglot shows a significant gender bias. We
argue that for languages with limited benchmark sets, data augmentation can be
particularly useful for obtaining more realistic and fine-grained performance
estimates. We provide a series of augmenters as a first step towards a more
thorough evaluation of language models for low and medium resource languages
and encourage further development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1"&gt;Kenneth Enevoldsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1"&gt;Lasse Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1"&gt;Kristoffer Nielbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metalearning Linear Bandits by Prior Update. (arXiv:2107.05320v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05320</id>
        <link href="http://arxiv.org/abs/2107.05320"/>
        <updated>2021-07-13T01:59:37.048Z</updated>
        <summary type="html"><![CDATA[Fully Bayesian approaches to sequential decision-making assume that problem
parameters are generated from a known prior, while in practice, such
information is often lacking, and needs to be estimated through learning. This
problem is exacerbated in decision-making setups with partial information,
where using a misspecified prior may lead to poor exploration and inferior
performance. In this work we prove, in the context of stochastic linear bandits
and Gaussian priors, that as long as the prior estimate is sufficiently close
to the true prior, the performance of an algorithm that uses the misspecified
prior is close to that of the algorithm that uses the true prior. Next, we
address the task of learning the prior through metalearning, where a learner
updates its estimate of the prior across multiple task instances in order to
improve performance on future tasks. The estimated prior is then updated within
each task based on incoming observations, while actions are selected in order
to maximize expected reward. In this work we apply this scheme within a linear
bandit setting, and provide algorithms and regret bounds, demonstrating its
effectiveness, as compared to an algorithm that knows the correct prior. Our
results hold for a broad class of algorithms, including, for example, Thompson
Sampling and Information Directed Sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Peleg_A/0/1/0/all/0/1"&gt;Amit Peleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pearl_N/0/1/0/all/0/1"&gt;Naama Pearl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meir_R/0/1/0/all/0/1"&gt;Ron Meir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Mean Embeddings. (arXiv:1805.08845v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1805.08845</id>
        <link href="http://arxiv.org/abs/1805.08845"/>
        <updated>2021-07-13T01:59:37.042Z</updated>
        <summary type="html"><![CDATA[Counterfactual inference has become a ubiquitous tool in online
advertisement, recommendation systems, medical diagnosis, and econometrics.
Accurate modeling of outcome distributions associated with different
interventions -- known as counterfactual distributions -- is crucial for the
success of these applications. In this work, we propose to model counterfactual
distributions using a novel Hilbert space representation called counterfactual
mean embedding (CME). The CME embeds the associated counterfactual distribution
into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite
kernel, which allows us to perform causal inference over the entire landscape
of the counterfactual distribution. Based on this representation, we propose a
distributional treatment effect (DTE) that can quantify the causal effect over
entire outcome distributions. Our approach is nonparametric as the CME can be
estimated under the unconfoundedness assumption from observational data without
requiring any parametric assumption about the underlying distributions. We also
establish a rate of convergence of the proposed estimator which depends on the
smoothness of the conditional mean and the Radon-Nikodym derivative of the
underlying marginal distributions. Furthermore, our framework allows for more
complex outcomes such as images, sequences, and graphs. Our experimental
results on synthetic data and off-policy evaluation tasks demonstrate the
advantages of the proposed estimator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1"&gt;Motonobu Kanagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saengkyongam_S/0/1/0/all/0/1"&gt;Sorawit Saengkyongam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marukatat_S/0/1/0/all/0/1"&gt;Sanparith Marukatat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05270</id>
        <link href="http://arxiv.org/abs/2107.05270"/>
        <updated>2021-07-13T01:59:37.035Z</updated>
        <summary type="html"><![CDATA[Breast cancer is the most common malignancy in women. Mammographic findings
such as microcalcifications and masses, as well as morphologic features of
masses in sonographic scans, are the main diagnostic targets for tumor
detection. However, improved specificity of these imaging modalities is
required. A leading alternative target is neoangiogenesis. When pathological,
it contributes to the development of numerous types of tumors, and the
formation of metastases. Hence, demonstrating neoangiogenesis by visualization
of the microvasculature may be of great importance. Super resolution ultrasound
localization microscopy enables imaging of the microvasculature at the
capillary level. Yet, challenges such as long reconstruction time, dependency
on prior knowledge of the system Point Spread Function (PSF), and separability
of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation
of super-resolution US into the clinic. In this work we use a deep neural
network architecture that makes effective use of signal structure to address
these challenges. We present in vivo human results of three different breast
lesions acquired with a clinical US scanner. By leveraging our trained network,
the microvasculature structure is recovered in a short time, without prior PSF
knowledge, and without requiring separability of the UCAs. Each of the
recoveries exhibits a different structure that corresponds with the known
histological structure. This study demonstrates the feasibility of in vivo
human super resolution, based on a clinical scanner, to increase US specificity
for different breast lesions and promotes the use of US in the diagnosis of
breast pathologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1"&gt;Or Bar-Shira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1"&gt;Ahuva Grubstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1"&gt;Yael Rapson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1"&gt;Dror Suhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1"&gt;Eli Atar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1"&gt;Keren Peri-Hanania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1"&gt;Ronnie Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HEMP: High-order Entropy Minimization for neural network comPression. (arXiv:2107.05298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05298</id>
        <link href="http://arxiv.org/abs/2107.05298"/>
        <updated>2021-07-13T01:59:37.029Z</updated>
        <summary type="html"><![CDATA[We formulate the entropy of a quantized artificial neural network as a
differentiable function that can be plugged as a regularization term into the
cost function minimized by gradient descent. Our formulation scales efficiently
beyond the first order and is agnostic of the quantization scheme. The network
can then be trained to minimize the entropy of the quantized parameters, so
that they can be optimally compressed via entropy coding. We experiment with
our entropy formulation at quantizing and compressing well-known network
architectures over multiple datasets. Our approach compares favorably over
similar methods, enjoying the benefits of higher order entropy estimate,
showing flexibility towards non-uniform quantization (we use Lloyd-max
quantization), scalability towards any entropy order to be minimized and
efficiency in terms of compression. We show that HEMP is able to work in
synergy with other approaches aiming at pruning or quantizing the model itself,
delivering significant benefits in terms of storage size compressibility
without harming the model's performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1"&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1"&gt;Attilio Fiandrotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cagnazzo_M/0/1/0/all/0/1"&gt;Marco Cagnazzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grangetto_M/0/1/0/all/0/1"&gt;Marco Grangetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Brownian motion in the transformer model. (arXiv:2107.05264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05264</id>
        <link href="http://arxiv.org/abs/2107.05264"/>
        <updated>2021-07-13T01:59:37.013Z</updated>
        <summary type="html"><![CDATA[Transformer is the state of the art model for many language and visual tasks.
In this paper, we give a deep analysis of its multi-head self-attention (MHSA)
module and find that: 1) Each token is a random variable in high dimensional
feature space. 2) After layer normalization, these variables are mapped to
points on the hyper-sphere. 3) The update of these tokens is a Brownian motion.
The Brownian motion has special properties, its second order item should not be
ignored. So we present a new second-order optimizer(an iterative K-FAC
algorithm) for the MHSA module.

In some short words: All tokens are mapped to high dimension hyper-sphere.
The Scaled Dot-Product Attention
$softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}})$ is just the Markov
transition matrix for the random walking on the sphere. And the deep learning
process would learn proper kernel function to get proper positions of these
tokens. The training process in the MHSA module corresponds to a Brownian
motion worthy of further study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingshi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2107.05252</id>
        <link href="http://arxiv.org/abs/2107.05252"/>
        <updated>2021-07-13T01:59:37.006Z</updated>
        <summary type="html"><![CDATA[We propose OmniLytics, a blockchain-based secure data trading marketplace for
machine learning applications. Utilizing OmniLytics, many distributed data
owners can contribute their private data to collectively train a ML model
requested by some model owners, and get compensated for data contribution.
OmniLytics enables such model training while simultaneously providing 1) model
security against curious data owners; 2) data security against curious model
and data owners; 3) resilience to malicious data owners who provide faulty
results to poison model training; and 4) resilience to malicious model owner
who intents to evade the payment. OmniLytics is implemented as a smart contract
on the Ethereum blockchain to guarantee the atomicity of payment. In
OmniLytics, a model owner publishes encrypted initial model on the contract,
over which the participating data owners compute gradients using their private
data, and securely aggregate the gradients through the contract. Finally, the
contract reimburses the data owners, and the model owner decrypts the
aggregated model update. We implement a working prototype of OmniLytics on
Ethereum, and perform extensive experiments to measure its gas cost and
execution time under various parameter combinations, demonstrating its high
computation and cost efficiency and strong practicality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiacheng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wensi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Songze Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05087</id>
        <link href="http://arxiv.org/abs/2107.05087"/>
        <updated>2021-07-13T01:59:37.000Z</updated>
        <summary type="html"><![CDATA[Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Joshua Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chau-Wai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05085</id>
        <link href="http://arxiv.org/abs/2107.05085"/>
        <updated>2021-07-13T01:59:36.993Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that lung cancer screening using annual low-dose
computed tomography (CT) reduces lung cancer mortality by 20% compared to
traditional chest radiography. Therefore, CT lung screening has started to be
used widely all across the world. However, analyzing these images is a serious
burden for radiologists. The number of slices in a CT scan can be up to 600.
Therefore, computer-aided-detection (CAD) systems are very important for faster
and more accurate assessment of the data. In this study, we proposed a
framework that analyzes CT lung screenings using convolutional neural networks
(CNNs) to reduce false positives. We trained our model with different volume
sizes and showed that volume size plays a critical role in the performance of
the system. We also used different fusions in order to show their power and
effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D
convolutional operations applied to 3D data could result in information loss.
The proposed framework has been tested on the dataset provided by the LUNA16
Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1"&gt;Gorkem Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1"&gt;Yesim Dogrusoz Serinagaoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1"&gt;Ugur Halici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05289</id>
        <link href="http://arxiv.org/abs/2107.05289"/>
        <updated>2021-07-13T01:59:36.982Z</updated>
        <summary type="html"><![CDATA[We consider a continuous-time multi-arm bandit problem (CTMAB), where the
learner can sample arms any number of times in a given interval and obtain a
random reward from each sample, however, increasing the frequency of sampling
incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining
large reward and incurring sampling cost as a function of the sampling
frequency. The goal is to design a learning algorithm that minimizes regret,
that is defined as the difference of the payoff of the oracle policy and that
of the learning algorithm. CTMAB is fundamentally different than the usual
multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial
in CTMAB, since the optimal sampling frequency depends on the mean of the arm,
which needs to be estimated. We first establish lower bounds on the regret
achievable with any algorithm and then propose algorithms that achieve the
lower bound up to logarithmic factors. For the single-arm case, we show that
the lower bound on the regret is $\Omega((\log T)^2/\mu)$, where $\mu$ is the
mean of the arm, and $T$ is the time horizon. For the multiple arms case, we
show that the lower bound on the regret is $\Omega((\log T)^2 \mu/\Delta^2)$,
where $\mu$ now represents the mean of the best arm, and $\Delta$ is the
difference of the mean of the best and the second-best arm. We then propose an
algorithm that achieves the bound up to constant terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaze_R/0/1/0/all/0/1"&gt;Rahul Vaze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1"&gt;Manjesh K. Hanawal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stateful Detection of Model Extraction Attacks. (arXiv:2107.05166v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05166</id>
        <link href="http://arxiv.org/abs/2107.05166"/>
        <updated>2021-07-13T01:59:36.966Z</updated>
        <summary type="html"><![CDATA[Machine-Learning-as-a-Service providers expose machine learning (ML) models
through application programming interfaces (APIs) to developers. Recent work
has shown that attackers can exploit these APIs to extract good approximations
of such ML models, by querying them with samples of their choosing. We propose
VarDetect, a stateful monitor that tracks the distribution of queries made by
users of such a service, to detect model extraction attacks. Harnessing the
latent distributions learned by a modified variational autoencoder, VarDetect
robustly separates three types of attacker samples from benign samples, and
successfully raises an alarm for each. Further, with VarDetect deployed as an
automated defense mechanism, the extracted substitute models are found to
exhibit poor performance and transferability, as intended. Finally, we
demonstrate that even adaptive attackers with prior knowledge of the deployment
of VarDetect, are detected by it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soham Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_Y/0/1/0/all/0/1"&gt;Yash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1"&gt;Aditya Kanade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1"&gt;Shirish Shevade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results. (arXiv:2107.04982v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04982</id>
        <link href="http://arxiv.org/abs/2107.04982"/>
        <updated>2021-07-13T01:59:36.960Z</updated>
        <summary type="html"><![CDATA[We study the problem of out-of-distribution dynamics (OODD) detection, which
involves detecting when the dynamics of a temporal process change compared to
the training-distribution dynamics. This is relevant to applications in
control, reinforcement learning (RL), and multi-variate time-series, where
changes to test time dynamics can impact the performance of learning
controllers/predictors in unknown ways. This problem is particularly important
in the context of deep RL, where learned controllers often overfit to the
training environment. Currently, however, there is a lack of established OODD
benchmarks for the types of environments commonly used in RL research. Our
first contribution is to design a set of OODD benchmarks derived from common RL
environments with varying types and intensities of OODD. Our second
contribution is to design a strong OODD baseline approach based on recurrent
implicit quantile networks (RIQNs), which monitors autoregressive prediction
errors for OODD detection. Our final contribution is to evaluate the RIQN
approach on the benchmarks to provide baseline results for future comparison.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1"&gt;Mohamad H Danesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fern_A/0/1/0/all/0/1"&gt;Alan Fern&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks. (arXiv:2107.05134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05134</id>
        <link href="http://arxiv.org/abs/2107.05134"/>
        <updated>2021-07-13T01:59:36.927Z</updated>
        <summary type="html"><![CDATA[Energy-based models (EBMs) are generative models that are usually trained via
maximum likelihood estimation. This approach becomes challenging in generic
situations where the trained energy is nonconvex, due to the need to sample the
Gibbs distribution associated with this energy. Using general Fenchel duality
results, we derive variational principles dual to maximum likelihood EBMs with
shallow overparametrized neural network energies, both in the active (aka
feature-learning) and lazy regimes. In the active regime, this dual formulation
leads to a training algorithm in which one updates concurrently the particles
in the sample space and the neurons in the parameter space of the energy. We
also consider a variant of this algorithm in which the particles are sometimes
restarted at random samples drawn from the data set, and show that performing
these restarts at every iteration step corresponds to score matching training.
Using intermediate parameter setups in our dual algorithm thereby gives a way
to interpolate between maximum likelihood and score matching training. These
results are illustrated in simple numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingo_Enrich_C/0/1/0/all/0/1"&gt;Carles Domingo-Enrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gabrie_M/0/1/0/all/0/1"&gt;Marylou Gabri&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanden_Eijnden_E/0/1/0/all/0/1"&gt;Eric Vanden-Eijnden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04914</id>
        <link href="http://arxiv.org/abs/2107.04914"/>
        <updated>2021-07-13T01:59:36.921Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1"&gt;Ivan Zakazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1"&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1"&gt;Alexey Chernyavskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Mean Estimation by Marginalized Corrupted Distributions. (arXiv:2107.04855v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04855</id>
        <link href="http://arxiv.org/abs/2107.04855"/>
        <updated>2021-07-13T01:59:36.915Z</updated>
        <summary type="html"><![CDATA[Estimating the kernel mean in a reproducing kernel Hilbert space is a
critical component in many kernel learning algorithms. Given a finite sample,
the standard estimate of the target kernel mean is the empirical average.
Previous works have shown that better estimators can be constructed by
shrinkage methods. In this work, we propose to corrupt data examples with noise
from known distributions and present a new kernel mean estimator, called the
marginalized kernel mean estimator, which estimates kernel mean under the
corrupted distribution. Theoretically, we show that the marginalized kernel
mean estimator introduces implicit regularization in kernel mean estimation.
Empirically, we show on a variety of datasets that the marginalized kernel mean
estimator obtains much lower estimation error than the existing estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shuo Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Haikun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05180</id>
        <link href="http://arxiv.org/abs/2107.05180"/>
        <updated>2021-07-13T01:59:36.898Z</updated>
        <summary type="html"><![CDATA[Real estate appraisal refers to the process of developing an unbiased opinion
for real property's market value, which plays a vital role in decision-making
for various players in the marketplace (e.g., real estate agents, appraisers,
lenders, and buyers). However, it is a nontrivial task for accurate real estate
appraisal because of three major challenges: (1) The complicated influencing
factors for property value; (2) The asynchronously spatiotemporal dependencies
among real estate transactions; (3) The diversified correlations between
residential communities. To this end, we propose a Multi-Task Hierarchical
Graph Representation Learning (MugRep) framework for accurate real estate
appraisal. Specifically, by acquiring and integrating multi-source urban data,
we first construct a rich feature set to comprehensively profile the real
estate from multiple perspectives (e.g., geographical distribution, human
mobility distribution, and resident demographics distribution). Then, an
evolving real estate transaction graph and a corresponding event graph
convolution module are proposed to incorporate asynchronously spatiotemporal
dependencies among real estate transactions. Moreover, to further incorporate
valuable knowledge from the view of residential communities, we devise a
hierarchical heterogeneous community graph convolution module to capture
diversified correlations between residential communities. Finally, an urban
district partitioned multi-task learning module is introduced to generate
differently distributed value opinions for real estate. Extensive experiments
on two real-world datasets demonstrate the effectiveness of MugRep and its
components and features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1"&gt;Lijun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05093</id>
        <link href="http://arxiv.org/abs/2107.05093"/>
        <updated>2021-07-13T01:59:36.891Z</updated>
        <summary type="html"><![CDATA[Recently, there has been a panoptic segmentation task combining semantic and
instance segmentation, in which the goal is to classify each pixel with the
corresponding instance ID. In this work, we propose a solution to tackle the
panoptic segmentation task. The overall structure combines the bottom-up method
and the top-down method. Therefore, not only can there be better performance,
but also the execution speed can be maintained. The network mainly pays
attention to the quality of the mask. In the previous work, we can see that the
uneven contour of the object is more likely to appear, resulting in low-quality
prediction. Accordingly, we propose enhancement features and corresponding loss
functions for the silhouette of objects and backgrounds to improve the mask.
Meanwhile, we use the new proposed confidence score to solve the occlusion
problem and make the network tend to use higher quality masks as prediction
results. To verify our research, we used the COCO dataset and CityScapes
dataset to do experiments and obtained competitive results with fast inference
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo-En Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;En-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1"&gt;Pei-Yung Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent. (arXiv:2107.05011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05011</id>
        <link href="http://arxiv.org/abs/2107.05011"/>
        <updated>2021-07-13T01:59:36.873Z</updated>
        <summary type="html"><![CDATA[Data representation techniques have made a substantial contribution to
advancing data processing and machine learning (ML). Improving predictive power
was the focus of previous representation techniques, which unfortunately
perform rather poorly on the interpretability in terms of extracting underlying
insights of the data. Recently, Kolmogorov model (KM) was studied, which is an
interpretable and predictable representation approach to learning the
underlying probabilistic structure of a set of random variables. The existing
KM learning algorithms using semi-definite relaxation with randomization
(SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility
to big data applications because they do not scale well computationally. In
this paper, we propose a computationally scalable KM learning algorithm, based
on the regularized dual optimization combined with enhanced gradient descent
(GD) method. To make our method more scalable to large-dimensional problems, we
propose two acceleration schemes, namely, eigenvalue decomposition (EVD)
elimination strategy and proximal EVD algorithm. Furthermore, a thresholding
technique by exploiting the approximation error analysis and leveraging the
normalized Minkowski $\ell_1$-norm and its bounds, is provided for the
selection of the number of iterations of the proximal EVD algorithm. When
applied to big data applications, it is demonstrated that the proposed method
can achieve compatible training/prediction performance with significantly
reduced computational complexity; roughly two orders of magnitude improvement
in terms of the time overhead, compared to the existing KM learning algorithms.
Furthermore, it is shown that the accuracy of logical relation mining for
interpretability by using the proposed KM learning algorithm exceeds $80\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1"&gt;Qiyou Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghauch_H/0/1/0/all/0/1"&gt;Hadi Ghauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taejoon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04714</id>
        <link href="http://arxiv.org/abs/2107.04714"/>
        <updated>2021-07-13T01:59:36.868Z</updated>
        <summary type="html"><![CDATA[As both machine learning models and the datasets on which they are evaluated
have grown in size and complexity, the practice of using a few summary
statistics to understand model performance has become increasingly problematic.
This is particularly true in real-world scenarios where understanding model
failure on certain subpopulations of the data is of critical importance. In
this paper we propose a topological framework for evaluating machine learning
models in which a dataset is treated as a "space" on which a model operates.
This provides us with a principled way to organize information about model
performance at both the global level (over the entire test set) and also the
local level (on specific subpopulations). Finally, we describe a topological
data structure, presheaves, which offer a convenient way to store and analyze
model performance between different subpopulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1"&gt;Colby Wight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1"&gt;Sarah Akers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Woongjo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1"&gt;Luke Gosink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1"&gt;Elizabeth Jurrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1"&gt;Keerti Kappagantula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1"&gt;Tegan H. Emerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOOCRep: A Unified Pre-trained Embedding of MOOC Entities. (arXiv:2107.05154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05154</id>
        <link href="http://arxiv.org/abs/2107.05154"/>
        <updated>2021-07-13T01:59:36.861Z</updated>
        <summary type="html"><![CDATA[Many machine learning models have been built to tackle information overload
issues on Massive Open Online Courses (MOOC) platforms. These models rely on
learning powerful representations of MOOC entities. However, they suffer from
the problem of scarce expert label data. To overcome this problem, we propose
to learn pre-trained representations of MOOC entities using abundant unlabeled
data from the structure of MOOCs which can directly be applied to the
downstream tasks. While existing pre-training methods have been successful in
NLP areas as they learn powerful textual representation, their models do not
leverage the richer information about MOOC entities. This richer information
includes the graph relationship between the lectures, concepts, and courses
along with the domain knowledge about the complexity of a concept. We develop
MOOCRep, a novel method based on Transformer language model trained with two
pre-training objectives : 1) graph-based objective to capture the powerful
signal of entities and relations that exist in the graph, and 2)
domain-oriented objective to effectively incorporate the complexity level of
concepts. Our experiments reveal that MOOCRep's embeddings outperform
state-of-the-art representation learning methods on two tasks important for
education community, concept pre-requisite prediction and lecture
recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1"&gt;Shalini Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_J/0/1/0/all/0/1"&gt;Jaideep Srivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04846</id>
        <link href="http://arxiv.org/abs/2107.04846"/>
        <updated>2021-07-13T01:59:36.843Z</updated>
        <summary type="html"><![CDATA[Social-aware recommendation approaches have been recognized as an effective
way to solve the data sparsity issue of traditional recommender systems. The
assumption behind is that the knowledge in social user-user connections can be
shared and transferred to the domain of user-item interactions, whereby to help
learn user preferences. However, most existing approaches merely adopt the
first-order connections among users during transfer learning, ignoring those
connections in higher orders. We argue that better recommendation performance
can also benefit from high-order social relations. In this paper, we propose a
novel Propagation-aware Transfer Learning Network (PTLN) based on the
propagation of social relations. We aim to better mine the sharing knowledge
hidden in social networks and thus further improve recommendation performance.
Specifically, we explore social influence in two aspects: (a) higher-order
friends have been taken into consideration by order bias; (b) different friends
in the same order will have distinct importance for recommendation by an
attention mechanism. Besides, we design a novel regularization to bridge the
gap between social relations and user-item interactions. We conduct extensive
experiments on two real-world datasets and beat other counterparts in terms of
ranking accuracy, especially for the cold-start users with few historical
interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Haodong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yabo Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating stable molecules using imitation and reinforcement learning. (arXiv:2107.05007v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2107.05007</id>
        <link href="http://arxiv.org/abs/2107.05007"/>
        <updated>2021-07-13T01:59:36.837Z</updated>
        <summary type="html"><![CDATA[Chemical space is routinely explored by machine learning methods to discover
interesting molecules, before time-consuming experimental synthesizing is
attempted. However, these methods often rely on a graph representation,
ignoring 3D information necessary for determining the stability of the
molecules. We propose a reinforcement learning approach for generating
molecules in cartesian coordinates allowing for quantum chemical prediction of
the stability. To improve sample-efficiency we learn basic chemical rules from
imitation learning on the GDB-11 database to create an initial model applicable
for all stoichiometries. We then deploy multiple copies of the model
conditioned on a specific stoichiometry in a reinforcement learning setting.
The models correctly identify low energy molecules in the database and produce
novel isomers not found in the training set. Finally, we apply the model to
larger molecules to show how reinforcement learning further refines the
imitation learning model in domains far from the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Meldgaard_S/0/1/0/all/0/1"&gt;S&amp;#xf8;ren Ager Meldgaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas K&amp;#xf6;hler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Mortensen_H/0/1/0/all/0/1"&gt;Henrik Lund Mortensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Christiansen_M/0/1/0/all/0/1"&gt;Mads-Peter V. Christiansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Noe_F/0/1/0/all/0/1"&gt;Frank No&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hammer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rk Hammer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Inductive Link Prediction Using Hyper-Relational Facts. (arXiv:2107.04894v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04894</id>
        <link href="http://arxiv.org/abs/2107.04894"/>
        <updated>2021-07-13T01:59:36.831Z</updated>
        <summary type="html"><![CDATA[For many years, link prediction on knowledge graphs (KGs) has been a purely
transductive task, not allowing for reasoning on unseen entities. Recently,
increasing efforts are put into exploring semi- and fully inductive scenarios,
enabling inference over unseen and emerging entities. Still, all these
approaches only consider triple-based \glspl{kg}, whereas their richer
counterparts, hyper-relational KGs (e.g., Wikidata), have not yet been properly
studied. In this work, we classify different inductive settings and study the
benefits of employing hyper-relational KGs on a wide range of semi- and fully
inductive link prediction tasks powered by recent advancements in graph neural
networks. Our experiments on a novel set of benchmarks show that qualifiers
over typed edges can lead to performance improvements of 6% of absolute gains
(for the Hits@10 metric) compared to triple-only baselines. Our code is
available at \url{https://github.com/mali-git/hyper_relational_ilp}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1"&gt;Mehdi Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1"&gt;Max Berrendorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1"&gt;Veronika Thost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1"&gt;Jens Lehmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-13T01:59:36.826Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05124</id>
        <link href="http://arxiv.org/abs/2107.05124"/>
        <updated>2021-07-13T01:59:36.819Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation is an important task for e-commerce services,
where a large number of users browse anonymously or may have very distinct
interests for different sessions. In this paper we present one of the winning
solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce
Data Challenge. Our solution was inspired by NLP techniques and consists of an
ensemble of two Transformer architectures - Transformer-XL and XLNet - trained
with autoregressive and autoencoding approaches. To leverage most of the rich
dataset made available for the competition, we describe how we prepared
multi-model features by combining tabular events with textual and image
vectors. We also present a model prediction analysis to better understand the
effectiveness of our architectures for the session-based recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1"&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1"&gt;Sara Rabhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1"&gt;Ronay Ak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Md Yasin Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1"&gt;Even Oldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence Analysis of Schr{\"o}dinger-F{\"o}llmer Sampler without Convexity. (arXiv:2107.04766v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2107.04766</id>
        <link href="http://arxiv.org/abs/2107.04766"/>
        <updated>2021-07-13T01:59:36.803Z</updated>
        <summary type="html"><![CDATA[Schr\"{o}dinger-F\"{o}llmer sampler (SFS) is a novel and efficient approach
for sampling from possibly unnormalized distributions without ergodicity. SFS
is based on the Euler-Maruyama discretization of Schr\"{o}dinger-F\"{o}llmer
diffusion process $$\mathrm{d} X_{t}=-\nabla U\left(X_t, t\right) \mathrm{d}
t+\mathrm{d} B_{t}, \quad t \in[0,1],\quad X_0=0$$ on the unit interval, which
transports the degenerate distribution at time zero to the target distribution
at time one. In \cite{sfs21}, the consistency of SFS is established under a
restricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is
uniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide
a nonasymptotic error bound of SFS in Wasserstein distance under some smooth
and bounded conditions on the density ratio of the target distribution over the
standard normal distribution, but without requiring the strongly convexity of
the potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kang_L/0/1/0/all/0/1"&gt;Lican Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Youzhou Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectro-Temporal RF Identification using Deep Learning. (arXiv:2107.05114v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.05114</id>
        <link href="http://arxiv.org/abs/2107.05114"/>
        <updated>2021-07-13T01:59:36.796Z</updated>
        <summary type="html"><![CDATA[RF emissions detection, classification, and spectro-temporal localization are
crucial not only for tasks relating to understanding, managing, and protecting
the RF spectrum, but also for safety and security applications such as
detecting intruding drones or jammers. Achieving this goal for wideband
spectrum and in real-time performance is a challenging problem. We present
WRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal
detection, framework and system. Our resulting deep learning model is capable
to detect, classify, and precisely locate RF emissions in time and frequency
using RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q
streams). Such capabilities are made feasible by leveraging a deep-learning
based one-stage object detection framework, and transfer learning to a
multi-channel image-based RF signals representation. We also introduce an
iterative training approach which leverages synthesized and augmented RF data
to efficiently build large labelled datasets of RF emissions (SPREAD). WRIST
detector achieves 90 mean Average Precision even in extremely congested
environment in the wild. WRIST model classifies five technologies (Bluetooth,
Lightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are
making our curated and annotated dataset available to the whole community. It
consists of nearly 1 million fully labelled RF emissions collected from various
off-the-shelf wireless radios in a range of environments and spanning the five
classes of emissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hai N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vomvas_M/0/1/0/all/0/1"&gt;Marinos Vomvas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_Huu_T/0/1/0/all/0/1"&gt;Triet Vo-Huu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noubir_G/0/1/0/all/0/1"&gt;Guevara Noubir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster Regularization via a Hierarchical Feature Regression. (arXiv:2107.04831v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04831</id>
        <link href="http://arxiv.org/abs/2107.04831"/>
        <updated>2021-07-13T01:59:36.790Z</updated>
        <summary type="html"><![CDATA[Prediction tasks with high-dimensional nonorthogonal predictor sets pose a
challenge for least squares based fitting procedures. A large and productive
literature exists, discussing various regularized approaches to improving the
out-of-sample robustness of parameter estimates. This paper proposes a novel
cluster-based regularization - the hierarchical feature regression (HFR) -,
which mobilizes insights from the domains of machine learning and graph theory
to estimate parameters along a supervised hierarchical representation of the
predictor set, shrinking parameters towards group targets. The method is
innovative in its ability to estimate optimal compositions of predictor groups,
as well as the group targets endogenously. The HFR can be viewed as a
supervised factor regression, with the strength of shrinkage governed by a
penalty on the extent of idiosyncratic variation captured in the fitting
process. The method demonstrates good predictive accuracy and versatility,
outperforming a panel of benchmark regularized estimators across a diverse set
of simulated regression tasks, including dense, sparse and grouped data
generating processes. An application to the prediction of economic growth is
used to illustrate the HFR's effectiveness in an empirical setting, with
favorable comparisons to several frequentist and Bayesian alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pfitzinger_J/0/1/0/all/0/1"&gt;Johann Pfitzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Crowds with Sparse and Imbalanced Annotations. (arXiv:2107.05039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05039</id>
        <link href="http://arxiv.org/abs/2107.05039"/>
        <updated>2021-07-13T01:59:36.783Z</updated>
        <summary type="html"><![CDATA[Traditional supervised learning requires ground truth labels for the training
data, whose collection can be difficult in many cases. Recently, crowdsourcing
has established itself as an efficient labeling solution through resorting to
non-expert crowds. To reduce the labeling error effects, one common practice is
to distribute each instance to multiple workers, whereas each worker only
annotates a subset of data, resulting in the {\it sparse annotation}
phenomenon. In this paper, we note that when meeting with class-imbalance,
i.e., when the ground truth labels are {\it class-imbalanced}, the sparse
annotations are prone to be skewly distributed, which thus can severely bias
the learning algorithm. To combat this issue, we propose one self-training
based approach named {\it Self-Crowd} by progressively adding confident
pseudo-annotations and rebalancing the annotation distribution. Specifically,
we propose one distribution aware confidence measure to select confident
pseudo-annotations, which adopts the resampling strategy to oversample the
minority annotations and undersample the majority annotations. On one
real-world crowdsourcing image classification task, we show that the proposed
method yields more balanced annotations throughout training than the
distribution agnostic methods and substantially improves the learning
performance at different annotation sparsity levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Ye Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shao-Yuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Sheng-Jun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Waveshaping Synthesis. (arXiv:2107.05050v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05050</id>
        <link href="http://arxiv.org/abs/2107.05050"/>
        <updated>2021-07-13T01:59:36.777Z</updated>
        <summary type="html"><![CDATA[We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully
causal approach to neural audio synthesis which operates directly in the
waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU
inference. The NEWT uses time-distributed multilayer perceptrons with periodic
activations to implicitly learn nonlinear transfer functions that encode the
characteristics of a target timbre. Once trained, a NEWT can produce complex
timbral evolutions by simple affine transformations of its input and output
signals. We paired the NEWT with a differentiable noise synthesiser and reverb
and found it capable of generating realistic musical instrument performances
with only 260k total model parameters, conditioned on F0 and loudness features.
We compared our method to state-of-the-art benchmarks with a multi-stimulus
listening test and the Fr\'echet Audio Distance and found it performed
competitively across the tested timbral domains. Our method significantly
outperformed the benchmarks in terms of generation speed, and achieved
real-time performance on a consumer CPU, both with and without FastNEWT,
suggesting it is a viable basis for future creative sound design tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_B/0/1/0/all/0/1"&gt;Ben Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1"&gt;Charalampos Saitis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04974</id>
        <link href="http://arxiv.org/abs/2107.04974"/>
        <updated>2021-07-13T01:59:36.771Z</updated>
        <summary type="html"><![CDATA[It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Rose McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process. (arXiv:2107.05071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05071</id>
        <link href="http://arxiv.org/abs/2107.05071"/>
        <updated>2021-07-13T01:59:36.754Z</updated>
        <summary type="html"><![CDATA[A cross-benchmark has been done on three critical aspects, data imputing,
feature selection and regression algorithms, for machine learning based
chemical vapor deposition (CVD) virtual metrology (VM). The result reveals that
linear feature selection regression algorithm would extensively under-fit the
VM data. Data imputing is also necessary to achieve a higher prediction
accuracy as the data availability is only ~70% when optimal accuracy is
obtained. This work suggests a nonlinear feature selection and regression
algorithm combined with nearest data imputing algorithm would provide a
prediction accuracy as high as 0.7. This would lead to 70% reduced CVD
processing variation, which is believed to will lead to reduced frequency of
physical metrology as well as more reliable mass-produced wafer with improved
quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yunsong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stearrett_R/0/1/0/all/0/1"&gt;Ryan Stearrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05033</id>
        <link href="http://arxiv.org/abs/2107.05033"/>
        <updated>2021-07-13T01:59:36.746Z</updated>
        <summary type="html"><![CDATA[The advancement of convolutional neural networks (CNNs) on various vision
applications has attracted lots of attention. Yet the majority of CNNs are
unable to satisfy the strict requirement for real-world deployment. To overcome
this, the recent popular network pruning is an effective method to reduce the
redundancy of the models. However, the ranking of filters according to their
"importance" on different pruning criteria may be inconsistent. One filter
could be important according to a certain criterion, while it is unnecessary
according to another one, which indicates that each criterion is only a partial
view of the comprehensive "importance". From this motivation, we propose a
novel framework to integrate the existing filter pruning criteria by exploring
the criteria diversity. The proposed framework contains two stages: Criteria
Clustering and Filters Importance Calibration. First, we condense the pruning
criteria via layerwise clustering based on the rank of "importance" score.
Second, within each cluster, we propose a calibration factor to adjust their
significance for each selected blending candidates and search for the optimal
blending criterion via Evolutionary Algorithm. Quantitative results on the
CIFAR-100 and ImageNet benchmarks show that our framework outperforms the
state-of-the-art baselines, regrading to the compact model performance after
pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QoS Prediction for 5G Connected and Automated Driving. (arXiv:2107.05000v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2107.05000</id>
        <link href="http://arxiv.org/abs/2107.05000"/>
        <updated>2021-07-13T01:59:36.707Z</updated>
        <summary type="html"><![CDATA[5G communication system can support the demanding quality-of-service (QoS)
requirements of many advanced vehicle-to-everything (V2X) use cases. However,
the safe and efficient driving, especially of automated vehicles, may be
affected by sudden changes of the provided QoS. For that reason, the prediction
of the QoS changes and the early notification of these predicted changes to the
vehicles have been recently enabled by 5G communication systems. This solution
enables the vehicles to avoid or mitigate the effect of sudden QoS changes at
the application level. This article describes how QoS prediction could be
generated by a 5G communication system and delivered to a V2X application. The
tele-operated driving use case is used as an example to analyze the feasibility
of a QoS prediction scheme. Useful recommendations for the development of a QoS
prediction solution are provided, while open research topics are identified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kousaridas_A/0/1/0/all/0/1"&gt;Apostolos Kousaridas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunath_R/0/1/0/all/0/1"&gt;Ramya Panthangi Manjunath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perdomo_J/0/1/0/all/0/1"&gt;Jose Mauricio Perdomo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zielinski_E/0/1/0/all/0/1"&gt;Ernst Zielinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitz_S/0/1/0/all/0/1"&gt;Steffen Schmitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfadler_A/0/1/0/all/0/1"&gt;Andreas Pfadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs. (arXiv:2107.05074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05074</id>
        <link href="http://arxiv.org/abs/2107.05074"/>
        <updated>2021-07-13T01:59:36.660Z</updated>
        <summary type="html"><![CDATA[Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the
method of choice for learning with large over-parameterized models. A popular
theory for explaining why SGD works well in practice is that the algorithm has
an implicit regularization that biases its output towards a good solution.
Perhaps the theoretically most well understood learning setting for SGD is that
of Stochastic Convex Optimization (SCO), where it is well known that SGD learns
at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this
paper, we consider the problem of SCO and explore the role of implicit
regularization, batch size and multiple epochs for SGD. Our main contributions
are threefold:

(a) We show that for any regularizer, there is an SCO problem for which
Regularized Empirical Risk Minimzation fails to learn. This automatically rules
out any implicit regularization based explanation for the success of SGD.

(b) We provide a separation between SGD and learning via Gradient Descent on
empirical loss (GD) in terms of sample complexity. We show that there is an SCO
problem such that GD with any step size and number of iterations can only learn
at a suboptimal rate: at least $\widetilde{\Omega}(1/n^{5/12})$.

(c) We present a multi-epoch variant of SGD commonly used in practice. We
prove that this algorithm is at least as good as single pass SGD in the worst
case. However, for certain SCO problems, taking multiple passes over the
dataset can significantly outperform single pass SGD.

We extend our results to the general learning setting by showing a problem
which is learnable for any data distribution, and for this problem, SGD is
strictly better than RERM for any regularization function. We conclude by
discussing the implications of our results for deep learning, and show a
separation between SGD and ERM for two layer diagonal neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_S/0/1/0/all/0/1"&gt;Satyen Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1"&gt;Ayush Sekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1"&gt;Karthik Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search. (arXiv:2107.04773v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2107.04773</id>
        <link href="http://arxiv.org/abs/2107.04773"/>
        <updated>2021-07-13T01:59:36.653Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning methods have become mainstream in code search since
they do better at capturing semantic correlations between code snippets and
search queries and have promising performance. However, code snippets have
diverse information from different dimensions, such as business logic, specific
algorithm, and hardware communication, so it is hard for a single code
representation module to cover all the perspectives. On the other hand, as a
specific query may focus on one or several perspectives, it is difficult for a
single query representation module to represent different user intents. In this
paper, we propose MuCoS, a multi-model ensemble learning architecture for
semantic code search. It combines several individual learners, each of which
emphasizes a specific perspective of code snippets. We train the individual
learners on different datasets which contain different perspectives of code
information, and we use a data augmentation strategy to get these different
datasets. Then we ensemble the learners to capture comprehensive features of
code snippets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1"&gt;Lun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiaozhou Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_E/0/1/0/all/0/1"&gt;Ensheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep-Bayesian Framework for Adaptive Speech Duration Modification. (arXiv:2107.04973v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04973</id>
        <link href="http://arxiv.org/abs/2107.04973"/>
        <updated>2021-07-13T01:59:36.645Z</updated>
        <summary type="html"><![CDATA[We propose the first method to adaptively modify the duration of a given
speech signal. Our approach uses a Bayesian framework to define a latent
attention map that links frames of the input and target utterances. We train a
masked convolutional encoder-decoder network to produce this attention map via
a stochastic version of the mean absolute error loss function; our model also
predicts the length of the target speech signal using the encoder embeddings.
The predicted length determines the number of steps for the decoder operation.
During inference, we generate the attention map as a proxy for the similarity
matrix between the given input speech and an unknown target speech signal.
Using this similarity matrix, we compute a warping path of alignment between
the two signals. Our experiments demonstrate that this adaptive framework
produces similar results to dynamic time warping, which relies on a known
target signal, on both voice conversion and emotion conversion tasks. We also
show that our technique results in a high quality of generated speech that is
on par with state-of-the-art vocoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shankar_R/0/1/0/all/0/1"&gt;Ravi Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Venkataraman_A/0/1/0/all/0/1"&gt;Archana Venkataraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate-wise Control Variates for Deep Policy Gradients. (arXiv:2107.04987v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04987</id>
        <link href="http://arxiv.org/abs/2107.04987"/>
        <updated>2021-07-13T01:59:36.639Z</updated>
        <summary type="html"><![CDATA[The control variates (CV) method is widely used in policy gradient estimation
to reduce the variance of the gradient estimators in practice. A control
variate is applied by subtracting a baseline function from the state-action
value estimates. Then the variance-reduced policy gradient presumably leads to
higher learning efficiency. Recent research on control variates with deep
neural net policies mainly focuses on scalar-valued baseline functions. The
effect of vector-valued baselines is under-explored. This paper investigates
variance reduction with coordinate-wise and layer-wise control variates
constructed from vector-valued baselines for neural net policies. We present
experimental evidence suggesting that lower variance can be obtained with such
baselines than with the conventional scalar-valued baseline. We demonstrate how
to equip the popular Proximal Policy Optimization (PPO) algorithm with these
new control variates. We show that the resulting algorithm with proper
regularization can achieve higher sample efficiency than scalar control
variates in continuous control benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yuanyi Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper. (arXiv:2107.05001v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.05001</id>
        <link href="http://arxiv.org/abs/2107.05001"/>
        <updated>2021-07-13T01:59:36.633Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important tool in many
branches of science. Under certain assumptions it allows scientists to explain
phenomena, predict, and make decisions. In the large sample limit, sound and
complete causal discovery algorithms have been previously introduced, where a
directed acyclic graph (DAG), or its equivalence class, representing causal
relations is searched. However, in real-world cases, only finite training data
is available, which limits the power of statistical tests used by these
algorithms, leading to errors in the inferred causal model. This is commonly
addressed by devising a strategy for using as few as possible statistical
tests. In this paper, we introduce such a strategy in the form of a recursive
wrapper for existing constraint-based causal discovery algorithms, which
preserves soundness and completeness. It recursively clusters the observed
variables using the normalized min-cut criterion from the outset, and uses a
baseline causal discovery algorithm during backtracking for learning local
sub-graphs. It then combines them and ensures completeness. By an ablation
study, using synthetic data, and by common real-world benchmarks, we
demonstrate that our approach requires significantly fewer statistical tests,
learns more accurate graphs, and requires shorter run-times than the baseline
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nisimov_S/0/1/0/all/0/1"&gt;Shami Nisimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gurwicz_Y/0/1/0/all/0/1"&gt;Yaniv Gurwicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rohekar_R/0/1/0/all/0/1"&gt;Raanan Y. Rohekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Novik_G/0/1/0/all/0/1"&gt;Gal Novik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. (arXiv:2107.04649v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04649</id>
        <link href="http://arxiv.org/abs/2107.04649"/>
        <updated>2021-07-13T01:59:36.627Z</updated>
        <summary type="html"><![CDATA[For machine learning systems to be reliable, we must understand their
performance in unseen, out-of-distribution environments. In this paper, we
empirically show that out-of-distribution performance is strongly correlated
with in-distribution performance for a wide range of models and distribution
shifts. Specifically, we demonstrate strong correlations between
in-distribution and out-of-distribution performance on variants of CIFAR-10 &
ImageNet, a synthetic pose estimation task derived from YCB objects, satellite
imagery classification in FMoW-WILDS, and wildlife classification in
iWildCam-WILDS. The strong correlations hold across model architectures,
hyperparameters, training set size, and training duration, and are more precise
than what is expected from existing domain adaptation theory. To complete the
picture, we also investigate cases where the correlation is weaker, for
instance some synthetic distribution shifts from CIFAR-10-C and the tissue
classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory
based on a Gaussian data model that shows how changes in the data covariance
arising from distribution shift can affect the observed correlations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;John Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1"&gt;Rohan Taori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1"&gt;Aditi Raghunathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1"&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1"&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1"&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1"&gt;Yair Carmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1"&gt;Ludwig Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of concept lengths for fast concept learning in description logics. (arXiv:2107.04911v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04911</id>
        <link href="http://arxiv.org/abs/2107.04911"/>
        <updated>2021-07-13T01:59:36.620Z</updated>
        <summary type="html"><![CDATA[Concept learning approaches based on refinement operators explore partially
ordered solution spaces to compute concepts, which are used as binary
classification models for individuals. However, the refinement trees spanned by
these approaches can easily grow to millions of nodes for complex learning
problems. This leads to refinement-based approaches often failing to detect
optimal concepts efficiently. In this paper, we propose a supervised machine
learning approach for learning concept lengths, which allows predicting the
length of the target concept and therefore facilitates the reduction of the
search space during concept learning. To achieve this goal, we compare four
neural architectures and evaluate them on four benchmark knowledge
graphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our
evaluation results suggest that recurrent neural network architectures perform
best at concept length prediction with an F-measure of up to 92%. We show that
integrating our concept length predictor into the CELOE (Class Expression
Learner for Ontology Engineering) algorithm improves CELOE's runtime by a
factor of up to 13.4 without any significant changes to the quality of the
results it generates. For reproducibility, we provide our implementation in the
public GitHub repository at
https://github.com/ConceptLengthLearner/ReproducibilityRepo]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kouagou_N/0/1/0/all/0/1"&gt;N&amp;#x27;Dah Jean Kouagou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1"&gt;Stefan Heindorf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1"&gt;Caglar Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1"&gt;Axel-Cyrille Ngonga Ngomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks. (arXiv:2107.04863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04863</id>
        <link href="http://arxiv.org/abs/2107.04863"/>
        <updated>2021-07-13T01:59:36.603Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNN) applications are increasingly becoming a part of
our everyday life, from medical applications to autonomous cars. Traditional
validation of DNN relies on accuracy measures, however, the existence of
adversarial examples has highlighted the limitations of these accuracy
measures, raising concerns especially when DNN are integrated into
safety-critical systems. In this paper, we present HOMRS, an approach to boost
metamorphic testing by automatically building a small optimized set of high
order metamorphic relations from an initial set of elementary metamorphic
relations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn
from traditional systems testing such as code coverage, test case, and path
diversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report
evidence that it builds a small but effective set of high order transformations
achieving a 95% kill ratio. Five raters manually labeled a pool of images
before and after high order transformation; Fleiss' Kappa and statistical tests
confirmed that they are metamorphic properties. HOMRS built-in relations are
also effective to confront adversarial or out-of-distribution examples; HOMRS
detected 92% of randomly sampled out-of-distribution images. HOMRS
transformations are also suitable for online real-time use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giulio Antoniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering. (arXiv:2107.04755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04755</id>
        <link href="http://arxiv.org/abs/2107.04755"/>
        <updated>2021-07-13T01:59:36.593Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks are becoming indispensable for deep learning
from graph-structured data. Most of the existing graph convolutional networks
share two big shortcomings. First, they are essentially low-pass filters, thus
the potentially useful middle and high frequency band of graph signals are
ignored. Second, the bandwidth of existing graph convolutional filters is
fixed. Parameters of a graph convolutional filter only transform the graph
inputs without changing the curvature of a graph convolutional filter function.
In reality, we are uncertain about whether we should retain or cut off the
frequency at a certain point unless we have expert domain knowledge. In this
paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture
the full spectrum of graph signals and automatically update the bandwidth of
graph convolutional filters. While it is based on graph spectral theory, our
AutoGCN is also localized in space and has a spatial form. Experimental results
show that AutoGCN achieves significant improvement over baseline methods which
only work as low-pass filters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zonghan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Guodong Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengqi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Agent Imitation Learning with Copulas. (arXiv:2107.04750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04750</id>
        <link href="http://arxiv.org/abs/2107.04750"/>
        <updated>2021-07-13T01:59:36.585Z</updated>
        <summary type="html"><![CDATA[Multi-agent imitation learning aims to train multiple agents to perform tasks
from demonstrations by learning a mapping between observations and actions,
which is essential for understanding physical, social, and team-play systems.
However, most existing works on modeling multi-agent interactions typically
assume that agents make independent decisions based on their observations,
ignoring the complex dependence among agents. In this paper, we propose to use
copula, a powerful statistical tool for capturing dependence among random
variables, to explicitly model the correlation and coordination in multi-agent
systems. Our proposed model is able to separately learn marginals that capture
the local behavioral patterns of each individual agent, as well as a copula
function that solely and fully captures the dependence structure among agents.
Extensive experiments on synthetic and real-world datasets show that our model
outperforms state-of-the-art baselines across various scenarios in the action
prediction task, and is able to generate new trajectories close to expert
demonstrations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhangjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04882</id>
        <link href="http://arxiv.org/abs/2107.04882"/>
        <updated>2021-07-13T01:59:36.576Z</updated>
        <summary type="html"><![CDATA[Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1"&gt;Anisie Uwimana1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1"&gt;Ransalu Senanayake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks. (arXiv:2107.04775v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04775</id>
        <link href="http://arxiv.org/abs/2107.04775"/>
        <updated>2021-07-13T01:59:36.558Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) algorithms have shown impressive success in
exploring high-dimensional environments to learn complex, long-horizon tasks,
but can often exhibit unsafe behaviors and require extensive environment
interaction when exploration is unconstrained. A promising strategy for safe
learning in dynamically uncertain environments is requiring that the agent can
robustly return to states where task success (and therefore safety) can be
guaranteed. While this approach has been successful in low-dimensions,
enforcing this constraint in environments with high-dimensional state spaces,
such as images, is challenging. We present Latent Space Safe Sets (LS3), which
extends this strategy to iterative, long-horizon tasks with image observations
by using suboptimal demonstrations and a learned dynamics model to restrict
exploration to the neighborhood of a learned Safe Set where task completion is
likely. We evaluate LS3 on 4 domains, including a challenging sequential
pushing task in simulation and a physical cable routing task. We find that LS3
can use prior task successes to restrict exploration and learn more efficiently
than prior algorithms while satisfying constraints. See
https://tinyurl.com/latent-ss for code and supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilcox_A/0/1/0/all/0/1"&gt;Albert Wilcox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04689</id>
        <link href="http://arxiv.org/abs/2107.04689"/>
        <updated>2021-07-13T01:59:36.535Z</updated>
        <summary type="html"><![CDATA[A unique cognitive capability of humans consists in their ability to acquire
new knowledge and skills from a sequence of experiences. Meanwhile, artificial
intelligence systems are good at learning only the last given task without
being able to remember the databases learnt in the past. We propose a novel
lifelong learning methodology by employing a Teacher-Student network framework.
While the Student module is trained with a new given database, the Teacher
module would remind the Student about the information learnt in the past. The
Teacher, implemented by a Generative Adversarial Network (GAN), is trained to
preserve and replay past knowledge corresponding to the probabilistic
representations of previously learn databases. Meanwhile, the Student module is
implemented by a Variational Autoencoder (VAE) which infers its latent variable
representation from both the output of the Teacher module as well as from the
newly available database. Moreover, the Student module is trained to capture
both continuous and discrete underlying data representations across different
domains. The proposed lifelong learning framework is applied in supervised,
semi-supervised and unsupervised training. The code is available~:
\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04952</id>
        <link href="http://arxiv.org/abs/2107.04952"/>
        <updated>2021-07-13T01:59:36.528Z</updated>
        <summary type="html"><![CDATA[A common problem with most zero and few-shot learning approaches is they
suffer from bias towards seen classes resulting in sub-optimal performance.
Existing efforts aim to utilize unlabeled images from unseen classes (i.e
transductive zero-shot) during training to enable generalization. However, this
limits their use in practical scenarios where data from target unseen classes
is unavailable or infeasible to collect. In this work, we present a practical
setting of inductive zero and few-shot learning, where unlabeled images from
other out-of-data classes, that do not belong to seen or unseen categories, can
be used to improve generalization in any-shot learning. We leverage a
formulation based on product-of-experts and introduce a new AUD module that
enables us to use unlabeled samples from out-of-data classes which are usually
easily available and practically entail no annotation cost. In addition, we
also demonstrate the applicability of our model to address a more practical and
challenging, Generalized Zero-shot under a limited supervision setting, where
even base seen classes do not have sufficient annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1"&gt;Gaurav Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1"&gt;Shivam Chandok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossibility of What? Formal and Substantive Equality in Algorithmic Fairness. (arXiv:2107.04642v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2107.04642</id>
        <link href="http://arxiv.org/abs/2107.04642"/>
        <updated>2021-07-13T01:59:36.507Z</updated>
        <summary type="html"><![CDATA[In the face of compounding crises of social and economic inequality, many
have turned to algorithmic decision-making to achieve greater fairness in
society. As these efforts intensify, reasoning within the burgeoning field of
"algorithmic fairness" increasingly shapes how fairness manifests in practice.
This paper interrogates whether algorithmic fairness provides the appropriate
conceptual and practical tools for enhancing social equality. I argue that the
dominant, "formal" approach to algorithmic fairness is ill-equipped as a
framework for pursuing equality, as its narrow frame of analysis generates
restrictive approaches to reform. In light of these shortcomings, I propose an
alternative: a "substantive" approach to algorithmic fairness that centers
opposition to social hierarchies and provides a more expansive analysis of how
to address inequality. This substantive approach enables more fruitful
theorizing about the role of algorithms in combatting oppression. The
distinction between formal and substantive algorithmic fairness is exemplified
by each approach's responses to the "impossibility of fairness" (an
incompatibility between mathematical definitions of algorithmic fairness).
While the formal approach requires us to accept the "impossibility of fairness"
as a harsh limit on efforts to enhance equality, the substantive approach
allows us to escape the "impossibility of fairness" by suggesting reforms that
are not subject to this false dilemma and that are better equipped to
ameliorate conditions of social oppression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1"&gt;Ben Green&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04795</id>
        <link href="http://arxiv.org/abs/2107.04795"/>
        <updated>2021-07-13T01:59:36.500Z</updated>
        <summary type="html"><![CDATA[Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. It works at the cost of training extra classifiers,
where the algorithm should be delicately designed to prevent individual
classifiers from collapsing into each other. In this paper, we present a simple
and efficient co-training algorithm, named Multi-Head Co-Training, for
semi-supervised image classification. By integrating base learners into a
multi-head structure, the model is in a minimal amount of extra parameters.
Every classification head in the unified model interacts with its peers through
a "Weak and Strong Augmentation" strategy, achieving single-view co-training
without promoting diversity explicitly. The effectiveness of Multi-Head
Co-Training is demonstrated in an empirical study on standard semi-supervised
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shuwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04827</id>
        <link href="http://arxiv.org/abs/2107.04827"/>
        <updated>2021-07-13T01:59:36.494Z</updated>
        <summary type="html"><![CDATA[Common neural network architectures are susceptible to attack by adversarial
samples. Neural network architectures are commonly thought of as divided into
low-level feature extraction layers and high-level classification layers;
susceptibility of networks to adversarial samples is often thought of as a
problem related to classification rather than feature extraction. We test this
idea by selectively retraining different portions of VGG and ResNet
architectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and
adversarial data. Our experimental results show that susceptibility to
adversarial samples is associated with low-level feature extraction layers.
Therefore, retraining high-level layers is insufficient for achieving
robustness. This phenomenon could have two explanations: either, adversarial
attacks yield outputs from early layers that are indistinguishable from
features found in the attack classes, or adversarial attacks yield outputs from
early layers that differ statistically from features for non-adversarial
samples and do not permit consistent classification by subsequent layers. We
test this question by large-scale non-linear dimensionality reduction and
density modeling on distributions of feature vectors in hidden layers and find
that the feature distributions between non-adversarial and adversarial samples
differ substantially. Our results provide new insights into the statistical
origins of adversarial samples and possible defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1"&gt;Thomas Breuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Computation Efficient Secure Aggregation for Federated Learning. (arXiv:2012.05433v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05433</id>
        <link href="http://arxiv.org/abs/2012.05433"/>
        <updated>2021-07-13T01:59:36.419Z</updated>
        <summary type="html"><![CDATA[Federated learning has been spotlighted as a way to train neural networks
using distributed data with no need for individual nodes to share data.
Unfortunately, it has also been shown that adversaries may be able to extract
local data contents off model parameters transmitted during federated learning.
A recent solution based on the secure aggregation primitive enabled
privacy-preserving federated learning, but at the expense of significant extra
communication/computational resources. In this paper, we propose a
low-complexity scheme that provides data privacy using substantially reduced
communication/computational resources relative to the existing secure solution.
The key idea behind the suggested scheme is to design the topology of
secret-sharing nodes as a sparse random graph instead of the complete graph
corresponding to the existing solution. We first obtain the necessary and
sufficient condition on the graph to guarantee both reliability and privacy. We
then suggest using the Erd\H{o}s-R\'enyi graph in particular and provide
theoretical guarantees on the reliability/privacy of the proposed scheme.
Through extensive real-world experiments, we demonstrate that our scheme, using
only $20 \sim 30\%$ of the resources required in the conventional scheme,
maintains virtually the same levels of reliability and data privacy in
practical federated learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_B/0/1/0/all/0/1"&gt;Beongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1"&gt;Jy-yong Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dong-Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jaekyun Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13202</id>
        <link href="http://arxiv.org/abs/2006.13202"/>
        <updated>2021-07-13T01:59:36.413Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning to determine the optimal amount of
information retained by the latent variable. We study the impact of calibrated
decoders, which learn the uncertainty of the decoding distribution and can
determine this amount of information automatically, on the VAE performance.
While many methods for learning calibrated decoders have been proposed, many of
the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc
modifications instead. We perform the first comprehensive comparative analysis
of calibrated decoder and provide recommendations for simple and effective VAE
training. Our analysis covers a range of image and video datasets and several
single-image and sequential VAE models. We further propose a simple but novel
modification to the commonly used Gaussian decoder, which computes the
prediction variance analytically. We observe empirically that using heuristic
modifications is not necessary with our method. Project website is at
https://orybkin.github.io/sigma-vae/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16188</id>
        <link href="http://arxiv.org/abs/2010.16188"/>
        <updated>2021-07-13T01:59:36.391Z</updated>
        <summary type="html"><![CDATA[Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between foreground and background images. We
find that a carefully designed composition route RSSN that aims to reduce the
discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the dataset will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen J. Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals. (arXiv:2006.16912v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16912</id>
        <link href="http://arxiv.org/abs/2006.16912"/>
        <updated>2021-07-13T01:59:36.384Z</updated>
        <summary type="html"><![CDATA[Learning the joint probability of random variables (RVs) is the cornerstone
of statistical signal processing and machine learning. However, direct
nonparametric estimation for high-dimensional joint probability is in general
impossible, due to the curse of dimensionality. Recent work has proposed to
recover the joint probability mass function (PMF) of an arbitrary number of RVs
from three-dimensional marginals, leveraging the algebraic properties of
low-rank tensor decomposition and the (unknown) dependence among the RVs.
Nonetheless, accurately estimating three-dimensional marginals can still be
costly in terms of sample complexity, affecting the performance of this line of
work in practice in the sample-starved regime. Using three-dimensional
marginals also involves challenging tensor decomposition problems whose
tractability is unclear. This work puts forth a new framework for learning the
joint PMF using only pairwise marginals, which naturally enjoys a lower sample
complexity relative to the third-order ones. A coupled nonnegative matrix
factorization (CNMF) framework is developed, and its joint PMF recovery
guarantees under various conditions are analyzed. Our method also features a
Gram--Schmidt (GS)-like algorithm that exhibits competitive runtime
performance. The algorithm is shown to provably recover the joint PMF up to
bounded error in finite iterations, under reasonable conditions. It is also
shown that a recently proposed economical expectation maximization (EM)
algorithm guarantees to improve upon the GS-like algorithm's output, thereby
further lifting up the accuracy and efficiency. Real-data experiments are
employed to showcase the effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Shahana Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05255</id>
        <link href="http://arxiv.org/abs/2107.05255"/>
        <updated>2021-07-13T01:59:36.378Z</updated>
        <summary type="html"><![CDATA[During pregnancy, ultrasound examination in the second trimester can assess
fetal size according to standardized charts. To achieve a reproducible and
accurate measurement, a sonographer needs to identify three standard 2D planes
of the fetal anatomy (head, abdomen, femur) and manually mark the key
anatomical landmarks on the image for accurate biometry and fetal weight
estimation. This can be a time-consuming operator-dependent task, especially
for a trainee sonographer. Computer-assisted techniques can help in automating
the fetal biometry computation process. In this paper, we present a unified
automated framework for estimating all measurements needed for the fetal weight
assessment. The proposed framework semantically segments the key fetal
anatomies using state-of-the-art segmentation models, followed by region
fitting and scale recovery for the biometry estimation. We present an ablation
study of segmentation algorithms to show their robustness through 4-fold
cross-validation on a dataset of 349 ultrasound standard plane images from 42
pregnancies. Moreover, we show that the network with the best segmentation
performance tends to be more accurate for biometry estimation. Furthermore, we
demonstrate that the error between clinically measured and predicted fetal
biometry is lower than the permissible error during routine clinical
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1"&gt;Brian Dromey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1"&gt;Raffaele Napolitano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1"&gt;Donald M. Peebles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data efficiency in graph networks through equivariance. (arXiv:2106.13786v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13786</id>
        <link href="http://arxiv.org/abs/2106.13786"/>
        <updated>2021-07-13T01:59:36.359Z</updated>
        <summary type="html"><![CDATA[We introduce a novel architecture for graph networks which is equivariant to
any transformation in the coordinate embeddings that preserves the distance
between neighbouring nodes. In particular, it is equivariant to the Euclidean
and conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance
properties, the proposed model is extremely more data efficient with respect to
classical graph architectures and also intrinsically equipped with a better
inductive bias. We show that, learning on a minimal amount of data, the
architecture we propose can perfectly generalise to unseen data in a synthetic
problem, while much more training data are required from a standard model to
reach comparable performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farina_F/0/1/0/all/0/1"&gt;Francesco Farina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1"&gt;Emma Slade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BAGUA: Scaling up Distributed Learning with System Relaxations. (arXiv:2107.01499v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01499</id>
        <link href="http://arxiv.org/abs/2107.01499"/>
        <updated>2021-07-13T01:59:36.353Z</updated>
        <summary type="html"><![CDATA[Recent years have witnessed a growing list of systems for distributed
data-parallel training. Existing systems largely fit into two paradigms, i.e.,
parameter server and MPI-style collective operations. On the algorithmic side,
researchers have proposed a wide range of techniques to lower the communication
via system relaxations: quantization, decentralization, and communication
delay. However, most, if not all, existing systems only rely on standard
synchronous and asynchronous stochastic gradient (SG) based optimization,
therefore, cannot take advantage of all possible optimizations that the machine
learning community has been developing recently. Given this emerging gap
between the current landscapes of systems and theory, we build BAGUA, a
communication framework whose design goal is to provide a system abstraction
that is both flexible and modular to support state-of-the-art system relaxation
techniques of distributed training. Powered by the new system design, BAGUA has
a great ability to implement and extend various state-of-the-art distributed
learning algorithms. In a production cluster with up to 16 machines (128 GPUs),
BAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training
time by a significant margin (up to 1.95 times) across a diverse range of
tasks. Moreover, we conduct a rigorous tradeoff exploration showing that
different algorithms and system relaxations achieve the best performance over
different network conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1"&gt;Shaoduo Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1"&gt;Xiangru Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;Jianbin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chengjun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hongmei Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianghong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tengxu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Binhang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulating the Time Projection Chamber responses at the MPD detector using Generative Adversarial Networks. (arXiv:2012.04595v2 [physics.ins-det] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04595</id>
        <link href="http://arxiv.org/abs/2012.04595"/>
        <updated>2021-07-13T01:59:36.346Z</updated>
        <summary type="html"><![CDATA[High energy physics experiments rely heavily on the detailed detector
simulation models in many tasks. Running these detailed models typically
requires a notable amount of the computing time available to the experiments.
In this work, we demonstrate a new approach to speed up the simulation of the
Time Projection Chamber tracker of the MPD experiment at the NICA accelerator
complex. Our method is based on a Generative Adversarial Network - a deep
learning technique allowing for implicit estimation of the population
distribution for a given set of objects. This approach lets us learn and then
sample from the distribution of raw detector responses, conditioned on the
parameters of the charged particle tracks. To evaluate the quality of the
proposed model, we integrate a prototype into the MPD software stack and
demonstrate that it produces high-quality events similar to the detailed
simulator, with a speed-up of at least an order of magnitude. The prototype is
trained on the responses from the inner part of the detector and, once expanded
to the full detector, should be ready for use in physics tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Maevskiy_A/0/1/0/all/0/1"&gt;A. Maevskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ratnikov_F/0/1/0/all/0/1"&gt;F. Ratnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zinchenko_A/0/1/0/all/0/1"&gt;A. Zinchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Riabov_V/0/1/0/all/0/1"&gt;V. Riabov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond. (arXiv:2004.11154v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.11154</id>
        <link href="http://arxiv.org/abs/2004.11154"/>
        <updated>2021-07-13T01:59:36.340Z</updated>
        <summary type="html"><![CDATA[Random features is one of the most popular techniques to speed up kernel
methods in large-scale problems. Related works have been recognized by the
NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019.
The body of work on random features has grown rapidly, and hence it is
desirable to have a comprehensive overview on this topic explaining the
connections among various algorithms and theoretical results. In this survey,
we systematically review the work on random features from the past ten years.
First, the motivations, characteristics and contributions of representative
random features based algorithms are summarized according to their sampling
schemes, learning procedures, variance reduction properties and how they
exploit training data. Second, we review theoretical results that center around
the following key question: how many random features are needed to ensure a
high approximation quality or no loss in the empirical/expected risks of the
learned estimator. Third, we provide a comprehensive evaluation of popular
random features based algorithms on several large-scale benchmark datasets and
discuss their approximation quality and prediction performance for
classification. Last, we discuss the relationship between random features and
modern over-parameterized deep neural networks (DNNs), including the use of
high dimensional random features in the analysis of DNNs as well as the gaps
between current theoretical and empirical results. This survey may serve as a
gentle introduction to this topic, and as a users' guide for practitioners
interested in applying the representative algorithms and understanding
theoretical results under various technical assumptions. We hope that this
survey will facilitate discussion on the open problems in this topic, and more
importantly, shed light on future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fanghui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yudong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suykens_J/0/1/0/all/0/1"&gt;Johan A.K. Suykens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution. (arXiv:2107.05132v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05132</id>
        <link href="http://arxiv.org/abs/2107.05132"/>
        <updated>2021-07-13T01:59:36.333Z</updated>
        <summary type="html"><![CDATA[Lexical substitution is the task of generating meaningful substitutes for a
word in a given textual context. Contextual word embedding models have achieved
state-of-the-art results in the lexical substitution task by relying on
contextual information extracted from the replaced word within the sentence.
However, such models do not take into account structured knowledge that exists
in external lexical databases.

We introduce LexSubCon, an end-to-end lexical substitution framework based on
contextual embedding models that can identify highly accurate substitute
candidates. This is achieved by combining contextual information with knowledge
from structured lexical resources. Our approach involves: (i) introducing a
novel mix-up embedding strategy in the creation of the input embedding of the
target word through linearly interpolating the pair of the target input
embedding and the average embedding of its probable synonyms; (ii) considering
the similarity of the sentence-definition embeddings of the target word and its
proposed candidates; and, (iii) calculating the effect of each substitution in
the semantics of the sentence through a fine-tuned sentence similarity model.
Our experiments show that LexSubCon outperforms previous state-of-the-art
methods on LS07 and CoInCo benchmark datasets that are widely used for lexical
substitution tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1"&gt;George Michalopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKillop_I/0/1/0/all/0/1"&gt;Ian McKillop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Helen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Multimodal System for Precision Agriculture using IoT and Machine Learning. (arXiv:2107.04895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04895</id>
        <link href="http://arxiv.org/abs/2107.04895"/>
        <updated>2021-07-13T01:59:36.315Z</updated>
        <summary type="html"><![CDATA[Precision agriculture system is an arising idea that refers to overseeing
farms utilizing current information and communication technologies to improve
the quantity and quality of yields while advancing the human work required. The
automation requires the assortment of information given by the sensors such as
soil, water, light, humidity, temperature for additional information to furnish
the operator with exact data to acquire excellent yield to farmers. In this
work, a study is proposed that incorporates all common state-of-the-art
approaches for precision agriculture use. Technologies like the Internet of
Things (IoT) for data collection, machine Learning for crop damage prediction,
and deep learning for crop disease detection is used. The data collection using
IoT is responsible for the measure of moisture levels for smart irrigation, n,
p, k estimations of fertilizers for best yield development. For crop damage
prediction, various algorithms like Random Forest (RF), Light gradient boosting
machine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN)
are used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models
such as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop
was tainted with some illness or not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Satvik Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pundir_P/0/1/0/all/0/1"&gt;Pradyumn Pundir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jindal_H/0/1/0/all/0/1"&gt;Himanshu Jindal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_H/0/1/0/all/0/1"&gt;Hemraj Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Somya Garg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05080</id>
        <link href="http://arxiv.org/abs/2107.05080"/>
        <updated>2021-07-13T01:59:36.309Z</updated>
        <summary type="html"><![CDATA[Relation prediction among entities in images is an important step in scene
graph generation (SGG), which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks, however, require heavy training yet
are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we
stress that such incapability is due to the lack of commonsense reasoning,i.e.,
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap, we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to
integrate commonsense knowledge for SGG, especially for zero-shot relation
prediction. Specifically, we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1"&gt;Xuan Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05115</id>
        <link href="http://arxiv.org/abs/2107.05115"/>
        <updated>2021-07-13T01:59:36.302Z</updated>
        <summary type="html"><![CDATA[In spite of the improvements achieved by the several denoising algorithms
over the years, many of them still fail at preserving the fine details of the
image after denoising. This is as a result of the smooth-out effect they have
on the images. Most neural network-based algorithms have achieved better
quantitative performance than the classical denoising algorithms. However, they
also suffer from qualitative (visual) performance as a result of the smooth-out
effect. In this paper, we propose an algorithm to address this shortcoming. We
propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image
denoising. This algorithm performs collaborative denoising of image patches in
the sparse domain using a set of optimized neural network models. This results
in a fast algorithm that is able to excellently obtain a trade-off between
noise removal and details preservation. Extensive experiments show that the
DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and
qualitatively (visually) better than many of the state-of-the-art denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning. (arXiv:2107.04971v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04971</id>
        <link href="http://arxiv.org/abs/2107.04971"/>
        <updated>2021-07-13T01:59:36.295Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms often produce models considered as complex
black-box models by both end users and developers. They fail to explain the
model in terms of the domain they are designed for. The proposed Iterative
Visual Logical Classifier (IVLC) is an interpretable machine learning algorithm
that allows end users to design a model and classify data with more confidence
and without having to compromise on the accuracy. Such technique is especially
helpful when dealing with sensitive and crucial data like cancer data in the
medical domain with high cost of errors. With the help of the proposed
interactive and lossless multidimensional visualization, end users can identify
the pattern in the data based on which they can make explainable decisions.
Such options would not be possible in black box machine learning methodologies.
The interpretable IVLC algorithm is supported by the Interactive Shifted Paired
Coordinates Software System (SPCVis). It is a lossless multidimensional data
visualization system with user interactive features. The interactive approach
provides flexibility to the end user to perform data classification as
self-service without having to rely on a machine learning expert. Interactive
pattern discovery becomes challenging while dealing with large data sets with
hundreds of dimensions/features. To overcome this problem, this chapter
proposes an automated classification approach combined with new Coordinate
Order Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm
automatically generates the coordinate pair sequences that best represent the
data separation and the genetic algorithm helps optimizing the proposed IVLC
algorithm by automatically generating the areas for data classification. The
feasibility of the approach is shown by experiments on benchmark datasets
covering both interactive and automated processes used for data classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagle_S/0/1/0/all/0/1"&gt;Sridevi Narayana Wagle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. (arXiv:2106.13008v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13008</id>
        <link href="http://arxiv.org/abs/2106.13008"/>
        <updated>2021-07-13T01:59:36.289Z</updated>
        <summary type="html"><![CDATA[Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haixu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiehui Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05097</id>
        <link href="http://arxiv.org/abs/2107.05097"/>
        <updated>2021-07-13T01:59:36.273Z</updated>
        <summary type="html"><![CDATA[Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Clubhouse. (arXiv:2106.13238v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13238</id>
        <link href="http://arxiv.org/abs/2106.13238"/>
        <updated>2021-07-13T01:59:36.267Z</updated>
        <summary type="html"><![CDATA[With the rise of voice chat rooms, a gigantic resource of data can be exposed
to the research community for natural language processing tasks. Moderators in
voice chat rooms actively monitor the discussions and remove the participants
with offensive language. However, it makes the hate speech detection even more
difficult since some participants try to find creative ways to articulate hate
speech. This makes the hate speech detection challenging in new social media
like Clubhouse. To the best of our knowledge all the hate speech datasets have
been collected from text resources like Twitter. In this paper, we take the
first step to collect a significant dataset from Clubhouse as the rising star
in social media industry. We analyze the collected instances from statistical
point of view using the Google Perspective Scores. Our experiments show that,
the Perspective Scores can outperform Bag of Words and Word2Vec as high level
text features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansourifar_H/0/1/0/all/0/1"&gt;Hadi Mansourifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsagheer_D/0/1/0/all/0/1"&gt;Dana Alsagheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathi_R/0/1/0/all/0/1"&gt;Reza Fathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weidong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1"&gt;Lan Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Function approximation by deep neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. (arXiv:2103.08659v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08659</id>
        <link href="http://arxiv.org/abs/2103.08659"/>
        <updated>2021-07-13T01:59:36.261Z</updated>
        <summary type="html"><![CDATA[In this paper it is shown that $C_\beta$-smooth functions can be approximated
by neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. The
depth, width and the number of active parameters of the constructed networks
have, up to a logarithmic factor, the same dependence on the approximation
error as the networks with parameters in $[-1,1]$. In particular, this means
that the nonparametric regression estimation with the constructed networks
attains the same convergence rate as with sparse networks with parameters in
$[-1,1]$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes. (arXiv:2105.14409v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14409</id>
        <link href="http://arxiv.org/abs/2105.14409"/>
        <updated>2021-07-13T01:59:36.256Z</updated>
        <summary type="html"><![CDATA[We propose a novel matrix autoencoder to map functional connectomes from
resting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor
Imaging (DTI), as guided by subject-level phenotypic measures. Our specialized
autoencoder infers a low dimensional manifold embedding for the rs-fMRI
correlation matrices that mimics a canonical outer-product decomposition. The
embedding is simultaneously used to reconstruct DTI tractography matrices via a
second manifold alignment decoder and to predict inter-subject phenotypic
variability via an artificial neural network. We validate our framework on a
dataset of 275 healthy individuals from the Human Connectome Project database
and on a second clinical dataset consisting of 57 subjects with Autism Spectrum
Disorder. We demonstrate that the model reliably recovers structural
connectivity patterns across individuals, while robustly extracting predictive
and interpretable brain biomarkers in a cross-validated setting. Finally, our
framework outperforms several baselines at predicting behavioral phenotypes in
both real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+DSouza_N/0/1/0/all/0/1"&gt;Niharika Shimona D&amp;#x27;Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nebel_M/0/1/0/all/0/1"&gt;Mary Beth Nebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Crocetti_D/0/1/0/all/0/1"&gt;Deana Crocetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Wymbs_N/0/1/0/all/0/1"&gt;Nicholas Wymbs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Robinson_J/0/1/0/all/0/1"&gt;Joshua Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mostofsky_S/0/1/0/all/0/1"&gt;Stewart Mostofsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Venkataraman_A/0/1/0/all/0/1"&gt;Archana Venkataraman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14435</id>
        <link href="http://arxiv.org/abs/2104.14435"/>
        <updated>2021-07-13T01:59:36.249Z</updated>
        <summary type="html"><![CDATA[Classification neural networks fail to detect inputs that do not fall inside
the classes they have been trained for. Runtime monitoring techniques on the
neuron activation pattern can be used to detect such inputs. We present an
approach for monitoring classification systems via data abstraction. Data
abstraction relies on the notion of box with a resolution. Box-based
abstraction consists in representing a set of values by its minimal and maximal
values in each dimension. We augment boxes with a notion of resolution and
define their clustering coverage, which is intuitively a quantitative metric
that indicates the abstraction quality. This allows studying the effect of
different clustering parameters on the constructed boxes and estimating an
interval of sub-optimal parameters. Moreover, we automatically construct
monitors that leverage both the correct and incorrect behaviors of a system.
This allows checking the size of the monitor abstractions and analyzing the
separability of the network. Monitors are obtained by combining the
sub-monitors of each class of the system placed at some selected layers. Our
experiments demonstrate the effectiveness of our clustering coverage estimation
and show how to assess the effectiveness and precision of monitors according to
the selected clustering parameter and monitored layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Changshun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1"&gt;Yli&amp;#xe8;s Falcone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1"&gt;Saddek Bensalem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Optimism and Delay. (arXiv:2106.06885v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06885</id>
        <link href="http://arxiv.org/abs/2106.06885"/>
        <updated>2021-07-13T01:59:36.243Z</updated>
        <summary type="html"><![CDATA[Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORM+, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Flaspohler_G/0/1/0/all/0/1"&gt;Genevieve Flaspohler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orabona_F/0/1/0/all/0/1"&gt;Francesco Orabona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Judah Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouatadid_S/0/1/0/all/0/1"&gt;Soukayna Mouatadid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oprescu_M/0/1/0/all/0/1"&gt;Miruna Oprescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1"&gt;Paulo Orenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-07-13T01:59:36.227Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The DEformer: An Order-Agnostic Distribution Estimating Transformer. (arXiv:2106.06989v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06989</id>
        <link href="http://arxiv.org/abs/2106.06989"/>
        <updated>2021-07-13T01:59:36.221Z</updated>
        <summary type="html"><![CDATA[Order-agnostic autoregressive distribution (density) estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity by assigning each feature to a
distinct fixed position in an input vector. As a result, architectures built
for these inputs must strategically mask either the input or model weights to
learn the various conditional distributions necessary for inferring the full
joint distribution of the dataset in an order-agnostic way. In this paper, we
propose an alternative approach for encoding feature identities, where each
feature's identity is included alongside its value in the input. This feature
identity encoding strategy allows neural architectures designed for sequential
data to be applied to the OADE task without modification. As a proof of
concept, we show that a Transformer trained on this input (which we refer to as
"the DEformer", i.e., the distribution estimating Transformer) can effectively
model binarized-MNIST, approaching the performance of fixed-order
autoregressive distribution estimating algorithms while still being entirely
order-agnostic. Additionally, we find that the DEformer surpasses the
performance of recent flow-based architectures when modeling a tabular dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alcorn_M/0/1/0/all/0/1"&gt;Michael A. Alcorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Mandarin Tone Classification with Short Term Context Information. (arXiv:2104.05657v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05657</id>
        <link href="http://arxiv.org/abs/2104.05657"/>
        <updated>2021-07-13T01:59:36.215Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end Mandarin tone classification method
from continuous speech utterances utilizing both the spectrogram and the
short-term context information as the input. Both spectrograms and context
segment features are used to train the tone classifier. We first divide the
spectrogram frames into syllable segments using force alignment results
produced by an ASR model. Then we extract the short-term segment features to
capture the context information across multiple syllables. Feeding both the
spectrogram and the short-term context segment features into an end-to-end
model could significantly improve the performance. Experiments are performed on
a large-scale open-source Mandarin speech dataset to evaluate the proposed
method. Results show that this method improves the classification accuracy from
79.5% to 92.6% on the AISHELL3 database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiyang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cold Posteriors Improve Bayesian Medical Image Post-Processing. (arXiv:2106.07533v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07533</id>
        <link href="http://arxiv.org/abs/2106.07533"/>
        <updated>2021-07-13T01:59:36.209Z</updated>
        <summary type="html"><![CDATA[Cold posteriors have been reported to perform better in practice in the
context of Bayesian deep learning (Wenzel et al., 2020). In variational
inference, it is common to employ only a partially tempered posterior by
scaling the complexity term in the log-evidence lower bound (ELBO). In this
work, we optimize the ELBO for a fully tempered posterior in mean-field
variational inference and use Bayesian optimization to automatically find the
optimal posterior temperature and prior scale. Choosing an appropriate
posterior temperature leads to better predictive performance and improved
uncertainty calibration, which we demonstrate for the task of denoising medical
X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Laves_M/0/1/0/all/0/1"&gt;Max-Heinrich Laves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tolle_M/0/1/0/all/0/1"&gt;Malte T&amp;#xf6;lle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1"&gt;Alexander Schlaefer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14251</id>
        <link href="http://arxiv.org/abs/2106.14251"/>
        <updated>2021-07-13T01:59:36.203Z</updated>
        <summary type="html"><![CDATA[Both conceptual modeling and machine learning have long been recognized as
important areas of research. With the increasing emphasis on digitizing and
processing large amounts of data for business and other applications, it would
be helpful to consider how these areas of research can complement each other.
To understand how they can be paired, we provide an overview of machine
learning foundations and development cycle. We then examine how conceptual
modeling can be applied to machine learning and propose a framework for
incorporating conceptual modeling into data science projects. The framework is
illustrated by applying it to a healthcare application. For the inverse
pairing, machine learning can impact conceptual modeling through text and rule
mining, as well as knowledge graphs. The pairing of conceptual modeling and
machine learning in this this way should help lay the foundations for future
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1"&gt;Wolfgang Maass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1"&gt;Veda C. Storey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning. (arXiv:2105.09540v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09540</id>
        <link href="http://arxiv.org/abs/2105.09540"/>
        <updated>2021-07-13T01:59:36.188Z</updated>
        <summary type="html"><![CDATA[The increasing concerns about data privacy and security drive an emerging
field of studying privacy-preserving machine learning from isolated data
sources, i.e., federated learning. A class of federated learning, vertical
federated learning, where different parties hold different features for common
users, has a great potential of driving a more variety of business cooperation
among enterprises in many fields. In machine learning, decision tree ensembles
such as gradient boosting decision tree (GBDT) and random forest are widely
applied powerful models with high interpretability and modeling efficiency.
However, the interpretability is compromised in state-of-the-art vertical
federated learning frameworks such as SecureBoost with anonymous features to
avoid possible data breaches. To address this issue in the inference process,
in this paper, we propose Fed-EINI to protect data privacy and allow the
disclosure of feature meaning by concealing decision paths with a
communication-efficient secure computation method for inference outputs. The
advantages of Fed-EINI will be demonstrated through both theoretical analysis
and extensive numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaolin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuai Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zejin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongji Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy. (arXiv:2104.14281v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14281</id>
        <link href="http://arxiv.org/abs/2104.14281"/>
        <updated>2021-07-13T01:59:36.182Z</updated>
        <summary type="html"><![CDATA[Ubiquitous internet access is reshaping the way we live, but it is
accompanied by unprecedented challenges in preventing chronic diseases planted
by long exposure to unhealthy lifestyles. This paper proposes leveraging online
shopping behaviors as a proxy for personal lifestyle choices to improve chronic
disease prevention literacy targeted for times when e-commerce user experience
has been assimilated into most people's daily lives. Here, retrospective
longitudinal query logs and purchase records from millions of online shoppers
were accessed, constructing a broad spectrum of lifestyle features covering
assorted product categories and buyer personas. Using the lifestyle-related
information preceding their first purchases of prescription drugs, we could
determine associations between online shoppers' past lifestyle choices and
whether they suffered from a particular chronic disease or not. Novel lifestyle
risk factors were discovered in two exemplars -- depression and diabetes, most
of which showed cognitive congruence with existing healthcare knowledge.
Further, such empirical findings could be adopted to locate online shoppers at
high risk of these chronic diseases with fair accuracy, closely matching the
performance of screening surveys benchmarked against medical diagnosis.
Unobtrusive chronic disease surveillance via e-commerce sites may soon meet
consenting individuals in the digital space they already inhabit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongzhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borner_K/0/1/0/all/0/1"&gt;Katy B&amp;#xf6;rner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1"&gt;Yingnan Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wasserstein Robust Classification with Fairness Constraints. (arXiv:2103.06828v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06828</id>
        <link href="http://arxiv.org/abs/2103.06828"/>
        <updated>2021-07-13T01:59:36.175Z</updated>
        <summary type="html"><![CDATA[We propose a distributionally robust classification model with a fairness
constraint that encourages the classifier to be fair in view of the equality of
opportunity criterion. We use a type-$\infty$ Wasserstein ambiguity set
centered at the empirical distribution to model distributional uncertainty and
derive a conservative reformulation for the worst-case equal opportunity
unfairness measure. We establish that the model is equivalent to a mixed binary
optimization problem, which can be solved by standard off-the-shelf solvers. To
improve scalability, we further propose a convex, hinge-loss-based model for
large problem instances whose reformulation does not incur any binary
variables. Moreover, we also consider the distributionally robust learning
problem with a generic ground transportation cost to hedge against the
uncertainties in the label and sensitive attribute. Finally, we numerically
demonstrate that our proposed approaches improve fairness with negligible loss
of predictive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanasusanto_G/0/1/0/all/0/1"&gt;Grani A. Hanasusanto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10897</id>
        <link href="http://arxiv.org/abs/2103.10897"/>
        <updated>2021-07-13T01:59:36.169Z</updated>
        <summary type="html"><![CDATA[This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1"&gt;Shachar Lovett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1"&gt;Gaurav Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruosong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data. (arXiv:2104.12284v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12284</id>
        <link href="http://arxiv.org/abs/2104.12284"/>
        <updated>2021-07-13T01:59:36.163Z</updated>
        <summary type="html"><![CDATA[Deep learning methods have shown suitability for time series classification
in the health and medical domain, with promising results for electrocardiogram
data classification. Successful identification of myocardial infarction holds
life saving potential and any meaningful improvement upon deep learning models
in this area is of great interest. Conventionally, data augmentation methods
are applied universally to the training set when data are limited in order to
ameliorate data resolution or sample size. In the method proposed in this
study, data augmentation was not applied in the context of data scarcity.
Instead, samples that yield low confidence predictions were selectively
augmented in order to bolster the model's sensitivity to features or patterns
less strongly associated with a given class. This approach was tested for
improving the performance of a Fully Convolutional Network. The proposed
approach achieved 90 percent accuracy for classifying myocardial infarction as
opposed to 82 percent accuracy for the baseline, a marked improvement. Further,
the accuracy of the proposed approach was optimal near a defined upper
threshold for qualifying low confidence samples and decreased as this threshold
was raised to include higher confidence samples. This suggests exclusively
selecting lower confidence samples for data augmentation comes with distinct
benefits for electrocardiogram data classification with Fully Convolutional
Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jacaruso_L/0/1/0/all/0/1"&gt;Lucas Cassiel Jacaruso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07396</id>
        <link href="http://arxiv.org/abs/2104.07396"/>
        <updated>2021-07-13T01:59:36.146Z</updated>
        <summary type="html"><![CDATA[We introduce a novel embedding model, named NoGE, which aims to integrate
co-occurrence among entities and relations into graph neural networks to
improve knowledge graph completion (i.e., link prediction). Given a knowledge
graph, NoGE constructs a single graph considering entities and relations as
individual nodes. NoGE then computes weights for edges among nodes based on the
co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion
Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector
representations for entity and relation nodes. NoGE then adopts a score
function to produce the triple scores. Comprehensive experimental results show
that NoGE obtains state-of-the-art results on three new and difficult benchmark
datasets CoDEx for knowledge graph completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dai Quoc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1"&gt;Vinh Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13924</id>
        <link href="http://arxiv.org/abs/2106.13924"/>
        <updated>2021-07-13T01:59:36.130Z</updated>
        <summary type="html"><![CDATA[Ensemble data from Earth system models has to be calibrated and
post-processed. I propose a novel member-by-member post-processing approach
with neural networks. I bridge ideas from ensemble data assimilation with
self-attention, resulting into the self-attentive ensemble transformer. Here,
interactions between ensemble members are represented as additive and dynamic
self-attentive part. As proof-of-concept, I regress global ECMWF ensemble
forecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate
that the ensemble transformer can calibrate the ensemble spread and extract
additional information from the ensemble. As it is a member-by-member approach,
the ensemble transformer directly outputs multivariate and spatially-coherent
ensemble members. Therefore, self-attention and the transformer technique can
be a missing piece for a non-parametric post-processing of ensemble data with
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1"&gt;Tobias Sebastian Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory-based Deep Reinforcement Learning for POMDP. (arXiv:2102.12344v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12344</id>
        <link href="http://arxiv.org/abs/2102.12344"/>
        <updated>2021-07-13T01:59:36.124Z</updated>
        <summary type="html"><![CDATA[A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Process (MDP). In real-world
robotics, this assumption is unpractical, because of the sensor issues such as
sensors' capacity limitation and sensor noise, and the lack of knowledge about
if the observation design is complete or not. These scenarios lead to Partially
Observable MDP (POMDP) and need special treatment. In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingheng Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1"&gt;Rob Gorbet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1"&gt;Dana Kuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosted Embeddings for Time Series Forecasting. (arXiv:2104.04781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04781</id>
        <link href="http://arxiv.org/abs/2104.04781"/>
        <updated>2021-07-13T01:59:36.084Z</updated>
        <summary type="html"><![CDATA[Time series forecasting is a fundamental task emerging from diverse
data-driven applications. Many advanced autoregressive methods such as ARIMA
were used to develop forecasting models. Recently, deep learning based methods
such as DeepAr, NeuralProphet, Seq2Seq have been explored for time series
forecasting problem. In this paper, we propose a novel time series forecast
model, DeepGB. We formulate and implement a variant of Gradient boosting
wherein the weak learners are DNNs whose weights are incrementally found in a
greedy manner over iterations. In particular, we develop a new embedding
architecture that improves the performance of many deep learning models on time
series using Gradient boosting variant. We demonstrate that our model
outperforms existing comparable state-of-the-art models using real-world sensor
data and public dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karingula_S/0/1/0/all/0/1"&gt;Sankeerth Rao Karingula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanan_N/0/1/0/all/0/1"&gt;Nandini Ramanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tahmasbi_R/0/1/0/all/0/1"&gt;Rasool Tahmasbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amjadi_M/0/1/0/all/0/1"&gt;Mehrnaz Amjadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Deokwoo Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_R/0/1/0/all/0/1"&gt;Ricky Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thimmisetty_C/0/1/0/all/0/1"&gt;Charanraj Thimmisetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabrera_L/0/1/0/all/0/1"&gt;Luisa Polania Cabrera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayer_M/0/1/0/all/0/1"&gt;Marjorie Sayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1"&gt;Claudionor Nunes Coelho Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Optimal Exploration in Linear Dynamical Systems. (arXiv:2102.05214v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05214</id>
        <link href="http://arxiv.org/abs/2102.05214"/>
        <updated>2021-07-13T01:59:36.068Z</updated>
        <summary type="html"><![CDATA[Exploration in unknown environments is a fundamental problem in reinforcement
learning and control. In this work, we study task-guided exploration and
determine what precisely an agent must learn about their environment in order
to complete a particular task. Formally, we study a broad class of
decision-making problems in the setting of linear dynamical systems, a class
that includes the linear quadratic regulator problem. We provide instance- and
task-dependent lower bounds which explicitly quantify the difficulty of
completing a task of interest. Motivated by our lower bound, we propose a
computationally efficient experiment-design based exploration algorithm. We
show that it optimally explores the environment, collecting precisely the
information needed to complete the task, and provide finite-time bounds
guaranteeing that it achieves the instance- and task-optimal sample complexity,
up to constant factors. Through several examples of the LQR problem, we show
that performing task-guided exploration provably improves on exploration
schemes which do not take into account the task of interest. Along the way, we
establish that certainty equivalence decision making is instance- and
task-optimal, and obtain the first algorithm for the linear quadratic regulator
problem which is instance-optimal. We conclude with several experiments
illustrating the effectiveness of our approach in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15069</id>
        <link href="http://arxiv.org/abs/2103.15069"/>
        <updated>2021-07-13T01:59:36.061Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering is an important research topic due to its capability to
utilize complementary information from multiple views. However, there are few
methods to consider the negative impact caused by certain views with unclear
clustering structures, resulting in poor multi-view clustering performance. To
address this drawback, we propose self-supervised discriminative feature
learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders
are applied to learn embedded features for each view independently. To leverage
the multi-view complementary information, we concatenate all views' embedded
features to form the global features, which can overcome the negative impact of
some views' unclear clustering structures. In a self-supervised manner,
pseudo-labels are obtained to build a unified target distribution to perform
multi-view discriminative feature learning. During this process, global
discriminative information can be mined to supervise all views to learn more
discriminative features, which in turn are used to update the target
distribution. Besides, this unified target distribution can make SDMVC learn
consistent cluster assignments, which accomplishes the clustering consistency
of multiple views while preserving their features' diversity. Experiments on
various types of multi-view datasets show that SDMVC achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhimeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. (arXiv:2103.11251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11251</id>
        <link href="http://arxiv.org/abs/2103.11251"/>
        <updated>2021-07-13T01:59:36.055Z</updated>
        <summary type="html"><![CDATA[Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the "Rashomon set" of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Semenova_L/0/1/0/all/0/1"&gt;Lesia Semenova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Chudi Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09792</id>
        <link href="http://arxiv.org/abs/2104.09792"/>
        <updated>2021-07-13T01:59:36.046Z</updated>
        <summary type="html"><![CDATA[In recent years online shopping has gained momentum and became an important
venue for customers wishing to save time and simplify their shopping process. A
key advantage of shopping online is the ability to read what other customers
are saying about products of interest. In this work, we aim to maintain this
advantage in situations where extreme brevity is needed, for example, when
shopping by voice. We suggest a novel task of extracting a single
representative helpful sentence from a set of reviews for a given product. The
selected sentence should meet two conditions: first, it should be helpful for a
purchase decision and second, the opinion it expresses should be supported by
multiple reviewers. This task is closely related to the task of Multi Document
Summarization in the product reviews domain but differs in its objective and
its level of conciseness. We collect a dataset in English of sentence
helpfulness scores via crowd-sourcing and demonstrate its reliability despite
the inherent subjectivity involved. Next, we describe a complete model that
extracts representative helpful sentences with positive and negative sentiment
towards the product and demonstrate that it outperforms several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1"&gt;Iftah Gamzu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1"&gt;Hila Gonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1"&gt;Gilad Kutiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Ran Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09550</id>
        <link href="http://arxiv.org/abs/2102.09550"/>
        <updated>2021-07-13T01:59:36.039Z</updated>
        <summary type="html"><![CDATA[We address the challenging problem of Natural Language Comprehension beyond
plain-text documents by introducing the TILT neural network architecture which
simultaneously learns layout information, visual features, and textual
semantics. Contrary to previous approaches, we rely on a decoder capable of
unifying a variety of problems involving natural language. The layout is
represented as an attention bias and complemented with contextualized visual
information, while the core of our model is a pretrained encoder-decoder
Transformer. Our novel approach achieves state-of-the-art results in extracting
information from documents and answering questions which demand layout
understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process
by employing an end-to-end model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1"&gt;Rafa&amp;#x142; Powalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Borchmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1"&gt;Dawid Jurkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1"&gt;Tomasz Dwojak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Pietruszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1"&gt;Gabriela Pa&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Online Reward Shaping in Sparse-Reward Environments. (arXiv:2103.04529v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04529</id>
        <link href="http://arxiv.org/abs/2103.04529"/>
        <updated>2021-07-13T01:59:36.032Z</updated>
        <summary type="html"><![CDATA[We propose a novel reinforcement learning framework that performs
self-supervised online reward shaping, yielding faster, sample efficient
performance in sparse-reward environments. The proposed framework alternates
between updating a policy and inferring a reward function. While the policy
update is performed with the inferred, potentially dense reward function, the
original sparse reward is used to provide a self-supervisory signal for the
reward update by serving as an ordering over the observed trajectories. The
proposed framework is based on the theory that altering the reward function
does not affect the optimal policy of the original MDP as long as certain
relations between the altered and the original reward are maintained. We name
the proposed framework ClAssification-based Reward Shaping (CaReS), since the
altered reward is learned in a self-supervised manner using classifier-based
reward inference. Experimental results on several sparse-reward environments
demonstrate that the proposed algorithm is not only significantly more sample
efficient than the state-of-the-art reinforcement learning baseline but also
achieves a similar sample efficiency to a baseline that uses hand-designed
dense reward functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1"&gt;Farzan Memarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goo_W/0/1/0/all/0/1"&gt;Wonjoon Goo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1"&gt;Rudolf Lioutikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling collaborative data science development with the Ballet framework. (arXiv:2012.07816v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07816</id>
        <link href="http://arxiv.org/abs/2012.07816"/>
        <updated>2021-07-13T01:59:36.000Z</updated>
        <summary type="html"><![CDATA[While the open-source software development model has led to successful
large-scale collaborations in building software systems, data science projects
are frequently developed by individuals or small teams. We describe challenges
to scaling data science collaborations and present a conceptual framework and
ML programming model to address them. We instantiate these ideas in Ballet, a
lightweight framework for collaborative, open-source data science through a
focus on feature engineering, and an accompanying cloud-based development
environment. Using our framework, collaborators incrementally propose feature
definitions to a repository which are each subjected to an ML performance
evaluation and can be automatically merged into an executable feature
engineering pipeline. We leverage Ballet to conduct a case study analysis of an
income prediction problem with 27 collaborators, and discuss implications for
future designers of collaborative projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1"&gt;Micah J. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cito_J/0/1/0/all/0/1"&gt;J&amp;#xfc;rgen Cito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kelvin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed Membership Graph Clustering via Systematic Edge Query. (arXiv:2011.12988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12988</id>
        <link href="http://arxiv.org/abs/2011.12988"/>
        <updated>2021-07-13T01:59:35.994Z</updated>
        <summary type="html"><![CDATA[This work considers clustering nodes of a largely incomplete graph. Under the
problem setting, only a small amount of queries about the edges can be made,
but the entire graph is not observable. This problem finds applications in
large-scale data clustering using limited annotations, community detection
under restricted survey resources, and graph topology inference under
hidden/removed node interactions. Prior works tackled this problem from various
perspectives, e.g., convex programming-based low-rank matrix completion and
active query-based clique finding. Nonetheless, many existing methods are
designed for estimating the single-cluster membership of the nodes, but nodes
may often have mixed (i.e., multi-cluster) membership in practice. Some query
and computational paradigms, e.g., the random query patterns and nuclear
norm-based optimization advocated in the convex approaches, may give rise to
scalability and implementation challenges. This work aims at learning mixed
membership of nodes using queried edges. The proposed method is developed
together with a systematic query principle that can be controlled and adjusted
by the system designers to accommodate implementation challenges -- e.g., to
avoid querying edges that are physically hard to acquire. Our framework also
features a lightweight and scalable algorithm with membership learning
guarantees. Real-data experiments on crowdclustering and community detection
are used to showcase the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Shahana Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1"&gt;Xiao Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14058</id>
        <link href="http://arxiv.org/abs/2011.14058"/>
        <updated>2021-07-13T01:59:35.987Z</updated>
        <summary type="html"><![CDATA[Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
https://github.com/gbup-group/EAN-efficient-attention-network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WheaCha: A Method for Explaining the Predictions of Code Summarization Models. (arXiv:2102.04625v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04625</id>
        <link href="http://arxiv.org/abs/2102.04625"/>
        <updated>2021-07-13T01:59:35.980Z</updated>
        <summary type="html"><![CDATA[The last decade has witnessed a rapid advance in machine learning models.
While the black-box nature of these systems allows powerful predictions, it
cannot be directly explained, posing a threat to the continuing democratization
of machine learning technology.

Tackling the challenge of model explainability, research has made significant
progress in demystifying the image classification models. In the same spirit of
these works, this paper studies code summarization models, particularly, given
an input program for which a model makes a prediction, our goal is to reveal
the key features that the model uses for predicting the label of the program.
We realize our approach in HouYi, which we use to evaluate four prominent code
summarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.
Results show that all models base their predictions on syntactic and lexical
properties with little to none semantic implication. Based on this finding, we
present a novel approach to explaining the predictions of code summarization
models through the lens of training data.

Our work opens up this exciting, new direction of studying what models have
learned from source code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Ke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linzhang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Graph Dictionary Learning. (arXiv:2102.06555v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06555</id>
        <link href="http://arxiv.org/abs/2102.06555"/>
        <updated>2021-07-13T01:59:35.966Z</updated>
        <summary type="html"><![CDATA[Dictionary learning is a key tool for representation learning, that explains
the data as linear combination of few basic elements. Yet, this analysis is not
amenable in the context of graph learning, as graphs usually belong to
different metric spaces. We fill this gap by proposing a new online Graph
Dictionary Learning approach, which uses the Gromov Wasserstein divergence for
the data fitting term. In our work, graphs are encoded through their nodes'
pairwise relations and modeled as convex combination of graph atoms, i.e.
dictionary elements, estimated thanks to an online stochastic algorithm, which
operates on a dataset of unregistered graphs with potentially different number
of nodes. Our approach naturally extends to labeled graphs, and is completed by
a novel upper bound that can be used as a fast approximation of Gromov
Wasserstein in the embedding space. We provide numerical evidences showing the
interest of our approach for unsupervised embedding of graph datasets and for
online graph subspace estimation and tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_Cuaz_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Vincent-Cuaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayer_T/0/1/0/all/0/1"&gt;Titouan Vayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Flamary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corneli_M/0/1/0/all/0/1"&gt;Marco Corneli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1"&gt;Nicolas Courty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder. (arXiv:2010.12876v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12876</id>
        <link href="http://arxiv.org/abs/2010.12876"/>
        <updated>2021-07-13T01:59:35.929Z</updated>
        <summary type="html"><![CDATA[Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.
To find a unique solution, traditional ESI methods impose a variety of priors
that may not reflect the actual source properties. Such limitations of
traditional ESI methods hinder their further applications. Inspired by deep
learning approaches, a novel data-synthesized spatio-temporal denoising
autoencoder method (DST-DAE) method was proposed to solve the ESI inverse
problem. Unlike the traditional methods, we utilize a neural network to
directly seek generalized mapping from the measured E/MEG signals to the
cortical sources. A novel data synthesis strategy is employed by introducing
the prior information of sources to the generated large-scale samples using the
forward model of ESI. All the generated data are used to drive the neural
network to automatically learn inverse mapping. To achieve better estimation
performance, a denoising autoencoder (DAE) architecture with spatio-temporal
feature extraction blocks is designed. Compared with the traditional methods,
we show (1) that the novel deep learning approach provides an effective and
easy-to-apply way to solve the ESI problem, that (2) compared to traditional
methods, DST-DAE with the data synthesis strategy can better consider the
characteristics of real sources than the mathematical formulation of prior
assumptions, and that (3) the specifically designed architecture of DAE can not
only provide a better estimation of source signals but also be robust to noise
pollution. Extensive numerical experiments show that the proposed method is
superior to the traditional knowledge-driven ESI methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gexin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhu Liang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1"&gt;Ke Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gu_Z/0/1/0/all/0/1"&gt;ZhengHui Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_F/0/1/0/all/0/1"&gt;Feifei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;YuanQing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jiawen Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06365</id>
        <link href="http://arxiv.org/abs/2012.06365"/>
        <updated>2021-07-13T01:59:35.923Z</updated>
        <summary type="html"><![CDATA[Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1"&gt;Peter Bugata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1"&gt;Peter Drotar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified lower bounds for interactive high-dimensional estimation under information constraints. (arXiv:2010.06562v4 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06562</id>
        <link href="http://arxiv.org/abs/2010.06562"/>
        <updated>2021-07-13T01:59:35.917Z</updated>
        <summary type="html"><![CDATA[We consider the task of distributed parameter estimation using interactive
protocols subject to local information constraints such as bandwidth
limitations, local differential privacy, and restricted measurements. We
provide a unified framework enabling us to derive a variety of (tight) minimax
lower bounds for different parametric families of distributions, both
continuous and discrete, under any $\ell_p$ loss. Our lower bound framework is
versatile and yields "plug-and-play" bounds that are widely applicable to a
large range of estimation problems. In particular, our approach recovers bounds
obtained using data processing inequalities and Cram\'er--Rao bounds, two other
alternative approaches for proving lower bounds in our setting of interest.
Further, for the families considered, we complement our lower bounds with
matching upper bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canonne_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment L. Canonne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Ziteng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1"&gt;Himanshu Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse sketches with small inversion bias. (arXiv:2011.10695v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10695</id>
        <link href="http://arxiv.org/abs/2011.10695"/>
        <updated>2021-07-13T01:59:35.910Z</updated>
        <summary type="html"><![CDATA[For a tall $n\times d$ matrix $A$ and a random $m\times n$ sketching matrix
$S$, the sketched estimate of the inverse covariance matrix $(A^\top A)^{-1}$
is typically biased: $E[(\tilde A^\top\tilde A)^{-1}]\ne(A^\top A)^{-1}$, where
$\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in
statistics and distributed optimization, when averaging multiple independently
constructed estimates of quantities that depend on the inverse covariance. We
develop a framework for analyzing inversion bias, based on our proposed concept
of an $(\epsilon,\delta)$-unbiased estimator for random matrices. We show that
when the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,
then after simple rescaling, the estimator $(\frac m{m-d}\tilde A^\top\tilde
A)^{-1}$ is $(\epsilon,\delta)$-unbiased for $(A^\top A)^{-1}$ with a sketch of
size $m=O(d+\sqrt d/\epsilon)$. This implies that for $m=O(d)$, the inversion
bias of this estimator is $O(1/\sqrt d)$, which is much smaller than the
$\Theta(1)$ approximation error obtained as a consequence of the subspace
embedding guarantee for sub-gaussian sketches. We then propose a new sketching
technique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas
from both data-oblivious sparse embeddings as well as data-aware leverage-based
row sampling methods, to get $\epsilon$ inversion bias for sketch size
$m=O(d\log d+\sqrt d/\epsilon)$ in time $O(\text{nnz}(A)\log n+md^2)$, where
nnz is the number of non-zeros. The key techniques enabling our analysis
include an extension of a classical inequality of Bai and Silverstein for
random quadratic forms, which we call the Restricted Bai-Silverstein
inequality; and anti-concentration of the Binomial distribution via the
Paley-Zygmund inequality, which we use to prove a lower bound showing that
leverage score sampling sketches generally do not achieve small inversion bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Derezi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1"&gt;Zhenyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00522</id>
        <link href="http://arxiv.org/abs/2101.00522"/>
        <updated>2021-07-13T01:59:35.904Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08743</id>
        <link href="http://arxiv.org/abs/2012.08743"/>
        <updated>2021-07-13T01:59:35.898Z</updated>
        <summary type="html"><![CDATA[Prior works have demonstrated that a low-resource language pair can benefit
from multilingual machine translation (MT) systems, which rely on many language
pairs' joint training. This paper proposes two simple strategies to address the
rare word issue in multilingual MT systems for two low-resource language pairs:
French-Vietnamese and English-Vietnamese. The first strategy is about dynamical
learning word similarity of tokens in the shared space among source languages
while another one attempts to augment the translation ability of rare words
through updating their embeddings during the training. Besides, we leverage
monolingual data for multilingual MT systems to increase the amount of
synthetic parallel corpora while dealing with the data sparsity problem. We
have shown significant improvements of up to +1.62 and +2.54 BLEU points over
the bilingual baseline systems for both language pairs and released our
datasets for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Thi-Vinh Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong-Thai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1"&gt;Thanh-Le Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1"&gt;Khac-Quy Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Le-Minh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12562</id>
        <link href="http://arxiv.org/abs/2010.12562"/>
        <updated>2021-07-13T01:59:35.876Z</updated>
        <summary type="html"><![CDATA[Due to the excessive cost of large-scale language model pre-training,
considerable efforts have been made to train BERT progressively -- start from
an inferior but low-cost model and gradually grow the model to increase the
computational complexity. Our objective is to advance the understanding of
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture search, Transformer growth
also favors compound scaling. Specifically, while existing methods only conduct
network growth in a single dimension, we observe that it is beneficial to use
compound growth operators and balance multiple dimensions (e.g., depth, width,
and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give operator
selection practical guidance. In light of our analyses, the proposed method
speeds up BERT pre-training by 73.6% and 82.2% for the base and large models
respectively, while achieving comparable performances]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiaotao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-tail learning via logit adjustment. (arXiv:2007.07314v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07314</id>
        <link href="http://arxiv.org/abs/2007.07314"/>
        <updated>2021-07-13T01:59:35.867Z</updated>
        <summary type="html"><![CDATA[Real-world classification problems typically exhibit an imbalanced or
long-tailed label distribution, wherein many labels are associated with only a
few samples. This poses a challenge for generalisation on such labels, and also
makes na\"ive learning biased towards dominant labels. In this paper, we
present two simple modifications of standard softmax cross-entropy training to
cope with these challenges. Our techniques revisit the classic idea of logit
adjustment based on the label frequencies, either applied post-hoc to a trained
model, or enforced in the loss during training. Such adjustment encourages a
large relative margin between logits of rare versus dominant labels. These
techniques unify and generalise several recent proposals in the literature,
while possessing firmer statistical grounding and empirical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1"&gt;Sadeep Jayasumana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1"&gt;Ankit Singh Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himanshu Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1"&gt;Andreas Veit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning. (arXiv:2012.01227v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01227</id>
        <link href="http://arxiv.org/abs/2012.01227"/>
        <updated>2021-07-13T01:59:35.830Z</updated>
        <summary type="html"><![CDATA[Active learning is widely used to reduce labeling effort and training time by
repeatedly querying only the most beneficial samples from unlabeled data. In
real-world problems where data cannot be stored indefinitely due to limited
storage or privacy issues, the query selection and the model update should be
performed as soon as a new data sample is observed. Various online active
learning methods have been studied to deal with these challenges; however,
there are difficulties in selecting representative query samples and updating
the model efficiently without forgetting. In this study, we propose Message
Passing Adaptive Resonance Theory (MPART) that learns the distribution and
topology of input data online. Through message passing on the topological
graph, MPART actively queries informative and representative samples, and
continuously improves the classification performance using both labeled and
unlabeled data. We evaluate our model in stream-based selective sampling
scenarios with comparable query selection strategies, showing that MPART
significantly outperforms competitive models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehyeong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1"&gt;Injune Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hyundo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunseo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Won-Seok Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Joseph J. Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Byoung-Tak Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v4 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01807</id>
        <link href="http://arxiv.org/abs/2102.01807"/>
        <updated>2021-07-13T01:59:35.812Z</updated>
        <summary type="html"><![CDATA[Modern recording technologies now enable simultaneous recording from large
numbers of neurons. This has driven the development of new statistical models
for analyzing and interpreting neural population activity. Here we provide a
broad overview of recent developments in this area. We compare and contrast
different approaches, highlight strengths and limitations, and discuss
biological and mechanistic insights that these methods provide.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1"&gt;Cole Hurwitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1"&gt;Nina Kudryashova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1"&gt;Arno Onken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1"&gt;Matthias H. Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted VAE: Variational and Targeted Learning for Causal Inference. (arXiv:2009.13472v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13472</id>
        <link href="http://arxiv.org/abs/2009.13472"/>
        <updated>2021-07-13T01:59:35.805Z</updated>
        <summary type="html"><![CDATA[Undertaking causal inference with observational data is incredibly useful
across a wide range of tasks including the development of medical treatments,
advertisements and marketing, and policy making. There are two significant
challenges associated with undertaking causal inference using observational
data: treatment assignment heterogeneity (i.e., differences between the treated
and untreated groups), and an absence of counterfactual data (i.e., not knowing
what would have happened if an individual who did get treatment, were instead
to have not been treated). We address these two challenges by combining
structured inference and targeted learning. In terms of structure, we factorize
the joint distribution into risk, confounding, instrumental, and miscellaneous
factors, and in terms of targeted learning, we apply a regularizer derived from
the influence curve in order to reduce residual bias. An ablation study is
undertaken, and an evaluation on benchmark datasets demonstrates that TVAE has
competitive and state of the art performance across.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew James Vowels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Camgoz_N/0/1/0/all/0/1"&gt;Necati Cihan Camgoz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense-Sparse Deep CNN Training for Image Denoising. (arXiv:2107.04857v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04857</id>
        <link href="http://arxiv.org/abs/2107.04857"/>
        <updated>2021-07-13T01:59:35.799Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL) methods such as convolutional neural networks
(CNNs) have gained prominence in the area of image denoising. This is owing to
their proven ability to surpass state-of-the-art classical image denoising
algorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward
convolution layers with added regularization methods of batch normalization and
residual learning to improve denoising performance significantly. However, this
comes at the expense of a huge number of trainable parameters. In this paper,
we address this issue by reducing the number of parameters while achieving a
comparable level of performance. We derive motivation from the improved
performance obtained by training networks using the dense-sparse-dense (DSD)
training approach. We extend this training approach to a reduced DnCNN (RDnCNN)
network resulting in a faster denoising network with significantly reduced
parameters and comparable performance to the DnCNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nondeterminism and Instability in Neural Network Optimization. (arXiv:2103.04514v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04514</id>
        <link href="http://arxiv.org/abs/2103.04514"/>
        <updated>2021-07-13T01:59:35.791Z</updated>
        <summary type="html"><![CDATA[Nondeterminism in neural network optimization produces uncertainty in
performance, making small improvements difficult to discern from run-to-run
variability. While uncertainty can be reduced by training multiple model
copies, doing so is time-consuming, costly, and harms reproducibility. In this
work, we establish an experimental protocol for understanding the effect of
optimization nondeterminism on model diversity, allowing us to isolate the
effects of a variety of sources of nondeterminism. Surprisingly, we find that
all sources of nondeterminism have similar effects on measures of model
diversity. To explain this intriguing fact, we identify the instability of
model training, taken as an end-to-end procedure, as the key determinant. We
show that even one-bit changes in initial parameters result in models
converging to vastly different values. Last, we propose two approaches for
reducing the effects of instability on run-to-run variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Summers_C/0/1/0/all/0/1"&gt;Cecilia Summers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinneen_M/0/1/0/all/0/1"&gt;Michael J. Dinneen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[General Invertible Transformations for Flow-based Generative Modeling. (arXiv:2011.15056v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15056</id>
        <link href="http://arxiv.org/abs/2011.15056"/>
        <updated>2021-07-13T01:59:35.775Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a new class of invertible transformations with an
application to flow-based generative models. We indicate that many well-known
invertible transformations in reversible logic and reversible neural networks
could be derived from our proposition. Next, we propose two new coupling layers
that are important building blocks of flow-based generative models. In the
experiments on digit data, we present how these new coupling layers could be
used in Integer Discrete Flows (IDF), and that they achieve better results than
standard coupling layers used in IDF and RealNVP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1"&gt;Jakub M. Tomczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Neural Models for Symbolic Regression at Scale. (arXiv:2007.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10784</id>
        <link href="http://arxiv.org/abs/2007.10784"/>
        <updated>2021-07-13T01:59:35.767Z</updated>
        <summary type="html"><![CDATA[Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, \`{a} la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
https://github.com/druidowm/OccamNet_Public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1"&gt;Allan Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dangovski_R/0/1/0/all/0/1"&gt;Rumen Dangovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dugan_O/0/1/0/all/0/1"&gt;Owen Dugan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Samuel Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soljacic_M/0/1/0/all/0/1"&gt;Marin Solja&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1"&gt;Joseph Jacobson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction. (arXiv:2107.04980v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04980</id>
        <link href="http://arxiv.org/abs/2107.04980"/>
        <updated>2021-07-13T01:59:35.760Z</updated>
        <summary type="html"><![CDATA[The metro ridership prediction has always received extensive attention from
governments and researchers. Recent works focus on designing complicated graph
convolutional recurrent network architectures to capture spatial and temporal
patterns. These works extract the information of spatial dimension well, but
the limitation of temporal dimension still exists. We extended Neural ODE
algorithms to the graph network and proposed the STR-GODEs network, which can
effectively learn spatial, temporal, and ridership correlations without the
limitation of dividing data into equal-sized intervals on the timeline. While
learning the spatial relations and the temporal correlations, we modify the
GODE-RNN cell to obtain the ridership feature and hidden states. Ridership
information and its hidden states are added to the GODESolve to reduce the
error accumulation caused by long time series in prediction. Extensive
experiments on two large-scale datasets demonstrate the efficacy and robustness
of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chuyu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces. (arXiv:2012.13196v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13196</id>
        <link href="http://arxiv.org/abs/2012.13196"/>
        <updated>2021-07-13T01:59:35.753Z</updated>
        <summary type="html"><![CDATA[Efficient sampling of complex data distributions can be achieved using
trained invertible flows (IF), where the model distribution is generated by
pushing a simple base distribution through multiple non-linear bijective
transformations. However, the iterative nature of the transformations in IFs
can limit the approximation to the target distribution. In this paper we seek
to mitigate this by implementing RBM-Flow, an IF model whose base distribution
is a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We
show that by using RBM-Flow we are able to improve the quality of samples
generated, quantified by the Inception Scores (IS) and Frechet Inception
Distance (FID), over baseline models with the same IF transformations, but with
less expressive base distributions. Furthermore, we also obtain D-Flow, an IF
model with uncorrelated discrete latent variables. We show that D-Flow achieves
similar likelihoods and FID/IS scores to those of a typical IF with Gaussian
base variables, but with the additional benefit that global features are
meaningfully encoded as discrete labels in the latent space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_D/0/1/0/all/0/1"&gt;Daniel O&amp;#x27;Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinci_W/0/1/0/all/0/1"&gt;Walter Vinci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Assumptions in Deep Anomaly Detection. (arXiv:2006.00339v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00339</id>
        <link href="http://arxiv.org/abs/2006.00339"/>
        <updated>2021-07-13T01:59:35.747Z</updated>
        <summary type="html"><![CDATA[Though anomaly detection (AD) can be viewed as a classification problem
(nominal vs. anomalous) it is usually treated in an unsupervised manner since
one typically does not have access to, or it is infeasible to utilize, a
dataset that sufficiently characterizes what it means to be "anomalous." In
this paper we present results demonstrating that this intuition surprisingly
seems not to extend to deep AD on images. For a recent AD benchmark on
ImageNet, classifiers trained to discern between normal samples and just a few
(64) random natural images are able to outperform the current state of the art
in deep AD. Experimentally we discover that the multiscale structure of image
data makes example anomalies exceptionally informative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruff_L/0/1/0/all/0/1"&gt;Lukas Ruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1"&gt;Robert A. Vandermeulen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franks_B/0/1/0/all/0/1"&gt;Billy Joe Franks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1"&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1"&gt;Marius Kloft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models. (arXiv:2104.10683v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10683</id>
        <link href="http://arxiv.org/abs/2104.10683"/>
        <updated>2021-07-13T01:59:35.730Z</updated>
        <summary type="html"><![CDATA[(Artificial) neural networks have become increasingly popular in mechanics to
accelerate computations with model order reduction techniques and as universal
models for a wide variety of materials. However, the major disadvantage of
neural networks remains: their numerous parameters are challenging to interpret
and explain. Thus, neural networks are often labeled as black boxes, and their
results often elude human interpretation. In mechanics, the new and active
field of physics-informed neural networks attempts to mitigate this
disadvantage by designing deep neural networks on the basis of mechanical
knowledge. By using this a priori knowledge, deeper and more complex neural
networks became feasible, since the mechanical assumptions could be explained.
However, the internal reasoning and explanation of neural network parameters
remain mysterious.

Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koeppe_A/0/1/0/all/0/1"&gt;Arnd Koeppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bamer_F/0/1/0/all/0/1"&gt;Franz Bamer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Selzer_M/0/1/0/all/0/1"&gt;Michael Selzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nestler_B/0/1/0/all/0/1"&gt;Britta Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markert_B/0/1/0/all/0/1"&gt;Bernd Markert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14536</id>
        <link href="http://arxiv.org/abs/2006.14536"/>
        <updated>2021-07-13T01:59:35.723Z</updated>
        <summary type="html"><![CDATA[It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.

Compared to standard adversarial training, SAT improves adversarial
robustness for "free", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
https://github.com/cihangxie/SmoothAdversarialTraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning. (arXiv:2102.08055v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08055</id>
        <link href="http://arxiv.org/abs/2102.08055"/>
        <updated>2021-07-13T01:59:35.716Z</updated>
        <summary type="html"><![CDATA[Millimeter wave (mmWave) beam-tracking based on machine learning enables the
development of accurate tracking policies while obviating the need to
periodically solve beam-optimization problems. However, its applicability is
still arguable when training-test gaps exist in terms of environmental
parameters that affect the node dynamics. From this skeptical point of view,
the contribution of this study is twofold. First, by considering an example
scenario, we confirm that the training-test gap adversely affects the
beam-tracking performance. More specifically, we consider nodes placed on
overhead messenger wires, where the node dynamics are affected by several
environmental parameters, e.g, the wire mass and tension. Although these are
particular scenarios, they yield insight into the validation of the
training-test gap problems. Second, we demonstrate the feasibility of
\textit{zero-shot adaptation} as a solution, where a learning agent adapts to
environmental parameters unseen during training. This is achieved by leveraging
a robust adversarial reinforcement learning (RARL) technique, where such
training-and-test gaps are regarded as disturbances by adversaries that are
jointly trained with a legitimate beam-tracking agent. Numerical evaluations
demonstrate that the beam-tracking policy learned via RARL can be applied to a
wide range of environmental parameters without severely degrading the received
power.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinzaki_M/0/1/0/all/0/1"&gt;Masao Shinzaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1"&gt;Yusuke Koda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Koji Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1"&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morikura_M/0/1/0/all/0/1"&gt;Masahiro Morikura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirato_Y/0/1/0/all/0/1"&gt;Yushi Shirato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_D/0/1/0/all/0/1"&gt;Daisei Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kita_N/0/1/0/all/0/1"&gt;Naoki Kita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification. (arXiv:2105.11043v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11043</id>
        <link href="http://arxiv.org/abs/2105.11043"/>
        <updated>2021-07-13T01:59:35.706Z</updated>
        <summary type="html"><![CDATA[Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1"&gt;Huy Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikkelsen_K/0/1/0/all/0/1"&gt;Kaare Mikkelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_O/0/1/0/all/0/1"&gt;Oliver Y. Ch&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1"&gt;Philipp Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mertins_A/0/1/0/all/0/1"&gt;Alfred Mertins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1"&gt;Maarten De Vos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Budget Matching for Sequential Budgeted Learning. (arXiv:2102.03400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03400</id>
        <link href="http://arxiv.org/abs/2102.03400"/>
        <updated>2021-07-13T01:59:35.700Z</updated>
        <summary type="html"><![CDATA[A core element in decision-making under uncertainty is the feedback on the
quality of the performed actions. However, in many applications, such feedback
is restricted. For example, in recommendation systems, repeatedly asking the
user to provide feedback on the quality of recommendations will annoy them. In
this work, we formalize decision-making problems with querying budget, where
there is a (possibly time-dependent) hard limit on the number of reward queries
allowed. Specifically, we consider multi-armed bandits, linear bandits, and
reinforcement learning problems. We start by analyzing the performance of
`greedy' algorithms that query a reward whenever they can. We show that in
fully stochastic settings, doing so performs surprisingly well, but in the
presence of any adversity, this might lead to linear regret. To overcome this
issue, we propose the Confidence-Budget Matching (CBM) principle that queries
rewards when the confidence intervals are wider than the inverse square root of
the available budget. We analyze the performance of CBM based algorithms in
different settings and show that they perform well in the presence of adversity
in the contexts, initial states, and budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1"&gt;Nadav Merlis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1"&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Mean Discrepancy Test is Aware of Adversarial Attacks. (arXiv:2010.11415v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11415</id>
        <link href="http://arxiv.org/abs/2010.11415"/>
        <updated>2021-07-13T01:59:35.657Z</updated>
        <summary type="html"><![CDATA[The maximum mean discrepancy (MMD) test could in principle detect any
distributional discrepancy between two datasets. However, it has been shown
that the MMD test is unaware of adversarial attacks -- the MMD test failed to
detect the discrepancy between natural and adversarial data. Given this
phenomenon, we raise a question: are natural and adversarial data really from
different distributions? The answer is affirmative -- the previous use of the
MMD test on the purpose missed three key factors, and accordingly, we propose
three components. Firstly, the Gaussian kernel has limited representation
power, and we replace it with an effective deep kernel. Secondly, the test
power of the MMD test was neglected, and we maximize it following asymptotic
statistics. Finally, adversarial data may be non-independent, and we overcome
this issue with the wild bootstrap. By taking care of the three factors, we
verify that the MMD test is aware of adversarial attacks, which lights up a
novel road for adversarial data detection based on two-sample tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1"&gt;Ruize Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better SGD using Second-order Momentum. (arXiv:2103.03265v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03265</id>
        <link href="http://arxiv.org/abs/2103.03265"/>
        <updated>2021-07-13T01:59:35.652Z</updated>
        <summary type="html"><![CDATA[We develop a new algorithm for non-convex stochastic optimization that finds
an $\epsilon$-critical point in the optimal $O(\epsilon^{-3})$ stochastic
gradient and Hessian-vector product computations. Our algorithm uses
Hessian-vector products to "correct" a bias term in the momentum of SGD with
momentum. This leads to better gradient estimates in a manner analogous to
variance reduction methods. In contrast to prior work, we do not require
excessively large batch sizes, and are able to provide an adaptive algorithm
whose convergence rate automatically improves with decreasing variance in the
gradient estimates. We validate our results on a variety of large-scale deep
learning architectures and benchmarks tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hoang Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1"&gt;Ashok Cutkosky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Directional Pruning via Perturbation Orthogonal Projection. (arXiv:2107.05328v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05328</id>
        <link href="http://arxiv.org/abs/2107.05328"/>
        <updated>2021-07-13T01:59:35.634Z</updated>
        <summary type="html"><![CDATA[Structured pruning is an effective compression technique to reduce the
computation of neural networks, which is usually achieved by adding
perturbations to reduce network parameters at the cost of slightly increasing
training loss. A more reasonable approach is to find a sparse minimizer along
the flat minimum valley found by optimizers, i.e. stochastic gradient descent,
which keeps the training loss constant. To achieve this goal, we propose the
structured directional pruning based on orthogonal projecting the perturbations
onto the flat minimum valley. We also propose a fast solver sDprun and further
prove that it achieves directional pruning asymptotically after sufficient
training. Experiments using VGG-Net and ResNet on CIFAR-10 and CIFAR-100
datasets show that our method obtains the state-of-the-art pruned accuracy
(i.e. 93.97% on VGG16, CIFAR-10 task) without retraining. Experiments using
DNN, VGG-Net and WRN28X10 on MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate
our method performs structured directional pruning, reaching the same minimum
valley as the optimizer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+YinchuanLi/0/1/0/all/0/1"&gt;YinchuanLi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XiaofengLiu/0/1/0/all/0/1"&gt;XiaofengLiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YunfengShao/0/1/0/all/0/1"&gt;YunfengShao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+QingWang/0/1/0/all/0/1"&gt;QingWang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+YanhuiGeng/0/1/0/all/0/1"&gt;YanhuiGeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning interaction rules from multi-animal trajectories via augmented behavioral models. (arXiv:2107.05326v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05326</id>
        <link href="http://arxiv.org/abs/2107.05326"/>
        <updated>2021-07-13T01:59:35.625Z</updated>
        <summary type="html"><![CDATA[Extracting the interaction rules of biological agents from moving sequences
pose challenges in various domains. Granger causality is a practical framework
for analyzing the interactions from observed time-series data; however, this
framework ignores the structures of the generative process in animal behaviors,
which may lead to interpretational problems and sometimes erroneous assessments
of causality. In this paper, we propose a new framework for learning Granger
causality from multi-animal trajectories via augmented theory-based behavioral
models with interpretable data-driven models. We adopt an approach for
augmenting incomplete multi-agent behavioral models described by time-varying
dynamical systems with neural networks. For efficient and interpretable
learning, our model leverages theory-based architectures separating navigation
and motion processes, and the theory-guided regularization for reliable
behavioral modeling. This can provide interpretable signs of Granger-causal
effects over time, i.e., when specific others cause the approach or separation.
In experiments using synthetic datasets, our method achieved better performance
than various baselines. We then analyzed multi-animal datasets of mice, flies,
birds, and bats, which verified our method and obtained novel biological
insights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1"&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeishi_N/0/1/0/all/0/1"&gt;Naoya Takeishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1"&gt;Kazushi Tsutsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujioka_E/0/1/0/all/0/1"&gt;Emyo Fujioka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishiumi_N/0/1/0/all/0/1"&gt;Nozomi Nishiumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_R/0/1/0/all/0/1"&gt;Ryooya Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukushiro_M/0/1/0/all/0/1"&gt;Mika Fukushiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ide_K/0/1/0/all/0/1"&gt;Kaoru Ide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohno_H/0/1/0/all/0/1"&gt;Hiroyoshi Kohno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoda_K/0/1/0/all/0/1"&gt;Ken Yoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Susumu Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiryu_S/0/1/0/all/0/1"&gt;Shizuko Hiryu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1"&gt;Yoshinobu Kawahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastically forced ensemble dynamic mode decomposition for forecasting and analysis of near-periodic systems. (arXiv:2010.04248v2 [physics.soc-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04248</id>
        <link href="http://arxiv.org/abs/2010.04248"/>
        <updated>2021-07-13T01:59:35.554Z</updated>
        <summary type="html"><![CDATA[Time series forecasting remains a central challenge problem in almost all
scientific disciplines. We introduce a novel load forecasting method in which
observed dynamics are modeled as a forced linear system using Dynamic Mode
Decomposition (DMD) in time delay coordinates. Central to this approach is the
insight that grid load, like many observables on complex real-world systems,
has an "almost-periodic" character, i.e., a continuous Fourier spectrum
punctuated by dominant peaks, which capture regular (e.g., daily or weekly)
recurrences in the dynamics. The forecasting method presented takes advantage
of this property by (i) regressing to a deterministic linear model whose
eigenspectrum maps onto those peaks, and (ii) simultaneously learning a
stochastic Gaussian process regression (GPR) process to actuate this system.
Our forecasting algorithm is compared against state-of-the-art forecasting
techniques not using additional explanatory variables and is shown to produce
superior performance. Moreover, its use of linear intrinsic dynamics offers a
number of desirable properties in terms of interpretability and parsimony.
Results are presented for a test case using load data from an electrical grid.
Load forecasting is an essential challenge in power systems engineering, with
major implications for real-time control, pricing, maintenance, and security
decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Dylewsky_D/0/1/0/all/0/1"&gt;Daniel Dylewsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Barajas_Solano_D/0/1/0/all/0/1"&gt;David Barajas-Solano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre M. Tartakovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kutz_J/0/1/0/all/0/1"&gt;J. Nathan Kutz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliable Post hoc Explanations: Modeling Uncertainty in Explainability. (arXiv:2008.05030v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05030</id>
        <link href="http://arxiv.org/abs/2008.05030"/>
        <updated>2021-07-13T01:59:35.535Z</updated>
        <summary type="html"><![CDATA[As black box explanations are increasingly being employed to establish model
credibility in high stakes settings, it is important to ensure that these
explanations are accurate and reliable. However, prior work demonstrates that
explanations generated by state-of-the-art techniques are inconsistent,
unstable, and provide very little insight into their correctness and
reliability. In addition, these methods are also computationally inefficient,
and require significant hyper-parameter tuning. In this paper, we address the
aforementioned challenges by developing a novel Bayesian framework for
generating local explanations along with their associated uncertainty. We
instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP
which output credible intervals for the feature importances, capturing the
associated uncertainty. The resulting explanations not only enable us to make
concrete inferences about their quality (e.g., there is a 95\% chance that the
feature importance lies within the given range), but are also highly consistent
and stable. We carry out a detailed theoretical analysis that leverages the
aforementioned uncertainty to estimate how many perturbations to sample, and
how to sample for faster convergence. This work makes the first attempt at
addressing several critical issues with popular explanation methods in one
shot, thereby generating consistent, stable, and reliable explanations with
guarantees in a computationally efficient manner. Experimental evaluation with
multiple real world datasets and user studies demonstrate that the efficacy of
the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers. (arXiv:2009.09217v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09217</id>
        <link href="http://arxiv.org/abs/2009.09217"/>
        <updated>2021-07-13T01:59:35.529Z</updated>
        <summary type="html"><![CDATA[The expressive power of Bayesian kernel-based methods has led them to become
an important tool across many different facets of artificial intelligence, and
useful to a plethora of modern application domains, providing both power and
interpretability via uncertainty analysis. This article introduces and
discusses two methods which straddle the areas of probabilistic Bayesian
schemes and kernel methods for regression: Gaussian Processes and Relevance
Vector Machines. Our focus is on developing a common framework with which to
view these methods, via intermediate methods a probabilistic version of the
well-known kernel ridge regression, and drawing connections among them, via
dual formulations, and discussion of their application in the context of major
tasks: regression, smoothing, interpolation, and filtering. Overall, we provide
understanding of the mathematical concepts behind these models, and we
summarize and discuss in depth different interpretations and highlight the
relationship to other methods, such as linear kernel smoothers, Kalman
filtering and Fourier approximations. Throughout, we provide numerous figures
to promote understanding, and we make numerous recommendations to
practitioners. Benefits and drawbacks of the different techniques are
highlighted. To our knowledge, this is the most in-depth study of its kind to
date focused on these two methods, and will be relevant to theoretical
understanding and practitioners throughout the domains of data-science, signal
processing, machine learning, and artificial intelligence in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1"&gt;Luca Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jesse Read&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04713</id>
        <link href="http://arxiv.org/abs/2107.04713"/>
        <updated>2021-07-13T01:59:35.523Z</updated>
        <summary type="html"><![CDATA[Owing to the remarkable capability of extracting effective graph embeddings,
graph convolutional network (GCN) and its variants have been successfully
applied to a broad range of tasks, such as node classification, link
prediction, and graph classification. Traditional GCN models suffer from the
issues of overfitting and oversmoothing, while some recent techniques like
DropEdge could alleviate these issues and thus enable the development of deep
GCN. However, training GCN models is non-trivial, as it is sensitive to the
choice of hyperparameters such as dropout rate and learning weight decay,
especially for deep GCN models. In this paper, we aim to automate the training
of GCN models through hyperparameter optimization. To be specific, we propose a
self-tuning GCN approach with an alternate training algorithm, and further
extend our approach by incorporating the population based training scheme.
Experimental results on three benchmark datasets demonstrate the effectiveness
of our approaches on optimizing multi-layer GCN, compared with several
representative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ronghang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhiqiang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[L2M: Practical posterior Laplace approximation with optimization-driven second moment estimation. (arXiv:2107.04695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04695</id>
        <link href="http://arxiv.org/abs/2107.04695"/>
        <updated>2021-07-13T01:59:35.517Z</updated>
        <summary type="html"><![CDATA[Uncertainty quantification for deep neural networks has recently evolved
through many techniques. In this work, we revisit Laplace approximation, a
classical approach for posterior approximation that is computationally
attractive. However, instead of computing the curvature matrix, we show that,
under some regularity conditions, the Laplace approximation can be easily
constructed using the gradient second moment. This quantity is already
estimated by many exponential moving average variants of Adagrad such as Adam
and RMSprop, but is traditionally discarded after training. We show that our
method (L2M) does not require changes in models or optimization, can be
implemented in a few lines of code to yield reasonable results, and it does not
require any extra computational steps besides what is already being computed by
optimizers, without introducing any new hyperparameter. We hope our method can
open new research directions on using quantities already computed by optimizers
for uncertainty estimation in deep neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perone_C/0/1/0/all/0/1"&gt;Christian S. Perone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silveira_R/0/1/0/all/0/1"&gt;Roberto Pereira Silveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paula_T/0/1/0/all/0/1"&gt;Thomas Paula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09646</id>
        <link href="http://arxiv.org/abs/2008.09646"/>
        <updated>2021-07-13T01:59:35.510Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel network for high resolution video
generation. Our network uses ideas from Wasserstein GANs by enforcing
k-Lipschitz constraint on the loss term and Conditional GANs using class labels
for training and testing. We present Generator and Discriminator network
layerwise details along with the combined network architecture, optimization
details and algorithm used in this work. Our network uses a combination of two
loss terms: mean square pixel loss and an adversarial loss. The datasets used
for training and testing our network are UCF101, Golf and Aeroplane Datasets.
Using Inception Score and Fr\'echet Inception Distance as the evaluation
metrics, our network outperforms previous state of the art networks on
unsupervised video generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04734</id>
        <link href="http://arxiv.org/abs/2107.04734"/>
        <updated>2021-07-13T01:59:35.504Z</updated>
        <summary type="html"><![CDATA[Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1"&gt;Ankita Pasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1"&gt;Ju-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1"&gt;Karen Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hack The Box: Fooling Deep Learning Abstraction-Based Monitors. (arXiv:2107.04764v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04764</id>
        <link href="http://arxiv.org/abs/2107.04764"/>
        <updated>2021-07-13T01:59:35.488Z</updated>
        <summary type="html"><![CDATA[Deep learning is a type of machine learning that adapts a deep hierarchy of
concepts. Deep learning classifiers link the most basic version of concepts at
the input layer to the most abstract version of concepts at the output layer,
also known as a class or label. However, once trained over a finite set of
classes, a deep learning model does not have the power to say that a given
input does not belong to any of the classes and simply cannot be linked.
Correctly invalidating the prediction of unrelated classes is a challenging
problem that has been tackled in many ways in the literature. Novelty detection
gives deep learning the ability to output "do not know" for novel/unseen
classes. Still, no attention has been given to the security aspects of novelty
detection. In this paper, we consider the case study of abstraction-based
novelty detection and show that it is not robust against adversarial samples.
Moreover, we show the feasibility of crafting adversarial samples that fool the
deep learning classifier and bypass the novelty detection monitoring at the
same time. In other words, these monitoring boxes are hackable. We demonstrate
that novelty detection itself ends up as an attack surface.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_S/0/1/0/all/0/1"&gt;Sara Hajj Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1"&gt;Mohamed Nassar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06801</id>
        <link href="http://arxiv.org/abs/2106.06801"/>
        <updated>2021-07-13T01:59:35.482Z</updated>
        <summary type="html"><![CDATA[Contrastive Learning (CL) is a recent representation learning approach, which
encourages inter-class separability and intra-class compactness in learned
image representations. Since medical images often contain multiple semantic
classes in an image, using CL to learn representations of local features (as
opposed to global) is important. In this work, we present a novel
semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization (CR) scheme, which
works in synergy with CL. It addresses the problem of confirmation bias, and
encourages better clustering in the feature space. We evaluate our method on
four public medical segmentation datasets and a novel histopathology dataset
that we introduce. Our method obtains consistent improvements over
state-of-the-art semi-supervised segmentation approaches for all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1"&gt;Ajey Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nisarg Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Prasenjit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1"&gt;Govind Makharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04619</id>
        <link href="http://arxiv.org/abs/2107.04619"/>
        <updated>2021-07-13T01:59:35.476Z</updated>
        <summary type="html"><![CDATA[Generating future frames given a few context (or past) frames is a
challenging task. It requires modeling the temporal coherence of videos and
multi-modality in terms of diversity in the potential future states. Current
variational approaches for video generation tend to marginalize over
multi-modal future outcomes. Instead, we propose to explicitly model the
multi-modality in the future outcomes and leverage it to sample diverse
futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to
learn priors on future states given the past and maintains a probability
distribution over possible futures given a particular sample. In addition, we
leverage the changes in this distribution over time to control the sampling of
diverse future states by estimating the end of ongoing sequences. That is, we
use the variance of GP over the output function space to trigger a change in an
action sequence. We achieve state-of-the-art results on diverse future frame
generation in terms of reconstruction quality and diversity of the generated
sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1"&gt;Gaurav Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation. (arXiv:2103.00738v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2103.00738</id>
        <link href="http://arxiv.org/abs/2103.00738"/>
        <updated>2021-07-13T01:59:35.470Z</updated>
        <summary type="html"><![CDATA[Scene understanding based on LiDAR point cloud is an essential task for
autonomous cars to drive safely, which often employs spherical projection to
map 3D point cloud into multi-channel 2D images for semantic segmentation. Most
existing methods simply stack different point attributes/modalities (e.g.
coordinates, intensity, depth, etc.) as image channels to increase information
capacity, but ignore distinct characteristics of point attributes in different
image channels. We design FPS-Net, a convolutional fusion network that exploits
the uniqueness and discrepancy among the projected image channels for optimal
point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead
of simply stacking multiple channel images as a single input, we group them
into different modalities to first learn modality-specific features separately
and then map the learned features into a common high-dimensional feature space
for pixel-level fusion and learning. Specifically, we design a residual dense
block with multiple receptive fields as a building block in the encoder which
preserves detailed information in each modality and learns hierarchical
modality-specific and fused features effectively. In the FPS-Net decoder, we
use a recurrent convolution block likewise to hierarchically decode fused
features into output space for pixel-level classification. Extensive
experiments conducted on two widely adopted point cloud datasets show that
FPS-Net achieves superior semantic segmentation as compared with
state-of-the-art projection-based methods. In addition, the proposed modality
fusion idea is compatible with typical projection-based methods and can be
incorporated into them with consistent performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaofei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Curious Case of Convex Neural Networks. (arXiv:2006.05103v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05103</id>
        <link href="http://arxiv.org/abs/2006.05103"/>
        <updated>2021-07-13T01:59:35.464Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a constrained formulation of neural networks
where the output is a convex function of the input. We show that the convexity
constraints can be enforced on both fully connected and convolutional layers,
making them applicable to most architectures. The convexity constraints include
restricting the weights (for all but the first layer) to be non-negative and
using a non-decreasing convex activation function. Albeit simple, these
constraints have profound implications on the generalization abilities of the
network. We draw three valuable insights: (a) Input Output Convex Neural
Networks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)
Although heavily constrained, they outperform the base multi layer perceptrons
and achieve similar performance as compared to base convolutional architectures
and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the
efficacy of the proposed idea using thorough experiments and ablation studies
on standard image classification datasets with three different neural network
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1"&gt;Sarath Sivaprasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ankur Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manwani_N/0/1/0/all/0/1"&gt;Naresh Manwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1"&gt;Vineet Gandhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SITHCon: A neural network robust to variations in input scaling on the time dimension. (arXiv:2107.04616v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04616</id>
        <link href="http://arxiv.org/abs/2107.04616"/>
        <updated>2021-07-13T01:59:35.450Z</updated>
        <summary type="html"><![CDATA[In machine learning, convolutional neural networks (CNNs) have been extremely
influential in both computer vision and in recognizing patterns extended over
time. In computer vision, part of the flexibility arises from the use of
max-pooling operations over the convolutions to attain translation invariance.
In the mammalian brain, neural representations of time use a set of temporal
basis functions. Critically, these basis functions appear to be arranged in a
geometric series such that the basis set is evenly distributed over logarithmic
time. This paper introduces a Scale-Invariant Temporal History Convolution
network (SITHCon) that uses a logarithmically-distributed temporal memory. A
max-pool over a logarithmically-distributed temporal memory results in
scale-invariance in time. We compare performance of SITHCon to a Temporal
Convolution Network (TCN) and demonstrate that, although both networks can
learn classification and regression problems on both univariate and
multivariate time series $f(t)$, only SITHCon has the property that it
generalizes without retraining to rescaled versions of the input $f(at)$. This
property, inspired by findings from neuroscience and psychology, could lead to
large-scale networks with dramatically different capabilities, including faster
training and greater generalizability, even with significantly fewer free
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jacques_B/0/1/0/all/0/1"&gt;Brandon G. Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiganj_Z/0/1/0/all/0/1"&gt;Zoran Tiganj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Aakash Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_M/0/1/0/all/0/1"&gt;Marc W. Howard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sederberg_P/0/1/0/all/0/1"&gt;Per B. Sederberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H\"older Bounds for Sensitivity Analysis in Causal Reasoning. (arXiv:2107.04661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04661</id>
        <link href="http://arxiv.org/abs/2107.04661"/>
        <updated>2021-07-13T01:59:35.443Z</updated>
        <summary type="html"><![CDATA[We examine interval estimation of the effect of a treatment T on an outcome Y
given the existence of an unobserved confounder U. Using H\"older's inequality,
we derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based
on the degree of unmeasured confounding (i.e., the strength of the connection
U->T, and the strength of U->Y). These bounds are tight either when U is
independent of T or when U is independent of Y given T (when there is no
unobserved confounding). We focus on a special case of this bound depending on
the total variation distance between the distributions p(U) and p(U|T=t), as
well as the maximum (over all possible values of U) deviation of the
conditional expected outcome E[Y|U=u,T=t] from the average expected outcome
E[Y|T=t]. We discuss possible calibration strategies for this bound to get
interval estimates for treatment effects, and experimentally validate the bound
using synthetic and semi-synthetic datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assaad_S/0/1/0/all/0/1"&gt;Serge Assaad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuxi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1"&gt;Henry Pfister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders. (arXiv:2107.04652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04652</id>
        <link href="http://arxiv.org/abs/2107.04652"/>
        <updated>2021-07-13T01:59:35.437Z</updated>
        <summary type="html"><![CDATA[Training and using modern neural-network based latent-variable generative
models (like Variational Autoencoders) often require simultaneously training a
generative direction along with an inferential(encoding) direction, which
approximates the posterior distribution over the latent variables. Thus, the
question arises: how complex does the inferential model need to be, in order to
be able to accurately model the posterior distribution of a given generative
model?

In this paper, we identify an important property of the generative map
impacting the required size of the encoder. We show that if the generative map
is "strongly invertible" (in a sense we suitably formalize), the inferential
model need not be much more complex. Conversely, we prove that there exist
non-invertible generative maps, for which the encoding direction needs to be
exponentially larger (under standard assumptions in computational complexity).
Importantly, we do not require the generative model to be layerwise invertible,
which a lot of the related literature assumes and isn't satisfied by many
architectures used in practice (e.g. convolution and pooling based networks).
Thus, we provide theoretical support for the empirical wisdom that learning
deep generative models is harder when data lies on a low-dimensional manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pareek_D/0/1/0/all/0/1"&gt;Divyansh Pareek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping. (arXiv:2002.02159v1 [cs.GR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2002.02159</id>
        <link href="http://arxiv.org/abs/2002.02159"/>
        <updated>2021-07-13T01:59:35.429Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel active marker for dynamic projection mapping (PM)
that emits a temporal blinking pattern of infrared (IR) light representing its
ID. We used a multi-material three dimensional (3D) printer to fabricate a
projection object with optical fibers that can guide IR light from LEDs
attached on the bottom of the object. The aperture of an optical fiber is
typically very small; thus, it is unnoticeable to human observers under
projection and can be placed on a strongly curved part of a projection surface.
In addition, the working range of our system can be larger than previous
marker-based methods as the blinking patterns can theoretically be recognized
by a camera placed at a wide range of distances from markers. We propose an
automatic marker placement algorithm to spread multiple active markers over the
surface of a projection object such that its pose can be robustly estimated
using captured images from arbitrary directions. We also propose an
optimization framework for determining the routes of the optical fibers in such
a way that collisions of the fibers can be avoided while minimizing the loss of
light intensity in the fibers. Through experiments conducted using three
fabricated objects containing strongly curved surfaces, we confirmed that the
proposed method can achieve accurate dynamic PMs in a significantly wide
working range.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tone_D/0/1/0/all/0/1"&gt;Daiki Tone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1"&gt;Daisuke Iwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hiura_S/0/1/0/all/0/1"&gt;Shinsaku Hiura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kosuke Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems. (arXiv:2107.05222v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.05222</id>
        <link href="http://arxiv.org/abs/2107.05222"/>
        <updated>2021-07-13T01:59:35.422Z</updated>
        <summary type="html"><![CDATA[In this paper we investigate speech denoising as a defense against
adversarial attacks on automatic speech recognition (ASR) systems. Adversarial
attacks attempt to force misclassification by adding small perturbations to the
original speech signal. We propose to counteract this by employing a
neural-network based denoiser as a pre-processor in the ASR pipeline. The
denoiser is independent of the downstream ASR model, and thus can be rapidly
deployed in existing systems. We found that training the denoisier using a
perceptually motivated loss function resulted in increased adversarial
robustness without compromising ASR performance on benign samples. Our defense
was evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack
strategy across a range of attack strengths and speech samples. An average
improvement in Word Error Rate (WER) of about 7.7% was observed over the
undefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sreeram_A/0/1/0/all/0/1"&gt;Anirudh Sreeram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehlman_N/0/1/0/all/0/1"&gt;Nicholas Mehlman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peri_R/0/1/0/all/0/1"&gt;Raghuveer Peri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Knox_D/0/1/0/all/0/1"&gt;Dillon Knox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12011</id>
        <link href="http://arxiv.org/abs/2106.12011"/>
        <updated>2021-07-13T01:59:35.406Z</updated>
        <summary type="html"><![CDATA[This paper jointly resolves two problems in vision transformer: i) the
computation of Multi-Head Self-Attention (MHSA) has high computational/space
complexity; ii) recent vision transformer networks are overly tuned for image
classification, ignoring the difference between image classification (simple
scenarios, more similar to NLP) and downstream scene understanding tasks
(complicated scenarios, rich structural and contextual information). To this
end, we note that pyramid pooling has been demonstrated to be effective in
various vision tasks owing to its powerful ability in context abstraction, and
its natural property of spatial invariance is also suitable to address the loss
of structural information (problem ii)). Hence, we propose to adapt pyramid
pooling to MHSA for alleviating its high requirement on computational resources
(problem i)). In this way, this pooling-based MHSA can well address the above
two problems and is thus flexible and powerful for downstream scene
understanding tasks. Plugged with our pooling-based MHSA, we build a
downstream-task-oriented transformer network, dubbed Pyramid Pooling
Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as
the backbone network, it shows substantial superiority in various downstream
scene understanding tasks such as semantic segmentation, object detection,
instance segmentation, and visual saliency detection, compared to previous CNN-
and transformer-based networks. The code will be released at
https://github.com/yuhuan-wu/P2T.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04705</id>
        <link href="http://arxiv.org/abs/2107.04705"/>
        <updated>2021-07-13T01:59:35.400Z</updated>
        <summary type="html"><![CDATA[Learning disentangled and interpretable representations is an important step
towards accomplishing comprehensive data representations on the manifold. In
this paper, we propose a novel representation learning algorithm which combines
the inference abilities of Variational Autoencoders (VAE) with the
generalization capability of Generative Adversarial Networks (GAN). The
proposed model, called InfoVAEGAN, consists of three networks~: Encoder,
Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and
continuous interpretable representations in an unsupervised manner by using two
different data-free log-likelihood functions onto the variables sampled from
the generator's distribution. We propose a two-stage algorithm for optimizing
the inference network separately from the generator training. Moreover, we
enforce the learning of interpretable representations through the maximization
of the mutual information between the existing latent variables and those
created through generative and inference processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cognitive Visual Commonsense Reasoning Using Dynamic Working Memory. (arXiv:2107.01671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01671</id>
        <link href="http://arxiv.org/abs/2107.01671"/>
        <updated>2021-07-13T01:59:35.394Z</updated>
        <summary type="html"><![CDATA[Visual Commonsense Reasoning (VCR) predicts an answer with corresponding
rationale, given a question-image input. VCR is a recently introduced visual
scene understanding task with a wide range of applications, including visual
question answering, automated vehicle systems, and clinical decision support.
Previous approaches to solving the VCR task generally rely on pre-training or
exploiting memory with long dependency relationship encoded models. However,
these approaches suffer from a lack of generalizability and prior knowledge. In
this paper we propose a dynamic working memory based cognitive VCR network,
which stores accumulated commonsense between sentences to provide prior
knowledge for inference. Extensive experiments show that the proposed model
yields significant improvements over existing methods on the benchmark VCR
dataset. Moreover, the proposed model provides intuitive interpretation into
visual commonsense reasoning. A Python implementation of our mechanism is
publicly available at https://github.com/tanjatang/DMVCR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Child_T/0/1/0/all/0/1"&gt;Travers B. Child&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiong Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes. (arXiv:2104.14435v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14435</id>
        <link href="http://arxiv.org/abs/2104.14435"/>
        <updated>2021-07-13T01:59:35.388Z</updated>
        <summary type="html"><![CDATA[Classification neural networks fail to detect inputs that do not fall inside
the classes they have been trained for. Runtime monitoring techniques on the
neuron activation pattern can be used to detect such inputs. We present an
approach for monitoring classification systems via data abstraction. Data
abstraction relies on the notion of box with a resolution. Box-based
abstraction consists in representing a set of values by its minimal and maximal
values in each dimension. We augment boxes with a notion of resolution and
define their clustering coverage, which is intuitively a quantitative metric
that indicates the abstraction quality. This allows studying the effect of
different clustering parameters on the constructed boxes and estimating an
interval of sub-optimal parameters. Moreover, we automatically construct
monitors that leverage both the correct and incorrect behaviors of a system.
This allows checking the size of the monitor abstractions and analyzing the
separability of the network. Monitors are obtained by combining the
sub-monitors of each class of the system placed at some selected layers. Our
experiments demonstrate the effectiveness of our clustering coverage estimation
and show how to assess the effectiveness and precision of monitors according to
the selected clustering parameter and monitored layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Changshun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcone_Y/0/1/0/all/0/1"&gt;Yli&amp;#xe8;s Falcone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensalem_S/0/1/0/all/0/1"&gt;Saddek Bensalem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Probabilistic Reward Machines from Non-Markovian Stochastic Reward Processes. (arXiv:2107.04633v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04633</id>
        <link href="http://arxiv.org/abs/2107.04633"/>
        <updated>2021-07-13T01:59:35.381Z</updated>
        <summary type="html"><![CDATA[The success of reinforcement learning in typical settings is, in part,
predicated on underlying Markovian assumptions on the reward signal by which an
agent learns optimal policies. In recent years, the use of reward machines has
relaxed this assumption by enabling a structured representation of
non-Markovian rewards. In particular, such representations can be used to
augment the state space of the underlying decision process, thereby
facilitating non-Markovian reinforcement learning. However, these reward
machines cannot capture the semantics of stochastic reward signals. In this
paper, we make progress on this front by introducing probabilistic reward
machines (PRMs) as a representation of non-Markovian stochastic rewards. We
present an algorithm to learn PRMs from the underlying decision process as well
as to learn the PRM representation of a given decision-making policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1"&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beckus_A/0/1/0/all/0/1"&gt;Andre Beckus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohmen_T/0/1/0/all/0/1"&gt;Taylor Dohmen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Ashutosh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topper_N/0/1/0/all/0/1"&gt;Noah Topper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1"&gt;George Atia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00420</id>
        <link href="http://arxiv.org/abs/2107.00420"/>
        <updated>2021-07-13T01:59:35.364Z</updated>
        <summary type="html"><![CDATA[Modern top-performing object detectors depend heavily on backbone networks,
whose advances bring consistent performance gains through exploring more
effective network structures. In this paper, we propose a novel and flexible
backbone framework, namely CBNetV2, to better train existing open-sourced
pre-trained backbones under the pre-training fine-tuning protocol. In
particular, CBNetV2 architecture groups multiple identical backbones, which are
connected through composite connections. Specifically, it integrates the high-
and low-level features of multiple backbone networks and gradually expands the
receptive field to more efficiently perform object detection. We also propose a
better training strategy with assistant supervision for CBNet-based detectors.
CBNetV2 has strong generalization capabilities for different backbones and head
designs of the detector architecture. Without additional pre-training, CBNetV2
can be adapted to various backbones, including manual-based and NAS-based, as
well as CNN-based and Transformer-based ones. Experiments provide strong
evidence showing that composite backbones are more efficient, effective, and
resource-friendly than wider and deeper networks. CBNetV2 is compatible with
the head designs of most mainstream detectors, including one-stage and
two-stage detectors, as well as anchor-based and anchor-free-based ones, and
significantly improve their performances by more than 3.0% AP over the baseline
on COCO. Particularly, under the single-model and single-scale testing
protocol, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO
test-dev, which is significantly better than the state-of-the-art result (i.e.,
57.7% box AP and 50.2% mask AP). Code is available at
https://github.com/VDIGPKU/CBNetV2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tingting Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xiaojie Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yudong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-07-13T01:59:35.358Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MVT: Mask Vision Transformer for Facial Expression Recognition in the wild. (arXiv:2106.04520v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04520</id>
        <link href="http://arxiv.org/abs/2106.04520"/>
        <updated>2021-07-13T01:59:35.352Z</updated>
        <summary type="html"><![CDATA[Facial Expression Recognition (FER) in the wild is an extremely challenging
task in computer vision due to variant backgrounds, low-quality facial images,
and the subjectiveness of annotators. These uncertainties make it difficult for
neural networks to learn robust features on limited-scale datasets. Moreover,
the networks can be easily distributed by the above factors and perform
incorrect decisions. Recently, vision transformer (ViT) and data-efficient
image transformers (DeiT) present their significant performance in traditional
classification tasks. The self-attention mechanism makes transformers obtain a
global receptive field in the first layer which dramatically enhances the
feature extraction capability. In this work, we first propose a novel pure
transformer-based mask vision transformer (MVT) for FER in the wild, which
consists of two modules: a transformer-based mask generation network (MGN) to
generate a mask that can filter out complex backgrounds and occlusion of face
images, and a dynamic relabeling module to rectify incorrect labels in FER
datasets in the wild. Extensive experimental results demonstrate that our MVT
outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with
89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable
result on AffectNet-8 with 61.40%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hanting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1"&gt;Mingzhe Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Feng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhengjun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Feng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00382</id>
        <link href="http://arxiv.org/abs/2107.00382"/>
        <updated>2021-07-13T01:59:35.345Z</updated>
        <summary type="html"><![CDATA[Place recognition gives a SLAM system the ability to correct cumulative
errors. Unlike images that contain rich texture features, point clouds are
almost pure geometric information which makes place recognition based on point
clouds challenging. Existing works usually encode low-level features such as
coordinate, normal, reflection intensity, etc., as local or global descriptors
to represent scenes. Besides, they often ignore the translation between point
clouds when matching descriptors. Different from most existing methods, we
explore the use of high-level features, namely semantics, to improve the
descriptor's representation ability. Also, when matching descriptors, we try to
correct the translation between point clouds to improve accuracy. Concretely,
we propose a novel global descriptor, Semantic Scan Context, which explores
semantic information to represent scenes more effectively. We also present a
two-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align
the point cloud to improve matching performance. Our experiments on the KITTI
dataset show that our approach outperforms the state-of-the-art methods with a
large margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangrui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianxin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weather and Light Level Classification for Autonomous Driving: Dataset, Baseline and Active Learning. (arXiv:2104.14042v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14042</id>
        <link href="http://arxiv.org/abs/2104.14042"/>
        <updated>2021-07-13T01:59:35.338Z</updated>
        <summary type="html"><![CDATA[Autonomous driving is rapidly advancing, and Level 2 functions are becoming a
standard feature. One of the foremost outstanding hurdles is to obtain robust
visual perception in harsh weather and low light conditions where accuracy
degradation is severe. It is critical to have a weather classification model to
decrease visual perception confidence during these scenarios. Thus, we have
built a new dataset for weather (fog, rain, and snow) classification and light
level (bright, moderate, and low) classification. Furthermore, we provide
street type (asphalt, grass, and cobblestone) classification, leading to 9
labels. Each image has three labels corresponding to weather, light level, and
street type. We recorded the data utilizing an industrial front camera of RCCC
(red/clear) format with a resolution of $1024\times1084$. We collected 15k
video sequences and sampled 60k images. We implement an active learning
framework to reduce the dataset's redundancy and find the optimal set of frames
for training a model. We distilled the 60k images further to 1.1k images, which
will be shared publicly after privacy anonymization. There is no public dataset
for weather and light level classification focused on autonomous driving to the
best of our knowledge. The baseline ResNet18 network used for weather
classification achieves state-of-the-art results in two non-automotive weather
classification public datasets but significantly lower accuracy on our proposed
dataset, demonstrating it is not saturated and needs further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhananjaya_M/0/1/0/all/0/1"&gt;Mahesh M Dhananjaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Varun Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data. (arXiv:2107.04680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04680</id>
        <link href="http://arxiv.org/abs/2107.04680"/>
        <updated>2021-07-13T01:59:35.321Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are viewed as an effective way to explain machine
learning predictions. This interest is reflected by a relatively young
literature with already dozens of algorithms aiming to generate such
explanations. These algorithms are focused on finding how features can be
modified to change the output classification. However, this rather general
objective can be achieved in different ways, which brings about the need for a
methodology to test and benchmark these algorithms. The contributions of this
work are manifold: First, a large benchmarking study of 10 algorithmic
approaches on 22 tabular datasets is performed, using 9 relevant evaluation
metrics. Second, the introduction of a novel, first of its kind, framework to
test counterfactual generation algorithms. Third, a set of objective metrics to
evaluate and compare counterfactual results. And finally, insight from the
benchmarking results that indicate which approaches obtain the best performance
on what type of dataset. This benchmarking study and framework can help
practitioners in determining which technique and building blocks most suit
their context, and can help researchers in the design and evaluation of current
and future counterfactual generation algorithms. Our findings show that,
overall, there's no single best algorithm to generate counterfactual
explanations as the performance highly depends on properties related to the
dataset, model, score and factual point specificities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzine_R/0/1/0/all/0/1"&gt;Raphael Mazzine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1"&gt;David Martens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06896</id>
        <link href="http://arxiv.org/abs/2106.06896"/>
        <updated>2021-07-13T01:59:35.315Z</updated>
        <summary type="html"><![CDATA[The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yunhao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianbu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weiwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Ran Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1"&gt;Qian Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Space Targeted Attacks by Statistic Alignment. (arXiv:2105.11645v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11645</id>
        <link href="http://arxiv.org/abs/2105.11645"/>
        <updated>2021-07-13T01:59:35.308Z</updated>
        <summary type="html"><![CDATA[By adding human-imperceptible perturbations to images, DNNs can be easily
fooled. As one of the mainstream methods, feature space targeted attacks
perturb images by modulating their intermediate feature maps, for the
discrepancy between the intermediate source and target features is minimized.
However, the current choice of pixel-wise Euclidean Distance to measure the
discrepancy is questionable because it unreasonably imposes a
spatial-consistency constraint on the source and target features. Intuitively,
an image can be categorized as "cat" no matter the cat is on the left or right
of the image. To address this issue, we propose to measure this discrepancy
using statistic alignment. Specifically, we design two novel approaches called
Pair-wise Alignment Attack and Global-wise Alignment Attack, which attempt to
measure similarities between feature maps by high-order statistics with
translation invariance. Furthermore, we systematically analyze the layer-wise
transferability with varied difficulties to obtain highly reliable attacks.
Extensive experiments verify the effectiveness of our proposed method, and it
outperforms the state-of-the-art algorithms by a large margin. Our code is
publicly available at https://github.com/yaya-cheng/PAA-GAA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1"&gt;Lianli Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yaya Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jingkuan Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15299</id>
        <link href="http://arxiv.org/abs/2106.15299"/>
        <updated>2021-07-13T01:59:35.300Z</updated>
        <summary type="html"><![CDATA[Digitization of histology images and the advent of new computational methods,
like deep learning, have helped the automatic grading of colorectal
adenocarcinoma cancer (CRA). Present automated CRA grading methods, however,
usually use tiny image patches and thus fail to integrate the entire tissue
micro-architecture for grading purposes. To tackle these challenges, we propose
to use a statistical network analysis method to describe the complex structure
of the tissue micro-environment by modelling nuclei and their connections as a
network. We show that by analyzing only the interactions between the cells in a
network, we can extract highly discriminative statistical features for CRA
grading. Unlike other deep learning or convolutional graph-based approaches,
our method is highly scalable (can be used for cell networks consist of
millions of nodes), completely explainable, and computationally inexpensive. We
create cell networks on a broad CRC histology image dataset, experiment with
our method, and report state-of-the-art performance for the prediction of
three-class CRA grading.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1"&gt;Neda Zamanitajeddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1"&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1"&gt;Nasir Rajpoot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Optimization of Hadamard Sensing and Reconstruction in Compressed Sensing Fluorescence Microscopy. (arXiv:2105.07961v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07961</id>
        <link href="http://arxiv.org/abs/2105.07961"/>
        <updated>2021-07-13T01:59:35.295Z</updated>
        <summary type="html"><![CDATA[Compressed sensing fluorescence microscopy (CS-FM) proposes a scheme whereby
less measurements are collected during sensing and reconstruction is performed
to recover the image. Much work has gone into optimizing the sensing and
reconstruction portions separately. We propose a method of jointly optimizing
both sensing and reconstruction end-to-end under a total measurement
constraint, enabling learning of the optimal sensing scheme concurrently with
the parameters of a neural network-based reconstruction network. We train our
model on a rich dataset of confocal, two-photon, and wide-field microscopy
images comprising of a variety of biological samples. We show that our method
outperforms several baseline sensing schemes and a regularized regression
reconstruction algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alan Q. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+LaViolette_A/0/1/0/all/0/1"&gt;Aaron K. LaViolette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moon_L/0/1/0/all/0/1"&gt;Leo Moon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chris Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabuncu_M/0/1/0/all/0/1"&gt;Mert R. Sabuncu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble-based Semi-supervised Learning to Improve Noisy Soiling Annotations in Autonomous Driving. (arXiv:2105.07930v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07930</id>
        <link href="http://arxiv.org/abs/2105.07930"/>
        <updated>2021-07-13T01:59:35.275Z</updated>
        <summary type="html"><![CDATA[Manual annotation of soiling on surround view cameras is a very challenging
and expensive task. The unclear boundary for various soiling categories like
water drops or mud particles usually results in a large variance in the
annotation quality. As a result, the models trained on such poorly annotated
data are far from being optimal. In this paper, we focus on handling such noisy
annotations via pseudo-label driven ensemble model which allow us to quickly
spot problematic annotations and in most cases also sufficiently fixing them.
We train a soiling segmentation model on both noisy and refined labels and
demonstrate significant improvements using the refined annotations. It also
illustrates that it is possible to effectively refine lower cost coarse
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uricar_M/0/1/0/all/0/1"&gt;Michal Uricar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1"&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahiaoui_L/0/1/0/all/0/1"&gt;Lucie Yahiaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04694</id>
        <link href="http://arxiv.org/abs/2107.04694"/>
        <updated>2021-07-13T01:59:35.269Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end lifelong learning mixture of experts.
Each expert is implemented by a Variational Autoencoder (VAE). The experts in
the mixture system are jointly trained by maximizing a mixture of individual
component evidence lower bounds (MELBO) on the log-likelihood of the given
training samples. The mixing coefficients in the mixture, control the
contributions of each expert in the goal representation. These are sampled from
a Dirichlet distribution whose parameters are determined through non-parametric
estimation during lifelong learning. The model can learn new tasks fast when
these are similar to those previously learnt. The proposed Lifelong mixture of
VAE (L-MVAE) expands its architecture with new components when learning a
completely new task. After the training, our model can automatically determine
the relevant expert to be used when fed with new data samples. This mechanism
benefits both the memory efficiency and the required computational cost as only
one expert is used during the inference. The L-MVAE inference model is able to
perform interpolation in the joint latent space across the data domains
associated with different tasks and is shown to be efficient for disentangled
learning representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14844</id>
        <link href="http://arxiv.org/abs/2106.14844"/>
        <updated>2021-07-13T01:59:35.258Z</updated>
        <summary type="html"><![CDATA[Low-light imaging on mobile devices is typically challenging due to
insufficient incident light coming through the relatively small aperture,
resulting in a low signal-to-noise ratio. Most of the previous works on
low-light image processing focus either only on a single task such as
illumination adjustment, color enhancement, or noise removal; or on a joint
illumination adjustment and denoising task that heavily relies on short-long
exposure image pairs collected from specific camera models, and thus these
approaches are less practical and generalizable in real-world settings where
camera-specific joint enhancement and restoration is required. To tackle this
problem, in this paper, we propose a low-light image processing framework that
performs joint illumination adjustment, color enhancement, and denoising.
Considering the difficulty in model-specific data collection and the ultra-high
definition of the captured images, we design two branches: a coefficient
estimation branch as well as a joint enhancement and denoising branch. The
coefficient estimation branch works in a low-resolution space and predicts the
coefficients for enhancement via bilateral learning, whereas the joint
enhancement and denoising branch works in a full-resolution space and
progressively performs joint enhancement and denoising. In contrast to existing
methods, our framework does not need to recollect massive data when being
adapted to another camera model, which significantly reduces the efforts
required to fine-tune our approach for practical usage. Through extensive
experiments, we demonstrate its great potential in real-world low-light imaging
applications when compared with current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seung-Won Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images. (arXiv:2107.05047v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05047</id>
        <link href="http://arxiv.org/abs/2107.05047"/>
        <updated>2021-07-13T01:59:35.239Z</updated>
        <summary type="html"><![CDATA[Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of AI models for clinical decision support. For medical
images, saliency maps are the most common form of explanation. The maps
highlight important features for AI model's prediction. Although many saliency
map methods have been proposed, it is unknown how well they perform on
explaining decisions on multi-modal medical images, where each modality/channel
carries distinct clinical meanings of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the MSFI
(Modality-Specific Feature Importance) metric to examine whether saliency maps
can highlight modality-specific important features. MSFI encodes the clinical
requirements on modality prioritization and modality-specific feature
localization. Our evaluations on 16 commonly used saliency map methods,
including a clinician user study, show that although most saliency map methods
captured modality importance information in general, most of them failed to
highlight modality-specific important features consistently and precisely. The
evaluation results guide the choices of saliency map methods and provide
insights to propose new ones targeting clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1"&gt;Weina Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1"&gt;Ghassan Hamarneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10829</id>
        <link href="http://arxiv.org/abs/2106.10829"/>
        <updated>2021-07-13T01:59:35.233Z</updated>
        <summary type="html"><![CDATA[This technical report presents our solution to the HACS Temporal Action
Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of
weakly-supervised temporal action localization is to temporally locate and
classify action of interest in untrimmed videos given only video-level labels.
We adopt the two-stream consensus network (TSCN) as the main framework in this
challenge. The TSCN consists of a two-stream base model training procedure and
a pseudo ground truth learning procedure. The base model training encourages
the model to predict reliable predictions based on single modality (i.e., RGB
or optical flow), based on the fusion of which a pseudo ground truth is
generated and in turn used as supervision to train the base models. On the HACS
v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our
method achieves 22.20% on the validation set and 21.68% on the testing set in
terms of average mAP. Our solution ranked the 2rd in this challenge, and we
hope our method can serve as a baseline for future academic research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1"&gt;Yuanhao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11810</id>
        <link href="http://arxiv.org/abs/2106.11810"/>
        <updated>2021-07-13T01:59:35.202Z</updated>
        <summary type="html"><![CDATA[In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1"&gt;Holger Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1"&gt;Juraj Kabzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kok Seang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1"&gt;Whye Kit Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1"&gt;Eric Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Alex Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1"&gt;Luke Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1"&gt;Sammy Omari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Surface Normal Constraint for Depth Estimation. (arXiv:2103.15483v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15483</id>
        <link href="http://arxiv.org/abs/2103.15483"/>
        <updated>2021-07-13T01:59:35.196Z</updated>
        <summary type="html"><![CDATA[We present a novel method for single image depth estimation using surface
normal constraints. Existing depth estimation methods either suffer from the
lack of geometric constraints, or are limited to the difficulty of reliably
capturing geometric context, which leads to a bottleneck of depth estimation
quality. We therefore introduce a simple yet effective method, named Adaptive
Surface Normal (ASN) constraint, to effectively correlate the depth estimation
with geometric consistency. Our key idea is to adaptively determine the
reliable local geometry from a set of randomly sampled candidates to derive
surface normal constraint, for which we measure the consistency of the
geometric contextual features. As a result, our method can faithfully
reconstruct the 3D geometry and is robust to local shape variations, such as
boundaries, sharp corners and noises. We conduct extensive evaluations and
comparisons using public datasets. The experimental results demonstrate our
method outperforms the state-of-the-art methods and has superior efficiency and
robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Cheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEVDetNet: Bird's Eye View LiDAR Point Cloud based Real-time 3D Object Detection for Autonomous Driving. (arXiv:2104.10780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10780</id>
        <link href="http://arxiv.org/abs/2104.10780"/>
        <updated>2021-07-13T01:59:35.184Z</updated>
        <summary type="html"><![CDATA[3D object detection based on LiDAR point clouds is a crucial module in
autonomous driving particularly for long range sensing. Most of the research is
focused on achieving higher accuracy and these models are not optimized for
deployment on embedded systems from the perspective of latency and power
efficiency. For high speed driving scenarios, latency is a crucial parameter as
it provides more time to react to dangerous situations. Typically a voxel or
point-cloud based 3D convolution approach is utilized for this module. Firstly,
they are inefficient on embedded platforms as they are not suitable for
efficient parallelization. Secondly, they have a variable runtime due to level
of sparsity of the scene which is against the determinism needed in a safety
system. In this work, we aim to develop a very low latency algorithm with fixed
runtime. We propose a novel semantic segmentation architecture as a single
unified model for object center detection using key points, box predictions and
orientation prediction using binned classification in a simpler Bird's Eye View
(BEV) 2D representation. The proposed architecture can be trivially extended to
include semantic segmentation classes like road without any additional
computation. The proposed model has a latency of 4 ms on the embedded Nvidia
Xavier platform. The model is 5X faster than other top accuracy models with a
minimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1"&gt;Sambit Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gotzig_H/0/1/0/all/0/1"&gt;Heinrich Gotzig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1"&gt;Stefan Milz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1"&gt;Patrick Mader&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security in Next Generation Mobile Payment Systems: A Comprehensive Survey. (arXiv:2105.12097v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12097</id>
        <link href="http://arxiv.org/abs/2105.12097"/>
        <updated>2021-07-13T01:59:35.166Z</updated>
        <summary type="html"><![CDATA[Cash payment is still king in several markets, accounting for more than 90\
of the payments in almost all the developing countries. The usage of mobile
phones is pretty ordinary in this present era. Mobile phones have become an
inseparable friend for many users, serving much more than just communication
tools. Every subsequent person is heavily relying on them due to multifaceted
usage and affordability. Every person wants to manage his/her daily
transactions and related issues by using his/her mobile phone. With the rise
and advancements of mobile-specific security, threats are evolving as well. In
this paper, we provide a survey of various security models for mobile phones.
We explore multiple proposed models of the mobile payment system (MPS), their
technologies and comparisons, payment methods, different security mechanisms
involved in MPS, and provide analysis of the encryption technologies,
authentication methods, and firewall in MPS. We also present current challenges
and future directions of mobile phone security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1"&gt;Waqas Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasool_A/0/1/0/all/0/1"&gt;Amir Rasool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+RehmanJaved_A/0/1/0/all/0/1"&gt;Abdul RehmanJaved&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1"&gt;Thippa Reddy Gadekallu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalil_Z/0/1/0/all/0/1"&gt;Zunera Jalil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kryvinska_N/0/1/0/all/0/1"&gt;Natalia Kryvinska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images. (arXiv:2101.00522v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00522</id>
        <link href="http://arxiv.org/abs/2101.00522"/>
        <updated>2021-07-13T01:59:35.157Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles. (arXiv:2104.00946v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00946</id>
        <link href="http://arxiv.org/abs/2104.00946"/>
        <updated>2021-07-13T01:59:35.149Z</updated>
        <summary type="html"><![CDATA[Human behavior understanding with unmanned aerial vehicles (UAVs) is of great
significance for a wide range of applications, which simultaneously brings an
urgent demand of large, challenging, and comprehensive benchmarks for the
development and evaluation of UAV-based models. However, existing benchmarks
have limitations in terms of the amount of captured data, types of data
modalities, categories of provided tasks, and diversities of subjects and
environments. Here we propose a new benchmark - UAVHuman - for human behavior
understanding with UAVs, which contains 67,428 multi-modal video sequences and
119 subjects for action recognition, 22,476 frames for pose estimation, 41,290
frames and 1,144 identities for person re-identification, and 22,263 frames for
attribute recognition. Our dataset was collected by a flying UAV in multiple
urban and rural districts in both daytime and nighttime over three months,
hence covering extensive diversities w.r.t subjects, backgrounds,
illuminations, weathers, occlusions, camera motions, and UAV flying attitudes.
Such a comprehensive and challenging benchmark shall be able to promote the
research of UAV-based human behavior understanding, including action
recognition, pose estimation, re-identification, and attribute recognition.
Furthermore, we propose a fisheye-based action recognition method that
mitigates the distortions in fisheye videos via learning unbounded
transformations guided by flat RGB videos. Experiments show the efficacy of our
method on the UAV-Human dataset. The project page:
https://github.com/SUTDCV/UAV-Human]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianjiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1"&gt;Yun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Sensing Image Change Detection with Transformers. (arXiv:2103.00208v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00208</id>
        <link href="http://arxiv.org/abs/2103.00208"/>
        <updated>2021-07-13T01:59:35.142Z</updated>
        <summary type="html"><![CDATA[Modern change detection (CD) has achieved remarkable success by the powerful
discriminative ability of deep convolutions. However, high-resolution remote
sensing CD remains challenging due to the complexity of objects in the scene.
Objects with the same semantic concept may show distinct spectral
characteristics at different times and spatial locations. Most recent CD
pipelines using pure convolutions are still struggling to relate long-range
concepts in space-time. Non-local self-attention approaches show promising
performance via modeling dense relations among pixels, yet are computationally
inefficient. Here, we propose a bitemporal image transformer (BIT) to
efficiently and effectively model contexts within the spatial-temporal domain.
Our intuition is that the high-level concepts of the change of interest can be
represented by a few visual words, i.e., semantic tokens. To achieve this, we
express the bitemporal image into a few tokens, and use a transformer encoder
to model contexts in the compact token-based space-time. The learned
context-rich tokens are then feedback to the pixel-space for refining the
original features via a transformer decoder. We incorporate BIT in a deep
feature differencing-based CD framework. Extensive experiments on three CD
datasets demonstrate the effectiveness and efficiency of the proposed method.
Notably, our BIT-based model significantly outperforms the purely convolutional
baseline using only 3 times lower computational costs and model parameters.
Based on a naive backbone (ResNet18) without sophisticated structures (e.g.,
FPN, UNet), our model surpasses several state-of-the-art CD methods, including
better than four recent attention-based methods in terms of efficiency and
accuracy. Our code is available at https://github.com/justchenhao/BIT\_CD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Zipeng Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhenwei Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:35.134Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation. (arXiv:2102.08005v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08005</id>
        <link href="http://arxiv.org/abs/2102.08005"/>
        <updated>2021-07-13T01:59:35.127Z</updated>
        <summary type="html"><![CDATA[Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yundong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huiye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1"&gt;Qiang Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination. (arXiv:2101.02639v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02639</id>
        <link href="http://arxiv.org/abs/2101.02639"/>
        <updated>2021-07-13T01:59:35.121Z</updated>
        <summary type="html"><![CDATA[Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jian Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuge Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1"&gt;Licong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiaona Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Desheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kehong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer. (arXiv:2103.09712v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09712</id>
        <link href="http://arxiv.org/abs/2103.09712"/>
        <updated>2021-07-13T01:59:35.095Z</updated>
        <summary type="html"><![CDATA[Real-time surgical phase recognition is a fundamental task in modern
operating rooms. Previous works tackle this task relying on architectures
arranged in spatio-temporal order, however, the supportive benefits of
intermediate spatial features are not considered. In this paper, we introduce,
for the first time in surgical workflow analysis, Transformer to reconsider the
ignored complementary effects of spatial and temporal features for accurate
surgical phase recognition. Our hybrid embedding aggregation Transformer fuses
cleverly designed spatial and temporal embeddings by allowing for active
queries based on spatial information from temporal embedding sequences. More
importantly, our framework processes the hybrid embeddings in parallel to
achieve a high inference speed. Our method is thoroughly validated on two large
surgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and
outperforms the state-of-the-art approaches at a processing speed of 91 fps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaojie Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yueming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yonghao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1"&gt;Qi Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1"&gt;Pheng-Ann Heng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering. (arXiv:2103.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15069</id>
        <link href="http://arxiv.org/abs/2103.15069"/>
        <updated>2021-07-13T01:59:35.075Z</updated>
        <summary type="html"><![CDATA[Multi-view clustering is an important research topic due to its capability to
utilize complementary information from multiple views. However, there are few
methods to consider the negative impact caused by certain views with unclear
clustering structures, resulting in poor multi-view clustering performance. To
address this drawback, we propose self-supervised discriminative feature
learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders
are applied to learn embedded features for each view independently. To leverage
the multi-view complementary information, we concatenate all views' embedded
features to form the global features, which can overcome the negative impact of
some views' unclear clustering structures. In a self-supervised manner,
pseudo-labels are obtained to build a unified target distribution to perform
multi-view discriminative feature learning. During this process, global
discriminative information can be mined to supervise all views to learn more
discriminative features, which in turn are used to update the target
distribution. Besides, this unified target distribution can make SDMVC learn
consistent cluster assignments, which accomplishes the clustering consistency
of multiple views while preserving their features' diversity. Experiments on
various types of multi-view datasets show that SDMVC achieves state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yazhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Huayi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhimeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lili Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_X/0/1/0/all/0/1"&gt;Xiaorong Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Task Mutual Learning for Semi-Supervised Medical Image Segmentation. (arXiv:2103.04708v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04708</id>
        <link href="http://arxiv.org/abs/2103.04708"/>
        <updated>2021-07-13T01:59:35.060Z</updated>
        <summary type="html"><![CDATA[The success of deep learning methods in medical image segmentation tasks
usually requires a large amount of labeled data. However, obtaining reliable
annotations is expensive and time-consuming. Semi-supervised learning has
attracted much attention in medical image segmentation by taking the advantage
of unlabeled data which is much easier to acquire. In this paper, we propose a
novel dual-task mutual learning framework for semi-supervised medical image
segmentation. Our framework can be formulated as an integration of two
individual segmentation networks based on two tasks: learning region-based
shape constraint and learning boundary-based surface mismatch. Different from
the one-way transfer between teacher and student networks, an ensemble of
dual-task students can learn collaboratively and implicitly explore useful
knowledge from each other during the training process. By jointly learning the
segmentation probability maps and signed distance maps of targets, our
framework can enforce the geometric shape constraint and learn more reliable
information. Experimental results demonstrate that our method achieves
performance gains by leveraging unlabeled data and outperforms the
state-of-the-art semi-supervised segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jicong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04537</id>
        <link href="http://arxiv.org/abs/2103.04537"/>
        <updated>2021-07-13T01:59:35.053Z</updated>
        <summary type="html"><![CDATA[We propose and demonstrate a representation learning approach by maximizing
the mutual information between local features of images and text. The goal of
this approach is to learn useful image representations by taking advantage of
the rich information contained in the free text that describes the findings in
the image. Our method trains image and text encoders by encouraging the
resulting representations to exhibit high local mutual information. We make use
of recent advances in mutual information estimation with neural network
discriminators. We argue that the sum of local mutual information is typically
a lower bound on the global mutual information. Our experimental results in the
downstream image classification tasks demonstrate the advantages of using local
features for image-text representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1"&gt;Ruizhi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1"&gt;Daniel Moyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1"&gt;Miriam Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1"&gt;Keegan Quigley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1"&gt;Seth Berkowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1"&gt;Steven Horng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1"&gt;William M. Wells&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Makes for End-to-End Object Detection?. (arXiv:2012.05780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05780</id>
        <link href="http://arxiv.org/abs/2012.05780"/>
        <updated>2021-07-13T01:59:35.036Z</updated>
        <summary type="html"><![CDATA[Object detection has recently achieved a breakthrough for removing the last
one non-differentiable component in the pipeline, Non-Maximum Suppression
(NMS), and building up an end-to-end system. However, what makes for its
one-to-one prediction has not been well understood. In this paper, we first
point out that one-to-one positive sample assignment is the key factor, while,
one-to-many assignment in previous detectors causes redundant predictions in
inference. Second, we surprisingly find that even training with one-to-one
assignment, previous detectors still produce redundant predictions. We identify
that classification cost in matching cost is the main ingredient: (1) previous
detectors only consider location cost, (2) by additionally introducing
classification cost, previous detectors immediately produce one-to-one
prediction during inference. We introduce the concept of score gap to explore
the effect of matching cost. Classification cost enlarges the score gap by
choosing positive samples as those of highest score in the training iteration
and reducing noisy positive samples brought by only location cost. Finally, we
demonstrate the advantages of end-to-end object detection on crowded scenes.
The code is available at: \url{https://github.com/PeizeSun/OneNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peize Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1"&gt;Wenqi Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zehuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Semantic Scene Completion: a Survey. (arXiv:2103.07466v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07466</id>
        <link href="http://arxiv.org/abs/2103.07466"/>
        <updated>2021-07-13T01:59:35.030Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion (SSC) aims to jointly estimate the complete
geometry and semantics of a scene, assuming partial sparse input. In the last
years following the multiplication of large-scale 3D datasets, SSC has gained
significant momentum in the research community because it holds unresolved
challenges. Specifically, SSC lies in the ambiguous completion of large
unobserved areas and the weak supervision signal of the ground truth. This led
to a substantially increasing number of papers on the matter. This survey aims
to identify, compare and analyze the techniques providing a critical analysis
of the SSC literature on both methods and datasets. Throughout the paper, we
provide an in-depth analysis of the existing works covering all choices made by
the authors while highlighting the remaining avenues of research. SSC
performance of the SoA on the most popular datasets is also evaluated and
analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1"&gt;Luis Roldao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verroust_Blondet_A/0/1/0/all/0/1"&gt;Anne Verroust-Blondet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A review of 3D human pose estimation algorithms for markerless motion capture. (arXiv:2010.06449v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06449</id>
        <link href="http://arxiv.org/abs/2010.06449"/>
        <updated>2021-07-13T01:59:35.023Z</updated>
        <summary type="html"><![CDATA[Human pose estimation is a very active research field, stimulated by its
important applications in robotics, entertainment or health and sports
sciences, among others. Advances in convolutional networks triggered noticeable
improvements in 2D pose estimation, leading modern 3D markerless motion capture
techniques to an average error per joint of 20 mm. However, with the
proliferation of methods, it is becoming increasingly difficult to make an
informed choice. Here, we review the leading human pose estimation methods of
the past five years, focusing on metrics, benchmarks and method structures. We
propose a taxonomy based on accuracy, speed and robustness that we use to
classify de methods and derive directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desmarais_Y/0/1/0/all/0/1"&gt;Yann Desmarais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottet_D/0/1/0/all/0/1"&gt;Denis Mottet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slangen_P/0/1/0/all/0/1"&gt;Pierre Slangen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesinos_P/0/1/0/all/0/1"&gt;Philippe Montesinos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multiscale Correlations for Human Motion Prediction. (arXiv:2103.10674v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10674</id>
        <link href="http://arxiv.org/abs/2103.10674"/>
        <updated>2021-07-13T01:59:35.017Z</updated>
        <summary type="html"><![CDATA[In spite of the great progress in human motion prediction, it is still a
challenging task to predict those aperiodic and complicated motions. We believe
that to capture the correlations among human body components is the key to
understand the human motion. In this paper, we propose a novel multiscale graph
convolution network (MGCN) to address this problem. Firstly, we design an
adaptive multiscale interactional encoding module (MIEM) which is composed of
two sub modules: scale transformation module and scale interaction module to
learn the human body correlations. Secondly, we apply a coarse-to-fine decoding
strategy to decode the motions sequentially. We evaluate our approach on two
standard benchmark datasets for human motion prediction: Human3.6M and CMU
motion capture dataset. The experiments show that the proposed approach
achieves the state-of-the-art performance for both short-term and long-term
prediction especially in those complicated action category.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Honghong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Caili Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task-Oriented Low-Dose CT Image Denoising. (arXiv:2103.13557v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13557</id>
        <link href="http://arxiv.org/abs/2103.13557"/>
        <updated>2021-07-13T01:59:35.010Z</updated>
        <summary type="html"><![CDATA[The extensive use of medical CT has raised a public concern over the
radiation dose to the patient. Reducing the radiation dose leads to increased
CT image noise and artifacts, which can adversely affect not only the
radiologists judgement but also the performance of downstream medical image
analysis tasks. Various low-dose CT denoising methods, especially the recent
deep learning based approaches, have produced impressive results. However, the
existing denoising methods are all downstream-task-agnostic and neglect the
diverse needs of the downstream applications. In this paper, we introduce a
novel Task-Oriented Denoising Network (TOD-Net) with a task-oriented loss
leveraging knowledge from the downstream tasks. Comprehensive empirical
analysis shows that the task-oriented loss complements other task agnostic
losses by steering the denoiser to enhance the image quality in the task
related regions of interest. Such enhancement in turn brings general boosts on
the performance of various methods for the downstream task. The presented work
may shed light on the future development of context-aware image denoising
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiajin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMINN: Autoencoder-based Multiple Instance Neural Network Improves Outcome Prediction of Multifocal Liver Metastases. (arXiv:2012.06875v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06875</id>
        <link href="http://arxiv.org/abs/2012.06875"/>
        <updated>2021-07-13T01:59:34.991Z</updated>
        <summary type="html"><![CDATA[Colorectal cancer is one of the most common and lethal cancers and colorectal
cancer liver metastases (CRLM) is the major cause of death in patients with
colorectal cancer. Multifocality occurs frequently in CRLM, but is relatively
unexplored in CRLM outcome prediction. Most existing clinical and imaging
biomarkers do not take the imaging features of all multifocal lesions into
account. In this paper, we present an end-to-end autoencoder-based multiple
instance neural network (AMINN) for the prediction of survival outcomes in
multifocal CRLM patients using radiomic features extracted from
contrast-enhanced MRIs. Specifically, we jointly train an autoencoder to
reconstruct input features and a multiple instance network to make predictions
by aggregating information from all tumour lesions of a patient. Also, we
incorporate a two-step normalization technique to improve the training of deep
neural networks, built on the observation that the distributions of radiomic
features are almost always severely skewed. Experimental results empirically
validated our hypothesis that incorporating imaging features of all lesions
improves outcome prediction for multifocal cancer. The proposed AMINN framework
achieved an area under the ROC curve (AUC) of 0.70, which is 11.4% higher than
the best baseline method. A risk score based on the outputs of AMINN achieved
superior prediction in our multifocal CRLM cohort. The effectiveness of
incorporating all lesions and applying two-step normalization is demonstrated
by a series of ablation studies. A Keras implementation of AMINN is released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_H/0/1/0/all/0/1"&gt;Helen M. C. Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milot_L/0/1/0/all/0/1"&gt;Laurent Milot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1"&gt;Anne L. Martel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. (arXiv:2012.06815v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06815</id>
        <link href="http://arxiv.org/abs/2012.06815"/>
        <updated>2021-07-13T01:59:34.985Z</updated>
        <summary type="html"><![CDATA[Visual object tracking aims to precisely estimate the bounding box for the
given target, which is a challenging problem due to factors such as deformation
and occlusion. Many recent trackers adopt the multiple-stage tracking strategy
to improve the quality of bounding box estimation. These methods first coarsely
locate the target and then refine the initial prediction in the following
stages. However, existing approaches still suffer from limited precision, and
the coupling of different stages severely restricts the method's
transferability. This work proposes a novel, flexible, and accurate refinement
module called Alpha-Refine (AR), which can significantly improve the base
trackers' box estimation quality. By exploring a series of design options, we
conclude that the key to successful refinement is extracting and maintaining
detailed spatial information as much as possible. Following this principle,
Alpha-Refine adopts a pixel-wise correlation, a corner prediction head, and an
auxiliary mask head as the core components. Comprehensive experiments on
TrackingNet, LaSOT, GOT-10K, and VOT2020 benchmarks with multiple base trackers
show that our approach significantly improves the base trackers' performance
with little extra latency. The proposed Alpha-Refine method leads to a series
of strengthened trackers, among which the ARSiamRPN (AR strengthened SiamRPNpp)
and the ARDiMP50 (ARstrengthened DiMP50) achieve good efficiency-precision
trade-off, while the ARDiMPsuper (AR strengthened DiMP-super) achieves very
competitive performance at a real-time speed. Code and pretrained models are
available at https://github.com/MasterBin-IIAU/AlphaRefine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huchuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyun Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Attention Network: Accelerate Attention by Searching Where to Plug. (arXiv:2011.14058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14058</id>
        <link href="http://arxiv.org/abs/2011.14058"/>
        <updated>2021-07-13T01:59:34.979Z</updated>
        <summary type="html"><![CDATA[Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
https://github.com/gbup-group/EAN-efficient-attention-network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-learning of Pooling Layers for Character Recognition. (arXiv:2103.09528v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09528</id>
        <link href="http://arxiv.org/abs/2103.09528"/>
        <updated>2021-07-13T01:59:34.972Z</updated>
        <summary type="html"><![CDATA[In convolutional neural network-based character recognition, pooling layers
play an important role in dimensionality reduction and deformation
compensation. However, their kernel shapes and pooling operations are
empirically predetermined; typically, a fixed-size square kernel shape and max
pooling operation are used. In this paper, we propose a meta-learning framework
for pooling layers. As part of our framework, a parameterized pooling layer is
proposed in which the kernel shape and pooling operation are trainable using
two parameters, thereby allowing flexible pooling of the input data. We also
propose a meta-learning algorithm for the parameterized pooling layer, which
allows us to acquire a suitable pooling layer across multiple tasks. In the
experiment, we applied the proposed meta-learning framework to character
recognition tasks. The results demonstrate that a pooling layer that is
suitable across character recognition tasks was obtained via meta-learning, and
the obtained pooling layer improved the performance of the model in both
few-shot character recognition and noisy image recognition tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Otsuzuki_T/0/1/0/all/0/1"&gt;Takato Otsuzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Heon Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1"&gt;Hideaki Hayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays. (arXiv:2011.12506v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12506</id>
        <link href="http://arxiv.org/abs/2011.12506"/>
        <updated>2021-07-13T01:59:34.967Z</updated>
        <summary type="html"><![CDATA[Chest X-ray becomes one of the most common medical diagnoses due to its
noninvasiveness. The number of chest X-ray images has skyrocketed, but reading
chest X-rays still have been manually performed by radiologists, which creates
huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology
that can extract a large number of quantitative features from medical images,
demonstrates its potential to facilitate medical imaging diagnosis before the
deep learning era. In this paper, we develop an end-to-end framework,
ChexRadiNet, that can utilize the radiomics features to improve the abnormality
classification performance. Specifically, ChexRadiNet first applies a
light-weight but efficient triplet-attention mechanism to classify the chest
X-rays and highlight the abnormal regions. Then it uses the generated class
activation map to extract radiomic features, which further guides our model to
learn more robust image features. After a number of iterations and with the
help of radiomic features, our framework can converge to more accurate image
regions. We evaluate the ChexRadiNet framework using three public datasets: NIH
ChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the
state-of-the-art on both disease detection (0.843 in AUC) and localization
(0.679 in T(IoU) = 0.1). We will make the code publicly available at
https://github.com/bionlplab/lung_disease_detection_amia2021, with the hope
that this method can facilitate the development of automatic systems with a
higher-level understanding of the radiological world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chongyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1"&gt;Liyan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingquan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1"&gt;Ajay Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Song Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tewfik_A/0/1/0/all/0/1"&gt;Ahmed Tewfik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1"&gt;George Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Ying Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yifan Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints. (arXiv:2012.06365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06365</id>
        <link href="http://arxiv.org/abs/2012.06365"/>
        <updated>2021-07-13T01:59:34.949Z</updated>
        <summary type="html"><![CDATA[Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bugata_P/0/1/0/all/0/1"&gt;Peter Bugata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drotar_P/0/1/0/all/0/1"&gt;Peter Drotar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive spectral image classification using 3D coded convolutional neural network. (arXiv:2009.11948v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.11948</id>
        <link href="http://arxiv.org/abs/2009.11948"/>
        <updated>2021-07-13T01:59:34.943Z</updated>
        <summary type="html"><![CDATA[Hyperspectral image classification (HIC) is an active research topic in
remote sensing. Hyperspectral images typically generate large data cubes posing
big challenges in data acquisition, storage, transmission and processing. To
overcome these limitations, this paper develops a novel deep learning HIC
approach based on compressive measurements of coded-aperture snapshot spectral
imagers (CASSI), without reconstructing the complete hyperspectral data cube. A
new kind of deep learning strategy, namely 3D coded convolutional neural
network (3D-CCNN) is proposed to efficiently solve for the classification
problem, where the hardware-based coded aperture is regarded as a pixel-wise
connected network layer. An end-to-end training method is developed to jointly
optimize the network parameters and the coded apertures with periodic
structures. The accuracy of classification is effectively improved by
exploiting the synergy between the deep learning network and coded apertures.
The superiority of the proposed method is assessed over the state-of-the-art
HIC methods on several hyperspectral datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xianhong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arce_G/0/1/0/all/0/1"&gt;Gonzalo R. Arce&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02265</id>
        <link href="http://arxiv.org/abs/2104.02265"/>
        <updated>2021-07-13T01:59:34.936Z</updated>
        <summary type="html"><![CDATA[Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1"&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yuzhuo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1"&gt;Mengyuan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks. (arXiv:2011.13118v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13118</id>
        <link href="http://arxiv.org/abs/2011.13118"/>
        <updated>2021-07-13T01:59:34.929Z</updated>
        <summary type="html"><![CDATA[We present a novel method for multi-view depth estimation from a single
video, which is a critical task in various applications, such as perception,
reconstruction and robot navigation. Although previous learning-based methods
have demonstrated compelling results, most works estimate depth maps of
individual video frames independently, without taking into consideration the
strong geometric and temporal coherence among the frames. Moreover, current
state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for
cost regularization and therefore require high computational cost, thus
limiting their deployment in real-world applications. Our method achieves
temporally coherent depth estimation results by using a novel Epipolar
Spatio-Temporal (EST) transformer to explicitly associate geometric and
temporal correlation with multiple estimated depth maps. Furthermore, to reduce
the computational cost, inspired by recent Mixture-of-Experts models, we design
a compact hybrid network consisting of a 2D context-aware network and a 3D
matching network which learn 2D context information and 3D disparity cues
separately. Extensive experiments demonstrate that our method achieves higher
accuracy in depth estimation and significant speedup than the SOTA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple and Effective VAE Training with Calibrated Decoders. (arXiv:2006.13202v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13202</id>
        <link href="http://arxiv.org/abs/2006.13202"/>
        <updated>2021-07-13T01:59:34.921Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning to determine the optimal amount of
information retained by the latent variable. We study the impact of calibrated
decoders, which learn the uncertainty of the decoding distribution and can
determine this amount of information automatically, on the VAE performance.
While many methods for learning calibrated decoders have been proposed, many of
the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc
modifications instead. We perform the first comprehensive comparative analysis
of calibrated decoder and provide recommendations for simple and effective VAE
training. Our analysis covers a range of image and video datasets and several
single-image and sequential VAE models. We further propose a simple but novel
modification to the commonly used Gaussian decoder, which computes the
prediction variance analytically. We observe empirically that using heuristic
modifications is not necessary with our method. Project website is at
https://orybkin.github.io/sigma-vae/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1"&gt;Oleh Rybkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1"&gt;Kostas Daniilidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Align Deep Features for Oriented Object Detection. (arXiv:2008.09397v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09397</id>
        <link href="http://arxiv.org/abs/2008.09397"/>
        <updated>2021-07-13T01:59:34.903Z</updated>
        <summary type="html"><![CDATA[The past decade has witnessed significant progress on detecting objects in
aerial images that are often distributed with large scale variations and
arbitrary orientations. However most of existing methods rely on heuristically
defined anchors with different scales, angles and aspect ratios and usually
suffer from severe misalignment between anchor boxes and axis-aligned
convolutional features, which leads to the common inconsistency between the
classification score and localization accuracy. To address this issue, we
propose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules:
a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The
FAM can generate high-quality anchors with an Anchor Refinement Network and
adaptively align the convolutional features according to the anchor boxes with
a novel Alignment Convolution. The ODM first adopts active rotating filters to
encode the orientation information and then produces orientation-sensitive and
orientation-invariant features to alleviate the inconsistency between
classification score and localization accuracy. Besides, we further explore the
approach to detect objects in large-size images, which leads to a better
trade-off between speed and accuracy. Extensive experiments demonstrate that
our method can achieve state-of-the-art performance on two commonly used aerial
objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The
code is available at https://github.com/csuhan/s2anet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiaming Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jian Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1"&gt;Gui-Song Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring intermediate representation for monocular vehicle pose estimation. (arXiv:2011.08464v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08464</id>
        <link href="http://arxiv.org/abs/2011.08464"/>
        <updated>2021-07-13T01:59:34.897Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based framework to recover vehicle pose in SO(3)
from a single RGB image. In contrast to previous works that map from local
appearance to observation angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) to
estimate egocentric vehicle orientation. This approach features a deep model
that transforms perceived intensities to IGRs, which are mapped to a 3D
representation encoding object orientation in the camera coordinate system.
Core problems are what IGRs to use and how to learn them more effectively. We
answer the former question by designing IGRs based on an interpolated cuboid
that derives from primitive 3D annotation readily. The latter question
motivates us to incorporate geometry knowledge with a new loss function based
on a projective invariant. This loss function allows unlabeled data to be used
in the training stage to improve representation learning. Without additional
labels, our system outperforms previous monocular RGB-based methods for joint
vehicle detection and pose estimation on the KITTI benchmark, achieving
performance even comparable to stereo methods. Code and pre-trained models are
available at this https URL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridge Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16188</id>
        <link href="http://arxiv.org/abs/2010.16188"/>
        <updated>2021-07-13T01:59:34.814Z</updated>
        <summary type="html"><![CDATA[Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between foreground and background images. We
find that a carefully designed composition route RSSN that aims to reduce the
discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the dataset will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jizhizi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1"&gt;Stephen J. Maybank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmenting overlapped cell clusters in biomedical images by concave point detection. (arXiv:2008.00997v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00997</id>
        <link href="http://arxiv.org/abs/2008.00997"/>
        <updated>2021-07-13T01:59:34.807Z</updated>
        <summary type="html"><![CDATA[In this paper we propose a method to detect concave points as a first step to
segment overlapped objects on images. Given an image of an object cluster we
compute the curvature on each point of its contour. Then, we select regions
with the highest probability to contain an interest point, that is, regions
with higher curvature. Finally we obtain an interest point from each region and
we classify them between convex and concave. In order to evaluate the quality
of the concave point detection algorithm we constructed a synthetic dataset to
simulate overlapping objects, providing the position of the concave points as a
ground truth. As a case study, the performance of a well-known application is
evaluated, such as the splitting of overlapped cells in images of peripheral
blood smears samples of patients with sickle cell anaemia. We used the proposed
method to detect the concave points in clusters of cells and then we separate
this clusters by ellipse fitting. Experimentally we demonstrate that our
proposal has a better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1"&gt;Miquel Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moya_Alcover_B/0/1/0/all/0/1"&gt;Biel Moy&amp;#xe0;-Alcover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Hidalgo_M/0/1/0/all/0/1"&gt;Manuel Gonz&amp;#xe1;lez-Hidalgo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1"&gt;Antoni Jaume-i-Cap&amp;#xf3;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review and Comparative Study on Probabilistic Object Detection in Autonomous Driving. (arXiv:2011.10671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10671</id>
        <link href="http://arxiv.org/abs/2011.10671"/>
        <updated>2021-07-13T01:59:34.800Z</updated>
        <summary type="html"><![CDATA[Capturing uncertainty in object detection is indispensable for safe
autonomous driving. In recent years, deep learning has become the de-facto
approach for object detection, and many probabilistic object detectors have
been proposed. However, there is no summary on uncertainty estimation in deep
object detection, and existing methods are not only built with different
network architectures and uncertainty estimation methods, but also evaluated on
different datasets with a wide range of evaluation metrics. As a result, a
comparison among methods remains challenging, as does the selection of a model
that best suits a particular application. This paper aims to alleviate this
problem by providing a review and comparative study on existing probabilistic
object detection methods for autonomous driving applications. First, we provide
an overview of generic uncertainty estimation in deep learning, and then
systematically survey existing methods and evaluation metrics for probabilistic
object detection. Next, we present a strict comparative study for probabilistic
object detection based on an image detector and three public autonomous driving
datasets. Finally, we present a discussion of the remaining challenges and
future works. Code has been made available at
https://github.com/asharakeh/pod_compare.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Di Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harakeh_A/0/1/0/all/0/1"&gt;Ali Harakeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1"&gt;Steven Waslander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1"&gt;Klaus Dietmayer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part-Aware Data Augmentation for 3D Object Detection in Point Cloud. (arXiv:2007.13373v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13373</id>
        <link href="http://arxiv.org/abs/2007.13373"/>
        <updated>2021-07-13T01:59:34.782Z</updated>
        <summary type="html"><![CDATA[Data augmentation has greatly contributed to improving the performance in
image recognition tasks, and a lot of related studies have been conducted.
However, data augmentation on 3D point cloud data has not been much explored.
3D label has more sophisticated and rich structural information than the 2D
label, so it enables more diverse and effective data augmentation. In this
paper, we propose part-aware data augmentation (PA-AUG) that can better utilize
rich information of 3D label to enhance the performance of 3D object detectors.
PA-AUG divides objects into partitions and stochastically applies five
augmentation methods to each local region. It is compatible with existing point
cloud data augmentation methods and can be used universally regardless of the
detector's architecture. PA-AUG has improved the performance of
state-of-the-art 3D object detector for all classes of the KITTI dataset and
has the equivalent effect of increasing the train data by about 2.5$\times$. We
also show that PA-AUG not only increases performance for a given dataset but
also is robust to corrupted data. The code is available at
https://github.com/sky77764/pa-aug.pytorch]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaeseok Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yeji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1"&gt;Nojun Kwak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Forward-Propagation for Large-Scale Temporal Video Modelling. (arXiv:2106.08318v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08318</id>
        <link href="http://arxiv.org/abs/2106.08318"/>
        <updated>2021-07-13T01:59:34.776Z</updated>
        <summary type="html"><![CDATA[How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1"&gt;Mateusz Malinowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1"&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swirszcz_G/0/1/0/all/0/1"&gt;Grzegorz Swirszcz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1"&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Convolutions Help Transformers See Better. (arXiv:2106.14881v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14881</id>
        <link href="http://arxiv.org/abs/2106.14881"/>
        <updated>2021-07-13T01:59:34.770Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) models exhibit substandard optimizability. In
particular, they are sensitive to the choice of optimizer (AdamW vs. SGD),
optimizer hyperparameters, and training schedule length. In comparison, modern
convolutional neural networks are far easier to optimize. Why is this the case?
In this work, we conjecture that the issue lies with the patchify stem of ViT
models, which is implemented by a stride-p pxp convolution (p=16 by default)
applied to the input image. This large-kernel plus large-stride convolution
runs counter to typical design choices of convolutional layers in neural
networks. To test whether this atypical design choice causes an issue, we
analyze the optimization behavior of ViT models with their original patchify
stem versus a simple counterpart where we replace the ViT stem by a small
number of stacked stride-two 3x3 convolutions. While the vast majority of
computation in the two ViT designs is identical, we find that this small change
in early visual processing results in markedly different training behavior in
terms of the sensitivity to optimization settings as well as the final model
accuracy. Using a convolutional stem in ViT dramatically increases optimization
stability and also improves peak performance (by ~1-2% top-1 accuracy on
ImageNet-1k), while maintaining flops and runtime. The improvement can be
observed across the wide spectrum of model complexities (from 1G to 36G flops)
and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us
to recommend using a standard, lightweight convolutional stem for ViT models as
a more robust architectural choice compared to the original ViT model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1"&gt;Tete Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mannat Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mintun_E/0/1/0/all/0/1"&gt;Eric Mintun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1"&gt;Trevor Darrell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1"&gt;Piotr Doll&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1"&gt;Ross Girshick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Matching for Domain Generalization. (arXiv:2104.09937v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09937</id>
        <link href="http://arxiv.org/abs/2104.09937"/>
        <updated>2021-07-13T01:59:34.764Z</updated>
        <summary type="html"><![CDATA[Machine learning systems typically assume that the distributions of training
and test sets match closely. However, a critical requirement of such systems in
the real world is their ability to generalize to unseen domains. Here, we
propose an inter-domain gradient matching objective that targets domain
generalization by maximizing the inner product between gradients from different
domains. Since direct optimization of the gradient inner product can be
computationally prohibitive -- requires computation of second-order derivatives
-- we derive a simpler first-order algorithm named Fish that approximates its
optimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds
benchmark, which captures distribution shift across a diverse range of
modalities. Our method produces competitive results on these datasets and
surpasses all baselines on 4 of them. We perform experiments on both the Wilds
benchmark, which captures distribution shift in the real world, as well as
datasets in DomainBed benchmark that focuses more on synthetic-to-real
transfer. Our method produces competitive results on both benchmarks,
demonstrating its effectiveness across a wide range of domain generalization
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seely_J/0/1/0/all/0/1"&gt;Jeffrey Seely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1"&gt;Awni Hannun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usunier_N/0/1/0/all/0/1"&gt;Nicolas Usunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1"&gt;Gabriel Synnaeve&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harmonization with Flow-based Causal Inference. (arXiv:2106.06845v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06845</id>
        <link href="http://arxiv.org/abs/2106.06845"/>
        <updated>2021-07-13T01:59:34.758Z</updated>
        <summary type="html"><![CDATA[Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper leverages a recently proposed
normalizing-flow-based method to perform counterfactual inference upon a
structural causal model (SCM), in order to achieve harmonization of such data.
A causal model is used to model observed effects (brain magnetic resonance
imaging data) that result from known confounders (site, gender and age) and
exogenous noise variables. Our formulation exploits the bijection induced by
flow for the purpose of harmonization. We infer the posterior of exogenous
variables, intervene on observations, and draw samples from the resultant SCM
to obtain counterfactuals. This approach is evaluated extensively on multiple,
large, real-world medical datasets and displayed better cross-domain
generalization compared to state-of-the-art algorithms. Further experiments
that evaluate the quality of confounder-independent data generated by our model
using regression and classification tasks are provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1"&gt;Christos Davatzikos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Neuromorphic Vision Sensors. (arXiv:2006.00422v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00422</id>
        <link href="http://arxiv.org/abs/2006.00422"/>
        <updated>2021-07-13T01:59:34.740Z</updated>
        <summary type="html"><![CDATA[Neuromorphic vision sensors (NVS) have been recently explored to tackle
scenarios where conventional sensors result in high data rate and processing
time. This paper presents a hybrid event-frame approach for detecting and
tracking objects recorded by a stationary neuromorphic sensor, thereby
exploiting the sparse NVS output in a low-power setting for traffic monitoring.
Specifically, we propose a hardware efficient processing pipeline that
optimizes memory and computational needs. The usage of NVS gives the advantage
of rejecting background while it has a unique disadvantage of fragmented
objects. To exploit the background removal, we propose an event-based binary
image creation that signals presence or absence of events in a frame duration.
This reduces memory requirement and enables usage of simple algorithms like
median filtering and connected component labeling for denoise and region
proposal respectively. To overcome the fragmentation issue, a YOLO-inspired
neural network based detector and classifier to merge fragmented region
proposals has been proposed. Finally, an overlap based tracker exploiting
overlap between detections and tracks is proposed with heuristics to overcome
occlusion. The proposed pipeline is evaluated with more than 5 hours of traffic
recording spanning three different locations on two different NVS and
demonstrate similar performance. Compared to existing event-based feature
trackers, our method provides similar accuracy while needing 6 times less
computes. To the best of our knowledge, this is the first time a stationary NVS
based traffic monitoring solution is extensively compared to simultaneously
recorded RGB frame-methods while showing tremendous promise by outperforming
state-of-the-art deep learning solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1"&gt;Deepak Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1"&gt;Vivek Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1"&gt;Tarun Pulluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1"&gt;Andres Ussa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1"&gt;Pradeep Kumar Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1"&gt;Bharath Ramesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1"&gt;Arindam Basu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN. (arXiv:2008.09646v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09646</id>
        <link href="http://arxiv.org/abs/2008.09646"/>
        <updated>2021-07-13T01:59:34.734Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel network for high resolution video
generation. Our network uses ideas from Wasserstein GANs by enforcing
k-Lipschitz constraint on the loss term and Conditional GANs using class labels
for training and testing. We present Generator and Discriminator network
layerwise details along with the combined network architecture, optimization
details and algorithm used in this work. Our network uses a combination of two
loss terms: mean square pixel loss and an adversarial loss. The datasets used
for training and testing our network are UCF101, Golf and Aeroplane Datasets.
Using Inception Score and Fr\'echet Inception Distance as the evaluation
metrics, our network outperforms previous state of the art networks on
unsupervised video generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. (arXiv:2004.00845v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.00845</id>
        <link href="http://arxiv.org/abs/2004.00845"/>
        <updated>2021-07-13T01:59:34.728Z</updated>
        <summary type="html"><![CDATA[We present a new learning-based method for multi-frame depth estimation from
a color video, which is a fundamental problem in scene understanding, robot
navigation or handheld 3D reconstruction. While recent learning-based methods
estimate depth at high accuracy, 3D point clouds exported from their depth maps
often fail to preserve important geometric feature (e.g., corners, edges,
planes) of man-made scenes. Widely-used pixel-wise depth errors do not
specifically penalize inconsistency on these features. These inaccuracies are
particularly severe when subsequent depth reconstructions are accumulated in an
attempt to scan a full environment with man-made objects with this kind of
features. Our depth estimation algorithm therefore introduces a Combined Normal
Map (CNM) constraint, which is designed to better preserve high-curvature
features and global planar regions. In order to further improve the depth
estimation accuracy, we introduce a new occlusion-aware strategy that
aggregates initial depth predictions from multiple adjacent views into one
final depth map and one occlusion probability map for the current reference
view. Our method outperforms the state-of-the-art in terms of depth estimation
accuracy, and preserves essential geometric features of man-made indoor scenes
much better than other algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1"&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14323</id>
        <link href="http://arxiv.org/abs/2106.14323"/>
        <updated>2021-07-13T01:59:34.722Z</updated>
        <summary type="html"><![CDATA[This work was developed aiming to employ Statistical techniques to the field
of Music Emotion Recognition, a well-recognized area within the Signal
Processing world, but hardly explored from the statistical point of view. Here,
we opened several possibilities within the field, applying modern Bayesian
Statistics techniques and developing efficient algorithms, focusing on the
applicability of the results obtained. Although the motivation for this project
was the development of a emotion-based music recommendation system, its main
contribution is a highly adaptable multivariate model that can be useful
interpreting any database where there is an interest in applying regularization
in an efficient manner. Broadly speaking, we will explore what role a sound
theoretical statistical analysis can play in the modeling of an algorithm that
is able to understand a well-known database and what can be gained with this
kind of approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1"&gt;Nathalie Deziderio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1"&gt;Hugo Tremonte de Carvalho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12931</id>
        <link href="http://arxiv.org/abs/2009.12931"/>
        <updated>2021-07-13T01:59:34.716Z</updated>
        <summary type="html"><![CDATA[Climate change has been a common interest and the forefront of crucial
political discussion and decision-making for many years. Shallow clouds play a
significant role in understanding the Earth's climate, but they are challenging
to interpret and represent in a climate model. By classifying these cloud
structures, there is a better possibility of understanding the physical
structures of the clouds, which would improve the climate model generation,
resulting in a better prediction of climate change or forecasting weather
update. Clouds organise in many forms, which makes it challenging to build
traditional rule-based algorithms to separate cloud features. In this paper,
classification of cloud organization patterns was performed using a new
scaled-up version of Convolutional Neural Network (CNN) named as EfficientNet
as the encoder and UNet as decoder where they worked as feature extractor and
reconstructor of fine grained feature map and was used as a classifier, which
will help experts to understand how clouds will shape the future climate. By
using a segmentation model in a classification task, it was shown that with a
good encoder alongside UNet, it is possible to obtain good performance from
this dataset. Dice coefficient has been used for the final evaluation metric,
which gave the score of 66.26\% and 66.02\% for public and private (test set)
leaderboard on Kaggle competition respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1"&gt;Tashin Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1"&gt;Noor Hossain Nuri Sabab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAG: Task-based Accumulated Gradients for Lifelong learning. (arXiv:2105.05155v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05155</id>
        <link href="http://arxiv.org/abs/2105.05155"/>
        <updated>2021-07-13T01:59:34.699Z</updated>
        <summary type="html"><![CDATA[When an agent encounters a continual stream of new tasks in the lifelong
learning setting, it leverages the knowledge it gained from the earlier tasks
to help learn the new tasks better. In such a scenario, identifying an
efficient knowledge representation becomes a challenging problem. Most research
works propose to either store a subset of examples from the past tasks in a
replay buffer, dedicate a separate set of parameters to each task or penalize
excessive updates over parameters by introducing a regularization term. While
existing methods employ the general task-agnostic stochastic gradient descent
update rule, we propose a task-aware optimizer that adapts the learning rate
based on the relatedness among tasks. We utilize the directions taken by the
parameters during the updates by accumulating the gradients specific to each
task. These task-based accumulated gradients act as a knowledge base that is
maintained and updated throughout the stream. We empirically show that our
proposed adaptive learning rate not only accounts for catastrophic forgetting
but also allows positive backward transfer. We also show that our method
performs better than several state-of-the-art methods in lifelong learning on
complex datasets with a large number of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Malviya_P/0/1/0/all/0/1"&gt;Pranshu Malviya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1"&gt;Balaraman Ravindran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.13200</id>
        <link href="http://arxiv.org/abs/1912.13200"/>
        <updated>2021-07-13T01:59:34.692Z</updated>
        <summary type="html"><![CDATA[Compared with cheap addition operation, multiplication operation is of much
higher computation complexity. The widely-used convolutions in deep neural
networks are exactly cross-correlation to measure the similarity between input
feature and convolution filters, which involves massive multiplications between
float values. In this paper, we present adder networks (AdderNets) to trade
these massive multiplications in deep neural networks, especially convolutional
neural networks (CNNs), for much cheaper additions to reduce computation costs.
In AdderNets, we take the $\ell_1$-norm distance between filters and input
feature as the output response. The influence of this new similarity measure on
the optimization of neural network have been thoroughly analyzed. To achieve a
better performance, we develop a special back-propagation approach for
AdderNets by investigating the full-precision gradient. We then propose an
adaptive learning rate strategy to enhance the training procedure of AdderNets
according to the magnitude of each neuron's gradient. As a result, the proposed
AdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50
on the ImageNet dataset without any multiplication in convolution layer. The
codes are publicly available at: https://github.com/huaweinoah/AdderNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Boxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:34.686Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Reward-free Approach to Constrained Reinforcement Learning. (arXiv:2107.05216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05216</id>
        <link href="http://arxiv.org/abs/2107.05216"/>
        <updated>2021-07-13T01:59:34.680Z</updated>
        <summary type="html"><![CDATA[In constrained reinforcement learning (RL), a learning agent seeks to not
only optimize the overall reward but also satisfy the additional safety,
diversity, or budget constraints. Consequently, existing constrained RL
solutions require several new algorithmic ingredients that are notably
different from standard RL. On the other hand, reward-free RL is independently
developed in the unconstrained literature, which learns the transition dynamics
without using the reward information, and thus naturally capable of addressing
RL with multiple objectives under the common dynamics. This paper bridges
reward-free RL and constrained RL. Particularly, we propose a simple
meta-algorithm such that given any reward-free RL oracle, the approachability
and constrained RL problems can be directly solved with negligible overheads in
sample complexity. Utilizing the existing reward-free RL solvers, our framework
provides sharp sample complexity results for constrained RL in the tabular MDP
setting, matching the best existing results up to a factor of horizon
dependence; our framework directly extends to a setting of tabular two-player
Markov games, and gives a new result for constrained RL with linear function
approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Differential Privacy. (arXiv:2107.01559v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01559</id>
        <link href="http://arxiv.org/abs/2107.01559"/>
        <updated>2021-07-13T01:59:34.674Z</updated>
        <summary type="html"><![CDATA[Differential privacy (DP) is a widely-accepted and widely-applied notion of
privacy based on worst-case analysis. Often, DP classifies most mechanisms
without external noise as non-private [Dwork et al., 2014], and external
noises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are
introduced to improve privacy. In many real-world applications, however, adding
external noise is undesirable and sometimes prohibited. For example,
presidential elections often require a deterministic rule to be used [Liu et
al., 2020], and small noises can lead to dramatic decreases in the prediction
accuracy of deep neural networks, especially the underrepresented classes
[Bagdasaryan et al., 2019].

In this paper, we propose a natural extension and relaxation of DP following
the worst average-case idea behind the celebrated smoothed analysis [Spielman
and Teng, 2004]. Our notion, the smoothed DP, can effectively measure the
privacy leakage of mechanisms without external noises under realistic settings.

We prove several strong properties of the smoothed DP, including
composability, robustness to post-processing and etc. We proved that any
discrete mechanism with sampling procedures is more private than what DP
predicts. In comparison, many continuous mechanisms with sampling procedures
are still non-private under smoothed DP. Experimentally, we first verified that
the discrete sampling mechanisms are private in real-world elections. Then, we
apply the smoothed DP notion on quantized gradient descent, which indicates
some neural networks can be private without adding any extra noises. We believe
that these results contribute to the theoretical foundation of realistic
privacy measures beyond worst-case analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Ao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1"&gt;Lirong Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Adversarial Training. (arXiv:2006.14536v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14536</id>
        <link href="http://arxiv.org/abs/2006.14536"/>
        <updated>2021-07-13T01:59:34.656Z</updated>
        <summary type="html"><![CDATA[It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.

Compared to standard adversarial training, SAT improves adversarial
robustness for "free", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
https://github.com/cihangxie/SmoothAdversarialTraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cihang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingxing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Explicit Concerning States for Reinforcement Learning in Visual Dialogue. (arXiv:2107.05250v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05250</id>
        <link href="http://arxiv.org/abs/2107.05250"/>
        <updated>2021-07-13T01:59:34.647Z</updated>
        <summary type="html"><![CDATA[To encourage AI agents to conduct meaningful Visual Dialogue (VD), the use of
Reinforcement Learning has been proven potential. In Reinforcement Learning, it
is crucial to represent states and assign rewards based on the action-caused
transitions of states. However, the state representation in previous Visual
Dialogue works uses the textual information only and its transitions are
implicit. In this paper, we propose Explicit Concerning States (ECS) to
represent what visual contents are concerned at each round and what have been
concerned throughout the Visual Dialogue. ECS is modeled from multimodal
information and is represented explicitly. Based on ECS, we formulate two
intuitive and interpretable rewards to encourage the Visual Dialogue agents to
converse on diverse and informative visual information. Experimental results on
the VisDial v1.0 dataset show our method enables the Visual Dialogue agents to
generate more visual coherent, less repetitive and more visual informative
dialogues compared with previous methods, according to multiple automatic
metrics, human study and qualitative analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zipeng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaojie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Duo Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1"&gt;Chenxu Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting sepsis in multi-site, multi-national intensive care cohorts using deep learning. (arXiv:2107.05230v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05230</id>
        <link href="http://arxiv.org/abs/2107.05230"/>
        <updated>2021-07-13T01:59:34.640Z</updated>
        <summary type="html"><![CDATA[Despite decades of clinical research, sepsis remains a global public health
crisis with high mortality, and morbidity. Currently, when sepsis is detected
and the underlying pathogen is identified, organ damage may have already
progressed to irreversible stages. Effective sepsis management is therefore
highly time-sensitive. By systematically analysing trends in the plethora of
clinical data available in the intensive care unit (ICU), an early prediction
of sepsis could lead to earlier pathogen identification, resistance testing,
and effective antibiotic and supportive treatment, and thereby become a
life-saving measure. Here, we developed and validated a machine learning (ML)
system for the prediction of sepsis in the ICU. Our analysis represents the
largest multi-national, multi-centre in-ICU study for sepsis prediction using
ML to date. Our dataset contains $156,309$ unique ICU admissions, which
represent a refined and harmonised subset of five large ICU databases
originating from three countries. Using the international consensus definition
Sepsis-3, we derived hourly-resolved sepsis label annotations, amounting to
$26,734$ ($17.1\%$) septic stays. We compared our approach, a deep
self-attention model, to several clinical baselines as well as ML baselines and
performed an extensive internal and external validation within and across
databases. On average, our model was able to predict sepsis with an AUROC of
$0.847 \pm 0.050$ (internal out-of sample validation) and $0.761 \pm 0.052$
(external validation). For a harmonised prevalence of $17\%$, at $80\%$ recall
our model detects septic patients with $39\%$ precision 3.7 hours in advance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennet_N/0/1/0/all/0/1"&gt;Nicolas Bennet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plecko_D/0/1/0/all/0/1"&gt;Drago Plecko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinshausen_N/0/1/0/all/0/1"&gt;Nicolai Meinshausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhlmann_P/0/1/0/all/0/1"&gt;Peter B&amp;#xfc;hlmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Super-Resolution System of 4K-Video Based on Deep Learning. (arXiv:2107.05307v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05307</id>
        <link href="http://arxiv.org/abs/2107.05307"/>
        <updated>2021-07-13T01:59:34.634Z</updated>
        <summary type="html"><![CDATA[Video super-resolution (VSR) technology excels in reconstructing low-quality
video, avoiding unpleasant blur effect caused by interpolation-based
algorithms. However, vast computation complexity and memory occupation hampers
the edge of deplorability and the runtime inference in real-life applications,
especially for large-scale VSR task. This paper explores the possibility of
real-time VSR system and designs an efficient and generic VSR network, termed
EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for
temporal coherence. In order to pursue faster VSR processing ability up to 4K
resolution, this paper tries to choose lightweight network structure and
efficient upsampling method to reduce the computation required by EGVSR network
under the guarantee of high visual quality. Besides, we implement the batch
normalization computation fusion, convolutional acceleration algorithm and
other neural network acceleration techniques on the actual hardware platform to
optimize the inference process of EGVSR network. Finally, our EGVSR achieves
the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the
most advanced VSR network at present, we achieve 85.04% reduction of
computation density and 7.92x performance speedups. In terms of visual quality,
the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,
etc.) on the public test dataset Vid4 and surpasses other state-of-the-art
methods in overall performance score. The source code of this project can be
found on https://github.com/Thmen/EGVSR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yanpeng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengcheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Changjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yongming Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Vehicles that Alert Humans to Take-Over Controls: Modeling with Real-World Data. (arXiv:2104.11489v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11489</id>
        <link href="http://arxiv.org/abs/2104.11489"/>
        <updated>2021-07-13T01:59:34.627Z</updated>
        <summary type="html"><![CDATA[With increasing automation in passenger vehicles, the study of safe and
smooth occupant-vehicle interaction and control transitions is key. In this
study, we focus on the development of contextual, semantically meaningful
representations of the driver state, which can then be used to determine the
appropriate timing and conditions for transfer of control between driver and
vehicle. To this end, we conduct a large-scale real-world controlled data study
where participants are instructed to take-over control from an autonomous agent
under different driving conditions while engaged in a variety of distracting
activities. These take-over events are captured using multiple driver-facing
cameras, which when labelled result in a dataset of control transitions and
their corresponding take-over times (TOTs). We then develop and train TOT
models that operate sequentially on mid to high-level features produced by
computer vision algorithms operating on different driver-facing camera views.
The proposed TOT model produces continuous estimates of take-over times without
delay, and shows promising qualitative and quantitative results in complex
real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rangesh_A/0/1/0/all/0/1"&gt;Akshay Rangesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1"&gt;Nachiket Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1"&gt;Ross Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunaratne_P/0/1/0/all/0/1"&gt;Pujitha Gunaratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1"&gt;Mohan M. Trivedi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VM-MODNet: Vehicle Motion aware Moving Object Detection for Autonomous Driving. (arXiv:2104.10985v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10985</id>
        <link href="http://arxiv.org/abs/2104.10985"/>
        <updated>2021-07-13T01:59:34.609Z</updated>
        <summary type="html"><![CDATA[Moving object Detection (MOD) is a critical task in autonomous driving as
moving agents around the ego-vehicle need to be accurately detected for safe
trajectory planning. It also enables appearance agnostic detection of objects
based on motion cues. There are geometric challenges like motion-parallax
ambiguity which makes it a difficult problem. In this work, we aim to leverage
the vehicle motion information and feed it into the model to have an adaptation
mechanism based on ego-motion. The motivation is to enable the model to
implicitly perform ego-motion compensation to improve performance. We convert
the six degrees of freedom vehicle motion into a pixel-wise tensor which can be
fed as input to the CNN model. The proposed model using Vehicle Motion Tensor
(VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline
architecture. We also achieve state-of-the-art results on the public
KITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR
and additional input frames. Our model is also lightweight and runs at 85 fps
on a TitanX GPU. Qualitative results are provided in
https://youtu.be/ezbfjti-kTk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1"&gt;Hazem Rashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1"&gt;Ahmad El Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09046</id>
        <link href="http://arxiv.org/abs/2001.09046"/>
        <updated>2021-07-13T01:59:34.603Z</updated>
        <summary type="html"><![CDATA[We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.

Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.

We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.

We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1"&gt;Bart Smets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1"&gt;Jim Portegies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1"&gt;Erik Bekkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1"&gt;Remco Duits&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction. (arXiv:2006.04047v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04047</id>
        <link href="http://arxiv.org/abs/2006.04047"/>
        <updated>2021-07-13T01:59:34.584Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a dense monocular SLAM system, named
DeepRelativeFusion, that is capable to recover a globally consistent 3D
structure. To this end, we use a visual SLAM algorithm to reliably recover the
camera poses and semi-dense depth maps of the keyframes, and then use relative
depth prediction to densify the semi-dense depth maps and refine the keyframe
pose-graph. To improve the semi-dense depth maps, we propose an adaptive
filtering scheme, which is a structure-preserving weighted average smoothing
filter that takes into account the pixel intensity and depth of the
neighbouring pixels, yielding substantial reconstruction accuracy gain in
densification. To perform densification, we introduce two incremental
improvements upon the energy minimization framework proposed by DeepFusion: (1)
an improved cost function, and (2) the use of single-image relative depth
prediction. After densification, we update the keyframes with two-view
consistent optimized semi-dense and dense depth maps to improve pose-graph
optimization, providing a feedback loop to refine the keyframe poses for
accurate scene reconstruction. Our system outperforms the state-of-the-art
dense SLAM systems quantitatively in dense reconstruction accuracy by a large
margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loo_S/0/1/0/all/0/1"&gt;Shing Yan Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mashohor_S/0/1/0/all/0/1"&gt;Syamsiah Mashohor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sai Hong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Split, embed and merge: An accurate table structure recognizer. (arXiv:2107.05214v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05214</id>
        <link href="http://arxiv.org/abs/2107.05214"/>
        <updated>2021-07-13T01:59:34.578Z</updated>
        <summary type="html"><![CDATA[The task of table structure recognition is to recognize the internal
structure of a table, which is a key step to make machines understand tables.
However, tabular data in unstructured digital documents, e.g. Portable Document
Format (PDF) and images, are difficult to parse into structured
machine-readable format, due to complexity and diversity in their structure and
style, especially for complex tables. In this paper, we introduce Split, Embed
and Merge (SEM), an accurate table structure recognizer. In the first stage, we
use the FCN to predict the potential regions of the table row (column)
separators, so as to obtain the bounding boxes of the basic grids in the table.
In the second stage, we not only extract the visual features corresponding to
each grid through RoIAlign, but also use the off-the-shelf recognizer and the
BERT to extract the semantic features. The fused features of both are used to
characterize each table grid. We find that by adding additional semantic
features to each grid, the ambiguity problem of the table structure from the
visual perspective can be solved to a certain extent and achieve higher
precision. Finally, we process the merging of these basic grids in a
self-regression manner. The correspondent merging results is learned by the
attention maps in attention mechanism. With the proposed method, we can
recognize the structure of tables well, even for complex tables. SEM can
achieve an average F-Measure of $96.9\%$ on the SciTSR dataset which
outperforms other methods by a large margin. Extensive experiments on other
publicly available table structure recognition datasets show that our model
achieves state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianshu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1"&gt;Jun Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Introduction to Deep Morphological Networks. (arXiv:1906.01751v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01751</id>
        <link href="http://arxiv.org/abs/1906.01751"/>
        <updated>2021-07-13T01:59:34.539Z</updated>
        <summary type="html"><![CDATA[The recent impressive results of deep learning-based methods on computer
vision applications brought fresh air to the research and industrial community.
This success is mainly due to the process that allows those methods to learn
data-driven features, generally based upon linear operations. However, in some
scenarios, such operations do not have a good performance because of their
inherited process that blurs edges, losing notions of corners, borders, and
geometry of objects. Overcoming this, non-linear operations, such as
morphological ones, may preserve such properties of the objects, being
preferable and even state-of-the-art in some applications. Encouraged by this,
in this work, we propose a novel network, called Deep Morphological Network
(DeepMorphNet), capable of doing non-linear morphological operations while
performing the feature learning process by optimizing the structuring elements.
The DeepMorphNets can be trained and optimized end-to-end using traditional
existing techniques commonly employed in the training of deep learning
approaches. A systematic evaluation of the proposed algorithm is conducted
using two synthetic and two traditional image classification datasets. Results
show that the proposed DeepMorphNets is a promising technique that can learn
distinct features when compared to the ones learned by current deep learning
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nogueira_K/0/1/0/all/0/1"&gt;Keiller Nogueira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mura_M/0/1/0/all/0/1"&gt;Mauro Dalla Mura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation. (arXiv:1912.04573v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04573</id>
        <link href="http://arxiv.org/abs/1912.04573"/>
        <updated>2021-07-13T01:59:34.522Z</updated>
        <summary type="html"><![CDATA[We introduce a method for simultaneously classifying, segmenting and tracking
object instances in a video sequence. Our method, named MaskProp, adapts the
popular Mask R-CNN to video by adding a mask propagation branch that propagates
frame-level object instance masks from each video frame to all the other frames
in a video clip. This allows our system to predict clip-level instance tracks
with respect to the object instances segmented in the middle frame of the clip.
Clip-level instance tracks generated densely for each frame in the sequence are
finally aggregated to produce video-level object instance segmentation and
classification. Our experiments demonstrate that our clip-level instance
segmentation makes our approach robust to motion blur and object occlusions in
video. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,
outperforming the ICCV 2019 video instance segmentation challenge winner
despite being much simpler and using orders of magnitude less labeled data
(1.3M vs 1B images and 860K vs 14M bounding boxes).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1"&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1"&gt;Lorenzo Torresani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking. (arXiv:2107.05150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05150</id>
        <link href="http://arxiv.org/abs/2107.05150"/>
        <updated>2021-07-13T01:59:34.515Z</updated>
        <summary type="html"><![CDATA[3D multi-object tracking is a crucial component in the perception system of
autonomous driving vehicles. Tracking all dynamic objects around the vehicle is
essential for tasks such as obstacle avoidance and path planning. Autonomous
vehicles are usually equipped with different sensor modalities to improve
accuracy and reliability. While sensor fusion has been widely used in object
detection networks in recent years, most existing multi-object tracking
algorithms either rely on a single input modality, or do not fully exploit the
information provided by multiple sensing modalities. In this work, we propose
an end-to-end network for joint object detection and tracking based on radar
and camera sensor fusion. Our proposed method uses a center-based radar-camera
fusion algorithm for object detection and utilizes a greedy algorithm for
object association. The proposed greedy algorithm uses the depth, velocity and
2D displacement of the detected objects to associate them through time. This
makes our tracking algorithm very robust to occluded and overlapping objects,
as the depth and velocity information can help the network in distinguishing
them. We evaluate our method on the challenging nuScenes dataset, where it
achieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the
benchmark, as well as the baseline LiDAR-based method. Our method is online
with a runtime of 35ms per image, making it very suitable for autonomous
driving applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabati_R/0/1/0/all/0/1"&gt;Ramin Nabati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harris_L/0/1/0/all/0/1"&gt;Landon Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hairong Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned super resolution ultrasound for improved breast lesion characterization. (arXiv:2107.05270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05270</id>
        <link href="http://arxiv.org/abs/2107.05270"/>
        <updated>2021-07-13T01:59:34.505Z</updated>
        <summary type="html"><![CDATA[Breast cancer is the most common malignancy in women. Mammographic findings
such as microcalcifications and masses, as well as morphologic features of
masses in sonographic scans, are the main diagnostic targets for tumor
detection. However, improved specificity of these imaging modalities is
required. A leading alternative target is neoangiogenesis. When pathological,
it contributes to the development of numerous types of tumors, and the
formation of metastases. Hence, demonstrating neoangiogenesis by visualization
of the microvasculature may be of great importance. Super resolution ultrasound
localization microscopy enables imaging of the microvasculature at the
capillary level. Yet, challenges such as long reconstruction time, dependency
on prior knowledge of the system Point Spread Function (PSF), and separability
of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation
of super-resolution US into the clinic. In this work we use a deep neural
network architecture that makes effective use of signal structure to address
these challenges. We present in vivo human results of three different breast
lesions acquired with a clinical US scanner. By leveraging our trained network,
the microvasculature structure is recovered in a short time, without prior PSF
knowledge, and without requiring separability of the UCAs. Each of the
recoveries exhibits a different structure that corresponds with the known
histological structure. This study demonstrates the feasibility of in vivo
human super resolution, based on a clinical scanner, to increase US specificity
for different breast lesions and promotes the use of US in the diagnosis of
breast pathologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_Shira_O/0/1/0/all/0/1"&gt;Or Bar-Shira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubstein_A/0/1/0/all/0/1"&gt;Ahuva Grubstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapson_Y/0/1/0/all/0/1"&gt;Yael Rapson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhami_D/0/1/0/all/0/1"&gt;Dror Suhami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atar_E/0/1/0/all/0/1"&gt;Eli Atar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peri_Hanania_K/0/1/0/all/0/1"&gt;Keren Peri-Hanania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosen_R/0/1/0/all/0/1"&gt;Ronnie Rosen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geographical Knowledge-driven Representation Learning for Remote Sensing Images. (arXiv:2107.05276v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05276</id>
        <link href="http://arxiv.org/abs/2107.05276"/>
        <updated>2021-07-13T01:59:34.497Z</updated>
        <summary type="html"><![CDATA[The proliferation of remote sensing satellites has resulted in a massive
amount of remote sensing images. However, due to human and material resource
constraints, the vast majority of remote sensing images remain unlabeled. As a
result, it cannot be applied to currently available deep learning methods. To
fully utilize the remaining unlabeled images, we propose a Geographical
Knowledge-driven Representation learning method for remote sensing images
(GeoKR), improving network performance and reduce the demand for annotated
data. The global land cover products and geographical location associated with
each remote sensing image are regarded as geographical knowledge to provide
supervision for representation learning and network pre-training. An efficient
pre-training framework is proposed to eliminate the supervision noises caused
by imaging times and resolutions difference between remote sensing images and
geographical knowledge. A large scale pre-training dataset Levir-KR is proposed
to support network pre-training. It contains 1,431,950 remote sensing images
from Gaofen series satellites with various resolutions. Experimental results
demonstrate that our proposed method outperforms ImageNet pre-training and
self-supervised representation learning methods and significantly reduces the
burden of data annotation on downstream tasks such as scene classification,
semantic segmentation, object detection, and cloud / snow detection. It
demonstrates that our proposed method can be used as a novel paradigm for
pre-training neural networks. Codes will be available on
https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Keyan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhenwei Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[eGHWT: The extended Generalized Haar-Walsh Transform. (arXiv:2107.05121v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2107.05121</id>
        <link href="http://arxiv.org/abs/2107.05121"/>
        <updated>2021-07-13T01:59:34.479Z</updated>
        <summary type="html"><![CDATA[Extending computational harmonic analysis tools from the classical setting of
regular lattices to the more general setting of graphs and networks is very
important and much research has been done recently. The Generalized Haar-Walsh
Transform (GHWT) developed by Irion and Saito (2014) is a multiscale transform
for signals on graphs, which is a generalization of the classical Haar and
Walsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh
Transform (eGHWT), which is a generalization of the adapted time-frequency
tilings of Thiele and Villemoes (1996). The eGHWT examines not only the
efficiency of graph-domain partitions but also that of "sequency-domain"
partitions simultaneously. Consequently, the eGHWT and its associated
best-basis selection algorithm for graph signals significantly improve the
performance of the previous GHWT with the similar computational cost, $O(N \log
N)$, where $N$ is the number of nodes of an input graph. While the GHWT
best-basis algorithm seeks the most suitable orthonormal basis for a given task
among more than $(1.5)^N$ possible orthonormal bases in $\mathbb{R}^N$, the
eGHWT best-basis algorithm can find a better one by searching through more than
$0.618\cdot(1.84)^N$ possible orthonormal bases in $\mathbb{R}^N$. This article
describes the details of the eGHWT best-basis algorithm and demonstrates its
superiority using several examples including genuine graph signals as well as
conventional digital images viewed as graph signals. Furthermore, we also show
how the eGHWT can be extended to 2D signals and matrix-form data by viewing
them as a tensor product of graphs generated from their columns and rows and
demonstrate its effectiveness on applications such as image approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1"&gt;Naoki Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yiqun Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05319</id>
        <link href="http://arxiv.org/abs/2107.05319"/>
        <updated>2021-07-13T01:59:34.472Z</updated>
        <summary type="html"><![CDATA[Video activity recognition by deep neural networks is impressive for many
classes. However, it falls short of human performance, especially for
challenging to discriminate activities. Humans differentiate these complex
activities by recognising critical spatio-temporal relations among explicitly
recognised objects and parts, for example, an object entering the aperture of a
container. Deep neural networks can struggle to learn such critical
relationships effectively. Therefore we propose a more human-like approach to
activity recognition, which interprets a video in sequential temporal phases
and extracts specific relationships among objects and hands in those phases.
Random forest classifiers are learnt from these extracted relationships. We
apply the method to a challenging subset of the something-something dataset and
achieve a more robust performance against neural network baselines on
challenging activities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chrol_Cannon_J/0/1/0/all/0/1"&gt;Joseph Chrol-Cannon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1"&gt;Andrew Gilbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazic_R/0/1/0/all/0/1"&gt;Ranko Lazic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhusoodanan_A/0/1/0/all/0/1"&gt;Adithya Madhusoodanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1"&gt;Frank Guerin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis. (arXiv:2107.05097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05097</id>
        <link href="http://arxiv.org/abs/2107.05097"/>
        <updated>2021-07-13T01:59:34.466Z</updated>
        <summary type="html"><![CDATA[Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yanqiao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Trainable Deep Neural Network for Robotic Grasp Detection and Semantic Segmentation from RGB. (arXiv:2107.05287v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05287</id>
        <link href="http://arxiv.org/abs/2107.05287"/>
        <updated>2021-07-13T01:59:34.459Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a novel, end-to-end trainable CNN-based
architecture to deliver high quality results for grasp detection suitable for a
parallel-plate gripper, and semantic segmentation. Utilizing this, we propose a
novel refinement module that takes advantage of previously calculated grasp
detection and semantic segmentation and further increases grasp detection
accuracy. Our proposed network delivers state-of-the-art accuracy on two
popular grasp dataset, namely Cornell and Jacquard. As additional contribution,
we provide a novel dataset extension for the OCID dataset, making it possible
to evaluate grasp detection in highly challenging scenes. Using this dataset,
we show that semantic segmentation can additionally be used to assign grasp
candidates to object classes, which can be used to pick specific objects in the
scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ainetter_S/0/1/0/all/0/1"&gt;Stefan Ainetter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1"&gt;Friedrich Fraundorfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delta Sampling R-BERT for limited data and low-light action recognition. (arXiv:2107.05202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05202</id>
        <link href="http://arxiv.org/abs/2107.05202"/>
        <updated>2021-07-13T01:59:34.453Z</updated>
        <summary type="html"><![CDATA[We present an approach to perform supervised action recognition in the dark.
In this work, we present our results on the ARID dataset. Most previous works
only evaluate performance on large, well illuminated datasets like Kinetics and
HMDB51. We demonstrate that our work is able to achieve a very low error rate
while being trained on a much smaller dataset of dark videos. We also explore a
variety of training and inference strategies including domain transfer
methodologies and also propose a simple but useful frame selection strategy.
Our empirical results demonstrate that we beat previously published baseline
models by 11%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hira_S/0/1/0/all/0/1"&gt;Sanchit Hira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Ritwik Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1"&gt;Abhinav Modi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pakhomov_D/0/1/0/all/0/1"&gt;Daniil Pakhomov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[R3L: Connecting Deep Reinforcement Learning to Recurrent Neural Networks for Image Denoising via Residual Recovery. (arXiv:2107.05318v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05318</id>
        <link href="http://arxiv.org/abs/2107.05318"/>
        <updated>2021-07-13T01:59:34.448Z</updated>
        <summary type="html"><![CDATA[State-of-the-art image denoisers exploit various types of deep neural
networks via deterministic training. Alternatively, very recent works utilize
deep reinforcement learning for restoring images with diverse or unknown
corruptions. Though deep reinforcement learning can generate effective policy
networks for operator selection or architecture search in image restoration,
how it is connected to the classic deterministic training in solving inverse
problems remains unclear. In this work, we propose a novel image denoising
scheme via Residual Recovery using Reinforcement Learning, dubbed R3L. We show
that R3L is equivalent to a deep recurrent neural network that is trained using
a stochastic reward, in contrast to many popular denoisers using supervised
learning with deterministic losses. To benchmark the effectiveness of
reinforcement learning in R3L, we train a recurrent neural network with the
same architecture for residual recovery using the deterministic loss, thus to
analyze how the two different training strategies affect the denoising
performance. With such a unified benchmarking system, we demonstrate that the
proposed R3L has better generalizability and robustness in image denoising when
the estimated noise level varies, comparing to its counterparts using
deterministic training, as well as various state-of-the-art image denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rongkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dauwels_J/0/1/0/all/0/1"&gt;Justin Dauwels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Anticipation with RBF Kernelized Feature Mapping RNN. (arXiv:1911.07806v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07806</id>
        <link href="http://arxiv.org/abs/1911.07806"/>
        <updated>2021-07-13T01:59:34.431Z</updated>
        <summary type="html"><![CDATA[We introduce a novel Recurrent Neural Network-based algorithm for future
video feature generation and action anticipation called feature mapping RNN.
Our novel RNN architecture builds upon three effective principles of machine
learning, namely parameter sharing, Radial Basis Function kernels and
adversarial training. Using only some of the earliest frames of a video, the
feature mapping RNN is able to generate future features with a fraction of the
parameters needed in traditional RNN. By feeding these future features into a
simple multi-layer perceptron facilitated with an RBF kernel layer, we are able
to accurately predict the action in the video. In our experiments, we obtain
18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on
UT-Interaction datasets over prior state-of-the-art for action anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1"&gt;Basura Fernando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1"&gt;Richard Hartley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Deep Feature Propagation for Early Action Recognition. (arXiv:2107.05122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05122</id>
        <link href="http://arxiv.org/abs/2107.05122"/>
        <updated>2021-07-13T01:59:34.425Z</updated>
        <summary type="html"><![CDATA[Early action recognition (action prediction) from limited preliminary
observations plays a critical role for streaming vision systems that demand
real-time inference, as video actions often possess elongated temporal spans
which cause undesired latency. In this study, we address action prediction by
investigating how action patterns evolve over time in a spatial feature space.
There are three key components to our system. First, we work with
intermediate-layer ConvNet features, which allow for abstraction from raw data,
while retaining spatial layout. Second, instead of propagating features per se,
we propagate their residuals across time, which allows for a compact
representation that reduces redundancy. Third, we employ a Kalman filter to
combat error build-up and unify across prediction start times. Extensive
experimental results on multiple benchmarks show that our approach leads to
competitive performance in action prediction. Notably, we investigate the
learned components of our system to shed light on their otherwise opaque
natures in two ways. First, we document that our learned feature propagation
module works as a spatial shifting mechanism under convolution to propagate
current observations into the future. Thus, it captures flow-based image motion
information. Second, the learned Kalman filter adaptively updates prior
estimation to aid the sequence learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;He Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05274</id>
        <link href="http://arxiv.org/abs/2107.05274"/>
        <updated>2021-07-13T01:59:34.417Z</updated>
        <summary type="html"><![CDATA[With the development of deep encoder-decoder architectures and large-scale
annotated medical datasets, great progress has been achieved in the development
of automatic medical image segmentation. Due to the stacking of convolution
layers and the consecutive sampling operations, existing standard models
inevitably encounter the information recession problem of feature
representations, which fails to fully model the global contextual feature
dependencies. To overcome the above challenges, this paper proposes a novel
Transformer based medical image semantic segmentation framework called
TransAttUnet, in which the multi-level guided attention and multi-scale skip
connection are jointly designed to effectively enhance the functionality and
flexibility of traditional U-shaped architecture. Inspired by Transformer, a
novel self-aware attention (SAA) module with both Transformer Self Attention
(TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to
effectively learn the non-local interactions between encoder features. In
particular, we also establish additional multi-scale skip connections between
decoder blocks to aggregate the different semantic-scale upsampling features.
In this way, the representation ability of multi-scale context information is
strengthened to generate discriminative features. Benefitting from these
complementary components, the proposed TransAttUnet can effectively alleviate
the loss of fine details caused by the information recession problem, improving
the diagnostic sensitivity and segmentation quality of medical image analysis.
Extensive experiments on multiple medical image segmentation datasets of
different imaging demonstrate that our method consistently outperforms the
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bingzhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yishu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1"&gt;Guangming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation. (arXiv:2107.05188v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05188</id>
        <link href="http://arxiv.org/abs/2107.05188"/>
        <updated>2021-07-13T01:59:34.411Z</updated>
        <summary type="html"><![CDATA[In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menghan_H/0/1/0/all/0/1"&gt;Hu Menghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guangtao_Z/0/1/0/all/0/1"&gt;Zhai Guangtao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Ping_Z/0/1/0/all/0/1"&gt;Zhang Xiao-Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising. (arXiv:2107.05115v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05115</id>
        <link href="http://arxiv.org/abs/2107.05115"/>
        <updated>2021-07-13T01:59:34.405Z</updated>
        <summary type="html"><![CDATA[In spite of the improvements achieved by the several denoising algorithms
over the years, many of them still fail at preserving the fine details of the
image after denoising. This is as a result of the smooth-out effect they have
on the images. Most neural network-based algorithms have achieved better
quantitative performance than the classical denoising algorithms. However, they
also suffer from qualitative (visual) performance as a result of the smooth-out
effect. In this paper, we propose an algorithm to address this shortcoming. We
propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image
denoising. This algorithm performs collaborative denoising of image patches in
the sparse domain using a set of optimized neural network models. This results
in a fast algorithm that is able to excellently obtain a trade-off between
noise removal and details preservation. Extensive experiments show that the
DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and
qualitatively (visually) better than many of the state-of-the-art denoising
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Alawode_B/0/1/0/all/0/1"&gt;Basit O. Alawode&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masood_M/0/1/0/all/0/1"&gt;Mudassir Masood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ballal_T/0/1/0/all/0/1"&gt;Tarig Ballal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Al_Naffouri_T/0/1/0/all/0/1"&gt;Tareq Al-Naffouri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on Integrated Circuit Text Spotting and Aesthetic Assessment. (arXiv:2107.05279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05279</id>
        <link href="http://arxiv.org/abs/2107.05279"/>
        <updated>2021-07-13T01:59:34.389Z</updated>
        <summary type="html"><![CDATA[With hundreds of thousands of electronic chip components are being
manufactured every day, chip manufacturers have seen an increasing demand in
seeking a more efficient and effective way of inspecting the quality of printed
texts on chip components. The major problem that deters this area of research
is the lacking of realistic text on chips datasets to act as a strong
foundation. Hence, a text on chips dataset, ICText is used as the main target
for the proposed Robust Reading Challenge on Integrated Circuit Text Spotting
and Aesthetic Assessment (RRC-ICText) 2021 to encourage the research on this
problem. Throughout the entire competition, we have received a total of 233
submissions from 10 unique teams/individuals. Details of the competition and
submission results are presented in this report.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1"&gt;Chun Chet Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nazaruddin_A/0/1/0/all/0/1"&gt;Akmalul Khairi Bin Nazaruddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yeong Khang Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yipeng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lixin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Review of Video Predictive Understanding: Early ActionRecognition and Future Action Prediction. (arXiv:2107.05140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05140</id>
        <link href="http://arxiv.org/abs/2107.05140"/>
        <updated>2021-07-13T01:59:34.383Z</updated>
        <summary type="html"><![CDATA[Video predictive understanding encompasses a wide range of efforts that are
concerned with the anticipation of the unobserved future from the current as
well as historical video observations. Action prediction is a major sub-area of
video predictive understanding and is the focus of this review. This sub-area
has two major subdivisions: early action recognition and future action
prediction. Early action recognition is concerned with recognizing an ongoing
action as soon as possible. Future action prediction is concerned with the
anticipation of actions that follow those previously observed. In either case,
the \textbf{\textit{causal}} relationship between the past, current, and
potential future information is the main focus. Various mathematical tools such
as Markov Chains, Gaussian Processes, Auto-Regressive modeling, and Bayesian
recursive filtering are widely adopted jointly with computer vision techniques
for these two tasks. However, these approaches face challenges such as the
curse of dimensionality, poor generalization, and constraints from
domain-specific knowledge. Recently, structures that rely on deep convolutional
neural networks and recurrent neural networks have been extensively proposed
for improving the performance of existing vision tasks, in general, and action
prediction tasks, in particular. However, they have their own shortcomings, \eg
reliance on massive training data and lack of strong theoretical underpinnings.
In this survey, we start by introducing the major sub-areas of the broad area
of video predictive understanding, which recently have received intensive
attention and proven to have practical value. Next, a thorough review of
various early action recognition and future action prediction algorithms are
provided with suitably organized divisions. Finally, we conclude our discussion
with future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;He Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wildes_R/0/1/0/all/0/1"&gt;Richard P. Wildes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EndoUDA: A modality independent segmentation approach for endoscopy imaging. (arXiv:2107.05342v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05342</id>
        <link href="http://arxiv.org/abs/2107.05342"/>
        <updated>2021-07-13T01:59:34.376Z</updated>
        <summary type="html"><![CDATA[Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Celik_N/0/1/0/all/0/1"&gt;Numan Celik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Soumya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Braden_B/0/1/0/all/0/1"&gt;Barbara Braden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rittscher_J/0/1/0/all/0/1"&gt;Jens Rittscher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05176</id>
        <link href="http://arxiv.org/abs/2107.05176"/>
        <updated>2021-07-13T01:59:34.369Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of recognizing compositional
attribute-object concepts within the zero-shot learning (ZSL) framework. We
propose an episode-based cross-attention (EpiCA) network which combines merits
of cross-attention mechanism and episode-based training strategy to recognize
novel compositional concepts. Firstly, EpiCA bases on cross-attention to
correlate concept-visual information and utilizes the gated pooling layer to
build contextualized representations for both images and concepts. The updated
representations are used for a more in-depth multi-modal relevance calculation
for concept recognition. Secondly, a two-phase episode training strategy,
especially the transductive phase, is adopted to utilize unlabeled test
examples to alleviate the low-resource learning problem. Experiments on two
widely-used zero-shot compositional learning (ZSCL) benchmarks have
demonstrated the effectiveness of the model compared with recent approaches on
both conventional and generalized ZSCL settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1"&gt;Parisa Kordjamshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Joyce Y. Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiveView: Dynamic Target-Centered MPI for View Synthesis. (arXiv:2107.05113v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05113</id>
        <link href="http://arxiv.org/abs/2107.05113"/>
        <updated>2021-07-13T01:59:34.362Z</updated>
        <summary type="html"><![CDATA[Existing Multi-Plane Image (MPI) based view-synthesis methods generate an MPI
aligned with the input view using a fixed number of planes in one forward pass.
These methods produce fast, high-quality rendering of novel views, but rely on
slow and computationally expensive MPI generation methods unsuitable for
real-time applications. In addition, most MPI techniques use fixed
depth/disparity planes which cannot be modified once the training is complete,
hence offering very little flexibility at run-time.

We propose LiveView - a novel MPI generation and rendering technique that
produces high-quality view synthesis in real-time. Our method can also offer
the flexibility to select scene-dependent MPI planes (number of planes and
spacing between them) at run-time. LiveView first warps input images to target
view (target-centered) and then learns to generate a target view centered MPI,
one depth plane at a time (dynamically). The method generates high-quality
renderings, while also enabling fast MPI generation and novel view synthesis.
As a result, LiveView enables real-time view synthesis applications where an
MPI needs to be updated frequently based on a video stream of input views. We
demonstrate that LiveView improves the quality of view synthesis while being 70
times faster at run-time compared to state-of-the-art MPI-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Sushobhan Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1"&gt;Zhaoyang Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1"&gt;Nathan Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1"&gt;Lei Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berkovich_A/0/1/0/all/0/1"&gt;Andrew Berkovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cossairt_O/0/1/0/all/0/1"&gt;Oliver Cossairt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks. (arXiv:2107.05085v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05085</id>
        <link href="http://arxiv.org/abs/2107.05085"/>
        <updated>2021-07-13T01:59:34.347Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown that lung cancer screening using annual low-dose
computed tomography (CT) reduces lung cancer mortality by 20% compared to
traditional chest radiography. Therefore, CT lung screening has started to be
used widely all across the world. However, analyzing these images is a serious
burden for radiologists. The number of slices in a CT scan can be up to 600.
Therefore, computer-aided-detection (CAD) systems are very important for faster
and more accurate assessment of the data. In this study, we proposed a
framework that analyzes CT lung screenings using convolutional neural networks
(CNNs) to reduce false positives. We trained our model with different volume
sizes and showed that volume size plays a critical role in the performance of
the system. We also used different fusions in order to show their power and
effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D
convolutional operations applied to 3D data could result in information loss.
The proposed framework has been tested on the dataset provided by the LUNA16
Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1"&gt;Gorkem Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Serinagaoglu_Y/0/1/0/all/0/1"&gt;Yesim Dogrusoz Serinagaoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halici_U/0/1/0/all/0/1"&gt;Ugur Halici&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2107.05255v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05255</id>
        <link href="http://arxiv.org/abs/2107.05255"/>
        <updated>2021-07-13T01:59:34.341Z</updated>
        <summary type="html"><![CDATA[During pregnancy, ultrasound examination in the second trimester can assess
fetal size according to standardized charts. To achieve a reproducible and
accurate measurement, a sonographer needs to identify three standard 2D planes
of the fetal anatomy (head, abdomen, femur) and manually mark the key
anatomical landmarks on the image for accurate biometry and fetal weight
estimation. This can be a time-consuming operator-dependent task, especially
for a trainee sonographer. Computer-assisted techniques can help in automating
the fetal biometry computation process. In this paper, we present a unified
automated framework for estimating all measurements needed for the fetal weight
assessment. The proposed framework semantically segments the key fetal
anatomies using state-of-the-art segmentation models, followed by region
fitting and scale recovery for the biometry estimation. We present an ablation
study of segmentation algorithms to show their robustness through 4-fold
cross-validation on a dataset of 349 ultrasound standard plane images from 42
pregnancies. Moreover, we show that the network with the best segmentation
performance tends to be more accurate for biometry estimation. Furthermore, we
demonstrate that the error between clinically measured and predicted fetal
biometry is lower than the permissible error during routine clinical
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bano_S/0/1/0/all/0/1"&gt;Sophia Bano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dromey_B/0/1/0/all/0/1"&gt;Brian Dromey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_F/0/1/0/all/0/1"&gt;Francisco Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napolitano_R/0/1/0/all/0/1"&gt;Raffaele Napolitano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+David_A/0/1/0/all/0/1"&gt;Anna L. David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peebles_D/0/1/0/all/0/1"&gt;Donald M. Peebles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration. (arXiv:2107.05080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05080</id>
        <link href="http://arxiv.org/abs/2107.05080"/>
        <updated>2021-07-13T01:59:34.335Z</updated>
        <summary type="html"><![CDATA[Relation prediction among entities in images is an important step in scene
graph generation (SGG), which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks, however, require heavy training yet
are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we
stress that such incapability is due to the lack of commonsense reasoning,i.e.,
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap, we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to
integrate commonsense knowledge for SGG, especially for zero-shot relation
prediction. Specifically, we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kan_X/0/1/0/all/0/1"&gt;Xuan Kan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1"&gt;Hejie Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network. (arXiv:2107.05093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05093</id>
        <link href="http://arxiv.org/abs/2107.05093"/>
        <updated>2021-07-13T01:59:34.328Z</updated>
        <summary type="html"><![CDATA[Recently, there has been a panoptic segmentation task combining semantic and
instance segmentation, in which the goal is to classify each pixel with the
corresponding instance ID. In this work, we propose a solution to tackle the
panoptic segmentation task. The overall structure combines the bottom-up method
and the top-down method. Therefore, not only can there be better performance,
but also the execution speed can be maintained. The network mainly pays
attention to the quality of the mask. In the previous work, we can see that the
uneven contour of the object is more likely to appear, resulting in low-quality
prediction. Accordingly, we propose enhancement features and corresponding loss
functions for the silhouette of objects and backgrounds to improve the mask.
Meanwhile, we use the new proposed confidence score to solve the occlusion
problem and make the network tend to use higher quality masks as prediction
results. To verify our research, we used the COCO dataset and CityScapes
dataset to do experiments and obtained competitive results with fast inference
time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo-En Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1"&gt;En-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_P/0/1/0/all/0/1"&gt;Pei-Yung Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Blood Oxygen Estimation From Videos Using Neural Networks. (arXiv:2107.05087v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05087</id>
        <link href="http://arxiv.org/abs/2107.05087"/>
        <updated>2021-07-13T01:59:34.321Z</updated>
        <summary type="html"><![CDATA[Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Joshua Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1"&gt;Xin Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chau-Wai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prb-GAN: A Probabilistic Framework for GAN Modelling. (arXiv:2107.05241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.05241</id>
        <link href="http://arxiv.org/abs/2107.05241"/>
        <updated>2021-07-13T01:59:34.305Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are very popular to generate realistic
images, but they often suffer from the training instability issues and the
phenomenon of mode loss. In order to attain greater diversity in GAN
synthesized data, it is critical to solving the problem of mode loss. Our work
explores probabilistic approaches to GAN modelling that could allow us to
tackle these issues. We present Prb-GANs, a new variation that uses dropout to
create a distribution over the network parameters with the posterior learnt
using variational inference. We describe theoretically and validate
experimentally using simple and complex datasets the benefits of such an
approach. We look into further improvements using the concept of uncertainty
measures. Through a set of further modifications to the loss functions for each
network of the GAN, we are able to get results that show the improvement of GAN
performance. Our methods are extremely simple and require very little
modification to existing GAN architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_B/0/1/0/all/0/1"&gt;Blessen George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K. Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Transformer with Statistical Test for COVID-19 Classification. (arXiv:2107.05334v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05334</id>
        <link href="http://arxiv.org/abs/2107.05334"/>
        <updated>2021-07-13T01:59:34.296Z</updated>
        <summary type="html"><![CDATA[With the massive damage in the world caused by Coronavirus Disease 2019
SARS-CoV-2 (COVID-19), many related research topics have been proposed in the
past two years. The Chest Computed Tomography (CT) scans are the most valuable
materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19
classification of Chest CT scan is based on a single-slice level, implying that
the most critical CT slice should be selected from the original CT scan volume
manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19
of CT scan to tickle this issue. In our 2-D model, we introduce the Deep
Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a
CT scan to overcome the issue mentioned previously. Furthermore, a
Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the
context of the slices fully. The frame-level feature is extracted from each CT
slice based on any backbone network and followed by feeding the features to our
within-slice-Transformer (WST) to discover the context information in the pixel
dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate
the extracted spatial-context features of every CT slice. A simple classifier
is then used to judge whether the Spatio-temporal features are COVID-19 or
non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and
DWCC significantly outperform the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chih-Chung Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guan-Lin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mei-Hsuan Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training deep cross-modality conversion models with a small amount of data and its application to MVCT to kVCT conversion. (arXiv:2107.05238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05238</id>
        <link href="http://arxiv.org/abs/2107.05238"/>
        <updated>2021-07-13T01:59:34.288Z</updated>
        <summary type="html"><![CDATA[Deep-learning-based image processing has emerged as a valuable tool in recent
years owing to its high performance. However, the quality of
deep-learning-based methods relies heavily on the amount of training data, and
the cost of acquiring a large amount of data is often prohibitive in medical
fields. Therefore, we performed CT modality conversion based on deep learning
requiring only a small number of unsupervised images. The proposed method is
based on generative adversarial networks (GANs) with several extensions
tailored for CT images. This method emphasizes the preservation of the
structure in the processed images and reduction in the amount of training data.
This method was applied to realize the conversion of mega-voltage computed
tomography (MVCT) to kilo-voltage computed tomography (kVCT) images. Training
was performed using several datasets acquired from patients with head and neck
cancer. The size of the datasets ranged from 16 slices (for two patients) to
2745 slices (for 137 patients) of MVCT and 2824 slices of kVCT for 98 patients.
The quality of the processed MVCT images was considerably enhanced, and the
structural changes in the images were minimized. With an increase in the size
of training data, the image quality exhibited a satisfactory convergence from a
few hundred slices. In addition to statistical and visual evaluations, these
results were clinically evaluated by medical doctors in terms of the accuracy
of contouring. We developed an MVCT to kVCT conversion model based on deep
learning, which can be trained using a few hundred unpaired images. The
stability of the model against the change in the data size was demonstrated.
This research promotes the reliable use of deep learning in clinical medicine
by partially answering the commonly asked questions: "Is our data enough? How
much data must we prepare?"]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1"&gt;Sho Ozaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaji_S/0/1/0/all/0/1"&gt;Shizuo Kaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nawa_K/0/1/0/all/0/1"&gt;Kanabu Nawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imae_T/0/1/0/all/0/1"&gt;Toshikazu Imae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aoki_A/0/1/0/all/0/1"&gt;Atsushi Aoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamoto_T/0/1/0/all/0/1"&gt;Takahiro Nakamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohta_T/0/1/0/all/0/1"&gt;Takeshi Ohta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nozawa_Y/0/1/0/all/0/1"&gt;Yuki Nozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_H/0/1/0/all/0/1"&gt;Hideomi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haga_A/0/1/0/all/0/1"&gt;Akihiro Haga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1"&gt;Keiichi Nakagawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early warning of pedestrians and cyclists. (arXiv:2107.05186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05186</id>
        <link href="http://arxiv.org/abs/2107.05186"/>
        <updated>2021-07-13T01:59:34.281Z</updated>
        <summary type="html"><![CDATA[State-of-the-art motor vehicles are able to break for pedestrians in an
emergency. We investigate what it would take to issue an early warning to the
driver so he/she has time to react. We have identified that predicting the
intention of a pedestrian reliably by position is a particularly hard
challenge. This paper describes an early pedestrian warning demonstration
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_J/0/1/0/all/0/1"&gt;Joerg Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-learning-based Hyperspectral imaging through a RGB camera. (arXiv:2107.05190v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05190</id>
        <link href="http://arxiv.org/abs/2107.05190"/>
        <updated>2021-07-13T01:59:34.243Z</updated>
        <summary type="html"><![CDATA[Hyperspectral image (HSI) contains both spatial pattern and spectral
information which has been widely used in food safety, remote sensing, and
medical detection. However, the acquisition of hyperspectral images is usually
costly due to the complicated apparatus for the acquisition of optical
spectrum. Recently, it has been reported that HSI can be reconstructed from
single RGB image using convolution neural network (CNN) algorithms. Compared
with the traditional hyperspectral cameras, the method based on CNN algorithms
is simple, portable and low cost. In this study, we focused on the influence of
the RGB camera spectral sensitivity (CSS) on the HSI. A Xenon lamp incorporated
with a monochromator were used as the standard light source to calibrate the
CSS. And the experimental results show that the CSS plays a significant role in
the reconstruction accuracy of an HSI. In addition, we proposed a new HSI
reconstruction network where the dimensional structure of the original
hyperspectral datacube was modified by 3D matrix transpose to improve the
reconstruction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianlang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1"&gt;Jinchao Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Y/0/1/0/all/0/1"&gt;Yanqing Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yanlong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mao_B/0/1/0/all/0/1"&gt;Banging Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pengwei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial and Temporal Networks for Facial Expression Recognition in the Wild Videos. (arXiv:2107.05160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05160</id>
        <link href="http://arxiv.org/abs/2107.05160"/>
        <updated>2021-07-13T01:59:34.224Z</updated>
        <summary type="html"><![CDATA[The paper describes our proposed methodology for the seven basic expression
classification track of Affective Behavior Analysis in-the-wild (ABAW)
Competition 2021. In this task, facial expression recognition (FER) methods aim
to classify the correct expression category from a diverse background, but
there are several challenges. First, to adapt the model to in-the-wild
scenarios, we use the knowledge from pre-trained large-scale face recognition
data. Second, we propose an ensemble model with a convolution neural network
(CNN), a CNN-recurrent neural network (CNN-RNN), and a CNN-Transformer
(CNN-Transformer), to incorporate both spatial and temporal information. Our
ensemble model achieved F1 as 0.4133, accuracy as 0.6216 and final metric as
0.4821 on the validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1"&gt;Shuyi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeoUNet: Towards accurate colon polyp segmentation and neoplasm detection. (arXiv:2107.05023v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.05023</id>
        <link href="http://arxiv.org/abs/2107.05023"/>
        <updated>2021-07-13T01:59:34.204Z</updated>
        <summary type="html"><![CDATA[Automatic polyp segmentation has proven to be immensely helpful for endoscopy
procedures, reducing the missing rate of adenoma detection for endoscopists
while increasing efficiency. However, classifying a polyp as being neoplasm or
not and segmenting it at the pixel level is still a challenging task for
doctors to perform in a limited time. In this work, we propose a fine-grained
formulation for the polyp segmentation problem. Our formulation aims to not
only segment polyp regions, but also identify those at high risk of malignancy
with high accuracy. In addition, we present a UNet-based neural network
architecture called NeoUNet, along with a hybrid loss function to solve this
problem. Experiments show highly competitive results for NeoUNet on our
benchmark dataset compared to existing polyp segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lan_P/0/1/0/all/0/1"&gt;Phan Ngoc Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+An_N/0/1/0/all/0/1"&gt;Nguyen Sy An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hang_D/0/1/0/all/0/1"&gt;Dao Viet Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_D/0/1/0/all/0/1"&gt;Dao Van Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trung_T/0/1/0/all/0/1"&gt;Tran Quang Trung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thuy_N/0/1/0/all/0/1"&gt;Nguyen Thi Thuy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sang_D/0/1/0/all/0/1"&gt;Dinh Viet Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training. (arXiv:2107.05031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05031</id>
        <link href="http://arxiv.org/abs/2107.05031"/>
        <updated>2021-07-13T01:59:34.197Z</updated>
        <summary type="html"><![CDATA[This study delves into semi-supervised object detection (SSOD) to improve
detector performance with additional unlabeled data. State-of-the-art SSOD
performance has been achieved recently by self-training, in which training
supervision consists of ground truths and pseudo-labels. In current studies, we
observe that class imbalance in SSOD severely impedes the effectiveness of
self-training. To address the class imbalance, we propose adaptive
class-rebalancing self-training (ACRST) with a novel memory module called
CropBank. ACRST adaptively rebalances the training data with foreground
instances extracted from the CropBank, thereby alleviating the class imbalance.
Owing to the high complexity of detection tasks, we observe that both
self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.
Therefore, we propose a novel two-stage filtering algorithm to generate
accurate pseudo-labels. Our method achieves satisfactory improvements on
MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our
method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP
improvement compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fangyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1"&gt;Tianxiang Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving. (arXiv:2107.04991v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04991</id>
        <link href="http://arxiv.org/abs/2107.04991"/>
        <updated>2021-07-13T01:59:34.191Z</updated>
        <summary type="html"><![CDATA[Object detection in autonomous cars is commonly based on camera images and
Lidar inputs, which are often used to train prediction models such as deep
artificial neural networks for decision making for object recognition,
adjusting speed, etc. A mistake in such decision making can be damaging; thus,
it is vital to measure the reliability of decisions made by such prediction
models via uncertainty measurement. Uncertainty, in deep learning models, is
often measured for classification problems. However, deep learning models in
autonomous driving are often multi-output regression models. Hence, we propose
a novel method called PURE (Prediction sURface uncErtainty) for measuring
prediction uncertainty of such regression models. We formulate the object
recognition problem as a regression model with more than one outputs for
finding object locations in a 2-dimensional camera view. For evaluation, we
modified three widely-applied object recognition models (i.e., YoLo, SSD300 and
SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET
datasets. Results showed the statistically significant negative correlation
between prediction surface uncertainty and prediction accuracy suggesting that
uncertainty significantly impacts the decisions made by autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Catak_F/0/1/0/all/0/1"&gt;Ferhat Ozgur Catak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_T/0/1/0/all/0/1"&gt;Tao Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1"&gt;Shaukat Ali&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locality Relationship Constrained Multi-view Clustering Framework. (arXiv:2107.05073v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05073</id>
        <link href="http://arxiv.org/abs/2107.05073"/>
        <updated>2021-07-13T01:59:34.175Z</updated>
        <summary type="html"><![CDATA[In most practical applications, it's common to utilize multiple features from
different views to represent one object. Among these works, multi-view
subspace-based clustering has gained extensive attention from many researchers,
which aims to provide clustering solutions to multi-view data. However, most
existing methods fail to take full use of the locality geometric structure and
similarity relationship among samples under the multi-view scenario. To solve
these issues, we propose a novel multi-view learning method with locality
relationship constraint to explore the problem of multi-view clustering, called
Locality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).
LRC-MCF aims to explore the diversity, geometric, consensus and complementary
information among different views, by capturing the locality relationship
information and the common similarity relationships among multiple views.
Moreover, LRC-MCF takes sufficient consideration to weights of different views
in finding the common-view locality structure and straightforwardly produce the
final clusters. To effectually reduce the redundancy of the learned
representations, the low-rank constraint on the common similarity matrix is
considered additionally. To solve the minimization problem of LRC-MCF, an
Alternating Direction Minimization (ADM) method is provided to iteratively
calculate all variables LRC-MCF. Extensive experimental results on seven
benchmark multi-view datasets validate the effectiveness of the LRC-MCF method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangzhu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenzhe Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cloud-Edge-Terminal Collaborative System for Temperature Measurement in COVID-19 Prevention. (arXiv:2107.05078v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05078</id>
        <link href="http://arxiv.org/abs/2107.05078"/>
        <updated>2021-07-13T01:59:34.167Z</updated>
        <summary type="html"><![CDATA[To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary
temperature measurement and mask detection in public areas are conducted.
However, the existing temperature measurement methods face the problems of
safety and deployment. In this paper, to realize safe and accurate temperature
measurement even when a person's face is partially obscured, we propose a
cloud-edge-terminal collaborative system with a lightweight infrared
temperature measurement model. A binocular camera with an RGB lens and a
thermal lens is utilized to simultaneously capture image pairs. Then, a mobile
detection model based on a multi-task cascaded convolutional network (MTCNN) is
proposed to realize face alignment and mask detection on the RGB images. For
accurate temperature measurement, we transform the facial landmarks on the RGB
images to the thermal images by an affine transformation and select a more
accurate temperature measurement area on the forehead. The collected
information is uploaded to the cloud in real time for COVID-19 prevention.
Experiments show that the detection model is only 6.1M and the average
detection speed is 257ms. At a distance of 1m, the error of indoor temperature
measurement is about 3%. That is, the proposed system can realize real-time
temperature measurement in public areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zheyi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wen Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiyong Bu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blending Pruning Criteria for Convolutional Neural Networks. (arXiv:2107.05033v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05033</id>
        <link href="http://arxiv.org/abs/2107.05033"/>
        <updated>2021-07-13T01:59:34.161Z</updated>
        <summary type="html"><![CDATA[The advancement of convolutional neural networks (CNNs) on various vision
applications has attracted lots of attention. Yet the majority of CNNs are
unable to satisfy the strict requirement for real-world deployment. To overcome
this, the recent popular network pruning is an effective method to reduce the
redundancy of the models. However, the ranking of filters according to their
"importance" on different pruning criteria may be inconsistent. One filter
could be important according to a certain criterion, while it is unnecessary
according to another one, which indicates that each criterion is only a partial
view of the comprehensive "importance". From this motivation, we propose a
novel framework to integrate the existing filter pruning criteria by exploring
the criteria diversity. The proposed framework contains two stages: Criteria
Clustering and Filters Importance Calibration. First, we condense the pruning
criteria via layerwise clustering based on the rank of "importance" score.
Second, within each cluster, we propose a calibration factor to adjust their
significance for each selected blending candidates and search for the optimal
blending criterion via Evolutionary Algorithm. Quantitative results on the
CIFAR-100 and ImageNet benchmarks show that our framework outperforms the
state-of-the-art baselines, regrading to the compact model performance after
pruning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Mingfu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Senwei Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haizhao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-07-13T01:59:34.154Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Projector-Camera System Using Hybrid Pixels with Projection and Capturing Capabilities. (arXiv:2107.05043v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05043</id>
        <link href="http://arxiv.org/abs/2107.05043"/>
        <updated>2021-07-13T01:59:34.147Z</updated>
        <summary type="html"><![CDATA[We propose a novel projector-camera system (ProCams) in which each pixel has
both projection and capturing capabilities. Our proposed ProCams solves the
difficulty of obtaining precise pixel correspondence between the projector and
the camera. We implemented a proof-of-concept ProCams prototype and
demonstrated its applicability to a dynamic projection mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Kenta Yamamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1"&gt;Daisuke Iwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1"&gt;Kosuke Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique. (arXiv:2107.04813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04813</id>
        <link href="http://arxiv.org/abs/2107.04813"/>
        <updated>2021-07-13T01:59:34.141Z</updated>
        <summary type="html"><![CDATA[Plant leaf diseases pose a significant danger to food security and they cause
depletion in quality and volume of production. Therefore accurate and timely
detection of leaf disease is very important to check the loss of the crops and
meet the growing food demand of the people. Conventional techniques depend on
lab investigation and human skills which are generally costly and inaccessible.
Recently, Deep Neural Networks have been exceptionally fruitful in image
classification. In this research paper, plant leaf disease detection employing
transfer learning is explored in the JPEG compressed domain. Here, the JPEG
compressed stream consisting of DCT coefficients is, directly fed into the
Neural Network to improve the efficiency of classification. The experimental
results on JPEG compressed leaf dataset demonstrate the efficacy of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Atul Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1"&gt;Bulla Rajesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSL-YOLO: A New Lightweight Object Detection System for Edge Computing. (arXiv:2107.04829v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04829</id>
        <link href="http://arxiv.org/abs/2107.04829"/>
        <updated>2021-07-13T01:59:34.135Z</updated>
        <summary type="html"><![CDATA[The development of lightweight object detectors is essential due to the
limited computation resources. To reduce the computation cost, how to generate
redundant features plays a significant role. This paper proposes a new
lightweight Convolution method Cross-Stage Lightweight (CSL) Module, to
generate redundant features from cheap operations. In the intermediate
expansion stage, we replaced Pointwise Convolution with Depthwise Convolution
to produce candidate features. The proposed CSL-Module can reduce the
computation cost significantly. Experiments conducted at MS-COCO show that the
proposed CSL-Module can approximate the fitting ability of Convolution-3x3.
Finally, we use the module to construct a lightweight detector CSL-YOLO,
achieving better detection performance with only 43% FLOPs and 52% parameters
than Tiny-YOLOv4.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu-Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chun-Chieh Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1"&gt;Jun-Wei Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1"&gt;Kuo-Chin Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Self-Supervised Learning for Medical Image Segmentation Based on Multi-Domain Data Aggregation. (arXiv:2107.04886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04886</id>
        <link href="http://arxiv.org/abs/2107.04886"/>
        <updated>2021-07-13T01:59:34.128Z</updated>
        <summary type="html"><![CDATA[A large labeled dataset is a key to the success of supervised deep learning,
but for medical image segmentation, it is highly challenging to obtain
sufficient annotated images for model training. In many scenarios, unannotated
images are abundant and easy to acquire. Self-supervised learning (SSL) has
shown great potentials in exploiting raw data information and representation
learning. In this paper, we propose Hierarchical Self-Supervised Learning
(HSSL), a new self-supervised framework that boosts medical image segmentation
by making good use of unannotated data. Unlike the current literature on
task-specific self-supervised pretraining followed by supervised fine-tuning,
we utilize SSL to learn task-agnostic knowledge from heterogeneous data for
various medical image segmentation tasks. Specifically, we first aggregate a
dataset from several medical challenges, then pre-train the network in a
self-supervised manner, and finally fine-tune on labeled data. We develop a new
loss function by combining contrastive loss and classification loss and
pretrain an encoder-decoder architecture for segmentation tasks. Our extensive
experiments show that multi-domain joint pre-training benefits downstream
segmentation tasks and outperforms single-domain pre-training significantly.
Compared to learning from scratch, our new method yields better performance on
various tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotated
data). With limited amounts of training data, our method can substantially
bridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% of
annotated data).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhuo Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chaoli Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danny Z. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Correlation Information for Domain Adaptation in Action Recognition. (arXiv:2107.04932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04932</id>
        <link href="http://arxiv.org/abs/2107.04932"/>
        <updated>2021-07-13T01:59:34.097Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) approaches address domain shift and enable networks to
be applied to different scenarios. Although various image DA approaches have
been proposed in recent years, there is limited research towards video DA. This
is partly due to the complexity in adapting the different modalities of
features in videos, which includes the correlation features extracted as
long-term dependencies of pixels across spatiotemporal dimensions. The
correlation features are highly associated with action classes and proven their
effectiveness in accurate video feature extraction through the supervised
action recognition task. Yet correlation features of the same action would
differ across domains due to domain shift. Therefore we propose a novel
Adversarial Correlation Adaptation Network (ACAN) to align action videos by
aligning pixel correlations. ACAN aims to minimize the distribution of
correlation information, termed as Pixel Correlation Discrepancy (PCD).
Additionally, video DA research is also limited by the lack of cross-domain
video datasets with larger domain shifts. We, therefore, introduce a novel
HMDB-ARID dataset with a larger domain shift caused by a larger statistical
difference between domains. This dataset is built in an effort to leverage
current datasets for dark video classification. Empirical results demonstrate
the state-of-the-art performance of our proposed ACAN for both existing and the
new video DA datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianxiong Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1"&gt;Simon See&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation. (arXiv:2107.04934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04934</id>
        <link href="http://arxiv.org/abs/2107.04934"/>
        <updated>2021-07-13T01:59:34.089Z</updated>
        <summary type="html"><![CDATA[The segmentation of medical images is a fundamental step in automated
clinical decision support systems. Existing medical image segmentation methods
based on supervised deep learning, however, remain problematic because of their
reliance on large amounts of labelled training data. Although medical imaging
data repositories continue to expand, there has not been a commensurate
increase in the amount of annotated data. Hence, we propose a new spatial
guided self-supervised clustering network (SGSCN) for medical image
segmentation, where we introduce multiple loss functions designed to aid in
grouping image pixels that are spatially connected and have similar feature
representations. It iteratively learns feature representations and clustering
assignment of each pixel in an end-to-end fashion from a single image. We also
propose a context-based consistency loss that better delineates the shape and
boundaries of image regions. It enforces all the pixels belonging to a cluster
to be spatially close to the cluster centre. We evaluated our method on 2
public medical image datasets and compared it to existing conventional and
self-supervised clustering methods. Experimental results show that our method
was most accurate for medical image segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_E/0/1/0/all/0/1"&gt;Euijoon Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1"&gt;Dagan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05025</id>
        <link href="http://arxiv.org/abs/2107.05025"/>
        <updated>2021-07-13T01:59:34.078Z</updated>
        <summary type="html"><![CDATA[Face image retrieval, which searches for images of the same identity from the
query input face image, is drawing more attention as the size of the image
database increases rapidly. In order to conduct fast and accurate retrieval, a
compact hash code-based methods have been proposed, and recently, deep face
image hashing methods with supervised classification training have shown
outstanding performance. However, classification-based scheme has a
disadvantage in that it cannot reveal complex similarities between face images
into the hash code learning. In this paper, we attempt to improve the face
image retrieval quality by proposing a Similarity Guided Hashing (SGH) method,
which gently considers self and pairwise-similarity simultaneously. SGH employs
various data augmentations designed to explore elaborate similarities between
face images, solving both intra and inter identity-wise difficulties. Extensive
experimental results on the protocols with existing benchmarks and an
additionally proposed large scale higher resolution face image dataset
demonstrate that our SGH delivers state-of-the-art retrieval performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young Kyun Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1"&gt;Nam Ik Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEV-MODNet: Monocular Camera based Bird's Eye View Moving Object Detection for Autonomous Driving. (arXiv:2107.04937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04937</id>
        <link href="http://arxiv.org/abs/2107.04937"/>
        <updated>2021-07-13T01:59:34.067Z</updated>
        <summary type="html"><![CDATA[Detection of moving objects is a very important task in autonomous driving
systems. After the perception phase, motion planning is typically performed in
Bird's Eye View (BEV) space. This would require projection of objects detected
on the image plane to top view BEV plane. Such a projection is prone to errors
due to lack of depth information and noisy mapping in far away areas. CNNs can
leverage the global context in the scene to project better. In this work, we
explore end-to-end Moving Object Detection (MOD) on the BEV map directly using
monocular images as input. To the best of our knowledge, such a dataset does
not exist and we create an extended KITTI-raw dataset consisting of 12.9k
images with annotations of moving object masks in BEV space for five classes.
The dataset is intended to be used for class agnostic motion cue based object
detection and classes are provided as meta-data for better tuning. We design
and implement a two-stream RGB and optical flow fusion architecture which
outputs motion segmentation directly in BEV space. We compare it with inverse
perspective mapping of state-of-the-art motion segmentation predictions on the
image plane. We observe a significant improvement of 13% in mIoU using the
simple baseline implementation. This demonstrates the ability to directly learn
motion segmentation output in BEV space. Qualitative results of our baseline
and the dataset annotations can be found in
https://sites.google.com/view/bev-modnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1"&gt;Hazem Rashed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essam_M/0/1/0/all/0/1"&gt;Mariam Essam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1"&gt;Maha Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sallab_A/0/1/0/all/0/1"&gt;Ahmad El Sallab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1"&gt;Senthil Yogamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision. (arXiv:2107.04952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04952</id>
        <link href="http://arxiv.org/abs/2107.04952"/>
        <updated>2021-07-13T01:59:34.057Z</updated>
        <summary type="html"><![CDATA[A common problem with most zero and few-shot learning approaches is they
suffer from bias towards seen classes resulting in sub-optimal performance.
Existing efforts aim to utilize unlabeled images from unseen classes (i.e
transductive zero-shot) during training to enable generalization. However, this
limits their use in practical scenarios where data from target unseen classes
is unavailable or infeasible to collect. In this work, we present a practical
setting of inductive zero and few-shot learning, where unlabeled images from
other out-of-data classes, that do not belong to seen or unseen categories, can
be used to improve generalization in any-shot learning. We leverage a
formulation based on product-of-experts and introduce a new AUD module that
enables us to use unlabeled samples from out-of-data classes which are usually
easily available and practically entail no annotation cost. In addition, we
also demonstrate the applicability of our model to address a more practical and
challenging, Generalized Zero-shot under a limited supervision setting, where
even base seen classes do not have sufficient annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1"&gt;Gaurav Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandok_S/0/1/0/all/0/1"&gt;Shivam Chandok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates. (arXiv:2107.04974v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04974</id>
        <link href="http://arxiv.org/abs/2107.04974"/>
        <updated>2021-07-13T01:59:34.020Z</updated>
        <summary type="html"><![CDATA[It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1"&gt;Rose McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1"&gt;Boris Kovalerchuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature-based Event Stereo Visual Odometry. (arXiv:2107.04921v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04921</id>
        <link href="http://arxiv.org/abs/2107.04921"/>
        <updated>2021-07-13T01:59:34.006Z</updated>
        <summary type="html"><![CDATA[Event-based cameras are biologically inspired sensors that output events,
i.e., asynchronous pixel-wise brightness changes in the scene. Their high
dynamic range and temporal resolution of a microsecond makes them more reliable
than standard cameras in environments of challenging illumination and in
high-speed scenarios, thus developing odometry algorithms based solely on event
cameras offers exciting new possibilities for autonomous systems and robots. In
this paper, we propose a novel stereo visual odometry method for event cameras
based on feature detection and matching with careful feature management, while
pose estimation is done by reprojection error minimization. We evaluate the
performance of the proposed method on two publicly available datasets: MVSEC
sequences captured by an indoor flying drone and DSEC outdoor driving
sequences. MVSEC offers accurate ground truth from motion capture, while for
DSEC, which does not offer ground truth, in order to obtain a reference
trajectory on the standard camera frames we used our SOFT visual odometry, one
of the highest ranking algorithms on the KITTI scoreboards. We compared our
method to the ESVO method, which is the first and still the only stereo event
odometry method, showing on par performance on the MVSEC sequences, while on
the DSEC dataset ESVO, unlike our method, was unable to handle outdoor driving
scenario with default parameters. Furthermore, two important advantages of our
method over ESVO are that it adapts tracking frequency to the asynchronous
event rate and does not require initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hadviger_A/0/1/0/all/0/1"&gt;Antea Hadviger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1"&gt;Igor Cvi&amp;#x161;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1"&gt;Ivan Markovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vrazic_S/0/1/0/all/0/1"&gt;Sacha Vra&amp;#x17e;i&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1"&gt;Ivan Petrovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04943</id>
        <link href="http://arxiv.org/abs/2107.04943"/>
        <updated>2021-07-13T01:59:33.999Z</updated>
        <summary type="html"><![CDATA[Compressed sensing (CS) is an efficient method to reconstruct MR image from
small sampled data in $k$-space and accelerate the acquisition of MRI. In this
work, we propose a novel deep geometric distillation network which combines the
merits of model-based and deep learning-based CS-MRI methods, it can be
theoretically guaranteed to improve geometric texture details of a linear
reconstruction. Firstly, we unfold the model-based CS-MRI optimization problem
into two sub-problems that consist of image linear approximation and image
geometric compensation. Secondly, geometric compensation sub-problem for
distilling lost texture details in approximation stage can be expanded by
Taylor expansion to design a geometric distillation module fusing features of
different geometric characteristic domains. Additionally, we use a learnable
version with adaptive initialization of the step-length parameter, which allows
model more flexibility that can lead to convergent smoothly. Numerical
experiments verify its superiority over other state-of-the-art CS-MRI
reconstruction approaches. The source code will be available at
\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaohong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianping Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images. (arXiv:2107.04847v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04847</id>
        <link href="http://arxiv.org/abs/2107.04847"/>
        <updated>2021-07-13T01:59:33.993Z</updated>
        <summary type="html"><![CDATA[In radiotherapy planning, manual contouring is labor-intensive and
time-consuming. Accurate and robust automated segmentation models improve the
efficiency and treatment outcome. We aim to develop a novel hybrid deep
learning approach, combining convolutional neural networks (CNNs) and the
self-attention mechanism, for rapid and accurate multi-organ segmentation on
head and neck computed tomography (CT) images. Head and neck CT images with
manual contours of 115 patients were retrospectively collected and used. We set
the training/validation/testing ratio to 81/9/25 and used the 10-fold
cross-validation strategy to select the best model parameters. The proposed
hybrid model segmented ten organs-at-risk (OARs) altogether for each case. The
performance of the model was evaluated by three metrics, i.e., the Dice
Similarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface
distance (MSD). We also tested the performance of the model on the Head and
Neck 2015 challenge dataset and compared it against several state-of-the-art
automated segmentation algorithms. The proposed method generated contours that
closely resemble the ground truth for ten OARs. Our results of the new Weaving
Attention U-net demonstrate superior or similar performance on the segmentation
of head and neck CT images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuangzhuang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1"&gt;Hiram Gay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weixiong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baozhou Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Industry and Academic Research in Computer Vision. (arXiv:2107.04902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04902</id>
        <link href="http://arxiv.org/abs/2107.04902"/>
        <updated>2021-07-13T01:59:33.987Z</updated>
        <summary type="html"><![CDATA[This work aims to study the dynamic between research in the industry and
academia in computer vision. The results are demonstrated on a set of top-5
vision conferences that are representative of the field. Since data for such
analysis was not readily available, significant effort was spent on gathering
and processing meta-data from the original publications. First, this study
quantifies the share of industry-sponsored research. Specifically, it shows
that the proportion of papers published by industry-affiliated researchers is
increasing and that more academics join companies or collaborate with them.
Next, the possible impact of industry presence is further explored, namely in
the distribution of research topics and citation patterns. The results indicate
that the distribution of the research topics is similar in industry and
academic papers. However, there is a strong preference towards citing industry
papers. Finally, possible reasons for citation bias, such as code availability
and influence, are investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kotseruba_I/0/1/0/all/0/1"&gt;Iuliia Kotseruba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SynPick: A Dataset for Dynamic Bin Picking Scene Understanding. (arXiv:2107.04852v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04852</id>
        <link href="http://arxiv.org/abs/2107.04852"/>
        <updated>2021-07-13T01:59:33.969Z</updated>
        <summary type="html"><![CDATA[We present SynPick, a synthetic dataset for dynamic scene understanding in
bin-picking scenarios. In contrast to existing datasets, our dataset is both
situated in a realistic industrial application domain -- inspired by the
well-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with
authentic picking actions as chosen by our picking heuristic developed for the
ARC 2017. The dataset is compatible with the popular BOP dataset format. We
describe the dataset generation process in detail, including object arrangement
generation and manipulation simulation using the NVIDIA PhysX physics engine.
To cover a large action space, we perform untargeted and targeted picking
actions, as well as random moving actions. To establish a baseline for object
perception, a state-of-the-art pose estimation approach is evaluated on the
dataset. We demonstrate the usefulness of tracking poses during manipulation
instead of single-shot estimation even with a naive filtering approach. The
generator source code and dataset are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1"&gt;Arul Selvam Periyasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1"&gt;Max Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1"&gt;Sven Behnke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis. (arXiv:2107.04882v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04882</id>
        <link href="http://arxiv.org/abs/2107.04882"/>
        <updated>2021-07-13T01:59:33.962Z</updated>
        <summary type="html"><![CDATA[Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uwimana1_A/0/1/0/all/0/1"&gt;Anisie Uwimana1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1"&gt;Ransalu Senanayake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks. (arXiv:2107.04648v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04648</id>
        <link href="http://arxiv.org/abs/2107.04648"/>
        <updated>2021-07-13T01:59:33.955Z</updated>
        <summary type="html"><![CDATA[Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention
due to their outstanding ability to be used in different sectors and serve in
difficult and dangerous areas. Moreover, the advancements in computer vision
and artificial intelligence have increased the use of UAVs in various
applications and solutions, such as forest fires detection and borders
monitoring. However, using deep neural networks (DNNs) with UAVs introduces
several challenges of processing deeper networks and complex models, which
restricts their on-board computation. In this work, we present a strategy
aiming at distributing inference requests to a swarm of resource-constrained
UAVs that classifies captured images on-board and finds the minimum
decision-making latency. We formulate the model as an optimization problem that
minimizes the latency between acquiring images and making the final decisions.
The formulated optimization solution is an NP-hard problem. Hence it is not
adequate for online resource allocation. Therefore, we introduce an online
heuristic solution, namely DistInference, to find the layers placement strategy
that gives the best latency among the available UAVs. The proposed approach is
general enough to be used for different low decision-latency applications as
well as for all CNN types organized into the pipeline of layers (e.g., VGG) or
based on residual blocks (e.g., ResNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhuheir_M/0/1/0/all/0/1"&gt;Marwan Dhuheir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabeeh_S/0/1/0/all/0/1"&gt;Sinan Sabeeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network. (arXiv:2107.04941v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04941</id>
        <link href="http://arxiv.org/abs/2107.04941"/>
        <updated>2021-07-13T01:59:33.948Z</updated>
        <summary type="html"><![CDATA[Partial Domain Adaptation (PDA) is a practical and general domain adaptation
scenario, which relaxes the fully shared label space assumption such that the
source label space subsumes the target one. The key challenge of PDA is the
issue of negative transfer caused by source-only classes. For videos, such
negative transfer could be triggered by both spatial and temporal features,
which leads to a more challenging Partial Video Domain Adaptation (PVDA)
problem. In this paper, we propose a novel Partial Adversarial Temporal
Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial
and temporal features for filtering source-only classes. Besides, PATAN
constructs effective overall temporal features by attending to local temporal
features that contribute more toward the class filtration process. We further
introduce new benchmarks to facilitate research on PVDA problems, covering a
wide range of PVDA scenarios. Empirical results demonstrate the
state-of-the-art performance of our proposed PATAN across the multiple PVDA
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuecong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1"&gt;Haozhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1"&gt;Kezhi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID Detection in Chest CTs: Improving the Baseline on COV19-CT-DB. (arXiv:2107.04808v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04808</id>
        <link href="http://arxiv.org/abs/2107.04808"/>
        <updated>2021-07-13T01:59:33.940Z</updated>
        <summary type="html"><![CDATA[The paper presents a comparative analysis of three distinct approaches based
on deep learning for COVID-19 detection in chest CTs. The first approach is a
volumetric one, involving 3D convolutions, while the other two approaches
perform at first slice-wise classification and then aggregate the results at
the volume level. The experiments are carried on the COV19-CT-DB dataset, with
the aim of addressing the challenge raised by the MIA-COV19D Competition within
ICCV 2021. Our best results on the validation subset reach a macro-F1 score of
0.92, which improves considerably the baseline score of 0.70 set by the
organizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miron_R/0/1/0/all/0/1"&gt;Radu Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moisii_C/0/1/0/all/0/1"&gt;Cosmin Moisii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dinu_S/0/1/0/all/0/1"&gt;Sergiu Dinu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Breaban_M/0/1/0/all/0/1"&gt;Mihaela Breaban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Fiber Clustering: Anatomically Informed Unsupervised Deep Learning for Fast and Effective White Matter Parcellation. (arXiv:2107.04938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04938</id>
        <link href="http://arxiv.org/abs/2107.04938"/>
        <updated>2021-07-13T01:59:33.914Z</updated>
        <summary type="html"><![CDATA[White matter fiber clustering (WMFC) enables parcellation of white matter
tractography for applications such as disease classification and anatomical
tract segmentation. However, the lack of ground truth and the ambiguity of
fiber data (the points along a fiber can equivalently be represented in forward
or reverse order) pose challenges to this task. We propose a novel WMFC
framework based on unsupervised deep learning. We solve the unsupervised
clustering problem as a self-supervised learning task. Specifically, we use a
convolutional neural network to learn embeddings of input fibers, using
pairwise fiber distances as pseudo annotations. This enables WMFC that is
insensitive to fiber point ordering. In addition, anatomical coherence of fiber
clusters is improved by incorporating brain anatomical segmentation data. The
proposed framework enables outlier removal in a natural way by rejecting fibers
with low cluster assignment probability. We train and evaluate our method using
200 datasets from the Human Connectome Project. Results demonstrate superior
performance and efficiency of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1"&gt;Nikos Makris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1"&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1"&gt;Lauren J. O&amp;#x27;Donnell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TeliNet, a simple and shallow Convolution Neural Network (CNN) to Classify CT Scans of COVID-19 patients. (arXiv:2107.04930v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04930</id>
        <link href="http://arxiv.org/abs/2107.04930"/>
        <updated>2021-07-13T01:59:33.905Z</updated>
        <summary type="html"><![CDATA[Hundreds of millions of cases and millions of deaths have occurred worldwide
due to COVID-19. The fight against this pandemic is on-going on multiple
fronts. While vaccinations are picking up speed, there are still billions of
unvaccinated people. In this fight diagnosis of the disease and isolation of
the patients to prevent any spreads play a huge role. Machine Learning
approaches have assisted the diagnosis of COVID-19 cases by analyzing chest
X-ray and CT-scan images of patients. In this research we present a simple and
shallow Convolutional Neural Network based approach, TeliNet, to classify
CT-scan images of COVID-19 patients. Our results outperform the F1 score of
VGGNet and the benchmark approaches. Our proposed solution is also more
lightweight in comparison to the other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Teli_M/0/1/0/all/0/1"&gt;Mohammad Nayeem Teli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning 3D Dense Correspondence via Canonical Point Autoencoder. (arXiv:2107.04867v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04867</id>
        <link href="http://arxiv.org/abs/2107.04867"/>
        <updated>2021-07-13T01:59:33.894Z</updated>
        <summary type="html"><![CDATA[We propose a canonical point autoencoder (CPAE) that predicts dense
correspondences between 3D shapes of the same category. The autoencoder
performs two key functions: (a) encoding an arbitrarily ordered point cloud to
a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to
the original input instance shape. As being placed in the bottleneck, this
primitive plays a key role to map all the unordered point clouds on the
canonical surface and to be reconstructed in an ordered fashion. Once trained,
points from different shape instances that are mapped to the same locations on
the primitive surface are determined to be a pair of correspondence. Our method
does not require any form of annotation or self-supervised part segmentation
network and can handle unaligned input point clouds. Experimental results on 3D
semantic keypoint transfer and part segmentation transfer show that our model
performs favorably against state-of-the-art correspondence learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1"&gt;An-Chieh Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xueting Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation. (arXiv:2107.04914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04914</id>
        <link href="http://arxiv.org/abs/2107.04914"/>
        <updated>2021-07-13T01:59:33.886Z</updated>
        <summary type="html"><![CDATA[Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakazov_I/0/1/0/all/0/1"&gt;Ivan Zakazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirokikh_B/0/1/0/all/0/1"&gt;Boris Shirokikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1"&gt;Alexey Chernyavskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BSDA-Net: A Boundary Shape and Distance Aware Joint Learning Framework for Segmenting and Classifying OCTA Images. (arXiv:2107.04823v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04823</id>
        <link href="http://arxiv.org/abs/2107.04823"/>
        <updated>2021-07-13T01:59:33.878Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography angiography (OCTA) is a novel non-invasive
imaging technique that allows visualizations of vasculature and foveal
avascular zone (FAZ) across retinal layers. Clinical researches suggest that
the morphology and contour irregularity of FAZ are important biomarkers of
various ocular pathologies. Therefore, precise segmentation of FAZ has great
clinical interest. Also, there is no existing research reporting that FAZ
features can improve the performance of deep diagnostic classification
networks. In this paper, we propose a novel multi-level boundary shape and
distance aware joint learning framework, named BSDA-Net, for FAZ segmentation
and diagnostic classification from OCTA images. Two auxiliary branches, namely
boundary heatmap regression and signed distance map reconstruction branches,
are constructed in addition to the segmentation branch to improve the
segmentation performance, resulting in more accurate FAZ contours and fewer
outliers. Moreover, both low-level and high-level features from the
aforementioned three branches, including shape, size, boundary, and signed
directional distance map of FAZ, are fused hierarchically with features from
the diagnostic classifier. Through extensive experiments, the proposed BSDA-Net
is found to yield state-of-the-art segmentation and classification results on
the OCTA-500, OCTAGON, and FAZID datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1"&gt;Li Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhonghua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiewei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yijin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Junyan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pujin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoying Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weakly-Supervised Depth Estimation Network Using Attention Mechanism. (arXiv:2107.04819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04819</id>
        <link href="http://arxiv.org/abs/2107.04819"/>
        <updated>2021-07-13T01:59:33.855Z</updated>
        <summary type="html"><![CDATA[Monocular depth estimation (MDE) is a fundamental task in many applications
such as scene understanding and reconstruction. However, most of the existing
methods rely on accurately labeled datasets. A weakly-supervised framework
based on attention nested U-net (ANU) named as ANUW is introduced in this paper
for cases with wrong labels. The ANUW is trained end-to-end to convert an input
single RGB image into a depth image. It consists of a dense residual network
structure, an adaptive weight channel attention (AWCA) module, a patch second
non-local (PSNL) module and a soft label generation method. The dense residual
network is the main body of the network to encode and decode the input. The
AWCA module can adaptively adjust the channel weights to extract important
features. The PSNL module implements the spatial attention mechanism through a
second-order non-local method. The proposed soft label generation method uses
the prior knowledge of the dataset to produce soft labels to replace false
ones. The proposed ANUW is trained on a defective monocular depth dataset and
the trained model is tested on three public datasets, and the results
demonstrate the superiority of ANUW in comparison with the state-of-the-art MDE
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiabao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaoxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shuang_F/0/1/0/all/0/1"&gt;Feng Shuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cumulative Assessment for Urban 3D Modeling. (arXiv:2107.04622v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04622</id>
        <link href="http://arxiv.org/abs/2107.04622"/>
        <updated>2021-07-13T01:59:33.849Z</updated>
        <summary type="html"><![CDATA[Urban 3D modeling from satellite images requires accurate semantic
segmentation to delineate urban features, multiple view stereo for 3D
reconstruction of surface heights, and 3D model fitting to produce compact
models with accurate surface slopes. In this work, we present a cumulative
assessment metric that succinctly captures error contributions from each of
these components. We demonstrate our approach by providing challenging public
datasets and extending two open source projects to provide an end-to-end 3D
modeling baseline solution to stimulate further research and evaluation with a
public leaderboard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagstrom_S/0/1/0/all/0/1"&gt;Shea Hagstrom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pak_H/0/1/0/all/0/1"&gt;Hee Won Pak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ku_S/0/1/0/all/0/1"&gt;Stephanie Ku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sean Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1"&gt;Myron Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Twin Generative Adversarial Networks. (arXiv:2107.04708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04708</id>
        <link href="http://arxiv.org/abs/2107.04708"/>
        <updated>2021-07-13T01:59:33.843Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new continuously learning generative model,
called the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs
learns a sequence of tasks from several databases and its architecture consists
of three components: two identical generators, namely the Teacher and
Assistant, and one Discriminator. In order to allow for the LT-GANs to learn
new concepts without forgetting, we introduce a new lifelong training approach,
namely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the
Teacher and Assistant to alternately teach each other, while learning a new
database. This training approach favours transferring knowledge from a more
knowledgeable player to another player which knows less information about a
previously given task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Domain Adaptation with Polymorphic Transformers. (arXiv:2107.04805v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04805</id>
        <link href="http://arxiv.org/abs/2107.04805"/>
        <updated>2021-07-13T01:59:33.836Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) trained on one set of medical images often
experience severe performance drop on unseen test images, due to various domain
discrepancy between the training images (source domain) and the test images
(target domain), which raises a domain adaptation issue. In clinical settings,
it is difficult to collect enough annotated target domain data in a short
period. Few-shot domain adaptation, i.e., adapting a trained model with a
handful of annotations, is highly practical and useful in this case. In this
paper, we propose a Polymorphic Transformer (Polyformer), which can be
incorporated into any DNN backbones for few-shot domain adaptation.
Specifically, after the polyformer layer is inserted into a model trained on
the source domain, it extracts a set of prototype embeddings, which can be
viewed as a "basis" of the source-domain features. On the target domain, the
polyformer layer adapts by only updating a projection layer which controls the
interactions between image features and the prototype embeddings. All other
model weights (except BatchNorm parameters) are frozen during adaptation. Thus,
the chance of overfitting the annotations is greatly reduced, and the model can
perform robustly on the target domain after being trained on a few annotated
images. We demonstrate the effectiveness of Polyformer on two medical
segmentation tasks (i.e., optic disc/cup segmentation, and polyp segmentation).
The source code of Polyformer is released at
https://github.com/askerlee/segtran.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1"&gt;Xiuchao Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yangqin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xinxing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1"&gt;Daniel Ting&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1"&gt;Rick Siow Mong Goh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Layers Susceptible to Adversarial Attacks. (arXiv:2107.04827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04827</id>
        <link href="http://arxiv.org/abs/2107.04827"/>
        <updated>2021-07-13T01:59:33.829Z</updated>
        <summary type="html"><![CDATA[Common neural network architectures are susceptible to attack by adversarial
samples. Neural network architectures are commonly thought of as divided into
low-level feature extraction layers and high-level classification layers;
susceptibility of networks to adversarial samples is often thought of as a
problem related to classification rather than feature extraction. We test this
idea by selectively retraining different portions of VGG and ResNet
architectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and
adversarial data. Our experimental results show that susceptibility to
adversarial samples is associated with low-level feature extraction layers.
Therefore, retraining high-level layers is insufficient for achieving
robustness. This phenomenon could have two explanations: either, adversarial
attacks yield outputs from early layers that are indistinguishable from
features found in the attack classes, or adversarial attacks yield outputs from
early layers that differ statistically from features for non-adversarial
samples and do not permit consistent classification by subsequent layers. We
test this question by large-scale non-linear dimensionality reduction and
density modeling on distributions of feature vectors in hidden layers and find
that the feature distributions between non-adversarial and adversarial samples
differ substantially. Our results provide new insights into the statistical
origins of adversarial samples and possible defenses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1"&gt;Shoaib Ahmed Siddiqui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1"&gt;Thomas Breuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications. (arXiv:2107.04834v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04834</id>
        <link href="http://arxiv.org/abs/2107.04834"/>
        <updated>2021-07-13T01:59:33.791Z</updated>
        <summary type="html"><![CDATA[The seven basic facial expression classifications are a basic way to express
complex human emotions and are an important part of artificial intelligence
research. Based on the traditional Bayesian neural network framework, the
ResNet-18_BNN network constructed in this paper has been improved in the
following three aspects: (1) A new objective function is proposed, which is
composed of the KL loss of uncertain parameters and the intersection of
specific parameters. Entropy loss composition. (2) Aiming at a special
objective function, a training scheme for alternately updating these two
parameters is proposed. (3) Only model the parameters of the last convolution
group. According to experimental analysis, our method achieves an accuracy of
98.28% on the evaluation set of the Aff-Wild2 database. Compared with the
traditional Bayesian Neural Network, our method brings the highest
classification accuracy gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1"&gt;Wei Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hailan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition. (arXiv:2107.04810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04810</id>
        <link href="http://arxiv.org/abs/2107.04810"/>
        <updated>2021-07-13T01:59:33.780Z</updated>
        <summary type="html"><![CDATA[Surgical phase recognition is of particular interest to computer assisted
surgery systems, in which the goal is to predict what phase is occurring at
each frame for a surgery video. Networks with multi-stage architecture have
been widely applied in many computer vision tasks with rich patterns, where a
predictor stage first outputs initial predictions and an additional refinement
stage operates on the initial predictions to perform further refinement.
Existing works show that surgical video contents are well ordered and contain
rich temporal patterns, making the multi-stage architecture well suited for the
surgical phase recognition task. However, we observe that when simply applying
the multi-stage architecture to the surgical phase recognition task, the
end-to-end training manner will make the refinement ability fall short of its
wishes. To address the problem, we propose a new non end-to-end training
strategy and explore different designs of multi-stage architecture for surgical
phase recognition task. For the non end-to-end training strategy, the
refinement stage is trained separately with proposed two types of disturbed
sequences. Meanwhile, we evaluate three different choices of refinement models
to show that our analysis and solution are robust to the choices of specific
multi-stage models. We conduct experiments on two public benchmarks, the
M2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that
multi-stage architecture trained with our strategy largely boosts the
performance of the current state-of-the-art single-stage model. Code is
available at \url{https://github.com/ChinaYi/casual_tcn}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yi_F/0/1/0/all/0/1"&gt;Fangqiu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tingting Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensual Collaborative Training And Knowledge Distillation Based Facial Expression Recognition Under Noisy Annotations. (arXiv:2107.04746v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04746</id>
        <link href="http://arxiv.org/abs/2107.04746"/>
        <updated>2021-07-13T01:59:33.759Z</updated>
        <summary type="html"><![CDATA[Presence of noise in the labels of large scale facial expression datasets has
been a key challenge towards Facial Expression Recognition (FER) in the wild.
During early learning stage, deep networks fit on clean data. Then, eventually,
they start overfitting on noisy labels due to their memorization ability, which
limits FER performance. This work proposes an effective training strategy in
the presence of noisy labels, called as Consensual Collaborative Training (CCT)
framework. CCT co-trains three networks jointly using a convex combination of
supervision loss and consistency loss, without making any assumption about the
noise distribution. A dynamic transition mechanism is used to move from
supervision loss in early learning to consistency loss for consensus of
predictions among networks in the later stage. Inference is done using a single
network based on a simple knowledge distillation scheme. Effectiveness of the
proposed framework is demonstrated on synthetic as well as real noisy FER
datasets. In addition, a large test subset of around 5K images is annotated
from the FEC dataset using crowd wisdom of 16 different annotators and reliable
labels are inferred. CCT is also validated on it. State-of-the-art performance
is reported on the benchmark FER datasets RAFDB (90.84%) FERPlus (89.99%) and
AffectNet (66%). Our codes are available at https://github.com/1980x/CCT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gera_D/0/1/0/all/0/1"&gt;Darshan Gera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_S/0/1/0/all/0/1"&gt;S. Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining GCN and Transformer for Chinese Grammatical Error Detection. (arXiv:2105.09085v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09085</id>
        <link href="http://arxiv.org/abs/2105.09085"/>
        <updated>2021-07-13T01:59:33.737Z</updated>
        <summary type="html"><![CDATA[This paper describes our system at NLPTEA-2020 Task: Chinese Grammatical
Error Diagnosis (CGED). The goal of CGED is to diagnose four types of
grammatical errors: word selection (S), redundant words (R), missing words (M),
and disordered words (W). The automatic CGED system contains two parts
including error detection and error correction and our system is designed to
solve the error detection problem. Our system is built on three models: 1) a
BERT-based model leveraging syntactic information; 2) a BERT-based model
leveraging contextual embeddings; 3) a lexicon-based graph neural network
leveraging lexical information. We also design an ensemble mechanism to improve
the single model's performance. Finally, our system achieves the highest F1
scores at detection level and identification level among all teams
participating in the CGED 2020 task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinhong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Graph Learning via Population Based Self-Tuning GCN. (arXiv:2107.04713v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04713</id>
        <link href="http://arxiv.org/abs/2107.04713"/>
        <updated>2021-07-13T01:59:33.731Z</updated>
        <summary type="html"><![CDATA[Owing to the remarkable capability of extracting effective graph embeddings,
graph convolutional network (GCN) and its variants have been successfully
applied to a broad range of tasks, such as node classification, link
prediction, and graph classification. Traditional GCN models suffer from the
issues of overfitting and oversmoothing, while some recent techniques like
DropEdge could alleviate these issues and thus enable the development of deep
GCN. However, training GCN models is non-trivial, as it is sensitive to the
choice of hyperparameters such as dropout rate and learning weight decay,
especially for deep GCN models. In this paper, we aim to automate the training
of GCN models through hyperparameter optimization. To be specific, we propose a
self-tuning GCN approach with an alternate training algorithm, and further
extend our approach by incorporating the population based training scheme.
Experimental results on three benchmark datasets demonstrate the effectiveness
of our approaches on optimizing multi-layer GCN, compared with several
representative baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Ronghang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1"&gt;Zhiqiang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Teacher-Student Network Learning. (arXiv:2107.04689v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04689</id>
        <link href="http://arxiv.org/abs/2107.04689"/>
        <updated>2021-07-13T01:59:33.711Z</updated>
        <summary type="html"><![CDATA[A unique cognitive capability of humans consists in their ability to acquire
new knowledge and skills from a sequence of experiences. Meanwhile, artificial
intelligence systems are good at learning only the last given task without
being able to remember the databases learnt in the past. We propose a novel
lifelong learning methodology by employing a Teacher-Student network framework.
While the Student module is trained with a new given database, the Teacher
module would remind the Student about the information learnt in the past. The
Teacher, implemented by a Generative Adversarial Network (GAN), is trained to
preserve and replay past knowledge corresponding to the probabilistic
representations of previously learn databases. Meanwhile, the Student module is
implemented by a Variational Autoencoder (VAE) which infers its latent variable
representation from both the output of the Teacher module as well as from the
newly available database. Moreover, the Student module is trained to capture
both continuous and discrete underlying data representations across different
domains. The proposed lifelong learning framework is applied in supervised,
semi-supervised and unsupervised training. The code is available~:
\url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lex2vec: making Explainable Word Embeddings via Lexical Resources. (arXiv:2103.02269v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02269</id>
        <link href="http://arxiv.org/abs/2103.02269"/>
        <updated>2021-07-13T01:59:33.699Z</updated>
        <summary type="html"><![CDATA[In this technical report, we propose an algorithm, called Lex2vec that
exploits lexical resources to inject information into word embeddings and name
the embedding dimensions by means of knowledge bases. We evaluate the optimal
parameters to extract a number of informative labels that is readable and has a
good coverage for the embedding dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1"&gt;Fabio Celli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese. (arXiv:2012.08743v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08743</id>
        <link href="http://arxiv.org/abs/2012.08743"/>
        <updated>2021-07-13T01:59:33.693Z</updated>
        <summary type="html"><![CDATA[Prior works have demonstrated that a low-resource language pair can benefit
from multilingual machine translation (MT) systems, which rely on many language
pairs' joint training. This paper proposes two simple strategies to address the
rare word issue in multilingual MT systems for two low-resource language pairs:
French-Vietnamese and English-Vietnamese. The first strategy is about dynamical
learning word similarity of tokens in the shared space among source languages
while another one attempts to augment the translation ability of rare words
through updating their embeddings during the training. Besides, we leverage
monolingual data for multilingual MT systems to increase the amount of
synthetic parallel corpora while dealing with the data sparsity problem. We
have shown significant improvements of up to +1.62 and +2.54 BLEU points over
the bilingual baseline systems for both language pairs and released our
datasets for the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1"&gt;Thi-Vinh Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phuong-Thai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1"&gt;Thanh-Le Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_K/0/1/0/all/0/1"&gt;Khac-Quy Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Le-Minh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resilience of Autonomous Vehicle Object Category Detection to Universal Adversarial Perturbations. (arXiv:2107.04749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04749</id>
        <link href="http://arxiv.org/abs/2107.04749"/>
        <updated>2021-07-13T01:59:33.685Z</updated>
        <summary type="html"><![CDATA[Due to the vulnerability of deep neural networks to adversarial examples,
numerous works on adversarial attacks and defenses have been burgeoning over
the past several years. However, there seem to be some conventional views
regarding adversarial attacks and object detection approaches that most
researchers take for granted. In this work, we bring a fresh perspective on
those procedures by evaluating the impact of universal perturbations on object
detection at a class-level. We apply it to a carefully curated data set related
to autonomous driving. We use Faster-RCNN object detector on images of five
different categories: person, car, truck, stop sign and traffic light from the
COCO data set, while carefully perturbing the images using Universal Dense
Object Suppression algorithm. Our results indicate that person, car, traffic
light, truck and stop sign are resilient in that order (most to least) to
universal perturbations. To the best of our knowledge, this is the first time
such a ranking has been established which is significant for the security of
the data sets pertaining to autonomous vehicles and object detection in
general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1"&gt;Mohammad Nayeem Teli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Seungwon Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaled-Time-Attention Robust Edge Network. (arXiv:2107.04688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04688</id>
        <link href="http://arxiv.org/abs/2107.04688"/>
        <updated>2021-07-13T01:59:33.662Z</updated>
        <summary type="html"><![CDATA[This paper describes a systematic approach towards building a new family of
neural networks based on a delay-loop version of a reservoir neural network.
The resulting architecture, called Scaled-Time-Attention Robust Edge (STARE)
network, exploits hyper dimensional space and non-multiply-and-add computation
to achieve a simpler architecture, which has shallow layers, is simple to
train, and is better suited for Edge applications, such as Internet of Things
(IoT), over traditional deep neural networks. STARE incorporates new AI
concepts such as Attention and Context, and is best suited for temporal feature
extraction and classification. We demonstrate that STARE is applicable to a
variety of applications with improved performance and lower implementation
complexity. In particular, we showed a novel way of applying a dual-loop
configuration to detection and identification of drone vs bird in a counter
Unmanned Air Systems (UAS) detection application by exploiting both spatial
(video frame) and temporal (trajectory) information. We also demonstrated that
the STARE performance approaches that of a State-of-the-Art deep neural network
in classifying RF modulations, and outperforms Long Short-term Memory (LSTM) in
a special case of Mackey Glass time series prediction. To demonstrate hardware
efficiency, we designed and developed an FPGA implementation of the STARE
algorithm to demonstrate its low-power and high-throughput operations. In
addition, we illustrate an efficient structure for integrating a massively
parallel implementation of the STARE algorithm for ASIC implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1"&gt;Richard Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lihan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huster_T/0/1/0/all/0/1"&gt;Todd Huster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_W/0/1/0/all/0/1"&gt;William Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arleth_S/0/1/0/all/0/1"&gt;Stephen Arleth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1"&gt;Justin Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ridge_D/0/1/0/all/0/1"&gt;Devin Ridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_M/0/1/0/all/0/1"&gt;Michael Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Headley_W/0/1/0/all/0/1"&gt;William C. Headley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04795</id>
        <link href="http://arxiv.org/abs/2107.04795"/>
        <updated>2021-07-13T01:59:33.650Z</updated>
        <summary type="html"><![CDATA[Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. It works at the cost of training extra classifiers,
where the algorithm should be delicately designed to prevent individual
classifiers from collapsing into each other. In this paper, we present a simple
and efficient co-training algorithm, named Multi-Head Co-Training, for
semi-supervised image classification. By integrating base learners into a
multi-head structure, the model is in a minimal amount of extra parameters.
Every classification head in the unified model interacts with its peers through
a "Weak and Strong Augmentation" strategy, achieving single-view co-training
without promoting diversity explicitly. The effectiveness of Multi-Head
Co-Training is demonstrated in an empirical study on standard semi-supervised
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yuntao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1"&gt;Shuwei Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chongjun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Helpful Sentences in Product Reviews. (arXiv:2104.09792v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09792</id>
        <link href="http://arxiv.org/abs/2104.09792"/>
        <updated>2021-07-13T01:59:33.642Z</updated>
        <summary type="html"><![CDATA[In recent years online shopping has gained momentum and became an important
venue for customers wishing to save time and simplify their shopping process. A
key advantage of shopping online is the ability to read what other customers
are saying about products of interest. In this work, we aim to maintain this
advantage in situations where extreme brevity is needed, for example, when
shopping by voice. We suggest a novel task of extracting a single
representative helpful sentence from a set of reviews for a given product. The
selected sentence should meet two conditions: first, it should be helpful for a
purchase decision and second, the opinion it expresses should be supported by
multiple reviewers. This task is closely related to the task of Multi Document
Summarization in the product reviews domain but differs in its objective and
its level of conciseness. We collect a dataset in English of sentence
helpfulness scores via crowd-sourcing and demonstrate its reliability despite
the inherent subjectivity involved. Next, we describe a complete model that
extracts representative helpful sentences with positive and negative sentiment
towards the product and demonstrate that it outperforms several baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gamzu_I/0/1/0/all/0/1"&gt;Iftah Gamzu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1"&gt;Hila Gonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutiel_G/0/1/0/all/0/1"&gt;Gilad Kutiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Ran Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agichtein_E/0/1/0/all/0/1"&gt;Eugene Agichtein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TTAN: Two-Stage Temporal Alignment Network for Few-shot Action Recognition. (arXiv:2107.04782v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04782</id>
        <link href="http://arxiv.org/abs/2107.04782"/>
        <updated>2021-07-13T01:59:33.636Z</updated>
        <summary type="html"><![CDATA[Few-shot action recognition aims to recognize novel action classes (query)
using just a few samples (support). The majority of current approaches follow
the metric learning paradigm, which learns to compare the similarity between
videos. Recently, it has been observed that directly measuring this similarity
is not ideal since different action instances may show distinctive temporal
distribution, resulting in severe misalignment issues across query and support
videos. In this paper, we arrest this problem from two distinct aspects --
action duration misalignment and motion evolution misalignment. We address them
sequentially through a Two-stage Temporal Alignment Network (TTAN). The first
stage performs temporal transformation with the predicted affine warp
parameters, while the second stage utilizes a cross-attention mechanism to
coordinate the features of the support and query to a consistent evolution.
Besides, we devise a novel multi-shot fusion strategy, which takes the
misalignment among support samples into consideration. Ablation studies and
visualizations demonstrate the role played by both stages in addressing the
misalignment. Extensive experiments on benchmark datasets show the potential of
the proposed method in achieving state-of-the-art performance for few-shot
action recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1"&gt;Mengjuan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaoyuan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Mixture of Variational Autoencoders. (arXiv:2107.04694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04694</id>
        <link href="http://arxiv.org/abs/2107.04694"/>
        <updated>2021-07-13T01:59:33.629Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end lifelong learning mixture of experts.
Each expert is implemented by a Variational Autoencoder (VAE). The experts in
the mixture system are jointly trained by maximizing a mixture of individual
component evidence lower bounds (MELBO) on the log-likelihood of the given
training samples. The mixing coefficients in the mixture, control the
contributions of each expert in the goal representation. These are sampled from
a Dirichlet distribution whose parameters are determined through non-parametric
estimation during lifelong learning. The model can learn new tasks fast when
these are similar to those previously learnt. The proposed Lifelong mixture of
VAE (L-MVAE) expands its architecture with new components when learning a
completely new task. After the training, our model can automatically determine
the relevant expert to be used when fed with new data samples. This mechanism
benefits both the memory efficiency and the required computational cost as only
one expert is used during the inference. The L-MVAE inference model is able to
perform interpolation in the joint latent space across the data domains
associated with different tasks and is shown to be efficient for disentangled
learning representation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07396</id>
        <link href="http://arxiv.org/abs/2104.07396"/>
        <updated>2021-07-13T01:59:33.622Z</updated>
        <summary type="html"><![CDATA[We introduce a novel embedding model, named NoGE, which aims to integrate
co-occurrence among entities and relations into graph neural networks to
improve knowledge graph completion (i.e., link prediction). Given a knowledge
graph, NoGE constructs a single graph considering entities and relations as
individual nodes. NoGE then computes weights for edges among nodes based on the
co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion
Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector
representations for entity and relation nodes. NoGE then adopts a score
function to produce the triple scores. Comprehensive experimental results show
that NoGE obtains state-of-the-art results on three new and difficult benchmark
datasets CoDEx for knowledge graph completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dai Quoc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1"&gt;Vinh Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat Quoc Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local-to-Global Self-Attention in Vision Transformers. (arXiv:2107.04735v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04735</id>
        <link href="http://arxiv.org/abs/2107.04735"/>
        <updated>2021-07-13T01:59:33.605Z</updated>
        <summary type="html"><![CDATA[Transformers have demonstrated great potential in computer vision tasks. To
avoid dense computations of self-attentions in high-resolution visual data,
some recent Transformer models adopt a hierarchical design, where
self-attentions are only computed within local windows. This design
significantly improves the efficiency but lacks global feature reasoning in
early stages. In this work, we design a multi-path structure of the
Transformer, which enables local-to-global reasoning at multiple granularities
in each stage. The proposed framework is computationally efficient and highly
effective. With a marginal increasement in computational overhead, our model
achieves notable improvements in both image classification and semantic
segmentation. Code is available at https://github.com/ljpadam/LG-Transformer]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yichao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shengcai Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04768</id>
        <link href="http://arxiv.org/abs/2107.04768"/>
        <updated>2021-07-13T01:59:33.598Z</updated>
        <summary type="html"><![CDATA[Video question answering is a challenging task, which requires agents to be
able to understand rich video contents and perform spatial-temporal reasoning.
However, existing graph-based methods fail to perform multi-step reasoning
well, neglecting two properties of VideoQA: (1) Even for the same video,
different questions may require different amount of video clips or objects to
infer the answer with relational reasoning; (2) During reasoning, appearance
and motion features have complicated interdependence which are correlated and
complementary to each other. Based on these observations, we propose a
Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an
end-to-end fashion. The first contribution of our DualVGR is the design of an
explainable Query Punishment Module, which can filter out irrelevant visual
features through multiple cycles of reasoning. The second contribution is the
proposed Video-based Multi-view Graph Attention Network, which captures the
relations between appearance and motion features. Our DualVGR network achieves
state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and
demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is
available at https://github.com/MMIR/DualVGR-VideoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1"&gt;Bing-Kun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood. (arXiv:2107.04705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04705</id>
        <link href="http://arxiv.org/abs/2107.04705"/>
        <updated>2021-07-13T01:59:33.590Z</updated>
        <summary type="html"><![CDATA[Learning disentangled and interpretable representations is an important step
towards accomplishing comprehensive data representations on the manifold. In
this paper, we propose a novel representation learning algorithm which combines
the inference abilities of Variational Autoencoders (VAE) with the
generalization capability of Generative Adversarial Networks (GAN). The
proposed model, called InfoVAEGAN, consists of three networks~: Encoder,
Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and
continuous interpretable representations in an unsupervised manner by using two
different data-free log-likelihood functions onto the variables sampled from
the generator's distribution. We propose a two-stage algorithm for optimizing
the inference network separately from the generator training. Moreover, we
enforce the learning of interpretable representations through the maximization
of the mutual information between the existing latent variables and those
created through generative and inference processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1"&gt;Adrian G. Bors&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer. (arXiv:2102.09550v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09550</id>
        <link href="http://arxiv.org/abs/2102.09550"/>
        <updated>2021-07-13T01:59:33.584Z</updated>
        <summary type="html"><![CDATA[We address the challenging problem of Natural Language Comprehension beyond
plain-text documents by introducing the TILT neural network architecture which
simultaneously learns layout information, visual features, and textual
semantics. Contrary to previous approaches, we rely on a decoder capable of
unifying a variety of problems involving natural language. The layout is
represented as an attention bias and complemented with contextualized visual
information, while the core of our model is a pretrained encoder-decoder
Transformer. Our novel approach achieves state-of-the-art results in extracting
information from documents and answering questions which demand layout
understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process
by employing an end-to-end model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powalski_R/0/1/0/all/0/1"&gt;Rafa&amp;#x142; Powalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Borchmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurkiewicz_D/0/1/0/all/0/1"&gt;Dawid Jurkiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dwojak_T/0/1/0/all/0/1"&gt;Tomasz Dwojak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Pietruszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palka_G/0/1/0/all/0/1"&gt;Gabriela Pa&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04767</id>
        <link href="http://arxiv.org/abs/2107.04767"/>
        <updated>2021-07-13T01:59:33.576Z</updated>
        <summary type="html"><![CDATA[Intelligent resident surveillance is one of the most essential smart
community services. The increasing demand for security needs surveillance
systems to be able to detect anomalies in surveillance scenes. Employing
high-capacity computational devices for intelligent surveillance in residential
societies is costly and not feasible. Therefore, we propose anomaly detection
for intelligent surveillance using CPU-only edge devices. A modular framework
to capture object-level inferences and tracking is developed. To cope with
partial occlusions, posture deformations, and complex scenes we employed
feature encoding and trajectory associations. Elements of the anomaly detection
framework are optimized to run on CPU-only edge devices with sufficient FPS.
The experimental results indicate the proposed method is feasible and achieves
satisfactory results in real-life scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1"&gt;Mayur R. Parate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1"&gt;Kishor M. Bhurchandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1"&gt;Ashwin G. Kothari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological-Framework to Improve Analysis of Machine Learning Model Performance. (arXiv:2107.04714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04714</id>
        <link href="http://arxiv.org/abs/2107.04714"/>
        <updated>2021-07-13T01:59:33.558Z</updated>
        <summary type="html"><![CDATA[As both machine learning models and the datasets on which they are evaluated
have grown in size and complexity, the practice of using a few summary
statistics to understand model performance has become increasingly problematic.
This is particularly true in real-world scenarios where understanding model
failure on certain subpopulations of the data is of critical importance. In
this paper we propose a topological framework for evaluating machine learning
models in which a dataset is treated as a "space" on which a model operates.
This provides us with a principled way to organize information about model
performance at both the global level (over the entire test set) and also the
local level (on specific subpopulations). Finally, we describe a topological
data structure, presheaves, which offer a convenient way to store and analyze
model performance between different subpopulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wight_C/0/1/0/all/0/1"&gt;Colby Wight&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akers_S/0/1/0/all/0/1"&gt;Sarah Akers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1"&gt;Woongjo Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gosink_L/0/1/0/all/0/1"&gt;Luke Gosink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurrus_E/0/1/0/all/0/1"&gt;Elizabeth Jurrus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kappagantula_K/0/1/0/all/0/1"&gt;Keerti Kappagantula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emerson_T/0/1/0/all/0/1"&gt;Tegan H. Emerson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DDCNet: Deep Dilated Convolutional Neural Network for Dense Prediction. (arXiv:2107.04715v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04715</id>
        <link href="http://arxiv.org/abs/2107.04715"/>
        <updated>2021-07-13T01:59:33.551Z</updated>
        <summary type="html"><![CDATA[Dense pixel matching problems such as optical flow and disparity estimation
are among the most challenging tasks in computer vision. Recently, several deep
learning methods designed for these problems have been successful. A
sufficiently larger effective receptive field (ERF) and a higher resolution of
spatial features within a network are essential for providing higher-resolution
dense estimates. In this work, we present a systemic approach to design network
architectures that can provide a larger receptive field while maintaining a
higher spatial feature resolution. To achieve a larger ERF, we utilized dilated
convolutional layers. By aggressively increasing dilation rates in the deeper
layers, we were able to achieve a sufficiently larger ERF with a significantly
fewer number of trainable parameters. We used optical flow estimation problem
as the primary benchmark to illustrate our network design strategy. The
benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact
networks can achieve comparable performance in the class of lightweight
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_A/0/1/0/all/0/1"&gt;Ali Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1"&gt;Madhusudhanan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Generative Adversarial Network for Depth Estimation in Laparoscopic Images. (arXiv:2107.04644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04644</id>
        <link href="http://arxiv.org/abs/2107.04644"/>
        <updated>2021-07-13T01:59:33.541Z</updated>
        <summary type="html"><![CDATA[Dense depth estimation and 3D reconstruction of a surgical scene are crucial
steps in computer assisted surgery. Recent work has shown that depth estimation
from a stereo images pair could be solved with convolutional neural networks.
However, most recent depth estimation models were trained on datasets with
per-pixel ground truth. Such data is especially rare for laparoscopic imaging,
making it hard to apply supervised depth estimation to real surgical
applications. To overcome this limitation, we propose SADepth, a new
self-supervised depth estimation method based on Generative Adversarial
Networks. It consists of an encoder-decoder generator and a discriminator to
incorporate geometry constraints during training. Multi-scale outputs from the
generator help to solve the local minima caused by the photometric reprojection
loss, while the adversarial learning improves the framework generation quality.
Extensive experiments on two public datasets show that SADepth outperforms
recent state-of-the-art unsupervised methods by a large margin, and reduces the
gap between supervised and unsupervised depth estimation in laparoscopic
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baoru Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1"&gt;Jianqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;Anh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuch_D/0/1/0/all/0/1"&gt;David Tuch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_K/0/1/0/all/0/1"&gt;Kunal Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elson_D/0/1/0/all/0/1"&gt;Daniel S. Elson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina. (arXiv:2107.04721v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04721</id>
        <link href="http://arxiv.org/abs/2107.04721"/>
        <updated>2021-07-13T01:59:33.534Z</updated>
        <summary type="html"><![CDATA[Fundus photography has routinely been used to document the presence and
severity of retinal degenerative diseases such as age-related macular
degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical
practice, for which the fovea and optic disc (OD) are important retinal
landmarks. However, the occurrence of lesions, drusen, and other retinal
abnormalities during retinal degeneration severely complicates automatic
landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net
backbone enriched with hierarchical bottleneck attention. The network consists
of a novel bottleneck attention block that combines and refines self-attention,
channel attention, and relative-position attention to highlight retinal
abnormalities that may be important for fovea and OD segmentation in the
degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea
detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of
25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for
AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:
ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for
landmark detection in the presence of a variety of retinal degenerative
diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shuyun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_Z/0/1/0/all/0/1"&gt;Ziming Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Granley_J/0/1/0/all/0/1"&gt;Jacob Granley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beyeler_M/0/1/0/all/0/1"&gt;Michael Beyeler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Streaming End-to-End Framework For Spoken Language Understanding. (arXiv:2105.10042v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10042</id>
        <link href="http://arxiv.org/abs/2105.10042"/>
        <updated>2021-07-13T01:59:33.511Z</updated>
        <summary type="html"><![CDATA[End-to-end spoken language understanding (SLU) has recently attracted
increasing interest. Compared to the conventional tandem-based approach that
combines speech recognition and language understanding as separate modules, the
new approach extracts users' intentions directly from the speech signals,
resulting in joint optimization and low latency. Such an approach, however, is
typically designed to process one intention at a time, which leads users to
take multiple rounds to fulfill their requirements while interacting with a
dialogue system. In this paper, we propose a streaming end-to-end framework
that can process multiple intentions in an online and incremental way. The
backbone of our framework is a unidirectional RNN trained with the
connectionist temporal classification (CTC) criterion. By this design, an
intention can be identified when sufficient evidence has been accumulated, and
multiple intentions can be identified sequentially. We evaluate our solution on
the Fluent Speech Commands (FSC) dataset and the intent detection accuracy is
about 97 % on all multi-intent settings. This result is comparable to the
performance of the state-of-the-art non-streaming models, but is achieved in an
online and incremental way. We also employ our model to a keyword spotting task
using the Google Speech Commands dataset and the results are also highly
promising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_N/0/1/0/all/0/1"&gt;Nihal Potdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avila_A/0/1/0/all/0/1"&gt;Anderson R. Avila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1"&gt;Chao Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yiran Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Video Generation using a Gaussian Process Trigger. (arXiv:2107.04619v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04619</id>
        <link href="http://arxiv.org/abs/2107.04619"/>
        <updated>2021-07-13T01:59:33.493Z</updated>
        <summary type="html"><![CDATA[Generating future frames given a few context (or past) frames is a
challenging task. It requires modeling the temporal coherence of videos and
multi-modality in terms of diversity in the potential future states. Current
variational approaches for video generation tend to marginalize over
multi-modal future outcomes. Instead, we propose to explicitly model the
multi-modality in the future outcomes and leverage it to sample diverse
futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to
learn priors on future states given the past and maintains a probability
distribution over possible futures given a particular sample. In addition, we
leverage the changes in this distribution over time to control the sampling of
diverse future states by estimating the end of ongoing sequences. That is, we
use the variance of GP over the output function space to trigger a change in an
action sequence. We achieve state-of-the-art results on diverse future frame
generation in terms of reconstruction quality and diversity of the generated
sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_G/0/1/0/all/0/1"&gt;Gaurav Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12637</id>
        <link href="http://arxiv.org/abs/2010.12637"/>
        <updated>2021-07-13T01:59:33.486Z</updated>
        <summary type="html"><![CDATA[Semantic textual similarity is one of the open research challenges in the
field of Natural Language Processing. Extensive research has been carried out
in this field and near-perfect results are achieved by recent transformer-based
models in existing benchmark datasets like the STS dataset and the SICK
dataset. In this paper, we study the sentences in these datasets and analyze
the sensitivity of various word embeddings with respect to the complexity of
the sentences. We build a complex sentences dataset comprising of 50 sentence
pairs with associated semantic similarity values provided by 15 human
annotators. Readability analysis is performed to highlight the increase in
complexity of the sentences in the existing benchmark datasets and those in the
proposed dataset. Further, we perform a comparative analysis of the performance
of various word embeddings and language models on the existing benchmark
datasets and the proposed dataset. The results show the increase in complexity
of the sentences has a significant impact on the performance of the embedding
models resulting in a 10-20% decrease in Pearson's and Spearman's correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1"&gt;Dhivya Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1"&gt;Vijay Mago&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Triangulation Method is Not Really Optimal. (arXiv:2107.04618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04618</id>
        <link href="http://arxiv.org/abs/2107.04618"/>
        <updated>2021-07-13T01:59:33.475Z</updated>
        <summary type="html"><![CDATA[Triangulation refers to the problem of finding a 3D point from its 2D
projections on multiple camera images. For solving this problem, it is the
common practice to use so-called optimal triangulation method, which we call
the L2 method in this paper. But, the method can be optimal only if we assume
no uncertainty in the camera parameters. Through extensive comparison on
synthetic and real data, we observed that the L2 method is actually not the
best choice when there is uncertainty in the camera parameters. Interestingly,
it can be observed that the simple mid-point method outperforms other methods.
Apart from its high performance, the mid-point method has a simple closed
formed solution for multiple camera images while the L2 method is hard to be
used for more than two camera images. Therefore, in contrast to the common
practice, we argue that the simple mid-point method should be used in
structure-from-motion applications where there is uncertainty in camera
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasiri_S/0/1/0/all/0/1"&gt;Seyed-Mahdi Nasiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1"&gt;Reshad Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1"&gt;Hadi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Transformer Growth for Progressive BERT Training. (arXiv:2010.12562v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12562</id>
        <link href="http://arxiv.org/abs/2010.12562"/>
        <updated>2021-07-13T01:59:33.466Z</updated>
        <summary type="html"><![CDATA[Due to the excessive cost of large-scale language model pre-training,
considerable efforts have been made to train BERT progressively -- start from
an inferior but low-cost model and gradually grow the model to increase the
computational complexity. Our objective is to advance the understanding of
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture search, Transformer growth
also favors compound scaling. Specifically, while existing methods only conduct
network growth in a single dimension, we observe that it is beneficial to use
compound growth operators and balance multiple dimensions (e.g., depth, width,
and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give operator
selection practical guidance. In light of our analyses, the proposed method
speeds up BERT pre-training by 73.6% and 82.2% for the base and large models
respectively, while achieving comparable performances]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiaotao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Noisy Self-Reports to Predict Twitter User Demographics. (arXiv:2005.00635v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00635</id>
        <link href="http://arxiv.org/abs/2005.00635"/>
        <updated>2021-07-13T01:59:33.459Z</updated>
        <summary type="html"><![CDATA[Computational social science studies often contextualize content analysis
within standard demographics. Since demographics are unavailable on many social
media platforms (e.g. Twitter) numerous studies have inferred demographics
automatically. Despite many studies presenting proof of concept inference of
race and ethnicity, training of practical systems remains elusive since there
are few annotated datasets. Existing datasets are small, inaccurate, or fail to
cover the four most common racial and ethnic groups in the United States. We
present a method to identify self-reports of race and ethnicity from Twitter
profile descriptions. Despite errors inherent in automated supervision, we
produce models with good performance when measured on gold standard self-report
survey data. The result is a reproducible method for creating large-scale
training resources for race and ethnicity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1"&gt;Zach Wood-Doughty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Paiheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1"&gt;Mark Dredze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning. (arXiv:2107.05382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05382</id>
        <link href="http://arxiv.org/abs/2107.05382"/>
        <updated>2021-07-13T01:59:33.442Z</updated>
        <summary type="html"><![CDATA[We propose a semi-supervised learning method for building end-to-end rich
transcription-style automatic speech recognition (RT-ASR) systems from
small-scale rich transcription-style and large-scale common transcription-style
datasets. In spontaneous speech tasks, various speech phenomena such as
fillers, word fragments, laughter and coughs, etc. are often included. While
common transcriptions do not give special awareness to these phenomena, rich
transcriptions explicitly convert them into special phenomenon tokens as well
as textual tokens. In previous studies, the textual and phenomenon tokens were
simultaneously estimated in an end-to-end manner. However, it is difficult to
build accurate RT-ASR systems because large-scale rich transcription-style
datasets are often unavailable. To solve this problem, our training method uses
a limited rich transcription-style dataset and common transcription-style
dataset simultaneously. The Key process in our semi-supervised learning is to
convert the common transcription-style dataset into a pseudo-rich
transcription-style dataset. To this end, we introduce style tokens which
control phenomenon tokens are generated or not into transformer-based
autoregressive modeling. We use this modeling for generating the pseudo-rich
transcription-style datasets and for building RT-ASR system from the pseudo and
original datasets. Our experiments on spontaneous ASR tasks showed the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05381</id>
        <link href="http://arxiv.org/abs/2107.05381"/>
        <updated>2021-07-13T01:59:33.435Z</updated>
        <summary type="html"><![CDATA[Departing from the postulate that Voynich Manuscript is not a hoax but rather
encodes authentic contents, our article presents an evolutionary algorithm
which aims to find the most optimal mapping between voynichian glyphs and
candidate phonemic values. Core component of the decoding algorithm is a
process of maximization of a fitness function which aims to find most optimal
set of substitution rules allowing to transcribe the part of the manuscript --
which we call the Calendar -- into lists of feminine names. This leads to sets
of character subsitution rules which allow us to consistently transcribe dozens
among three hundred calendar tokens into feminine names: a result far
surpassing both ``popular'' as well as "state of the art" tentatives to crack
the manuscript. What's more, by using name lists stemming from different
languages as potential cribs, our ``adaptive'' method can also be useful in
identification of the language in which the manuscript is written.

As far as we can currently tell, results of our experiments indicate that the
Calendar part of the manuscript contains names from baltoslavic, balkanic or
hebrew language strata. Two further indications are also given: primo, highest
fitness values were obtained when the crib list contains names with specific
infixes at token's penultimate position as is the case, for example, for slavic
\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the
most successful scenario, 240 characters contained in 35 distinct Voynichese
tokens were successfully transcribed. Secundo, in case of crib stemming from
Hebrew language, whole adaptation process converges to significantly better
fitness values when transcribing voynichian tokens whose order of individual
characters have been reversed, and when lists feminine and not masculine names
are used as the crib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1"&gt;Daniel Devatman Hromada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation. (arXiv:2107.05380v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05380</id>
        <link href="http://arxiv.org/abs/2107.05380"/>
        <updated>2021-07-13T01:59:33.426Z</updated>
        <summary type="html"><![CDATA[In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1"&gt;Rudrajit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Visual Models using a Knowledge Graph as a Trainer. (arXiv:2102.08747v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08747</id>
        <link href="http://arxiv.org/abs/2102.08747"/>
        <updated>2021-07-13T01:59:33.414Z</updated>
        <summary type="html"><![CDATA[Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1"&gt;Sebastian Monka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1"&gt;Lavdim Halilaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmid_S/0/1/0/all/0/1"&gt;Stefan Schmid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1"&gt;Achim Rettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Social Media Information Sharing for Natural Disaster Response. (arXiv:2005.07019v5 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07019</id>
        <link href="http://arxiv.org/abs/2005.07019"/>
        <updated>2021-07-13T01:59:33.407Z</updated>
        <summary type="html"><![CDATA[Social media has become an essential channel for posting disaster-related
information, which provide governments and relief agencies real-time data for
better disaster management. However, research in this field has not received
sufficient attention and extracting useful information is still challenging.
This paper aims to improve disaster relief efficiency via mining and analyzing
social media data like public attitudes towards disaster response and public
demands for targeted relief supplies during different types of disasters. We
focus on different natural disasters based on properties such as types,
durations, and damages, which contains a total of 41,993 tweets. In this paper,
public perception is assessed qualitatively by manually classified tweets,
which contain information like the demand for targeted relief supplies,
satisfactions of disaster response, and public fear. Public attitudes to
natural disasters are studied via a quantitative analysis using eight machine
learning models. To better provide decision-makers with the appropriate model,
the comparison of machine learning models based on computational time and
prediction accuracy is conducted. The change of public opinion during different
natural disasters and the evolution of people's behavior of using social media
for disaster relief in the face of the identical type of natural disasters as
Twitter continues to evolve are studied. The results in this paper demonstrate
the feasibility and validation of the proposed research approach and provide
relief agencies with insights into better disaster management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhijie Sasha Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christenson_L/0/1/0/all/0/1"&gt;Lauren Christenson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fulton_L/0/1/0/all/0/1"&gt;Lawrence Fulton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oriental Language Recognition (OLR) 2020: Summary and Analysis. (arXiv:2107.05365v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05365</id>
        <link href="http://arxiv.org/abs/2107.05365"/>
        <updated>2021-07-13T01:59:33.383Z</updated>
        <summary type="html"><![CDATA[The fifth Oriental Language Recognition (OLR) Challenge focuses on language
recognition in a variety of complex environments to promote its development.
The OLR 2020 Challenge includes three tasks: (1) cross-channel language
identification, (2) dialect identification, and (3) noisy language
identification. We choose Cavg as the principle evaluation metric, and the
Equal Error Rate (EER) as the secondary metric. There were 58 teams
participating in this challenge and one third of the teams submitted valid
results. Compared with the best baseline, the Cavg values of Top 1 system for
the three tasks were relatively reduced by 82%, 62% and 48%, respectively. This
paper describes the three tasks, the database profile, and the final results.
We also outline the novel approaches that improve the performance of language
recognition systems most significantly, such as the utilization of auxiliary
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Binling Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_Y/0/1/0/all/0/1"&gt;Yiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1"&gt;Qingyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling. (arXiv:2012.00857v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00857</id>
        <link href="http://arxiv.org/abs/2012.00857"/>
        <updated>2021-07-13T01:59:33.377Z</updated>
        <summary type="html"><![CDATA[There are two major classes of natural language grammar -- the dependency
grammar that models one-to-one correspondences between words and the
constituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only
inducing one class of grammars, we introduce a novel model, StructFormer, that
can simultaneously induce dependency and constituency structure. To achieve
this, we propose a new parsing framework that can jointly generate a
constituency tree and dependency graph. Then we integrate the induced
dependency relations into the transformer, in a differentiable manner, through
a novel dependency-constrained self-attention mechanism. Experimental results
show that our model can achieve strong results on unsupervised constituency
parsing, unsupervised dependency parsing, and masked language modeling at the
same time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yikang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1"&gt;Yi Tay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Che Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1"&gt;Donald Metzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DaCy: A Unified Framework for Danish NLP. (arXiv:2107.05295v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05295</id>
        <link href="http://arxiv.org/abs/2107.05295"/>
        <updated>2021-07-13T01:59:33.354Z</updated>
        <summary type="html"><![CDATA[Danish natural language processing (NLP) has in recent years obtained
considerable improvements with the addition of multiple new datasets and
models. However, at present, there is no coherent framework for applying
state-of-the-art models for Danish. We present DaCy: a unified framework for
Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain
state-of-the-art performance on named entity recognition, part-of-speech
tagging, and dependency parsing. DaCy contains tools for easy integration of
existing models such as for polarity, emotion, or subjectivity detection. In
addition, we conduct a series of tests for biases and robustness of Danish NLP
pipelines through augmentation of the test set of DaNE. DaCy large compares
favorably and is especially robust to long input lengths and spelling
variations and errors. All models except DaCy large display significant biases
related to ethnicity while only Polyglot shows a significant gender bias. We
argue that for languages with limited benchmark sets, data augmentation can be
particularly useful for obtaining more realistic and fine-grained performance
estimates. We provide a series of augmenters as a first step towards a more
thorough evaluation of language models for low and medium resource languages
and encourage further development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Enevoldsen_K/0/1/0/all/0/1"&gt;Kenneth Enevoldsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1"&gt;Lasse Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielbo_K/0/1/0/all/0/1"&gt;Kristoffer Nielbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Flexible Multi-Task Model for BERT Serving. (arXiv:2107.05377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05377</id>
        <link href="http://arxiv.org/abs/2107.05377"/>
        <updated>2021-07-13T01:59:33.347Z</updated>
        <summary type="html"><![CDATA[In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianwen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianwei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shenghuan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05219</id>
        <link href="http://arxiv.org/abs/2107.05219"/>
        <updated>2021-07-13T01:59:33.333Z</updated>
        <summary type="html"><![CDATA[Controlling the model to generate texts of different categories is a
challenging task that is getting more and more attention. Recently, generative
adversarial net (GAN) has shown promising results in category text generation.
However, the texts generated by GANs usually suffer from the problems of mode
collapse and training instability. To avoid the above problems, we propose a
novel model named category-aware variational recurrent neural network
(CatVRNN), which is inspired by multi-task learning. In our model, generation
and classification are trained simultaneously, aiming at generating texts of
different categories. Moreover, the use of multi-task learning can improve the
quality of generated texts, when the classification task is appropriate. And we
propose a function to initialize the hidden state of CatVRNN to force model to
generate texts of a specific category. Experimental results on three datasets
demonstrate that our model can do better than several state-of-the-art text
generation methods based GAN in the category accuracy and quality of generated
texts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pengsen Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jinqiao Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate versus Politics: Detection of Hate against Policy makers in Italian tweets. (arXiv:2107.05357v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05357</id>
        <link href="http://arxiv.org/abs/2107.05357"/>
        <updated>2021-07-13T01:59:33.314Z</updated>
        <summary type="html"><![CDATA[Accurate detection of hate speech against politicians, policy making and
political ideas is crucial to maintain democracy and free speech.
Unfortunately, the amount of labelled data necessary for training models to
detect hate speech are limited and domain-dependent. In this paper, we address
the issue of classification of hate speech against policy makers from Twitter
in Italian, producing the first resource of this type in this language. We
collected and annotated 1264 tweets, examined the cases of disagreements
between annotators, and performed in-domain and cross-domain hate speech
classifications with different features and algorithms. We achieved a
performance of ROC AUC 0.83 and analyzed the most predictive attributes, also
finding the different language features in the anti-policymakers and
anti-immigration domains. Finally, we visualized networks of hashtags to
capture the topics used in hateful and normal tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duzha_A/0/1/0/all/0/1"&gt;Armend Duzha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casadei_C/0/1/0/all/0/1"&gt;Cristiano Casadei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosi_M/0/1/0/all/0/1"&gt;Michael Tosi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celli_F/0/1/0/all/0/1"&gt;Fabio Celli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Putting words into the system's mouth: A targeted attack on neural machine translation using monolingual data poisoning. (arXiv:2107.05243v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05243</id>
        <link href="http://arxiv.org/abs/2107.05243"/>
        <updated>2021-07-13T01:59:33.307Z</updated>
        <summary type="html"><![CDATA[Neural machine translation systems are known to be vulnerable to adversarial
test inputs, however, as we show in this paper, these systems are also
vulnerable to training attacks. Specifically, we propose a poisoning attack in
which a malicious adversary inserts a small poisoned sample of monolingual text
into the training set of a system trained using back-translation. This sample
is designed to induce a specific, targeted translation behaviour, such as
peddling misinformation. We present two methods for crafting poisoned examples,
and show that only a tiny handful of instances, amounting to only 0.02% of the
training set, is sufficient to enact a successful attack. We outline a defence
method against said attacks, which partly ameliorates the problem. However, we
stress that this is a blind-spot in modern NMT, demanding immediate attention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1"&gt;Francisco Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1"&gt;Ahmed El-Kishky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yuqing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1"&gt;Trevor Cohn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computational Paremiology: Charting the temporal, ecological dynamics of proverb use in books, news articles, and tweets. (arXiv:2107.04929v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04929</id>
        <link href="http://arxiv.org/abs/2107.04929"/>
        <updated>2021-07-13T01:59:33.301Z</updated>
        <summary type="html"><![CDATA[Proverbs are an essential component of language and culture, and though much
attention has been paid to their history and currency, there has been
comparatively little quantitative work on changes in the frequency with which
they are used over time. With wider availability of large corpora reflecting
many diverse genres of documents, it is now possible to take a broad and
dynamic view of the importance of the proverb. Here, we measure temporal
changes in the relevance of proverbs within three corpora, differing in kind,
scale, and time frame: Millions of books over centuries; hundreds of millions
of news articles over twenty years; and billions of tweets over a decade. We
find that proverbs present heavy-tailed frequency-of-usage rank distributions
in each venue; exhibit trends reflecting the cultural dynamics of the eras
covered; and have evolved into contemporary forms on social media.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1"&gt;E. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1"&gt;C. M. Danforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mieder_W/0/1/0/all/0/1"&gt;W. Mieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1"&gt;P. S. Dodds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Legal Judgment Prediction with Multi-Stage CaseRepresentation Learning in the Real Court Setting. (arXiv:2107.05192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05192</id>
        <link href="http://arxiv.org/abs/2107.05192"/>
        <updated>2021-07-13T01:59:33.292Z</updated>
        <summary type="html"><![CDATA[Legal judgment prediction(LJP) is an essential task for legal AI. While prior
methods studied on this topic in a pseudo setting by employing the
judge-summarized case narrative as the input to predict the judgment,
neglecting critical case life-cycle information in real court setting could
threaten the case logic representation quality and prediction correctness. In
this paper, we introduce a novel challenging dataset from real courtrooms to
predict the legal judgment in a reasonably encyclopedic manner by leveraging
the genuine input of the case -- plaintiff's claims and court debate data, from
which the case's facts are automatically recognized by comprehensively
understanding the multi-role dialogues of the court debate, and then learnt to
discriminate the claims so as to reach the final judgment through multi-task
learning. An extensive set of experiments with a large civil trial data set
shows that the proposed model can more accurately characterize the interactions
among claims, fact and debate for legal judgment prediction, achieving
significant improvements over strong state-of-the-art baselines. Moreover, the
user study conducted with real judges and law school students shows the neural
predictions can also be interpretable and easily observed, and thus enhancing
the trial efficiency and judgment quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Luyao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yating Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1"&gt;Wei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shikun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Compositional Concept Learning. (arXiv:2107.05176v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05176</id>
        <link href="http://arxiv.org/abs/2107.05176"/>
        <updated>2021-07-13T01:59:33.279Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of recognizing compositional
attribute-object concepts within the zero-shot learning (ZSL) framework. We
propose an episode-based cross-attention (EpiCA) network which combines merits
of cross-attention mechanism and episode-based training strategy to recognize
novel compositional concepts. Firstly, EpiCA bases on cross-attention to
correlate concept-visual information and utilizes the gated pooling layer to
build contextualized representations for both images and concepts. The updated
representations are used for a more in-depth multi-modal relevance calculation
for concept recognition. Secondly, a two-phase episode training strategy,
especially the transductive phase, is adopted to utilize unlabeled test
examples to alleviate the low-resource learning problem. Experiments on two
widely-used zero-shot compositional learning (ZSCL) benchmarks have
demonstrated the effectiveness of the model compared with recent approaches on
both conventional and generalized ZSCL settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1"&gt;Parisa Kordjamshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Joyce Y. Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual and crosslingual speech recognition using phonological-vector based phone embeddings. (arXiv:2107.05038v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05038</id>
        <link href="http://arxiv.org/abs/2107.05038"/>
        <updated>2021-07-13T01:59:33.262Z</updated>
        <summary type="html"><![CDATA[The use of phonological features (PFs) potentially allows language-specific
phones to remain linked in training, which is highly desirable for information
sharing for multilingual and crosslingual speech recognition methods for
low-resourced languages. A drawback suffered by previous methods in using
phonological features is that the acoustic-to-PF extraction in a bottom-up way
is itself difficult. In this paper, we propose to join phonology driven phone
embedding (top-down) and deep neural network (DNN) based acoustic feature
extraction (bottom-up) to calculate phone probabilities. The new method is
called JoinAP (Joining of Acoustics and Phonology). Remarkably, no inversion
from acoustics to phonological features is required for speech recognition. For
each phone in the IPA (International Phonetic Alphabet) table, we encode its
phonological features to a phonological-vector, and then apply linear or
nonlinear transformation of the phonological-vector to obtain the phone
embedding. A series of multilingual and crosslingual (both zero-shot and
few-shot) speech recognition experiments are conducted on the CommonVoice
dataset (German, French, Spanish and Italian) and the AISHLL-1 dataset
(Mandarin), and demonstrate the superiority of JoinAP with nonlinear phone
embeddings over both JoinAP with linear phone embeddings and the traditional
method with flat phone embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chengrui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1"&gt;Keyu An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Huahuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1"&gt;Zhijian Ou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05133</id>
        <link href="http://arxiv.org/abs/2107.05133"/>
        <updated>2021-07-13T01:59:33.252Z</updated>
        <summary type="html"><![CDATA[The number of research articles in business and management has dramatically
increased along with terminology, constructs, and measures. Proper
classification of organizational performance constructs from research articles
plays an important role in categorizing the literature and understanding to
whom its research implications may be relevant. In this work, we classify
constructs (i.e., concepts and terminology used to capture different aspects of
organizational performance) in research articles into a three-level
categorization: (a) performance and non-performance categories (Level 0); (b)
for performance constructs, stakeholder group-level of performance concerning
investors, customers, employees, and the society (community and natural
environment) (Level 1); and (c) for each stakeholder group-level, subcategories
of different ways of measurement (Level 2). We observed that increasing
contextual information with features extracted from surrounding sentences and
external references improves classification of disaggregate-level labels, given
limited training data. Our research has implications for computer-assisted
construct identification and classification - an essential step for research
synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1"&gt;Seethalakshmi Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Victor Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_Powell_G/0/1/0/all/0/1"&gt;Gus Hahn-Powell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tirunagar_B/0/1/0/all/0/1"&gt;Bharadwaj Tirunagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States and Conversations. (arXiv:2107.05168v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05168</id>
        <link href="http://arxiv.org/abs/2107.05168"/>
        <updated>2021-07-13T01:59:33.246Z</updated>
        <summary type="html"><![CDATA[Most recently proposed approaches in dialogue state tracking (DST) leverage
the context and the last dialogue states to track current dialogue states,
which are often slot-value pairs. Although the context contains the complete
dialogue information, the information is usually indirect and even requires
reasoning to obtain. The information in the lastly predicted dialogue states is
direct, but when there is a prediction error, the dialogue information from
this source will be incomplete or erroneous. In this paper, we propose the
Dialogue State Tracking with Multi-Level Fusion of Predicted Dialogue States
and Conversations network (FPDSC). This model extracts information of each
dialogue turn by modeling interactions among each turn utterance, the
corresponding last dialogue states, and dialogue slots. Then the representation
of each dialogue turn is aggregated by a hierarchical structure to form the
passage information, which is utilized in the current turn of DST. Experimental
results validate the effectiveness of the fusion network with 55.03% and 59.07%
joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, which reaches the
state-of-the-art performance. Furthermore, we conduct the deleted-value and
related-slot experiments on MultiWOZ 2.1 to evaluate our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingyao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haipang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zehao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guodun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05002</id>
        <link href="http://arxiv.org/abs/2107.05002"/>
        <updated>2021-07-13T01:59:33.238Z</updated>
        <summary type="html"><![CDATA[Extractive Reading Comprehension (ERC) has made tremendous advances enabled
by the availability of large-scale high-quality ERC training data. Despite of
such rapid progress and widespread application, the datasets in languages other
than high-resource languages such as English remain scarce. To address this
issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by
modelling existing high-quality extractive reading comprehension datasets in a
multilingual environment. To be specific, we present multilingual adaptive
attention (MAA) to combine intra-attention and inter-attention to learn more
general generalizable semantic and lexical knowledge from each pair of language
families. Furthermore, to make full use of existing datasets, we adopt a new
training framework to train our model by calculating task-level similarities
between each existing dataset and target dataset. The experimental results show
that our XLTT model surpasses six baselines on two multilingual ERC benchmarks,
especially more effective for low-resource languages with 3.9 and 4.1 average
improvement in F1 and EM, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu1_B/0/1/0/all/0/1"&gt;Bin Xu1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuxin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fei Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bangchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongwen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dejie Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Formal context reduction in deriving concept hierarchies from corpora using adaptive evolutionary clustering algorithm star. (arXiv:2107.04781v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.04781</id>
        <link href="http://arxiv.org/abs/2107.04781"/>
        <updated>2021-07-13T01:59:33.228Z</updated>
        <summary type="html"><![CDATA[It is beneficial to automate the process of deriving concept hierarchies from
corpora since a manual construction of concept hierarchies is typically a
time-consuming and resource-intensive process. As such, the overall process of
learning concept hierarchies from corpora encompasses a set of steps: parsing
the text into sentences, splitting the sentences and then tokenising it. After
the lemmatisation step, the pairs are extracted using FCA. However, there might
be some uninteresting and erroneous pairs in the formal context. Generating
formal context may lead to a time-consuming process, so formal context size
reduction is required to remove uninterested and erroneous pairs, taking less
time to extract the concept lattice and concept hierarchies accordingly. In
this premise, this study aims to propose two frameworks: (1) A framework to
review the current process of deriving concept hierarchies from corpus
utilising FCA; (2) A framework to decrease the formal contexts ambiguity of the
first framework using an adaptive version of ECA*. Experiments are conducted by
applying 385 sample corpora from Wikipedia on the two frameworks to examine the
reducing size of formal context, which leads to yield concept lattice and
concept hierarchy. The resulting lattice of formal context is evaluated to the
standard one using concept lattice-invariants. Accordingly, the homomorphic
between the two lattices preserves the quality of resulting concept hierarchies
by 89% in contrast to the basic ones, and the reduced concept lattice inherits
the structural relation of the standard one. The adaptive ECA* is examined
against its four counterpart baseline algorithms to measure the execution time
on random datasets with different densities (fill ratios). The results show
that adaptive ECA* performs concept lattice faster than other mentioned
competitive techniques in different fill ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1"&gt;Bryar A. Hassan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirjalili_S/0/1/0/all/0/1"&gt;Seyedali Mirjalili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Axiomatic Explanations for Neural Ranking Models. (arXiv:2106.08019v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08019</id>
        <link href="http://arxiv.org/abs/2106.08019"/>
        <updated>2021-07-13T01:59:33.207Z</updated>
        <summary type="html"><![CDATA[Recently, neural networks have been successfully employed to improve upon
state-of-the-art performance in ad-hoc retrieval tasks via machine-learned
ranking functions. While neural retrieval models grow in complexity and impact,
little is understood about their correspondence with well-studied IR
principles. Recent work on interpretability in machine learning has provided
tools and techniques to understand neural models in general, yet there has been
little progress towards explaining ranking models.

We investigate whether one can explain the behavior of neural ranking models
in terms of their congruence with well understood principles of document
ranking by using established theories from axiomatic IR. Axiomatic analysis of
information retrieval models has formalized a set of constraints on ranking
decisions that reasonable retrieval models should fulfill. We operationalize
this axiomatic thinking to reproduce rankings based on combinations of
elementary constraints. This allows us to investigate to what extent the
ranking decisions of neural rankers can be explained in terms of retrieval
axioms, and which axioms apply in which situations. Our experimental study
considers a comprehensive set of axioms over several representative neural
rankers. While the existing axioms can already explain the particularly
confident ranking decisions rather well, future work should extend the axiom
set to also cover the other still "unexplainable" neural IR rank decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volske_M/0/1/0/all/0/1"&gt;Michael V&amp;#xf6;lske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bondarenko_A/0/1/0/all/0/1"&gt;Alexander Bondarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frobe_M/0/1/0/all/0/1"&gt;Maik Fr&amp;#xf6;be&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1"&gt;Matthias Hagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1"&gt;Benno Stein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04734</id>
        <link href="http://arxiv.org/abs/2107.04734"/>
        <updated>2021-07-13T01:59:33.186Z</updated>
        <summary type="html"><![CDATA[Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1"&gt;Ankita Pasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1"&gt;Ju-Chieh Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1"&gt;Karen Livescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatentMiner: Patent Vacancy Mining via Context-enhanced and Knowledge-guided Graph Attention. (arXiv:2107.04880v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04880</id>
        <link href="http://arxiv.org/abs/2107.04880"/>
        <updated>2021-07-13T01:59:33.177Z</updated>
        <summary type="html"><![CDATA[Although there are a small number of work to conduct patent research by
building knowledge graph, but without constructing patent knowledge graph using
patent documents and combining latest natural language processing methods to
mine hidden rich semantic relationships in existing patents and predict new
possible patents. In this paper, we propose a new patent vacancy prediction
approach named PatentMiner to mine rich semantic knowledge and predict new
potential patents based on knowledge graph (KG) and graph attention mechanism.
Firstly, patent knowledge graph over time (e.g. year) is constructed by
carrying out named entity recognition and relation extrac-tion from patent
documents. Secondly, Common Neighbor Method (CNM), Graph Attention Networks
(GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to
perform link prediction in the constructed knowledge graph to dig out the
potential triples. Finally, patents are defined on the knowledge graph by means
of co-occurrence relationship, that is, each patent is represented as a fully
connected subgraph containing all its entities and co-occurrence relationships
of the patent in the knowledge graph; Furthermore, we propose a new patent
prediction task which predicts a fully connected subgraph with newly added
prediction links as a new pa-tent. The experimental results demonstrate that
our proposed patent predic-tion approach can correctly predict new patents and
Context-enhanced Graph Attention Networks is much better than the baseline.
Meanwhile, our proposed patent vacancy prediction task still has significant
room to im-prove.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1"&gt;Gaochen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yuxin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fei Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bangchang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hongwen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Dejie Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Evolutionary Computation Help us to Crib the Voynich Manuscript ?. (arXiv:2107.05381v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.05381</id>
        <link href="http://arxiv.org/abs/2107.05381"/>
        <updated>2021-07-13T01:59:33.170Z</updated>
        <summary type="html"><![CDATA[Departing from the postulate that Voynich Manuscript is not a hoax but rather
encodes authentic contents, our article presents an evolutionary algorithm
which aims to find the most optimal mapping between voynichian glyphs and
candidate phonemic values. Core component of the decoding algorithm is a
process of maximization of a fitness function which aims to find most optimal
set of substitution rules allowing to transcribe the part of the manuscript --
which we call the Calendar -- into lists of feminine names. This leads to sets
of character subsitution rules which allow us to consistently transcribe dozens
among three hundred calendar tokens into feminine names: a result far
surpassing both ``popular'' as well as "state of the art" tentatives to crack
the manuscript. What's more, by using name lists stemming from different
languages as potential cribs, our ``adaptive'' method can also be useful in
identification of the language in which the manuscript is written.

As far as we can currently tell, results of our experiments indicate that the
Calendar part of the manuscript contains names from baltoslavic, balkanic or
hebrew language strata. Two further indications are also given: primo, highest
fitness values were obtained when the crib list contains names with specific
infixes at token's penultimate position as is the case, for example, for slavic
\textbf{feminine diminutives} (i.e. names ending with -ka and not -a). In the
most successful scenario, 240 characters contained in 35 distinct Voynichese
tokens were successfully transcribed. Secundo, in case of crib stemming from
Hebrew language, whole adaptation process converges to significantly better
fitness values when transcribing voynichian tokens whose order of individual
characters have been reversed, and when lists feminine and not masculine names
are used as the crib.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hromada_D/0/1/0/all/0/1"&gt;Daniel Devatman Hromada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation. (arXiv:2107.05366v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05366</id>
        <link href="http://arxiv.org/abs/2107.05366"/>
        <updated>2021-07-13T01:59:33.155Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation (SBR) learns users' preferences by capturing the
short-term and sequential patterns from the evolution of user behaviors. Among
the studies in the SBR field, graph-based approaches are a relatively powerful
kind of way, which generally extract item information by message aggregation
under Euclidean space. However, such methods can't effectively extract the
hierarchical information contained among consecutive items in a session, which
is critical to represent users' preferences. In this paper, we present a
hyperbolic contrastive graph recommender (HCGR), a principled session-based
recommendation framework involving Lorentz hyperbolic space to adequately
capture the coherence and hierarchical representations of the items. Within
this framework, we design a novel adaptive hyperbolic attention computation to
aggregate the graph message of each user's preference in a session-based
behavior sequence. In addition, contrastive learning is leveraged to optimize
the item representation by considering the geodesic distance between positive
and negative samples in hyperbolic space. Extensive experiments on four
real-world datasets demonstrate that HCGR consistently outperforms
state-of-the-art baselines by 0.43$\%$-28.84$\%$ in terms of $HitRate$, $NDCG$
and $MRR$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1"&gt;Naicheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaoshuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qiongxu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1"&gt;Kaixin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaobo Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Initial Investigation of Non-Native Spoken Question-Answering. (arXiv:2107.04691v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04691</id>
        <link href="http://arxiv.org/abs/2107.04691"/>
        <updated>2021-07-13T01:59:33.140Z</updated>
        <summary type="html"><![CDATA[Text-based machine comprehension (MC) systems have a wide-range of
applications, and standard corpora exist for developing and evaluating
approaches. There has been far less research on spoken question answering (SQA)
systems. The SQA task considered in this paper is to extract the answer from a
candidate$\text{'}$s spoken response to a question in a prompt-response style
language assessment test. Applying these MC approaches to this SQA task rather
than, for example, off-topic response detection provides far more detailed
information that can be used for further downstream processing. One significant
challenge is the lack of appropriately annotated speech corpora to train
systems for this task. Hence, a transfer-learning style approach is adopted
where a system trained on text-based MC is evaluated on an SQA task with
non-native speakers. Mismatches must be considered between text documents and
spoken responses; non-native spoken grammar and written grammar. In practical
SQA, ASR systems are used, necessitating an investigation of the impact of ASR
errors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0
transfers well for SQA. It is found that there is an approximately linear
relationship between ASR errors and the SQA assessment scores but grammar
mismatches have minimal impact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raina_V/0/1/0/all/0/1"&gt;Vatsal Raina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1"&gt;Mark J.F. Gales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.16102</id>
        <link href="http://arxiv.org/abs/2106.16102"/>
        <updated>2021-07-13T01:59:33.132Z</updated>
        <summary type="html"><![CDATA[The volume of scientific publications in organizational research becomes
exceedingly overwhelming for human researchers who seek to timely extract and
review knowledge. This paper introduces natural language processing (NLP)
models to accelerate the discovery, extraction, and organization of theoretical
developments (i.e., hypotheses) from social science publications. We illustrate
and evaluate NLP models in the context of a systematic review of stakeholder
value constructs and hypotheses. Specifically, we develop NLP models to
automatically 1) detect sentences in scholarly documents as hypotheses or not
(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)
and links (causal/associative relationships) (Relationship Deconstruction ),
and 3) classify the features of links in terms causality (versus association)
and direction (positive, negative, versus nonlinear) (Feature Classification).
Our models have reported high performance metrics for all three tasks. While
our models are built in Python, we have made the pre-trained models fully
accessible for non-programmers. We have provided instructions on installing and
using our pre-trained models via an R Shiny app graphic user interface (GUI).
Finally, we suggest the next paths to extend our methodology for
computer-assisted knowledge synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1"&gt;Victor Zitian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1"&gt;Felipe Montano-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1"&gt;Wlodek Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1"&gt;Evan Canfield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparative analysis of word embeddings in assessing semantic similarity of complex sentences. (arXiv:2010.12637v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12637</id>
        <link href="http://arxiv.org/abs/2010.12637"/>
        <updated>2021-07-13T01:59:33.120Z</updated>
        <summary type="html"><![CDATA[Semantic textual similarity is one of the open research challenges in the
field of Natural Language Processing. Extensive research has been carried out
in this field and near-perfect results are achieved by recent transformer-based
models in existing benchmark datasets like the STS dataset and the SICK
dataset. In this paper, we study the sentences in these datasets and analyze
the sensitivity of various word embeddings with respect to the complexity of
the sentences. We build a complex sentences dataset comprising of 50 sentence
pairs with associated semantic similarity values provided by 15 human
annotators. Readability analysis is performed to highlight the increase in
complexity of the sentences in the existing benchmark datasets and those in the
proposed dataset. Further, we perform a comparative analysis of the performance
of various word embeddings and language models on the existing benchmark
datasets and the proposed dataset. The results show the increase in complexity
of the sentences has a significant impact on the performance of the embedding
models resulting in a 10-20% decrease in Pearson's and Spearman's correlation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_D/0/1/0/all/0/1"&gt;Dhivya Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1"&gt;Vijay Mago&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Training Improves E2E ASR for the Edge. (arXiv:2107.04677v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04677</id>
        <link href="http://arxiv.org/abs/2107.04677"/>
        <updated>2021-07-13T01:59:33.110Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) has become increasingly ubiquitous on
modern edge devices. Past work developed streaming End-to-End (E2E) all-neural
speech recognizers that can run compactly on edge devices. However, E2E ASR
models are prone to overfitting and have difficulties in generalizing to unseen
testing data. Various techniques have been proposed to regularize the training
of ASR models, including layer normalization, dropout, spectrum data
augmentation and speed distortions in the inputs. In this work, we present a
simple yet effective noisy training strategy to further improve the E2E ASR
model training. By introducing random noise to the parameter space during
training, our method can produce smoother models at convergence that generalize
better. We apply noisy training to improve both dense and sparse
state-of-the-art Emformer models and observe consistent WER reduction.
Specifically, when training Emformers with 90% sparsity, we achieve 12% and 14%
WER improvements on the LibriSpeech Test-other and Test-clean data set,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dilin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1"&gt;Yuan Shangguan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haichuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1"&gt;Pierce Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiatong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Meng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1"&gt;Ozlem Kalinli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1"&gt;Vikas Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing Data Efficiency in Task-Oriented Semantic Parsing. (arXiv:2107.04736v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04736</id>
        <link href="http://arxiv.org/abs/2107.04736"/>
        <updated>2021-07-13T01:59:33.084Z</updated>
        <summary type="html"><![CDATA[Data efficiency, despite being an attractive characteristic, is often
challenging to measure and optimize for in task-oriented semantic parsing;
unlike exact match, it can require both model- and domain-specific setups,
which have, historically, varied widely across experiments. In our work, as a
step towards providing a unified solution to data-efficiency-related questions,
we introduce a four-stage protocol which gives an approximate measure of how
much in-domain, "target" data a parser requires to achieve a certain quality
bar. Specifically, our protocol consists of (1) sampling target subsets of
different cardinalities, (2) fine-tuning parsers on each subset, (3) obtaining
a smooth curve relating target subset (%) vs. exact match (%), and (4)
referencing the curve to mine ad-hoc (target subset, exact match) points. We
apply our protocol in two real-world case studies -- model generalizability and
intent complexity -- illustrating its flexibility and applicability to
practitioners in task-oriented semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Shrey Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Akshat Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rill_J/0/1/0/all/0/1"&gt;Justin Rill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_B/0/1/0/all/0/1"&gt;Brian Moran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleem_S/0/1/0/all/0/1"&gt;Safiyyah Saleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1"&gt;Alexander Zotov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1"&gt;Ahmed Aly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Stability Regularization for Improving BERT Fine-tuning. (arXiv:2107.04835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04835</id>
        <link href="http://arxiv.org/abs/2107.04835"/>
        <updated>2021-07-13T01:59:33.076Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pre-trained language models such as BERT has become a common
practice dominating leaderboards across various NLP tasks. Despite its recent
success and wide adoption, this process is unstable when there are only a small
number of training samples available. The brittleness of this process is often
reflected by the sensitivity to random seeds. In this paper, we propose to
tackle this problem based on the noise stability property of deep nets, which
is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020).
Specifically, we introduce a novel and effective regularization method to
improve fine-tuning on NLP tasks, referred to as Layer-wise Noise Stability
Regularization (LNSR). We extend the theories about adding noise to the input
and prove that our method gives a stabler regularization effect. We provide
supportive evidence by experimentally confirming that well-performing models
show a low sensitivity to noise and fine-tuning with LNSR exhibits clearly
higher generalizability and stability. Furthermore, our method also
demonstrates advantages over other state-of-the-art algorithms including L2-SP
(Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1"&gt;Hang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xingjian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cheng-Zhong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Three Phase Semantic Web Matchmaker. (arXiv:2107.05368v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05368</id>
        <link href="http://arxiv.org/abs/2107.05368"/>
        <updated>2021-07-13T01:59:32.950Z</updated>
        <summary type="html"><![CDATA[Since using environments that are made according to the service oriented
architecture, we have more effective and dynamic applications. Semantic
matchmaking process is finding valuable service candidates for substitution. It
is a very important aspect of using semantic Web Services. Our proposed
matchmaker algorithm performs semantic matching of Web Services on the basis of
input and output descriptions of semantic Web Services matching. This technique
takes advantages from a graph structure and flow networks. Our novel approach
is assigning matchmaking scores to semantics of the inputs and outputs
parameters and their types. It makes a flow network in which the weights of the
edges are these scores, using FordFulkerson algorithm, we find matching rate of
two web services. So, all services should be described in the same Ontology Web
Language. Among these candidates, best one is chosen for substitution in the
case of an execution failure. Our approach uses the algorithm that has the
least running time among all others that can be used for bipartite matching.
The importance of problem is that in real systems, many fundamental problems
will occur by late answering. So system`s service should always be on and if
one of them crashes, it would be replaced fast. Semantic web matchmaker eases
this process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_G/0/1/0/all/0/1"&gt;Golsa Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamanifar_K/0/1/0/all/0/1"&gt;Kamran Zamanifar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sliding Spectrum Decomposition for Diversified Recommendation. (arXiv:2107.05204v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05204</id>
        <link href="http://arxiv.org/abs/2107.05204"/>
        <updated>2021-07-13T01:59:32.926Z</updated>
        <summary type="html"><![CDATA[Content feed, a type of product that recommends a sequence of items for users
to browse and engage with, has gained tremendous popularity among social media
platforms. In this paper, we propose to study the diversity problem in such a
scenario from an item sequence perspective using time series analysis
techniques. We derive a method called sliding spectrum decomposition (SSD) that
captures users' perception of diversity in browsing a long item sequence. We
also share our experiences in designing and implementing a suitable item
embedding method for accurate similarity measurement under long tail effect.
Combined together, they are now fully implemented and deployed in Xiaohongshu
App's production recommender system that serves the main Explore Feed product
for tens of millions of users every day. We demonstrate the effectiveness and
efficiency of the method through theoretical analysis, offline experiments and
online A/B tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yanhua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weikun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruiwen Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05315</id>
        <link href="http://arxiv.org/abs/2107.05315"/>
        <updated>2021-07-13T01:59:32.896Z</updated>
        <summary type="html"><![CDATA[Recommending cold-start items is a long-standing and fundamental challenge in
recommender systems. Without any historical interaction on cold-start items, CF
scheme fails to use collaborative signals to infer user preference on these
items. To solve this problem, extensive studies have been conducted to
incorporate side information into the CF scheme. Specifically, they employ
modern neural network techniques (e.g., dropout, consistency constraint) to
discover and exploit the coalition effect of content features and collaborative
representations. However, we argue that these works less explore the mutual
dependencies between content features and collaborative representations and
lack sufficient theoretical supports, thus resulting in unsatisfactory
performance. In this work, we reformulate the cold-start item representation
learning from an information-theoretic standpoint. It aims to maximize the
mutual dependencies between item content and collaborative signals.
Specifically, the representation learning is theoretically lower-bounded by the
integration of two terms: mutual information between collaborative embeddings
of users and items, and mutual information between collaborative embeddings and
feature representations of items. To model such a learning process, we devise a
new objective function founded upon contrastive learning and develop a simple
yet effective Contrastive Learning-based Cold-start Recommendation
framework(CLCRec). In particular, CLCRec consists of three components:
contrastive pair organization, contrastive embedding, and contrastive
optimization modules. It allows us to preserve collaborative signals in the
content representations for both warm and cold-start items. Through extensive
experiments on four publicly accessible datasets, we observe that CLCRec
achieves significant improvements over state-of-the-art approaches in both
warm- and cold-start scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yinwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Recommender Systems to Depolarize. (arXiv:2107.04953v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04953</id>
        <link href="http://arxiv.org/abs/2107.04953"/>
        <updated>2021-07-13T01:59:32.859Z</updated>
        <summary type="html"><![CDATA[Polarization is implicated in the erosion of democracy and the progression to
violence, which makes the polarization properties of large algorithmic content
selection systems (recommender systems) a matter of concern for peace and
security. While algorithm-driven social media does not seem to be a primary
driver of polarization at the country level, it could be a useful intervention
point in polarized societies. This paper examines algorithmic depolarization
interventions with the goal of conflict transformation: not suppressing or
eliminating conflict but moving towards more constructive conflict. Algorithmic
intervention is considered at three stages: which content is available
(moderation), how content is selected and personalized (ranking), and content
presentation and controls (user interface). Empirical studies of online
conflict suggest that the exposure diversity intervention proposed as an
antidote to "filter bubbles" can be improved and can even worsen polarization
under some conditions. Using civility metrics in conjunction with diversity in
content selection may be more effective. However, diversity-based interventions
have not been tested at scale and may not work in the diverse and dynamic
contexts of real platforms. Instead, intervening in platform polarization
dynamics will likely require continuous monitoring of polarization metrics,
such as the widely used "feeling thermometer." These metrics can be used to
evaluate product features, and potentially engineered as algorithmic
objectives. It may further prove necessary to include polarization measures in
the objective functions of recommender algorithms to prevent optimization
processes from creating conflict as a side effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stray_J/0/1/0/all/0/1"&gt;Jonathan Stray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations. (arXiv:2107.05235v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05235</id>
        <link href="http://arxiv.org/abs/2107.05235"/>
        <updated>2021-07-13T01:59:32.850Z</updated>
        <summary type="html"><![CDATA[Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liwei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yutao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yanbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Deyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v4 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09883</id>
        <link href="http://arxiv.org/abs/2008.09883"/>
        <updated>2021-07-13T01:59:32.823Z</updated>
        <summary type="html"><![CDATA[Obesity is known to lower the quality of life substantially. It is often
associated with increased chances of non-communicable diseases such as
diabetes, cardiovascular problems, various cancers, etc. Evidence suggests that
diet-related mobile applications play a vital role in assisting individuals in
making healthier choices and keeping track of food intake. However, due to an
abundance of similar applications, it becomes pertinent to evaluate each of
them in terms of functionality, usability, and possible design issues to truly
determine state-of-the-art solutions for the future. Since these applications
involve implementing multiple user requirements and recommendations from
different dietitians, the evaluation becomes quite complex. Therefore, this
study aims to review existing dietary applications at length to highlight key
features and problems that enhance or undermine an application's usability. For
this purpose, we have examined the published literature from various scientific
databases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct
(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six
primary studies met our inclusion criteria after identification, screening,
eligibility and full-text evaluation. We analyzed 35 apps from the selected
studies and extracted the data of each of the identified apps.Following our
detailed analysis on the comprehensiveness of freely available mHealth
applications, we specified potential future research challenges and stated
recommendations to help grow clinically accurate diet-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Ahmed Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1"&gt;Foong Ming Moy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1"&gt;Nadine Kong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding. (arXiv:2107.05223v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.05223</id>
        <link href="http://arxiv.org/abs/2107.05223"/>
        <updated>2021-07-13T01:59:32.810Z</updated>
        <summary type="html"><![CDATA[This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yi-Hui Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1"&gt;I-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chin-Jui Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMSys'21 Grand Challenge on Detecting Cheapfakes. (arXiv:2107.05297v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.05297</id>
        <link href="http://arxiv.org/abs/2107.05297"/>
        <updated>2021-07-13T01:59:32.800Z</updated>
        <summary type="html"><![CDATA[Cheapfake is a recently coined term that encompasses non-AI ("cheap")
manipulations of multimedia content. Cheapfakes are known to be more prevalent
than deepfakes. Cheapfake media can be created using editing software for
image/video manipulations, or even without using any software, by simply
altering the context of an image/video by sharing the media alongside
misleading claims. This alteration of context is referred to as out-of-context
(OOC) misuse} of media. OOC media is much harder to detect than fake media,
since the images and videos are not tampered. In this challenge, we focus on
detecting OOC images, and more specifically the misuse of real photographs with
conflicting image captions in news items. The aim of this challenge is to
develop and benchmark models that can be used to detect whether given samples
(news image and associated captions) are OOC, based on the recently compiled
COSMOS dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1"&gt;Shivangi Aneja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Midoglu_C/0/1/0/all/0/1"&gt;Cise Midoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_Nguyen_D/0/1/0/all/0/1"&gt;Duc-Tien Dang-Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael Alexander Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;Paal Halvorsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Niessner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1"&gt;Balu Adsumilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bregler_C/0/1/0/all/0/1"&gt;Chris Bregler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inductive Representation Based Graph Convolution Network for Collaborative Filtering. (arXiv:2107.05247v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05247</id>
        <link href="http://arxiv.org/abs/2107.05247"/>
        <updated>2021-07-13T01:59:32.788Z</updated>
        <summary type="html"><![CDATA[In recent years, graph neural networks (GNNs) have shown powerful ability in
collaborative filtering, which is a widely adopted recommendation scenario.
While without any side information, existing graph neural network based methods
generally learn a one-hot embedding for each user or item as the initial input
representation of GNNs. However, such one-hot embedding is intrinsically
transductive, making these methods with no inductive ability, i.e., failing to
deal with new users or new items that are unseen during training. Besides, the
number of model parameters depends on the number of users and items, which is
expensive and not scalable. In this paper, we give a formal definition of
inductive recommendation and solve the above problems by proposing Inductive
representation based Graph Convolutional Network (IGCN) for collaborative
filtering. Specifically, we design an inductive representation layer, which
utilizes the interaction behavior with core users or items as the initial
representation, improving the general recommendation performance while bringing
inductive ability. Note that, the number of parameters of IGCN only depends on
the number of core users or items, which is adjustable and scalable. Extensive
experiments on three public benchmarks demonstrate the state-of-the-art
performance of IGCN in both transductive and inductive recommendation
scenarios, while with remarkably fewer model parameters. Our implementations
are available here in PyTorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huawei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1"&gt;Shuchang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Accurate Localization by Instance Search. (arXiv:2107.05005v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05005</id>
        <link href="http://arxiv.org/abs/2107.05005"/>
        <updated>2021-07-13T01:59:32.764Z</updated>
        <summary type="html"><![CDATA[Visual object localization is the key step in a series of object detection
tasks. In the literature, high localization accuracy is achieved with the
mainstream strongly supervised frameworks. However, such methods require
object-level annotations and are unable to detect objects of unknown
categories. Weakly supervised methods face similar difficulties. In this paper,
a self-paced learning framework is proposed to achieve accurate object
localization on the rank list returned by instance search. The proposed
framework mines the target instance gradually from the queries and their
corresponding top-ranked search results. Since a common instance is shared
between the query and the images in the rank list, the target visual instance
can be accurately localized even without knowing what the object category is.
In addition to performing localization on instance search, the issue of
few-shot object detection is also addressed under the same framework. Superior
performance over state-of-the-art methods is observed on both tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi-Geng Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Hui-Chu Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wan-Lei Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Similarity Guided Deep Face Image Retrieval. (arXiv:2107.05025v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.05025</id>
        <link href="http://arxiv.org/abs/2107.05025"/>
        <updated>2021-07-13T01:59:32.754Z</updated>
        <summary type="html"><![CDATA[Face image retrieval, which searches for images of the same identity from the
query input face image, is drawing more attention as the size of the image
database increases rapidly. In order to conduct fast and accurate retrieval, a
compact hash code-based methods have been proposed, and recently, deep face
image hashing methods with supervised classification training have shown
outstanding performance. However, classification-based scheme has a
disadvantage in that it cannot reveal complex similarities between face images
into the hash code learning. In this paper, we attempt to improve the face
image retrieval quality by proposing a Similarity Guided Hashing (SGH) method,
which gently considers self and pairwise-similarity simultaneously. SGH employs
various data augmentations designed to explore elaborate similarities between
face images, solving both intra and inter identity-wise difficulties. Extensive
experimental results on the protocols with existing benchmarks and an
additionally proposed large scale higher resolution face image dataset
demonstrate that our SGH delivers state-of-the-art retrieval performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young Kyun Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1"&gt;Nam Ik Cho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. (arXiv:2107.05124v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05124</id>
        <link href="http://arxiv.org/abs/2107.05124"/>
        <updated>2021-07-13T01:59:32.739Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation is an important task for e-commerce services,
where a large number of users browse anonymously or may have very distinct
interests for different sessions. In this paper we present one of the winning
solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce
Data Challenge. Our solution was inspired by NLP techniques and consists of an
ensemble of two Transformer architectures - Transformer-XL and XLNet - trained
with autoregressive and autoencoding approaches. To leverage most of the rich
dataset made available for the competition, we describe how we prepared
multi-model features by combining tabular events with textual and image
vectors. We also present a model prediction analysis to better understand the
effectiveness of our architectures for the session-based recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1"&gt;Gabriel de Souza P. Moreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabhi_S/0/1/0/all/0/1"&gt;Sara Rabhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ak_R/0/1/0/all/0/1"&gt;Ronay Ak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1"&gt;Md Yasin Kabir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oldridge_E/0/1/0/all/0/1"&gt;Even Oldridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SVP-CF: Selection via Proxy for Collaborative Filtering Data. (arXiv:2107.04984v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04984</id>
        <link href="http://arxiv.org/abs/2107.04984"/>
        <updated>2021-07-13T01:59:32.720Z</updated>
        <summary type="html"><![CDATA[We study the practical consequences of dataset sampling strategies on the
performance of recommendation algorithms. Recommender systems are generally
trained and evaluated on samples of larger datasets. Samples are often taken in
a naive or ad-hoc fashion: e.g. by sampling a dataset randomly or by selecting
users or items with many interactions. As we demonstrate, commonly-used data
sampling schemes can have significant consequences on algorithm performance --
masking performance deficiencies in algorithms or altering the relative
performance of algorithms, as compared to models trained on the complete
dataset. Following this observation, this paper makes the following main
contributions: (1) characterizing the effect of sampling on algorithm
performance, in terms of algorithm and dataset characteristics (e.g. sparsity
characteristics, sequential dynamics, etc.); and (2) designing SVP-CF, which is
a data-specific sampling strategy, that aims to preserve the relative
performance of models after sampling, and is especially suited to long-tail
interaction data. Detailed experiments show that SVP-CF is more accurate than
commonly used sampling schemes in retaining the relative ranking of different
recommendation algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Propagation-aware Social Recommendation by Transfer Learning. (arXiv:2107.04846v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.04846</id>
        <link href="http://arxiv.org/abs/2107.04846"/>
        <updated>2021-07-13T01:59:32.708Z</updated>
        <summary type="html"><![CDATA[Social-aware recommendation approaches have been recognized as an effective
way to solve the data sparsity issue of traditional recommender systems. The
assumption behind is that the knowledge in social user-user connections can be
shared and transferred to the domain of user-item interactions, whereby to help
learn user preferences. However, most existing approaches merely adopt the
first-order connections among users during transfer learning, ignoring those
connections in higher orders. We argue that better recommendation performance
can also benefit from high-order social relations. In this paper, we propose a
novel Propagation-aware Transfer Learning Network (PTLN) based on the
propagation of social relations. We aim to better mine the sharing knowledge
hidden in social networks and thus further improve recommendation performance.
Specifically, we explore social influence in two aspects: (a) higher-order
friends have been taken into consideration by order bias; (b) different friends
in the same order will have distinct importance for recommendation by an
attention mechanism. Besides, we design a novel regularization to bridge the
gap between social relations and user-item interactions. We conduct extensive
experiments on two real-world datasets and beat other counterparts in terms of
ranking accuracy, especially for the cold-start users with few historical
interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Haodong Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1"&gt;Yabo Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Cold-Start Recommendation. (arXiv:2107.05315v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2107.05315</id>
        <link href="http://arxiv.org/abs/2107.05315"/>
        <updated>2021-07-13T01:59:32.618Z</updated>
        <summary type="html"><![CDATA[Recommending cold-start items is a long-standing and fundamental challenge in
recommender systems. Without any historical interaction on cold-start items, CF
scheme fails to use collaborative signals to infer user preference on these
items. To solve this problem, extensive studies have been conducted to
incorporate side information into the CF scheme. Specifically, they employ
modern neural network techniques (e.g., dropout, consistency constraint) to
discover and exploit the coalition effect of content features and collaborative
representations. However, we argue that these works less explore the mutual
dependencies between content features and collaborative representations and
lack sufficient theoretical supports, thus resulting in unsatisfactory
performance. In this work, we reformulate the cold-start item representation
learning from an information-theoretic standpoint. It aims to maximize the
mutual dependencies between item content and collaborative signals.
Specifically, the representation learning is theoretically lower-bounded by the
integration of two terms: mutual information between collaborative embeddings
of users and items, and mutual information between collaborative embeddings and
feature representations of items. To model such a learning process, we devise a
new objective function founded upon contrastive learning and develop a simple
yet effective Contrastive Learning-based Cold-start Recommendation
framework(CLCRec). In particular, CLCRec consists of three components:
contrastive pair organization, contrastive embedding, and contrastive
optimization modules. It allows us to preserve collaborative signals in the
content representations for both warm and cold-start items. Through extensive
experiments on four publicly accessible datasets, we observe that CLCRec
achieves significant improvements over state-of-the-art approaches in both
warm- and cold-start scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yinwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Liqiang Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanping Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data. (arXiv:2107.04954v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04954</id>
        <link href="http://arxiv.org/abs/2107.04954"/>
        <updated>2021-07-13T01:59:32.430Z</updated>
        <summary type="html"><![CDATA[Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheuk_K/0/1/0/all/0/1"&gt;Kin Wai Cheuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Li Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer Vision-assisted Decimeter-level Single-antenna RSSI Localization Harnessing Dynamic Blockage Events. (arXiv:2107.04770v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04770</id>
        <link href="http://arxiv.org/abs/2107.04770"/>
        <updated>2021-07-13T01:59:32.408Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates the feasibility of received power strength indicator
(RSSI)-based single-antenna localization (R-SAL) with decimeter-level
localization accuracy. To achieve decimeter-level accuracy, either fine-grained
radio frequency (RF) information (e.g., channel state information) or
coarse-grained RF information (e.g., RSSI) from more than multiple antennas is
required. Meanwhile, owing to deficiency of single-antenna RSSI which only
indicates a distance between a receiver and a transmitter, realizing
fine-grained localization accuracy with single coarse-grained RF information is
challenging. Our key idea to address this challenge is to leverage computer
vision (CV) and to estimate the most likely Fresnel zone between the receiver
and transmitter, where the role of RSSI is to detect blockage timings.
Specifically, historical positions of an obstacle that dynamically blocks the
Fresnel zone are detected by the CV technique, and we estimate positions at
which a blockage starts and ends via a time series of RSSI. These estimated
obstacle positions, in principle, coincide with points on the Fresnel zone
boundaries, enabling the estimation of the Fresnel zone and localization of the
transmitter. The experimental evaluation revealed that the proposed R-SAL
achieved decimeter-level localization in an indoor environment, which is
comparable to that of a simple previous RSSI-based localization with three
receivers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sunami_T/0/1/0/all/0/1"&gt;Tomoya Sunami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itahara_S/0/1/0/all/0/1"&gt;Sohei Itahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koda_Y/0/1/0/all/0/1"&gt;Yusuke Koda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nishio_T/0/1/0/all/0/1"&gt;Takayuki Nishio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamamoto_K/0/1/0/all/0/1"&gt;Koji Yamamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering. (arXiv:2107.04768v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04768</id>
        <link href="http://arxiv.org/abs/2107.04768"/>
        <updated>2021-07-13T01:59:32.384Z</updated>
        <summary type="html"><![CDATA[Video question answering is a challenging task, which requires agents to be
able to understand rich video contents and perform spatial-temporal reasoning.
However, existing graph-based methods fail to perform multi-step reasoning
well, neglecting two properties of VideoQA: (1) Even for the same video,
different questions may require different amount of video clips or objects to
infer the answer with relational reasoning; (2) During reasoning, appearance
and motion features have complicated interdependence which are correlated and
complementary to each other. Based on these observations, we propose a
Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an
end-to-end fashion. The first contribution of our DualVGR is the design of an
explainable Query Punishment Module, which can filter out irrelevant visual
features through multiple cycles of reasoning. The second contribution is the
proposed Video-based Multi-view Graph Attention Network, which captures the
relations between appearance and motion features. Our DualVGR network achieves
state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and
demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is
available at https://github.com/MMIR/DualVGR-VideoQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1"&gt;Bing-Kun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Classification and Detection of Bird Sounds in the Wild. A BirdCLEF 2021 Solution. (arXiv:2107.04878v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04878</id>
        <link href="http://arxiv.org/abs/2107.04878"/>
        <updated>2021-07-13T01:59:32.340Z</updated>
        <summary type="html"><![CDATA[It is easier to hear birds than see them, however, they still play an
essential role in nature and they are excellent indicators of deteriorating
environmental quality and pollution. Recent advances in Machine Learning and
Convolutional Neural Networks allow us to detect and classify bird sounds, by
doing this, we can assist researchers in monitoring the status and trends of
bird populations and biodiversity in ecosystems. We propose a sound detection
and classification pipeline for analyzing complex soundscape recordings and
identify birdcalls in the background. Our pipeline learns from weak labels,
classifies fine-grained bird vocalizations in the wild, and is robust against
background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th
place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shubham_K/0/1/0/all/0/1"&gt;Kumar Shubham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agnihotri_P/0/1/0/all/0/1"&gt;Prateek Agnihotri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Movva_N/0/1/0/all/0/1"&gt;Nitin D. Movva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bessenyei_S/0/1/0/all/0/1"&gt;Szilard Bessenyei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Sensor Fusion Algorithms Against Voice Command Attacks in Autonomous Vehicles. (arXiv:2104.09872v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09872</id>
        <link href="http://arxiv.org/abs/2104.09872"/>
        <updated>2021-07-12T01:55:17.162Z</updated>
        <summary type="html"><![CDATA[With recent advances in autonomous driving, Voice Control Systems have become
increasingly adopted as human-vehicle interaction methods. This technology
enables drivers to use voice commands to control the vehicle and will be soon
available in Advanced Driver Assistance Systems (ADAS). Prior work has shown
that Siri, Alexa and Cortana, are highly vulnerable to inaudible command
attacks. This could be extended to ADAS in real-world applications and such
inaudible command threat is difficult to detect due to microphone
nonlinearities. In this paper, we aim to develop a more practical solution by
using camera views to defend against inaudible command attacks where ADAS are
capable of detecting their environment via multi-sensors. To this end, we
propose a novel multimodal deep learning classification system to defend
against inaudible command attacks. Our experimental results confirm the
feasibility of the proposed defense methods and the best classification
accuracy reaches 89.2%. Code is available at
https://github.com/ITSEG-MQ/Sensor-Fusion-Against-VoiceCommand-Attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiwei Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jolfa_A/0/1/0/all/0/1"&gt;Alireza Jolfa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adaptation to Label Distribution Shift. (arXiv:2107.04520v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04520</id>
        <link href="http://arxiv.org/abs/2107.04520"/>
        <updated>2021-07-12T01:55:17.137Z</updated>
        <summary type="html"><![CDATA[Machine learning models often encounter distribution shifts when deployed in
the real world. In this paper, we focus on adaptation to label distribution
shift in the online setting, where the test-time label distribution is
continually changing and the model must dynamically adapt to it without
observing the true label. Leveraging a novel analysis, we show that the lack of
true label does not hinder estimation of the expected test loss, which enables
the reduction of online label shift adaptation to conventional online learning.
Informed by this observation, we propose adaptation algorithms inspired by
classical online learning techniques such as Follow The Leader (FTL) and Online
Gradient Descent (OGD) and derive their regret bounds. We empirically verify
our findings under both simulated and real world label distribution shifts and
show that OGD is particularly effective and robust to a variety of challenging
label shift scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruihan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annealed Flow Transport Monte Carlo. (arXiv:2102.07501v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07501</id>
        <link href="http://arxiv.org/abs/2102.07501"/>
        <updated>2021-07-12T01:55:17.130Z</updated>
        <summary type="html"><![CDATA[Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC)
extensions are state-of-the-art methods for estimating normalizing constants of
probability distributions. We propose here a novel Monte Carlo algorithm,
Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them
with normalizing flows (NFs) for improved performance. This method transports a
set of particles using not only importance sampling (IS), Markov chain Monte
Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are
learned sequentially to push particles towards the successive annealed targets.
We provide limit theorems for the resulting Monte Carlo estimates of the
normalizing constant and expectations with respect to the target distribution.
Additionally, we show that a continuous-time scaling limit of the population
version of AFT is given by a Feynman--Kac measure which simplifies to the law
of a controlled diffusion for expressive NFs. We demonstrate experimentally the
benefits and limitations of our methodology on a variety of applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Arbel_M/0/1/0/all/0/1"&gt;Michael Arbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Matthews_A/0/1/0/all/0/1"&gt;Alexander G. D. G. Matthews&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization of the Change of Variables Formula with Applications to Residual Flows. (arXiv:2107.04346v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04346</id>
        <link href="http://arxiv.org/abs/2107.04346"/>
        <updated>2021-07-12T01:55:17.122Z</updated>
        <summary type="html"><![CDATA[Normalizing flows leverage the Change of Variables Formula (CVF) to define
flexible density models. Yet, the requirement of smooth transformations
(diffeomorphisms) in the CVF poses a significant challenge in the construction
of these models. To enlarge the design space of flows, we introduce
$\mathcal{L}$-diffeomorphisms as generalized transformations which may violate
these requirements on zero Lebesgue-measure sets. This relaxation allows e.g.
the use of non-smooth activation functions such as ReLU. Finally, we apply the
obtained results to planar, radial, and contractive residual flows.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koenen_N/0/1/0/all/0/1"&gt;Niklas Koenen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wright_M/0/1/0/all/0/1"&gt;Marvin N. Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maass_P/0/1/0/all/0/1"&gt;Peter Maa&amp;#xdf;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Behrmann_J/0/1/0/all/0/1"&gt;Jens Behrmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.10109</id>
        <link href="http://arxiv.org/abs/1906.10109"/>
        <updated>2021-07-12T01:55:17.114Z</updated>
        <summary type="html"><![CDATA[In this paper we present CMRNet, a realtime approach based on a Convolutional
Neural Network to localize an RGB image of a scene in a map built from LiDAR
data. Our network is not trained in the working area, i.e. CMRNet does not
learn the map. Instead it learns to match an image to the map. We validate our
approach on the KITTI dataset, processing each frame independently without any
tracking procedure. CMRNet achieves 0.27m and 1.07deg median localization
accuracy on the sequence 00 of the odometry dataset, starting from a rough pose
estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is
the first CNN-based approach that learns to match images from a monocular
camera to a given, preexisting 3D LiDAR-map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1"&gt;Augusto Luis Ballardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1"&gt;Simone Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1"&gt;Domenico Giorgio Sorrenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1"&gt;Wolfram Burgard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An $O(s^r)$-Resolution ODE Framework for Understanding Discrete-Time Algorithms and Applications to the Linear Convergence of Minimax Problems. (arXiv:2001.08826v7 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.08826</id>
        <link href="http://arxiv.org/abs/2001.08826"/>
        <updated>2021-07-12T01:55:17.107Z</updated>
        <summary type="html"><![CDATA[There has been a long history of using ordinary differential equations (ODEs)
to understand the dynamics of discrete-time algorithms (DTAs). Surprisingly,
there are still two fundamental and unanswered questions: (i) it is unclear how
to obtain a \emph{suitable} ODE from a given DTA, and (ii) it is unclear the
connection between the convergence of a DTA and its corresponding ODEs. In this
paper, we propose a new machinery -- an $O(s^r)$-resolution ODE framework --
for analyzing the behavior of a generic DTA, which (partially) answers the
above two questions. The framework contains three steps: 1. To obtain a
suitable ODE from a given DTA, we define a hierarchy of $O(s^r)$-resolution
ODEs of a DTA parameterized by the degree $r$, where $s$ is the step-size of
the DTA. We present a principal approach to construct the unique
$O(s^r)$-resolution ODEs from a DTA; 2. To analyze the resulting ODE, we
propose the $O(s^r)$-linear-convergence condition of a DTA with respect to an
energy function, under which the $O(s^r)$-resolution ODE converges linearly to
an optimal solution; 3. To bridge the convergence properties of a DTA and its
corresponding ODEs, we define the properness of an energy function and show
that the linear convergence of the $O(s^r)$-resolution ODE with respect to a
proper energy function can automatically guarantee the linear convergence of
the DTA. To better illustrate this machinery, we utilize it to study three
classic algorithms -- gradient descent ascent (GDA), proximal point method
(PPM) and extra-gradient method (EGM) -- for solving the unconstrained minimax
problem $\min_{x\in\RR^n} \max_{y\in \RR^m} L(x,y)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haihao Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias in Machine Learning Software: Why? How? What to do?. (arXiv:2105.12195v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12195</id>
        <link href="http://arxiv.org/abs/2105.12195"/>
        <updated>2021-07-12T01:55:17.086Z</updated>
        <summary type="html"><![CDATA[Increasingly, software is making autonomous decisions in case of criminal
sentencing, approving credit cards, hiring employees, and so on. Some of these
decisions show bias and adversely affect certain social groups (e.g. those
defined by sex, race, age, marital status). Many prior works on bias mitigation
take the following form: change the data or learners in multiple ways, then see
if any of that improves fairness. Perhaps a better approach is to postulate
root causes of bias and then applying some resolution strategy. This paper
postulates that the root causes of bias are the prior decisions that affect-
(a) what data was selected and (b) the labels assigned to those examples. Our
Fair-SMOTE algorithm removes biased labels; and rebalances internal
distributions such that based on sensitive attribute, examples are equal in
both positive and negative classes. On testing, it was seen that this method
was just as effective at reducing bias as prior approaches. Further, models
generated via Fair-SMOTE achieve higher performance (measured in terms of
recall and F1) than other state-of-the-art fairness improvement algorithms. To
the best of our knowledge, measured in terms of number of analyzed learners and
datasets, this study is one of the largest studies on bias mitigation yet
presented in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_J/0/1/0/all/0/1"&gt;Joymallya Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1"&gt;Suvodeep Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menzies_T/0/1/0/all/0/1"&gt;Tim Menzies&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Gradient-based Algorithms for Non-concave Bandit Optimization. (arXiv:2107.04518v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04518</id>
        <link href="http://arxiv.org/abs/2107.04518"/>
        <updated>2021-07-12T01:55:17.079Z</updated>
        <summary type="html"><![CDATA[Bandit problems with linear or concave reward have been extensively studied,
but relatively few works have studied bandits with non-concave reward. This
work considers a large family of bandit problems where the unknown underlying
reward function is non-concave, including the low-rank generalized linear
bandit problems and two-layer neural network with polynomial activation bandit
problem. For the low-rank generalized linear bandit problem, we provide a
minimax-optimal algorithm in the dimension, refuting both conjectures in
[LMT21, JWWN19]. Our algorithms are based on a unified zeroth-order
optimization paradigm that applies in great generality and attains optimal
rates in several structured polynomial settings (in the dimension). We further
demonstrate the applicability of our algorithms in RL in the generative model
setting, resulting in improved sample complexity over prior approaches.
Finally, we show that the standard optimistic algorithms (e.g., UCB) are
sub-optimal by dimension factors. In the neural net setting (with polynomial
activation functions) with noiseless reward, we provide a bandit algorithm with
sample complexity equal to the intrinsic algebraic dimension. Again, we show
that optimistic approaches have worse sample complexity, polynomial in the
extrinsic dimension (which could be exponentially worse in the polynomial
degree).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Baihe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runzhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HPNet: Deep Primitive Segmentation Using Hybrid Representations. (arXiv:2105.10620v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10620</id>
        <link href="http://arxiv.org/abs/2105.10620"/>
        <updated>2021-07-12T01:55:17.072Z</updated>
        <summary type="html"><![CDATA[This paper introduces HPNet, a novel deep-learning approach for segmenting a
3D shape represented as a point cloud into primitive patches. The key to deep
primitive segmentation is learning a feature representation that can separate
points of different primitives. Unlike utilizing a single feature
representation, HPNet leverages hybrid representations that combine one learned
semantic descriptor, two spectral descriptors derived from predicted geometric
parameters, as well as an adjacency matrix that encodes sharp edges. Moreover,
instead of merely concatenating the descriptors, HPNet optimally combines
hybrid representations by learning combination weights. This weighting module
builds on the entropy of input features. The output primitive segmentation is
obtained from a mean-shift clustering module. Experimental results on benchmark
datasets ANSI and ABCParts show that HPNet leads to significant performance
gains from baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Siming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhenpei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1"&gt;Etienne Vouga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qixing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, Featuring Prognostic Stratification Boosting. (arXiv:2101.11085v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11085</id>
        <link href="http://arxiv.org/abs/2101.11085"/>
        <updated>2021-07-12T01:55:17.065Z</updated>
        <summary type="html"><![CDATA[Histopathology-based survival modelling has two major hurdles. Firstly, a
well-performing survival model has minimal clinical application if it does not
contribute to the stratification of a cancer patient cohort into different risk
groups, preferably driven by histologic morphologies. In the clinical setting,
individuals are not given specific prognostic predictions, but are rather
predicted to lie within a risk group which has a general survival trend. Thus,
It is imperative that a survival model produces well-stratified risk groups.
Secondly, until now, survival modelling was done in a two-stage approach
(encoding and aggregation). The massive amount of pixels in digitized whole
slide images were never utilized to their fullest extent due to technological
constraints on data processing, forcing decoupled learning. EPIC-Survival
bridges encoding and aggregation into an end-to-end survival modelling
approach, while introducing stratification boosting to encourage the model to
not only optimize ranking, but also to discriminate between risk groups. In
this study we show that EPIC-Survival performs better than other approaches in
modelling intrahepatic cholangiocarcinoma, a historically difficult cancer to
model. Further, we show that stratification boosting improves further improves
model performance, resulting in a concordance-index of 0.880 on a held-out test
set. Finally, we were able to identify specific histologic differences, not
commonly sought out in ICC, between low and high risk groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muhammad_H/0/1/0/all/0/1"&gt;Hassan Muhammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Chensu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigel_C/0/1/0/all/0/1"&gt;Carlie S. Sigel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doukas_M/0/1/0/all/0/1"&gt;Michael Doukas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alpert_L/0/1/0/all/0/1"&gt;Lindsay Alpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jarnagin_W/0/1/0/all/0/1"&gt;William R. Jarnagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simpson_A/0/1/0/all/0/1"&gt;Amber Simpson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuchs_T/0/1/0/all/0/1"&gt;Thomas J. Fuchs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Multilayer Network Exploration by Random Walk with Restart. (arXiv:2107.04565v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04565</id>
        <link href="http://arxiv.org/abs/2107.04565"/>
        <updated>2021-07-12T01:55:17.059Z</updated>
        <summary type="html"><![CDATA[The amount and variety of data is increasing drastically for several years.
These data are often represented as networks, which are then explored with
approaches arising from network theory. Recent years have witnessed the
extension of network exploration methods to leverage more complex and richer
network frameworks. Random walks, for instance, have been extended to explore
multilayer networks. However, current random walk approaches are limited in the
combination and heterogeneity of network layers they can handle. New analytical
and numerical random walk methods are needed to cope with the increasing
diversity and complexity of multilayer networks. We propose here MultiXrank, a
Python package that enables Random Walk with Restart (RWR) on any kind of
multilayer network with an optimized implementation. This package is supported
by a universal mathematical formulation of the RWR. We evaluated MultiXrank
with leave-one-out cross-validation and link prediction, and introduced
protocols to measure the impact of the addition or removal of multilayer
network data on prediction performances. We further measured the sensitivity of
MultiXrank to input parameters by in-depth exploration of the parameter space.
Finally, we illustrate the versatility of MultiXrank with different use-cases
of unsupervised node prioritization and supervised classification in the
context of human genetic diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1"&gt;Anthony Baptista&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1"&gt;Aitor Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baudot_A/0/1/0/all/0/1"&gt;Ana&amp;#xef;s Baudot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-12T01:55:17.040Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Delegate for Large-scale Vehicle Routing. (arXiv:2107.04139v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04139</id>
        <link href="http://arxiv.org/abs/2107.04139"/>
        <updated>2021-07-12T01:55:17.032Z</updated>
        <summary type="html"><![CDATA[Vehicle routing problems (VRPs) are a class of combinatorial problems with
wide practical applications. While previous heuristic or learning-based works
achieve decent solutions on small problem instances of up to 100 customers,
their performance does not scale to large problems. This article presents a
novel learning-augmented local search algorithm to solve large-scale VRP. The
method iteratively improves the solution by identifying appropriate subproblems
and $\textit{delegating}$ their improvement to a black box subsolver. At each
step, we leverage spatial locality to consider only a linear number of
subproblems, rather than exponential. We frame subproblem selection as a
regression problem and train a Transformer on a generated training set of
problem instances. We show that our method achieves state-of-the-art
performance, with a speed-up of up to 15 times over strong baselines, on VRPs
with sizes ranging from 500 to 3000.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhongxia Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cathy Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning. (arXiv:2107.04271v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2107.04271</id>
        <link href="http://arxiv.org/abs/2107.04271"/>
        <updated>2021-07-12T01:55:17.026Z</updated>
        <summary type="html"><![CDATA[Applying Federated Learning (FL) on Internet-of-Things devices is
necessitated by the large volumes of data they produce and growing concerns of
data privacy. However, there are three challenges that need to be addressed to
make FL efficient: (i) execute on devices with limited computational
capabilities, (ii) account for stragglers due to computational heterogeneity of
devices, and (iii) adapt to the changing network bandwidths. This paper
presents FedAdapt, an adaptive offloading FL framework to mitigate the
aforementioned challenges. FedAdapt accelerates local training in
computationally constrained devices by leveraging layer offloading of deep
neural networks (DNNs) to servers. Further, FedAdapt adopts reinforcement
learning-based optimization and clustering to adaptively identify which layers
of the DNN should be offloaded for each individual device on to a server to
tackle the challenges of computational heterogeneity and changing network
bandwidth. Experimental studies are carried out on a lab-based testbed
comprising five IoT devices. By offloading a DNN from the device to the server
FedAdapt reduces the training time of a typical IoT device by over half
compared to classic FL. The training time of extreme stragglers and the overall
training time can be reduced by up to 57%. Furthermore, with changing network
bandwidth, FedAdapt is demonstrated to reduce the training time by up to 40%
when compared to classic FL, without sacrificing accuracy. FedAdapt can be
downloaded from https://github.com/qub-blesson/FedAdapt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullah_R/0/1/0/all/0/1"&gt;Rehmat Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_P/0/1/0/all/0/1"&gt;Paul Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilpatrick_P/0/1/0/all/0/1"&gt;Peter Kilpatrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spence_I/0/1/0/all/0/1"&gt;Ivor Spence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varghese_B/0/1/0/all/0/1"&gt;Blesson Varghese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lithography Hotspot Detection via Heterogeneous Federated Learning with Local Adaptation. (arXiv:2107.04367v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04367</id>
        <link href="http://arxiv.org/abs/2107.04367"/>
        <updated>2021-07-12T01:55:17.016Z</updated>
        <summary type="html"><![CDATA[As technology scaling is approaching the physical limit, lithography hotspot
detection has become an essential task in design for manufacturability. While
the deployment of pattern matching or machine learning in hotspot detection can
help save significant simulation time, such methods typically demand for
non-trivial quality data to build the model, which most design houses are short
of. Moreover, the design houses are also unwilling to directly share such data
with the other houses to build a unified model, which can be ineffective for
the design house with unique design patterns due to data insufficiency. On the
other hand, with data homogeneity in each design house, the locally trained
models can be easily over-fitted, losing generalization ability and robustness.
In this paper, we propose a heterogeneous federated learning framework for
lithography hotspot detection that can address the aforementioned issues. On
one hand, the framework can build a more robust centralized global sub-model
through heterogeneous knowledge sharing while keeping local data private. On
the other hand, the global sub-model can be combined with a local sub-model to
better adapt to local data heterogeneity. The experimental results show that
the proposed framework can overcome the challenge of non-independent and
identically distributed (non-IID) data and heterogeneous communication to
achieve very high performance in comparison to other state-of-the-art methods
while guaranteeing a good convergence rate in various scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuezhong Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jingyu Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiran Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1"&gt;Cheng Zhuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Domain Adaptation with Self-Training for EEG-based Sleep Stage Classification. (arXiv:2107.04470v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04470</id>
        <link href="http://arxiv.org/abs/2107.04470"/>
        <updated>2021-07-12T01:55:17.009Z</updated>
        <summary type="html"><![CDATA[Sleep staging is of great importance in the diagnosis and treatment of sleep
disorders. Recently, numerous data driven deep learning models have been
proposed for automatic sleep staging. They mainly rely on the assumption that
training and testing data are drawn from the same distribution which may not
hold in real-world scenarios. Unsupervised domain adaption (UDA) has been
recently developed to handle this domain shift problem. However, previous UDA
methods applied for sleep staging has two main limitations. First, they rely on
a totally shared model for the domain alignment, which may lose the
domain-specific information during feature extraction. Second, they only align
the source and target distributions globally without considering the class
information in the target domain, which hinders the classification performance
of the model. In this work, we propose a novel adversarial learning framework
to tackle the domain shift problem in the unlabeled target domain. First, we
develop unshared attention mechanisms to preserve the domain-specific features
in the source and target domains. Second, we design a self-training strategy to
align the fine-grained class distributions for the source and target domains
via target domain pseudo labels. We also propose dual distinct classifiers to
increase the robustness and quality of the pseudo labels. The experimental
results on six cross-domain scenarios validate the efficacy of our proposed
framework for sleep staging and its advantage over state-of-the-art UDA
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1"&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1"&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenghua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Min Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1"&gt;Chee-Keong Kwoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation. (arXiv:2107.04331v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04331</id>
        <link href="http://arxiv.org/abs/2107.04331"/>
        <updated>2021-07-12T01:55:16.992Z</updated>
        <summary type="html"><![CDATA[We present a caricature generation framework based on shape and style
manipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically
creates a realistic and detailed caricature from an input photo with optional
controls on shape exaggeration degree and color stylization type. The key
component of our method is shape exaggeration blocks that are used for
modulating coarse layer feature maps of StyleGAN to produce desirable
caricature shape exaggerations. We first build a layer-mixed StyleGAN for
photo-to-caricature style conversion by swapping fine layers of the StyleGAN
for photos to the corresponding layers of the StyleGAN trained to generate
caricatures. Given an input photo, the layer-mixed model produces detailed
color stylization for a caricature but without shape exaggerations. We then
append shape exaggeration blocks to the coarse layers of the layer-mixed model
and train the blocks to create shape exaggerations while preserving the
characteristic appearances of the input. Experimental results show that our
StyleCariGAN generates realistic and detailed caricatures compared to the
current state-of-the-art methods. We demonstrate StyleCariGAN also supports
other StyleGAN-based image manipulations, such as facial expression control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1"&gt;Wonjong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_G/0/1/0/all/0/1"&gt;Gwangjin Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1"&gt;Yucheol Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seungyong Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04333</id>
        <link href="http://arxiv.org/abs/2107.04333"/>
        <updated>2021-07-12T01:55:16.985Z</updated>
        <summary type="html"><![CDATA[This paper seeks to tackle the bin packing problem (BPP) through a learning
perspective. Building on self-attention-based encoding and deep reinforcement
learning algorithms, we propose a new end-to-end learning model for this task
of interest. By decomposing the combinatorial action space, as well as
utilizing a new training technique denoted as prioritized oversampling, which
is a general scheme to speed up on-policy learning, we achieve state-of-the-art
performance in a range of experimental settings. Moreover, although the
proposed approach attend2pack targets offline-BPP, we strip our method down to
the strict online-BPP setting where it is also able to achieve state-of-the-art
performance. With a set of ablation studies as well as comparisons against a
range of previous works, we hope to offer as a valid baseline approach to this
field of study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1"&gt;Bin Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xiaoyu Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scopeformer: n-CNN-ViT Hybrid Model for Intracranial Hemorrhage Classification. (arXiv:2107.04575v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04575</id>
        <link href="http://arxiv.org/abs/2107.04575"/>
        <updated>2021-07-12T01:55:16.979Z</updated>
        <summary type="html"><![CDATA[We propose a feature generator backbone composed of an ensemble of
convolutional neuralnetworks (CNNs) to improve the recently emerging Vision
Transformer (ViT) models. We tackled the RSNA intracranial hemorrhage
classification problem, i.e., identifying various hemorrhage types from
computed tomography (CT) slices. We show that by gradually stacking several
feature maps extracted using multiple Xception CNNs, we can develop a
feature-rich input for the ViT model. Our approach allowed the ViT model to pay
attention to relevant features at multiple levels. Moreover, pretraining the n
CNNs using various paradigms leads to a diverse feature set and further
improves the performance of the proposed n-CNN-ViT. We achieved a test accuracy
of 98.04% with a weighted logarithmic loss value of 0.0708. The proposed
architecture is modular and scalable in both the number of CNNs used for
feature extraction and the size of the ViT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barhoumi_Y/0/1/0/all/0/1"&gt;Yassine Barhoumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghulam_R/0/1/0/all/0/1"&gt;Rasool Ghulam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Active Feature Acquisition. (arXiv:2107.04163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04163</id>
        <link href="http://arxiv.org/abs/2107.04163"/>
        <updated>2021-07-12T01:55:16.972Z</updated>
        <summary type="html"><![CDATA[Truly intelligent systems are expected to make critical decisions with
incomplete and uncertain data. Active feature acquisition (AFA), where features
are sequentially acquired to improve the prediction, is a step towards this
goal. However, current AFA models all deal with a small set of candidate
features and have difficulty scaling to a large feature space. Moreover, they
are ignorant about the valid domains where they can predict confidently, thus
they can be vulnerable to out-of-distribution (OOD) inputs. In order to remedy
these deficiencies and bring AFA models closer to practical use, we propose
several techniques to advance the current AFA approaches. Our framework can
easily handle a large number of features using a hierarchical acquisition
policy and is more robust to OOD inputs with the help of an OOD detector for
partially observed data. Extensive experiments demonstrate the efficacy of our
framework over strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Siyuan Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1"&gt;Junier B. Oliva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event-Based Feature Tracking in Continuous Time with Sliding Window Optimization. (arXiv:2107.04536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04536</id>
        <link href="http://arxiv.org/abs/2107.04536"/>
        <updated>2021-07-12T01:55:16.966Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for continuous-time feature tracking in event
cameras. To this end, we track features by aligning events along an estimated
trajectory in space-time such that the projection on the image plane results in
maximally sharp event patch images. The trajectory is parameterized by $n^{th}$
order B-splines, which are continuous up to $(n-2)^{th}$ derivative. In
contrast to previous work, we optimize the curve parameters in a sliding window
fashion. On a public dataset we experimentally confirm that the proposed
sliding-window B-spline optimization leads to longer and more accurate feature
tracks than in previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chui_J/0/1/0/all/0/1"&gt;Jason Chui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klenk_S/0/1/0/all/0/1"&gt;Simon Klenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04458</id>
        <link href="http://arxiv.org/abs/2107.04458"/>
        <updated>2021-07-12T01:55:16.960Z</updated>
        <summary type="html"><![CDATA[The process of aggregation is ubiquitous in almost all deep nets models. It
functions as an important mechanism for consolidating deep features into a more
compact representation, whilst increasing robustness to overfitting and
providing spatial invariance in deep nets. In particular, the proximity of
global aggregation layers to the output layers of DNNs mean that aggregated
features have a direct influence on the performance of a deep net. A better
understanding of this relationship can be obtained using information theoretic
methods. However, this requires the knowledge of the distributions of the
activations of aggregation layers. To achieve this, we propose a novel
mathematical formulation for analytically modelling the probability
distributions of output values of layers involved with deep feature
aggregation. An important outcome is our ability to analytically predict the
KL-divergence of output nodes in a DNN. We also experimentally verify our
theoretical predictions against empirical observations across a range of
different classification tasks and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Eng-Jon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1"&gt;Sameed Husain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1"&gt;Miroslaw Bober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-07-12T01:55:16.942Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seven Basic Expression Recognition Using ResNet-18. (arXiv:2107.04569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04569</id>
        <link href="http://arxiv.org/abs/2107.04569"/>
        <updated>2021-07-12T01:55:16.935Z</updated>
        <summary type="html"><![CDATA[We propose to use a ResNet-18 architecture that was pre-trained on the FER+
dataset for tackling the problem of affective behavior analysis in-the-wild
(ABAW) for classification of the seven basic expressions, namely, neutral,
anger, disgust, fear, happiness, sadness and surprise. As part of the second
workshop and competition on affective behavior analysis in-the-wild (ABAW2), a
database consisting of 564 videos with around 2.8M frames is provided along
with labels for these seven basic expressions. We resampled the dataset to
counter class-imbalances by under-sampling the over-represented classes and
over-sampling the under-represented classes along with class-wise weights. To
avoid overfitting we performed data-augmentation and used L2 regularisation.
Our classifier reaches an ABAW2 score of 0.4 and therefore exceeds the baseline
results provided by the hosts of the competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satnam Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schicker_D/0/1/0/all/0/1"&gt;Doris Schicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Distributions of Aggregation Layers in Deep Neural Networks. (arXiv:2107.04458v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04458</id>
        <link href="http://arxiv.org/abs/2107.04458"/>
        <updated>2021-07-12T01:55:16.929Z</updated>
        <summary type="html"><![CDATA[The process of aggregation is ubiquitous in almost all deep nets models. It
functions as an important mechanism for consolidating deep features into a more
compact representation, whilst increasing robustness to overfitting and
providing spatial invariance in deep nets. In particular, the proximity of
global aggregation layers to the output layers of DNNs mean that aggregated
features have a direct influence on the performance of a deep net. A better
understanding of this relationship can be obtained using information theoretic
methods. However, this requires the knowledge of the distributions of the
activations of aggregation layers. To achieve this, we propose a novel
mathematical formulation for analytically modelling the probability
distributions of output values of layers involved with deep feature
aggregation. An important outcome is our ability to analytically predict the
KL-divergence of output nodes in a DNN. We also experimentally verify our
theoretical predictions against empirical observations across a range of
different classification tasks and datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Eng-Jon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1"&gt;Sameed Husain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1"&gt;Miroslaw Bober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset. (arXiv:2107.04313v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04313</id>
        <link href="http://arxiv.org/abs/2107.04313"/>
        <updated>2021-07-12T01:55:16.922Z</updated>
        <summary type="html"><![CDATA[Hateful memes pose a unique challenge for current machine learning systems
because their message is derived from both text- and visual-modalities. To this
effect, Facebook released the Hateful Memes Challenge, a dataset of memes with
pre-extracted text captions, but it is unclear whether these synthetic examples
generalize to `memes in the wild'. In this paper, we collect hateful and
non-hateful memes from Pinterest to evaluate out-of-sample performance on
models pre-trained on the Facebook dataset. We find that memes in the wild
differ in two key aspects: 1) Captions must be extracted via OCR, injecting
noise and diminishing performance of multimodal models, and 2) Memes are more
diverse than `traditional memes', including screenshots of conversations or
text on a plain background. This paper thus serves as a reality check for the
current benchmark of hateful meme detection and its applicability for detecting
real world hate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1"&gt;Hannah Rose Kirk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1"&gt;Yennie Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauba_P/0/1/0/all/0/1"&gt;Paulius Rauba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachtel_G/0/1/0/all/0/1"&gt;Gal Wachtel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruining Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xingjian Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broestl_N/0/1/0/all/0/1"&gt;Noah Broestl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doff_Sotta_M/0/1/0/all/0/1"&gt;Martin Doff-Sotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1"&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1"&gt;Yuki M. Asano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction. (arXiv:2107.04119v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2107.04119</id>
        <link href="http://arxiv.org/abs/2107.04119"/>
        <updated>2021-07-12T01:55:16.905Z</updated>
        <summary type="html"><![CDATA[Advances in machine learning have led to graph neural network-based methods
for drug discovery, yielding promising results in molecular design, chemical
synthesis planning, and molecular property prediction. However, current graph
neural networks (GNNs) remain of limited acceptance in drug discovery is
limited due to their lack of interpretability. Although this major weakness has
been mitigated by the development of explainable artificial intelligence (XAI)
techniques, the "ground truth" assignment in most explainable tasks ultimately
rests with subjective judgments by humans so that the quality of model
interpretation is hard to evaluate in quantity. In this work, we first build
three levels of benchmark datasets to quantitatively assess the
interpretability of the state-of-the-art GNN models. Then we implemented recent
XAI methods in combination with different GNN algorithms to highlight the
benefits, limitations, and future opportunities for drug discovery. As a
result, GradInput and IG generally provide the best model interpretability for
GNNs, especially when combined with GraphNet and CMPNN. The integrated and
developed XAI package is fully open-sourced and can be used by practitioners to
train new models on other drug discovery tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Rao_J/0/1/0/all/0/1"&gt;Jiahua Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuedong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs. (arXiv:2102.08540v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08540</id>
        <link href="http://arxiv.org/abs/2102.08540"/>
        <updated>2021-07-12T01:55:16.899Z</updated>
        <summary type="html"><![CDATA[Interpretability methods aim to help users build trust in and understand the
capabilities of machine learning models. However, existing approaches often
rely on abstract, complex visualizations that poorly map to the task at hand or
require non-trivial ML expertise to interpret. Here, we present two visual
analytics modules that facilitate an intuitive assessment of model reliability.
To help users better characterize and reason about a model's uncertainty, we
visualize raw and aggregate information about a given input's nearest
neighbors. Using an interactive editor, users can manipulate this input in
semantically-meaningful ways, determine the effect on the output, and compare
against their prior expectations. We evaluate our interface using an
electrocardiogram beat classification case study. Compared to a baseline
feature importance interface, we find that 14 physicians are better able to
align the model's uncertainty with domain-relevant factors and build intuition
about its capabilities and limitations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_H/0/1/0/all/0/1"&gt;Harini Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1"&gt;Kathleen M. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guttag_J/0/1/0/all/0/1"&gt;John V. Guttag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1"&gt;Arvind Satyanarayan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stock Market Analysis with Text Data: A Review. (arXiv:2106.12985v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12985</id>
        <link href="http://arxiv.org/abs/2106.12985"/>
        <updated>2021-07-12T01:55:16.891Z</updated>
        <summary type="html"><![CDATA[Stock market movements are influenced by public and private information
shared through news articles, company reports, and social media discussions.
Analyzing these vast sources of data can give market participants an edge to
make profit. However, the majority of the studies in the literature are based
on traditional approaches that come short in analyzing unstructured, vast
textual data. In this study, we provide a review on the immense amount of
existing literature of text-based stock market analysis. We present input data
types and cover main textual data sources and variations. Feature
representation techniques are then presented. Then, we cover the analysis
techniques and create a taxonomy of the main stock market forecast models.
Importantly, we discuss representative work in each category of the taxonomy,
analyzing their respective contributions. Finally, this paper shows the
findings on unaddressed open problems and gives suggestions for future work.
The aim of this study is to survey the main stock market analysis models, text
representation techniques for financial market prediction, shortcomings of
existing techniques, and propose promising directions for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Fataliyev_K/0/1/0/all/0/1"&gt;Kamaladdin Fataliyev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chivukula_A/0/1/0/all/0/1"&gt;Aneesh Chivukula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Prasad_M/0/1/0/all/0/1"&gt;Mukesh Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11825</id>
        <link href="http://arxiv.org/abs/2008.11825"/>
        <updated>2021-07-12T01:55:16.884Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly used in natural language processing
(NLP) models. However, the need to interpret and explain the results from
complex algorithms are limiting their widespread adoption in regulated
industries such as banking. There has been recent work on interpretability of
machine learning algorithms with structured data. But there are only limited
techniques for NLP applications where the problem is more challenging due to
the size of the vocabulary, high-dimensional nature, and the need to consider
textual coherence and language structure. This paper develops a methodology to
compute SHAP values for local explainability of CNN-based text classification
models. The approach is also extended to compute global scores to assess the
importance of features. The results are illustrated on sentiment analysis of
Amazon Electronic Review data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Model Pruning of Convolutional Networks on Tensor Processing Units. (arXiv:2107.04191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04191</id>
        <link href="http://arxiv.org/abs/2107.04191"/>
        <updated>2021-07-12T01:55:16.877Z</updated>
        <summary type="html"><![CDATA[The deployment of convolutional neural networks is often hindered by high
computational and storage requirements. Structured model pruning is a promising
approach to alleviate these requirements. Using the VGG-16 model as an example,
we measure the accuracy-efficiency trade-off for various structured model
pruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units
(TPUs). To measure the actual performance of models, we develop a structured
model pruning library for TensorFlow2 to modify models in place (instead of
adding mask layers). We show that structured model pruning can significantly
improve model memory usage and speed on TPUs without losing accuracy,
especially for small datasets (e.g., CIFAR-10).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kongtao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franko_K/0/1/0/all/0/1"&gt;Ken Franko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_R/0/1/0/all/0/1"&gt;Ruoxin Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality specific U-Net variants for biomedical image segmentation: A survey. (arXiv:2107.04537v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04537</id>
        <link href="http://arxiv.org/abs/2107.04537"/>
        <updated>2021-07-12T01:55:16.869Z</updated>
        <summary type="html"><![CDATA[With the advent of advancements in deep learning approaches, such as deep
convolution neural network, residual neural network, adversarial network; U-Net
architectures are most widely utilized in biomedical image segmentation to
address the automation in identification and detection of the target regions or
sub-regions. In recent studies, U-Net based approaches have illustrated
state-of-the-art performance in different applications for the development of
computer-aided diagnosis systems for early diagnosis and treatment of diseases
such as brain tumor, lung cancer, alzheimer, breast cancer, etc. This article
contributes to present the success of these approaches by describing the U-Net
framework, followed by the comprehensive analysis of the U-Net variants for
different medical imaging or modalities such as magnetic resonance imaging,
X-ray, computerized tomography/computerized axial tomography, ultrasound,
positron emission tomography, etc. Besides, this article also highlights the
contribution of U-Net based frameworks in the on-going pandemic, severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Finite-temperature Kohn-Sham Density Functional Theory with Deep Neural Networks. (arXiv:2010.04905v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04905</id>
        <link href="http://arxiv.org/abs/2010.04905"/>
        <updated>2021-07-12T01:55:16.861Z</updated>
        <summary type="html"><![CDATA[We present a numerical modeling workflow based on machine learning (ML) which
reproduces the the total energies produced by Kohn-Sham density functional
theory (DFT) at finite electronic temperature to within chemical accuracy at
negligible computational cost. Based on deep neural networks, our workflow
yields the local density of states (LDOS) for a given atomic configuration.
From the LDOS, spatially-resolved, energy-resolved, and integrated quantities
can be calculated, including the DFT total free energy, which serves as the
Born-Oppenheimer potential energy surface for the atoms. We demonstrate the
efficacy of this approach for both solid and liquid metals and compare results
between independent and unified machine-learning models for solid and liquid
aluminum. Our machine-learning density functional theory framework opens up the
path towards multiscale materials modeling for matter under ambient and extreme
conditions at a computational scale and cost that is unattainable with current
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ellis_J/0/1/0/all/0/1"&gt;J. Austin Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fiedler_L/0/1/0/all/0/1"&gt;Lenz Fiedler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Popoola_G/0/1/0/all/0/1"&gt;Gabriel A. Popoola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Modine_N/0/1/0/all/0/1"&gt;Normand A. Modine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Stephens_J/0/1/0/all/0/1"&gt;J. Adam Stephens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Thompson_A/0/1/0/all/0/1"&gt;Aidan P. Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Cangi_A/0/1/0/all/0/1"&gt;Attila Cangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rajamanickam_S/0/1/0/all/0/1"&gt;Sivasankaran Rajamanickam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning. (arXiv:2105.12722v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12722</id>
        <link href="http://arxiv.org/abs/2105.12722"/>
        <updated>2021-07-12T01:55:16.854Z</updated>
        <summary type="html"><![CDATA[The objective of this work is to segment any arbitrary structures of interest
(SOI) in 3D volumes by only annotating a single slice, (i.e. semi-automatic 3D
segmentation). We show that high accuracy can be achieved by simply propagating
the 2D slice segmentation with an affinity matrix between consecutive slices,
which can be learnt in a self-supervised manner, namely slice reconstruction.
Specifically, we compare the proposed framework, termed as Sli2Vol, with
supervised approaches and two other unsupervised/ self-supervised slice
registration approaches, on 8 public datasets (both CT and MRI scans), spanning
9 different SOIs. Without any parameter-tuning, the same model achieves
superior performance with Dice scores (0-100 scale) of over 80 for most of the
benchmarks, including the ones that are unseen during training. Our results
show generalizability of the proposed approach across data from different
machines and with different SOIs: a major use case of semi-automatic
segmentation methods where fully supervised approaches would normally struggle.
The source code will be made publicly available at
https://github.com/pakheiyeung/Sli2Vol.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_P/0/1/0/all/0/1"&gt;Pak-Hei Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namburete_A/0/1/0/all/0/1"&gt;Ana I.L. Namburete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weidi Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04212</id>
        <link href="http://arxiv.org/abs/2107.04212"/>
        <updated>2021-07-12T01:55:16.828Z</updated>
        <summary type="html"><![CDATA[Content moderation is often performed by a collaboration between humans and
machine learning models. However, it is not well understood how to design the
collaborative process so as to maximize the combined moderator-model system
performance. This work presents a rigorous study of this problem, focusing on
an approach that incorporates model uncertainty into the collaborative process.
First, we introduce principled metrics to describe the performance of the
collaborative system under capacity constraints on the human moderator,
quantifying how efficiently the combined system utilizes human decisions. Using
these metrics, we conduct a large benchmark study evaluating the performance of
state-of-the-art uncertainty models under different collaborative review
strategies. We find that an uncertainty-based strategy consistently outperforms
the widely used strategy based on toxicity scores, and moreover that the choice
of review strategy drastically changes the overall system performance. Our
results demonstrate the importance of rigorous metrics for understanding and
developing effective moderator-model systems for content moderation, as well as
the utility of uncertainty estimation in this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1"&gt;Ian D. Kivlichan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1"&gt;Lucy Vasserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-headed Neural Ensemble Search. (arXiv:2107.04369v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04369</id>
        <link href="http://arxiv.org/abs/2107.04369"/>
        <updated>2021-07-12T01:55:16.817Z</updated>
        <summary type="html"><![CDATA[Ensembles of CNN models trained with different seeds (also known as Deep
Ensembles) are known to achieve superior performance over a single copy of the
CNN. Neural Ensemble Search (NES) can further boost performance by adding
architectural diversity. However, the scope of NES remains prohibitive under
limited computational resources. In this work, we extend NES to multi-headed
ensembles, which consist of a shared backbone attached to multiple prediction
heads. Unlike Deep Ensembles, these multi-headed ensembles can be trained end
to end, which enables us to leverage one-shot NAS methods to optimize an
ensemble objective. With extensive empirical evaluations, we demonstrate that
multi-headed ensemble search finds robust ensembles 3 times faster, while
having comparable performance to other ensemble search methods, in both
predictive performance and uncertainty calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Ashwin Raaghav Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zela_A/0/1/0/all/0/1"&gt;Arber Zela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikia_T/0/1/0/all/0/1"&gt;Tonmoy Saikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARC: Adversarially Robust Control Policies for Autonomous Vehicles. (arXiv:2107.04487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04487</id>
        <link href="http://arxiv.org/abs/2107.04487"/>
        <updated>2021-07-12T01:55:16.793Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have demonstrated their capability to learn control
policies for a variety of tasks. However, these neural network-based policies
have been shown to be susceptible to exploitation by adversarial agents.
Therefore, there is a need to develop techniques to learn control policies that
are robust against adversaries. We introduce Adversarially Robust Control
(ARC), which trains the protagonist policy and the adversarial policy
end-to-end on the same loss. The aim of the protagonist is to maximise this
loss, whilst the adversary is attempting to minimise it. We demonstrate the
proposed ARC training in a highway driving scenario, where the protagonist
controls the follower vehicle whilst the adversary controls the lead vehicle.
By training the protagonist against an ensemble of adversaries, it learns a
significantly more robust control policy, which generalises to a variety of
adversarial strategies. The approach is shown to reduce the amount of
collisions against new adversaries by up to 90.25%, compared to the original
policy. Moreover, by utilising an auxiliary distillation loss, we show that the
fine-tuned control policy shows no drop in performance across its original
training distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting the Auditory Attention in a Dual-Speaker Scenario from EEG using a Joint CNN-LSTM Model. (arXiv:2102.03957v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03957</id>
        <link href="http://arxiv.org/abs/2102.03957"/>
        <updated>2021-07-12T01:55:16.782Z</updated>
        <summary type="html"><![CDATA[Human brain performs remarkably well in segregating a particular speaker from
interfering ones in a multi-speaker scenario. It has been recently shown that
we can quantitatively evaluate the segregation capability by modelling the
relationship between the speech signals present in an auditory scene and the
cortical signals of the listener measured using electroencephalography (EEG).
This has opened up avenues to integrate neuro-feedback into hearing aids
whereby the device can infer user's attention and enhance the attended speaker.
Commonly used algorithms to infer the auditory attention are based on linear
systems theory where the speech cues such as envelopes are mapped on to the EEG
signals. Here, we present a joint convolutional neural network (CNN) - long
short-term memory (LSTM) model to infer the auditory attention. Our joint
CNN-LSTM model takes the EEG signals and the spectrogram of the multiple
speakers as inputs and classifies the attention to one of the speakers. We
evaluated the reliability of our neural network using three different datasets
comprising of 61 subjects where, each subject undertook a dual-speaker
experiment. The three datasets analysed corresponded to speech stimuli
presented in three different languages namely German, Danish and Dutch. Using
the proposed joint CNN-LSTM model, we obtained a median decoding accuracy of
77.2% at a trial duration of three seconds. Furthermore, we evaluated the
amount of sparsity that our model can tolerate by means of magnitude pruning
and found that the model can tolerate up to 50% sparsity without substantial
loss of decoding accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuruvila_I/0/1/0/all/0/1"&gt;Ivine Kuruvila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muncke_J/0/1/0/all/0/1"&gt;Jan Muncke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_E/0/1/0/all/0/1"&gt;Eghart Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoppe_U/0/1/0/all/0/1"&gt;Ulrich Hoppe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Spherical k-Means. (arXiv:2107.04074v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04074</id>
        <link href="http://arxiv.org/abs/2107.04074"/>
        <updated>2021-07-12T01:55:16.764Z</updated>
        <summary type="html"><![CDATA[Spherical k-means is a widely used clustering algorithm for sparse and
high-dimensional data such as document vectors. While several improvements and
accelerations have been introduced for the original k-means algorithm, not all
easily translate to the spherical variant: Many acceleration techniques, such
as the algorithms of Elkan and Hamerly, rely on the triangle inequality of
Euclidean distances. However, spherical k-means uses Cosine similarities
instead of distances for computational efficiency. In this paper, we
incorporate the Elkan and Hamerly accelerations to the spherical k-means
algorithm working directly with the Cosines instead of Euclidean distances to
obtain a substantial speedup and evaluate these spherical accelerations on real
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Andreas Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feher_G/0/1/0/all/0/1"&gt;Gloria Feher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When is Particle Filtering Efficient for Planning in Partially Observed Linear Dynamical Systems?. (arXiv:2006.05975v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05975</id>
        <link href="http://arxiv.org/abs/2006.05975"/>
        <updated>2021-07-12T01:55:16.747Z</updated>
        <summary type="html"><![CDATA[Particle filtering is a popular method for inferring latent states in
stochastic dynamical systems, whose theoretical properties have been well
studied in machine learning and statistics communities. In many control
problems, e.g., partially observed linear dynamical systems (POLDS), oftentimes
the inferred latent state is further used for planning at each step. This paper
initiates a rigorous study on the efficiency of particle filtering for
sequential planning, and gives the first particle complexity bounds. Though
errors in past actions may affect the future, we are able to bound the number
of particles needed so that the long-run reward of the policy based on particle
filtering is close to that based on exact inference. In particular, we show
that, in stable systems, polynomially many particles suffice. Key in our proof
is a coupling of the ideal sequence based on the exact planning and the
sequence generated by approximate planning based on particle filtering. We
believe this technique can be useful in other sequential decision-making
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ruoqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiajun Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04551</id>
        <link href="http://arxiv.org/abs/2107.04551"/>
        <updated>2021-07-12T01:55:16.741Z</updated>
        <summary type="html"><![CDATA[In the present study, we propose to implement a new framework for estimating
generative models via an adversarial process to extend an existing GAN
framework and develop a white-box controllable image cartoonization, which can
generate high-quality cartooned images/videos from real-world photos and
videos. The learning purposes of our system are based on three distinct
representations: surface representation, structure representation, and texture
representation. The surface representation refers to the smooth surface of the
images. The structure representation relates to the sparse colour blocks and
compresses generic content. The texture representation shows the texture,
curves, and features in cartoon images. Generative Adversarial Network (GAN)
framework decomposes the images into different representations and learns from
them to generate cartoon images. This decomposition makes the framework more
controllable and flexible which allows users to make changes based on the
required output. This approach overcomes any previous system in terms of
maintaining clarity, colours, textures, shapes of images yet showing the
characteristics of cartoon images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1"&gt;Hasan Rizvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-07-12T01:55:16.720Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;Mohammad Javad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-12T01:55:16.713Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, \ie it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04231</id>
        <link href="http://arxiv.org/abs/2107.04231"/>
        <updated>2021-07-12T01:55:16.706Z</updated>
        <summary type="html"><![CDATA[Adaptation of a classifier to new domains is one of the challenging problems
in machine learning. This has been addressed using many deep and non-deep
learning based methods. Among the methodologies used, that of adversarial
learning is widely applied to solve many deep learning problems along with
domain adaptation. These methods are based on a discriminator that ensures
source and target distributions are close. However, here we suggest that rather
than using a point estimate obtaining by a single discriminator, it would be
useful if a distribution based on ensembles of discriminators could be used to
bridge this gap. This could be achieved using multiple classifiers or using
traditional ensemble methods. In contrast, we suggest that a Monte Carlo
dropout based ensemble discriminator could suffice to obtain the distribution
based discriminator. Specifically, we propose a curriculum based dropout
discriminator that gradually increases the variance of the sample based
distribution and the corresponding reverse gradients are used to align the
source and target feature representations. An ensemble of discriminators helps
the model to learn the data distribution efficiently. It also provides a better
gradient estimates to train the feature extractor. The detailed results and
thorough ablation analysis show that our model outperforms state-of-the-art
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1"&gt;Venkatesh K Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Form2Seq : A Framework for Higher-Order Form Structure Extraction. (arXiv:2107.04419v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04419</id>
        <link href="http://arxiv.org/abs/2107.04419"/>
        <updated>2021-07-12T01:55:16.700Z</updated>
        <summary type="html"><![CDATA[Document structure extraction has been a widely researched area for decades
with recent works performing it as a semantic segmentation task over document
images using fully-convolution networks. Such methods are limited by image
resolution due to which they fail to disambiguate structures in dense regions
which appear commonly in forms. To mitigate this, we propose Form2Seq, a novel
sequence-to-sequence (Seq2Seq) inspired framework for structure extraction
using text, with a specific focus on forms, which leverages relative spatial
arrangement of structures. We discuss two tasks; 1) Classification of low-level
constituent elements (TextBlock and empty fillable Widget) into ten types such
as field captions, list items, and others; 2) Grouping lower-level elements
into higher-order constructs, such as Text Fields, ChoiceFields and
ChoiceGroups, used as information collection mechanism in forms. To achieve
this, we arrange the constituent elements linearly in natural reading order,
feed their spatial and textual representations to Seq2Seq framework, which
sequentially outputs prediction of each element depending on the final task. We
modify Seq2Seq for grouping task and discuss improvements obtained through
cascaded end-to-end training of two tasks versus training in isolation.
Experimental results show the effectiveness of our text-based approach
achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01,
61.63 on groups discussed above respectively, outperforming segmentation
baselines. Further we show our framework achieves state of the results for
table structure recognition on ICDAR 2013 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Hiresh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Likelihood ratio-based policy gradient methods for distorted risk measures: A non-asymptotic analysis. (arXiv:2107.04422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04422</id>
        <link href="http://arxiv.org/abs/2107.04422"/>
        <updated>2021-07-12T01:55:16.693Z</updated>
        <summary type="html"><![CDATA[We propose policy-gradient algorithms for solving the problem of control in a
risk-sensitive reinforcement learning (RL) context. The objective of our
algorithm is to maximize the distorted risk measure (DRM) of the cumulative
reward in an episodic Markov decision process (MDP). We derive a variant of the
policy gradient theorem that caters to the DRM objective. Using this theorem in
conjunction with a likelihood ratio (LR) based gradient estimation scheme, we
propose policy gradient algorithms for optimizing DRM in both on-policy and
off-policy RL settings. We derive non-asymptotic bounds that establish the
convergence of our algorithms to an approximate stationary point of the DRM
objective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vijayan_N/0/1/0/all/0/1"&gt;Nithia Vijayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+A_P/0/1/0/all/0/1"&gt;Prashanth L. A&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Self-Organization Supports Task Inference for Continual Robot Learning. (arXiv:2107.04533v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04533</id>
        <link href="http://arxiv.org/abs/2107.04533"/>
        <updated>2021-07-12T01:55:16.687Z</updated>
        <summary type="html"><![CDATA[Recent advances in robot learning have enabled robots to become increasingly
better at mastering a predefined set of tasks. On the other hand, as humans, we
have the ability to learn a growing set of tasks over our lifetime. Continual
robot learning is an emerging research direction with the goal of endowing
robots with this ability. In order to learn new tasks over time, the robot
first needs to infer the task at hand. Task inference, however, has received
little attention in the multi-task learning literature. In this paper, we
propose a novel approach to continual learning of robotic control tasks. Our
approach performs unsupervised learning of behavior embeddings by incrementally
self-organizing demonstrated behaviors. Task inference is made by finding the
nearest behavior embedding to a demonstrated behavior, which is used together
with the environment state as input to a multi-task policy trained with
reinforcement learning to optimize performance over tasks. Unlike previous
approaches, our approach makes no assumptions about task distribution and
requires no task exploration to infer tasks. We evaluate our approach in
experiments with concurrently and sequentially presented tasks and show that it
outperforms other multi-task learning approaches in terms of generalization
performance and convergence speed, particularly in the continual learning
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1"&gt;Muhammad Burhan Hafez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1st Place Solution for YouTubeVOS Challenge 2021:Video Instance Segmentation. (arXiv:2106.06649v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06649</id>
        <link href="http://arxiv.org/abs/2106.06649"/>
        <updated>2021-07-12T01:55:16.668Z</updated>
        <summary type="html"><![CDATA[Video Instance Segmentation (VIS) is a multi-task problem performing
detection, segmentation, and tracking simultaneously. Extended from image set
applications, video data additionally induces the temporal information, which,
if handled appropriately, is very useful to identify and predict object
motions. In this work, we design a unified model to mutually learn these tasks.
Specifically, we propose two modules, named Temporally Correlated Instance
Segmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit
of the temporal correlation between the object's instance masks across adjacent
frames. On the other hand, video data is often redundant due to the frame's
overlap. Our analysis shows that this problem is particularly severe for the
YoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)
training mechanism to compensate for the data deficiency. By combining these
techniques with a bag of tricks, the network performance is significantly
boosted compared to the baseline, and outperforms other methods by a
considerable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuy C. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tuan N. Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_N/0/1/0/all/0/1"&gt;Nam LH. Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1"&gt;Chuong H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamazaki_M/0/1/0/all/0/1"&gt;Masayuki Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamanaka_M/0/1/0/all/0/1"&gt;Masao Yamanaka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-07-12T01:55:16.661Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuefang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Program and Layout Transformations to enable DNN Operators on Specialized Hardware based on Constraint Programming. (arXiv:2104.04731v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04731</id>
        <link href="http://arxiv.org/abs/2104.04731"/>
        <updated>2021-07-12T01:55:16.655Z</updated>
        <summary type="html"><![CDATA[The success of Deep Artificial Neural Networks (DNNs) in many domains created
a rich body of research concerned with hardwareaccelerators for
compute-intensive DNN operators. However, implementing such operators
efficiently with complex hardwareintrinsics such as matrix multiply is a task
not yet automated gracefully. Solving this task often requires joint program
and data layouttransformations. First solutions to this problem have been
proposed, such as TVM, UNIT or ISAMIR, which work on a loop-levelrepresentation
of operators and specify data layout and possible program transformations
before the embedding into the operator isperformed. This top-down approach
creates a tension between exploration range and search space complexity,
especially when alsoexploring data layout transformations such as im2col,
channel packing or padding.In this work, we propose a new approach to this
problem. We created a bottom-up method that allows the joint transformation
ofboth compuation and data layout based on the found embedding. By formulating
the embedding as a constraint satisfaction problemover the scalar dataflow,
every possible embedding solution is contained in the search space. Adding
additional constraints andoptmization targets to the solver generates the
subset of preferable solutions.An evaluation using the VTA hardware accelerator
with the Baidu DeepBench inference benchmark shows that our approach
canautomatically generate code competitive to reference implementations.
Further, we show that dynamically determining the data layoutbased on intrinsic
and workload is beneficial for hardware utilization and performance. In cases
where the reference implementationhas low hardware utilization due to its fixed
deployment strategy, we achieve a geomean speedup of up to x2.813, while
individualoperators can improve as much as x170.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rieber_D/0/1/0/all/0/1"&gt;Dennis Rieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acosta_A/0/1/0/all/0/1"&gt;Axel Acosta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1"&gt;Holger Fr&amp;#xf6;ning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-07-12T01:55:16.648Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning an optical interferometer with beam divergence control and continuous action space. (arXiv:2107.04457v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04457</id>
        <link href="http://arxiv.org/abs/2107.04457"/>
        <updated>2021-07-12T01:55:16.640Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarenko_S/0/1/0/all/0/1"&gt;Stepan Makarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorokin_D/0/1/0/all/0/1"&gt;Dmitry Sorokin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulanov_A/0/1/0/all/0/1"&gt;Alexander Ulanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lvovsky_A/0/1/0/all/0/1"&gt;A. I. Lvovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Counterfactual Explanations on Graph Neural Networks. (arXiv:2107.04086v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04086</id>
        <link href="http://arxiv.org/abs/2107.04086"/>
        <updated>2021-07-12T01:55:16.615Z</updated>
        <summary type="html"><![CDATA[Massive deployment of Graph Neural Networks (GNNs) in high-stake applications
generates a strong demand for explanations that are robust to noise and align
well with human intuition. Most existing methods generate explanations by
identifying a subgraph of an input graph that has a strong correlation with the
prediction. These explanations are not robust to noise because independently
optimizing the correlation for a single input can easily overfit noise.
Moreover, they do not align well with human intuition because removing an
identified subgraph from an input graph does not necessarily change the
prediction result. In this paper, we propose a novel method to generate robust
counterfactual explanations on GNNs by explicitly modelling the common decision
logic of GNNs on similar input graphs. Our explanations are naturally robust to
noise because they are produced from the common decision boundaries of a GNN
that govern the predictions of many similar input graphs. The explanations also
align well with human intuition because removing the set of edges identified by
an explanation from the input graph changes the prediction significantly.
Exhaustive experiments on many public datasets demonstrate the superior
performance of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_M/0/1/0/all/0/1"&gt;Mohit Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1"&gt;Lingyang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zi Yu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lanjun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_P/0/1/0/all/0/1"&gt;Peter Cho-Ho Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Breath Phase and Continuous Adventitious Sound Detection in Lung and Tracheal Sound Using Mixed Set Training and Domain Adaptation. (arXiv:2107.04229v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04229</id>
        <link href="http://arxiv.org/abs/2107.04229"/>
        <updated>2021-07-12T01:55:16.609Z</updated>
        <summary type="html"><![CDATA[Previously, we established a lung sound database, HF_Lung_V2 and proposed
convolutional bidirectional gated recurrent unit (CNN-BiGRU) models with
adequate ability for inhalation, exhalation, continuous adventitious sound
(CAS), and discontinuous adventitious sound detection in the lung sound. In
this study, we proceeded to build a tracheal sound database, HF_Tracheal_V1,
containing 11107 of 15-second tracheal sound recordings, 23087 inhalation
labels, 16728 exhalation labels, and 6874 CAS labels. The tracheal sound in
HF_Tracheal_V1 and the lung sound in HF_Lung_V2 were either combined or used
alone to train the CNN-BiGRU models for respective lung and tracheal sound
analysis. Different training strategies were investigated and compared: (1)
using full training (training from scratch) to train the lung sound models
using lung sound alone and train the tracheal sound models using tracheal sound
alone, (2) using a mixed set that contains both the lung and tracheal sound to
train the models, and (3) using domain adaptation that finetuned the
pre-trained lung sound models with the tracheal sound data and vice versa.
Results showed that the models trained only by lung sound performed poorly in
the tracheal sound analysis and vice versa. However, the mixed set training and
domain adaptation can improve the performance of exhalation and CAS detection
in the lung sound, and inhalation, exhalation, and CAS detection in the
tracheal sound compared to positive controls (lung models trained only by lung
sound and vice versa). Especially, a model derived from the mixed set training
prevails in the situation of killing two birds with one stone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chang-Fu Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chun-Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chung-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yen-Chun Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1"&gt;Tang-Wei Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1"&gt;Nian-Jhen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_W/0/1/0/all/0/1"&gt;Wan-Ling Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Ching-Shiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Convolutional Networks for Model-Based Learning in Nonlinear Inverse Problems. (arXiv:2103.15138v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15138</id>
        <link href="http://arxiv.org/abs/2103.15138"/>
        <updated>2021-07-12T01:55:16.601Z</updated>
        <summary type="html"><![CDATA[The majority of model-based learned image reconstruction methods in medical
imaging have been limited to uniform domains, such as pixelated images. If the
underlying model is solved on nonuniform meshes, arising from a finite element
method typical for nonlinear inverse problems, interpolation and embeddings are
needed. To overcome this, we present a flexible framework to extend model-based
learning directly to nonuniform meshes, by interpreting the mesh as a graph and
formulating our network architectures using graph convolutional neural
networks. This gives rise to the proposed iterative Graph Convolutional
Newton-type Method (GCNM), which includes the forward model in the solution of
the inverse problem, while all updates are directly computed by the network on
the problem specific mesh. We present results for Electrical Impedance
Tomography, a severely ill-posed nonlinear inverse problem that is frequently
solved via optimization-based methods, where the forward problem is solved by
finite element methods. Results for absolute EIT imaging are compared to
standard iterative methods as well as a graph residual network. We show that
the GCNM has strong generalizability to different domain shapes and meshes, out
of distribution data as well as experimental data, from purely simulated
training data and without transfer training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Herzberg_W/0/1/0/all/0/1"&gt;William Herzberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rowe_D/0/1/0/all/0/1"&gt;Daniel B. Rowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1"&gt;Andreas Hauptmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamilton_S/0/1/0/all/0/1"&gt;Sarah J. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Variance of the Fisher Information for Deep Learning. (arXiv:2107.04205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04205</id>
        <link href="http://arxiv.org/abs/2107.04205"/>
        <updated>2021-07-12T01:55:16.594Z</updated>
        <summary type="html"><![CDATA[The Fisher information matrix (FIM) has been applied to the realm of deep
learning. It is closely related to the loss landscape, the variance of the
parameters, second order optimization, and deep learning theory. The exact FIM
is either unavailable in closed form or too expensive to compute. In practice,
it is almost always estimated based on empirical samples. We investigate two
such estimators based on two equivalent representations of the FIM. They are
both unbiased and consistent with respect to the underlying "true" FIM. Their
estimation quality is characterized by their variance given in closed form. We
bound their variances and analyze how the parametric structure of a deep neural
network can impact the variance. We discuss the meaning of this variance
measure and our bounds in the context of deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soen_A/0/1/0/all/0/1"&gt;Alexander Soen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1"&gt;Ke Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[(Un)Masked COVID-19 Trends from Social Media. (arXiv:2011.00052v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00052</id>
        <link href="http://arxiv.org/abs/2011.00052"/>
        <updated>2021-07-12T01:55:16.587Z</updated>
        <summary type="html"><![CDATA[Wearing masks is a useful protection method against COVID-19, which has
caused widespread economic and social impact worldwide. Across the globe,
governments have put mandates for the use of face masks, which have received
both positive and negative reaction. Online social media provides an exciting
platform to study the use of masks and analyze underlying mask-wearing
patterns. In this article, we analyze 2.04 million social media images for six
US cities. An increase in masks worn in images is seen as the COVID-19 cases
rose, particularly when their respective states imposed strict regulations. We
also found a decrease in the posting of group pictures as stay-at-home laws
were put into place. Furthermore, mask compliance in the Black Lives Matter
protest was analyzed, eliciting that 40% of the people in group photos wore
masks, and 45% of them wore the masks with a fit score of greater than 80%. We
introduce two new datasets, VAriety MAsks - Classification (VAMA-C) and VAriety
MAsks - Segmentation (VAMA-S), for mask detection and mask fit analysis tasks,
respectively. For the analysis, we create two frameworks, face mask detector
(for classifying masked and unmasked faces) and mask fit analyzer (a semantic
segmentation based model to calculate a mask-fit score). The face mask detector
achieved a classification accuracy of 98%, and the semantic segmentation model
for the mask fit analyzer achieved an Intersection Over Union (IOU) score of
98%. We conclude that such a framework can be used to evaluate the
effectiveness of such public health strategies using social media platforms in
times of pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Asmit Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehan_P/0/1/0/all/0/1"&gt;Paras Mehan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Divyanshu Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1"&gt;Rohan Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1"&gt;Tavpritesh Sethi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGBD-Net: Predicting color and depth images for novel views synthesis. (arXiv:2011.14398v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14398</id>
        <link href="http://arxiv.org/abs/2011.14398"/>
        <updated>2021-07-12T01:55:16.568Z</updated>
        <summary type="html"><![CDATA[We propose a new cascaded architecture for novel view synthesis, called
RGBD-Net, which consists of two core components: a hierarchical depth
regression network and a depth-aware generator network. The former one predicts
depth maps of the target views by using adaptive depth scaling, while the
latter one leverages the predicted depths and renders spatially and temporally
consistent target images. In the experimental evaluation on standard datasets,
RGBD-Net not only outperforms the state-of-the-art by a clear margin, but it
also generalizes well to new scenes without per-scene optimization. Moreover,
we show that RGBD-Net can be optionally trained without depth supervision while
still retaining high-quality rendering. Thanks to the depth regression network,
RGBD-Net can be also used for creating dense 3D point clouds that are more
accurate than those produced by some state-of-the-art multi-view stereo
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1"&gt;Phong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnewar_A/0/1/0/all/0/1"&gt;Animesh Karnewar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1"&gt;Lam Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1"&gt;Janne Heikkila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Contrastive Motion Learning for Video Action Recognition. (arXiv:2007.10321v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10321</id>
        <link href="http://arxiv.org/abs/2007.10321"/>
        <updated>2021-07-12T01:55:16.561Z</updated>
        <summary type="html"><![CDATA[One central question for video action recognition is how to model motion. In
this paper, we present hierarchical contrastive motion learning, a new
self-supervised learning framework to extract effective motion representations
from raw video frames. Our approach progressively learns a hierarchy of motion
features that correspond to different abstraction levels in a network. This
hierarchical design bridges the semantic gap between low-level motion cues and
high-level recognition tasks, and promotes the fusion of appearance and motion
information at multiple levels. At each level, an explicit motion
self-supervision is provided via contrastive learning to enforce the motion
features at the current level to predict the future ones at the previous level.
Thus, the motion features at higher levels are trained to gradually capture
semantic dynamics and evolve more discriminative for action recognition. Our
motion learning module is lightweight and flexible to be embedded into various
backbone networks. Extensive experiments on four benchmarks show that the
proposed approach consistently achieves superior results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xitong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sifei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1"&gt;Deqing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1"&gt;Jan Kautz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retinal OCT Denoising with Pseudo-Multimodal Fusion Network. (arXiv:2107.04288v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04288</id>
        <link href="http://arxiv.org/abs/2107.04288"/>
        <updated>2021-07-12T01:55:16.553Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography (OCT) is a prevalent imaging technique for
retina. However, it is affected by multiplicative speckle noise that can
degrade the visibility of essential anatomical structures, including blood
vessels and tissue layers. Although averaging repeated B-scan frames can
significantly improve the signal-to-noise-ratio (SNR), this requires longer
acquisition time, which can introduce motion artifacts and cause discomfort to
patients. In this study, we propose a learning-based method that exploits
information from the single-frame noisy B-scan and a pseudo-modality that is
created with the aid of the self-fusion method. The pseudo-modality provides
good SNR for layers that are barely perceptible in the noisy B-scan but can
over-smooth fine features such as small vessels. By using a fusion network,
desired features from each modality can be combined, and the weight of their
contribution is adjustable. Evaluated by intensity-based and structural
metrics, the result shows that our method can effectively suppress the speckle
noise and enhance the contrast between retina layers while the overall
structure and small blood vessels are preserved. Compared to the single
modality network, our method improves the structural similarity with low noise
B-scan from 0.559 +\- 0.033 to 0.576 +\- 0.031.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dewei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malone_J/0/1/0/all/0/1"&gt;Joseph D. Malone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Atay_Y/0/1/0/all/0/1"&gt;Yigit Atay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ipek Oguz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Triangle Inequality for Cosine Similarity. (arXiv:2107.04071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04071</id>
        <link href="http://arxiv.org/abs/2107.04071"/>
        <updated>2021-07-12T01:55:16.546Z</updated>
        <summary type="html"><![CDATA[Similarity search is a fundamental problem for many data analysis techniques.
Many efficient search techniques rely on the triangle inequality of metrics,
which allows pruning parts of the search space based on transitive bounds on
distances. Recently, Cosine similarity has become a popular alternative choice
to the standard Euclidean metric, in particular in the context of textual data
and neural network embeddings. Unfortunately, Cosine similarity is not metric
and does not satisfy the standard triangle inequality. Instead, many search
techniques for Cosine rely on approximation techniques such as locality
sensitive hashing. In this paper, we derive a triangle inequality for Cosine
similarity that is suitable for efficient similarity search with many standard
search structures (such as the VP-tree, Cover-tree, and M-tree); show that this
bound is tight and discuss fast approximations for it. We hope that this spurs
new research on accelerating exact similarity search for cosine similarity, and
possible other similarity measures beyond the existing work for distance
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction. (arXiv:2007.09327v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09327</id>
        <link href="http://arxiv.org/abs/2007.09327"/>
        <updated>2021-07-12T01:55:16.540Z</updated>
        <summary type="html"><![CDATA[Current methods for authentication and key agreement based on public-key
cryptography are vulnerable to quantum computing. We propose a novel approach
based on artificial intelligence research in which communicating parties are
viewed as autonomous agents which interact repeatedly using their private
decision models. Authentication and key agreement are decided based on the
agents' observed behaviors during the interaction. The security of this
approach rests upon the difficulty of modeling the decisions of interacting
agents from limited observations, a problem which we conjecture is also hard
for quantum computing. We release PyAMI, a prototype authentication and key
agreement system based on the proposed method. We empirically validate our
method for authenticating legitimate users while detecting different types of
adversarial attacks. Finally, we show how reinforcement learning techniques can
be used to train server models which effectively probe a client's decisions to
achieve more sample-efficient authentication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_I/0/1/0/all/0/1"&gt;Ibrahim H. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanna_J/0/1/0/all/0/1"&gt;Josiah P. Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fosong_E/0/1/0/all/0/1"&gt;Elliot Fosong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Discontinuity-Preserving Image Registration Network. (arXiv:2107.04440v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04440</id>
        <link href="http://arxiv.org/abs/2107.04440"/>
        <updated>2021-07-12T01:55:16.522Z</updated>
        <summary type="html"><![CDATA[Image registration aims to establish spatial correspondence across pairs, or
groups of images, and is a cornerstone of medical image computing and
computer-assisted-interventions. Currently, most deep learning-based
registration methods assume that the desired deformation fields are globally
smooth and continuous, which is not always valid for real-world scenarios,
especially in medical image registration (e.g. cardiac imaging and abdominal
imaging). Such a global constraint can lead to artefacts and increased errors
at discontinuous tissue interfaces. To tackle this issue, we propose a
weakly-supervised Deep Discontinuity-preserving Image Registration network
(DDIR), to obtain better registration performance and realistic deformation
fields. We demonstrate that our method achieves significant improvements in
registration accuracy and predicts more realistic deformations, in registration
experiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging
Study (UKBB), than state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06171</id>
        <link href="http://arxiv.org/abs/2106.06171"/>
        <updated>2021-07-12T01:55:16.502Z</updated>
        <summary type="html"><![CDATA[Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1"&gt;Luu Huu Phuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1"&gt;Seiji Okajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1"&gt;Arseny Tolmachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1"&gt;Tomoyoshi Takebayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1"&gt;Koji Maruhashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03541</id>
        <link href="http://arxiv.org/abs/2103.03541"/>
        <updated>2021-07-12T01:55:16.495Z</updated>
        <summary type="html"><![CDATA[To scale neural speech synthesis to various real-world languages, we present
a multilingual end-to-end framework that maps byte inputs to spectrograms, thus
allowing arbitrary input scripts. Besides strong results on 40+ languages, the
framework demonstrates capabilities to adapt to new languages under extreme
low-resource and even few-shot scenarios of merely 40s transcribed recording,
without the need of per-language resources like lexicon, extra corpus,
auxiliary models, or linguistic expertise, thus ensuring scalability. While it
retains satisfactory intelligibility and naturalness matching rich-resource
models. Exhaustive comparative and ablation studies are performed to reveal the
potential of the framework for low-resource languages. Furthermore, we propose
a novel method to extract language-specific sub-networks in a multilingual
model for a better understanding of its mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mutian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingzhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank K. Soong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust General Medical Image Segmentation. (arXiv:2107.04263v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04263</id>
        <link href="http://arxiv.org/abs/2107.04263"/>
        <updated>2021-07-12T01:55:16.489Z</updated>
        <summary type="html"><![CDATA[The reliability of Deep Learning systems depends on their accuracy but also
on their robustness against adversarial perturbations to the input data.
Several attacks and defenses have been proposed to improve the performance of
Deep Neural Networks under the presence of adversarial noise in the natural
image domain. However, robustness in computer-aided diagnosis for volumetric
data has only been explored for specific tasks and with limited attacks. We
propose a new framework to assess the robustness of general medical image
segmentation systems. Our contributions are two-fold: (i) we propose a new
benchmark to evaluate robustness in the context of the Medical Segmentation
Decathlon (MSD) by extending the recent AutoAttack natural image classification
framework to the domain of volumetric data segmentation, and (ii) we present a
novel lattice architecture for RObust Generic medical image segmentation (ROG).
Our results show that ROG is capable of generalizing across different tasks of
the MSD and largely surpasses the state-of-the-art under sophisticated
adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1"&gt;Laura Daza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1"&gt;Juan C. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Best of Many Worlds: Dual Mirror Descent for Online Allocation Problems. (arXiv:2011.10124v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10124</id>
        <link href="http://arxiv.org/abs/2011.10124"/>
        <updated>2021-07-12T01:55:16.471Z</updated>
        <summary type="html"><![CDATA[Online allocation problems with resource constraints are central problems in
revenue management and online advertising. In these problems, requests arrive
sequentially during a finite horizon and, for each request, a decision maker
needs to choose an action that consumes a certain amount of resources and
generates reward. The objective is to maximize cumulative rewards subject to a
constraint on the total consumption of resources. In this paper, we consider a
data-driven setting in which the reward and resource consumption of each
request are generated using an input model that is unknown to the decision
maker.

We design a general class of algorithms that attain good performance in
various inputs models without knowing which type of input they are facing. In
particular, our algorithms are asymptotically optimal under stochastic i.i.d.
input model as well as various non-stationary stochastic input models, and they
attain an asymptotically optimal fixed competitive ratio when the input is
adversarial. Our algorithms operate in the Lagrangian dual space: they maintain
a dual multiplier for each resource that is updated using online mirror
descent. By choosing the reference function accordingly, we recover dual
sub-gradient descent and dual exponential weights algorithm. The resulting
algorithms are simple, fast, and have minimal requirements on the reward
functions, consumption functions and the action space, in contrast to existing
methods for online allocation problems. We discuss applications to network
revenue management, online bidding in repeated auctions with budget
constraints, online proportional matching with high entropy, and personalized
assortment optimization with limited inventories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balseiro_S/0/1/0/all/0/1"&gt;Santiago Balseiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haihao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-07-12T01:55:16.463Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) tries to utilize data or knowledge from one or more
source domains to facilitate the learning in a target domain. It is
particularly useful when the target domain has few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., the source domain
data/knowledge cause reduced learning performance in the target domain, has
been a long-standing and challenging problem in TL. Various approaches to
handle NT have been proposed in the literature. However, this filed lacks a
systematic survey on the formalization of NT, their factors and the algorithms
that handle NT. This paper proposes to fill this gap. First, the definition of
negative transfer is considered and a taxonomy of the factors are discussed.
Then, near fifty representative approaches for handling NT are categorized and
reviewed, from four perspectives: secure transfer, domain similarity
estimation, distant transfer and negative transfer mitigation. NT in related
fields, e.g., multi-task learning, lifelong learning, and adversarial attacks
are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Speech Recognition from Federated Acoustic Models. (arXiv:2104.14297v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14297</id>
        <link href="http://arxiv.org/abs/2104.14297"/>
        <updated>2021-07-12T01:55:16.456Z</updated>
        <summary type="html"><![CDATA[Training Automatic Speech Recognition (ASR) models under federated learning
(FL) settings has attracted a lot of attention recently. However, the FL
scenarios often presented in the literature are artificial and fail to capture
the complexity of real FL systems. In this paper, we construct a challenging
and realistic ASR federated experimental setup consisting of clients with
heterogeneous data distributions using the French and Italian sets of the
CommonVoice dataset, a large heterogeneous dataset containing thousands of
different speakers, acoustic environments and noises. We present the first
empirical study on attention-based sequence-to-sequence End-to-End (E2E) ASR
model with three aggregation weighting strategies -- standard FedAvg,
loss-based aggregation and a novel word error rate (WER)-based aggregation,
compared in two realistic FL scenarios: cross-silo with 10 clients and
cross-device with 2K and 4K clients. Our analysis on E2E ASR from heterogeneous
and realistic federated acoustic models provides the foundations for future
research and development of realistic FL-based ASR applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1"&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiem_S/0/1/0/all/0/1"&gt;Salah Zaiem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1"&gt;Javier Fernandez-Marques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1"&gt;Pedro P. B. de Gusmao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1"&gt;Daniel J. Beutel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04144</id>
        <link href="http://arxiv.org/abs/2107.04144"/>
        <updated>2021-07-12T01:55:16.449Z</updated>
        <summary type="html"><![CDATA[The fine-grained relationship between form and function with respect to deep
neural network architecture design and hardware-specific acceleration is one
area that is not well studied in the research literature, with form often
dictated by accuracy as opposed to hardware function. In this study, a
comprehensive empirical exploration is conducted to investigate the impact of
deep neural network architecture design on the degree of inference speedup that
can be achieved via hardware-specific acceleration. More specifically, we
empirically study the impact of a variety of commonly used macro-architecture
design patterns across different architectural depths through the lens of
OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental
results showed that while leveraging hardware-specific acceleration achieved an
average inference speed-up of 380%, the degree of inference speed-up varied
drastically depending on the macro-architecture design pattern, with the
greatest speedup achieved on the depthwise bottleneck convolution design
pattern at 550%. Furthermore, we conduct an in-depth exploration of the
correlation between FLOPs requirement, level 3 cache efficacy, and network
latency with increasing architectural depth and width. Finally, we analyze the
inference time reductions using hardware-specific acceleration when compared to
native deep learning frameworks across a wide variety of hand-crafted deep
convolutional neural network architecture designs as well as ones found via
neural architecture search strategies. We found that the DARTS-derived
architecture to benefit from the greatest improvement from hardware-specific
software acceleration (1200%) while the depthwise bottleneck convolution-based
MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1"&gt;Ellick Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Generative Models for Two-Dimensional Datasets. (arXiv:2106.00203v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00203</id>
        <link href="http://arxiv.org/abs/2106.00203"/>
        <updated>2021-07-12T01:55:16.439Z</updated>
        <summary type="html"><![CDATA[Two-dimensional array-based datasets are pervasive in a variety of domains.
Current approaches for generative modeling have typically been limited to
conventional image datasets and performed in the pixel domain which do not
explicitly capture the correlation between pixels. Additionally, these
approaches do not extend to scientific and other applications where each
element value is continuous and is not limited to a fixed range. In this paper,
we propose a novel approach for generating two-dimensional datasets by moving
the computations to the space of representation bases and show its usefulness
for two different datasets, one from imaging and another from scientific
computing. The proposed approach is general and can be applied to any dataset,
representation basis, or generative model. We provide a comprehensive
performance comparison of various combinations of generative models and
representation basis spaces. We also propose a new evaluation metric which
captures the deficiency of generating images in pixel space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1"&gt;Hoda Shajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaemoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1"&gt;Sanjay Ranka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04062</id>
        <link href="http://arxiv.org/abs/2107.04062"/>
        <updated>2021-07-12T01:55:16.420Z</updated>
        <summary type="html"><![CDATA[A two-step concept for 3D segmentation on 5 abdominal organs inside
volumetric CT images is presented. First each relevant organ's volume of
interest is extracted as bounding box. The extracted volume acts as input for a
second stage, wherein two compared U-Nets with different architectural
dimensions re-construct an organ segmentation as label mask. In this work, we
focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results
indicate Dice improvements of about 6\% at maximum. In this study to our
surprise, liver and kidneys for instance were tackled significantly better
using the faster and GPU-memory saving 2D U-Nets. For other abdominal key
organs, there were no significant differences, but we observe highly
significant advantages for the 2D U-Net in terms of GPU computational efforts
for all organs under study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1"&gt;Nico Zettler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1"&gt;Andre Mastmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02810</id>
        <link href="http://arxiv.org/abs/2102.02810"/>
        <updated>2021-07-12T01:55:16.410Z</updated>
        <summary type="html"><![CDATA[Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.

Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Rebuffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1"&gt;Marco Roberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1"&gt;Laure Soulier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1"&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired Machine Learning on high-energy physics data. (arXiv:2004.13747v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13747</id>
        <link href="http://arxiv.org/abs/2004.13747"/>
        <updated>2021-07-12T01:55:16.402Z</updated>
        <summary type="html"><![CDATA[Tensor Networks, a numerical tool originally designed for simulating quantum
many-body systems, have recently been applied to solve Machine Learning
problems. Exploiting a tree tensor network, we apply a quantum-inspired machine
learning technique to a very important and challenging big data problem in high
energy physics: the analysis and classification of data produced by the Large
Hadron Collider at CERN. In particular, we present how to effectively classify
so-called b-jets, jets originating from b-quarks from proton-proton collisions
in the LHCb experiment, and how to interpret the classification results. We
exploit the Tensor Network approach to select important features and adapt the
network geometry based on information acquired in the learning process.
Finally, we show how to adapt the tree tensor network to achieve optimal
precision or fast response in time without the need of repeating the learning
process. These results pave the way to the implementation of high-frequency
real-time applications, a key ingredient needed among others for current and
future LHCb event classification able to trigger events at the tens of MHz
scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Felser_T/0/1/0/all/0/1"&gt;Timo Felser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trenti_M/0/1/0/all/0/1"&gt;Marco Trenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sestini_L/0/1/0/all/0/1"&gt;Lorenzo Sestini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gianelle_A/0/1/0/all/0/1"&gt;Alessio Gianelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zuliani_D/0/1/0/all/0/1"&gt;Davide Zuliani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lucchesi_D/0/1/0/all/0/1"&gt;Donatella Lucchesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Montangero_S/0/1/0/all/0/1"&gt;Simone Montangero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. (arXiv:2102.02049v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02049</id>
        <link href="http://arxiv.org/abs/2102.02049"/>
        <updated>2021-07-12T01:55:16.396Z</updated>
        <summary type="html"><![CDATA[We consider local planning in fixed-horizon MDPs with a generative model
under the assumption that the optimal value function lies close to the span of
a feature map. The generative model provides a local access to the MDP: The
planner can ask for random transitions from previously returned states and
arbitrary actions, and features are only accessible for states that are
encountered in this process. As opposed to previous work (e.g. Lattimore et al.
(2020)) where linear realizability of all policies was assumed, we consider the
significantly relaxed assumption of a single linearly realizable
(deterministic) policy. A recent lower bound by Weisz et al. (2020) established
that the related problem when the action-value function of the optimal policy
is linearly realizable requires an exponential number of queries, either in $H$
(the horizon of the MDP) or $d$ (the dimension of the feature mapping). Their
construction crucially relies on having an exponentially large action set. In
contrast, in this work, we establish that poly$(H,d)$ planning is possible with
state value function realizability whenever the action set has a constant size.
In particular, we present the TensorPlan algorithm which uses
poly$((dH/\delta)^A)$ simulator queries to find a $\delta$-optimal policy
relative to any deterministic policy for which the value function is linearly
realizable with some bounded parameter. This is the first algorithm to give a
polynomial query complexity guarantee using only linear-realizability of a
single competing value function. Whether the computation cost is similarly
bounded remains an open question. We extend the upper bound to the
near-realizable case and to the infinite-horizon discounted setup. We also
present a lower bound in the infinite-horizon episodic setting: Planners that
achieve constant suboptimality need exponentially many queries, either in $d$
or the number of actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weisz_G/0/1/0/all/0/1"&gt;Gell&amp;#xe9;rt Weisz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amortila_P/0/1/0/all/0/1"&gt;Philip Amortila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janzer_B/0/1/0/all/0/1"&gt;Barnab&amp;#xe1;s Janzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_Yadkori_Y/0/1/0/all/0/1"&gt;Yasin Abbasi-Yadkori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesv&amp;#xe1;ri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autocalibration and Tweedie-dominance for Insurance Pricing with Machine Learning. (arXiv:2103.03635v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03635</id>
        <link href="http://arxiv.org/abs/2103.03635"/>
        <updated>2021-07-12T01:55:16.371Z</updated>
        <summary type="html"><![CDATA[Boosting techniques and neural networks are particularly effective machine
learning methods for insurance pricing. Often in practice, there are
nevertheless endless debates about the choice of the right loss function to be
used to train the machine learning model, as well as about the appropriate
metric to assess the performances of competing models. Also, the sum of fitted
values can depart from the observed totals to a large extent and this often
confuses actuarial analysts. The lack of balance inherent to training models by
minimizing deviance outside the familiar GLM with canonical link setting has
been empirically documented in W\"uthrich (2019, 2020) who attributes it to the
early stopping rule in gradient descent methods for model fitting. The present
paper aims to further study this phenomenon when learning proceeds by
minimizing Tweedie deviance. It is shown that minimizing deviance involves a
trade-off between the integral of weighted differences of lower partial moments
and the bias measured on a specific scale. Autocalibration is then proposed as
a remedy. This new method to correct for bias adds an extra local GLM step to
the analysis. Theoretically, it is shown that it implements the autocalibration
concept in pure premium calculation and ensures that balance also holds on a
local scale, not only at portfolio level as with existing bias-correction
techniques. The convex order appears to be the natural tool to compare
competing models, putting a new light on the diagnostic graphs and associated
metrics proposed by Denuit et al. (2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Denuit_M/0/1/0/all/0/1"&gt;Michel Denuit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Charpentier_A/0/1/0/all/0/1"&gt;Arthur Charpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Trufin_J/0/1/0/all/0/1"&gt;Julien Trufin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04055</id>
        <link href="http://arxiv.org/abs/2107.04055"/>
        <updated>2021-07-12T01:55:16.363Z</updated>
        <summary type="html"><![CDATA[In this paper, a 3D-RegNet-based neural network is proposed for diagnosing
the physical condition of patients with coronavirus (Covid-19) infection. In
the application of clinical medicine, lung CT images are utilized by
practitioners to determine whether a patient is infected with coronavirus.
However, there are some laybacks can be considered regarding to this diagnostic
method, such as time consuming and low accuracy. As a relatively large organ of
human body, important spatial features would be lost if the lungs were
diagnosed utilizing two dimensional slice image. Therefore, in this paper, a
deep learning model with 3D image was designed. The 3D image as input data was
comprised of two-dimensional pulmonary image sequence and from which relevant
coronavirus infection 3D features were extracted and classified. The results
show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC
value of 0.8807 have been achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haibo Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11385</id>
        <link href="http://arxiv.org/abs/2006.11385"/>
        <updated>2021-07-12T01:55:16.356Z</updated>
        <summary type="html"><![CDATA[We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05600</id>
        <link href="http://arxiv.org/abs/2104.05600"/>
        <updated>2021-07-12T01:55:16.349Z</updated>
        <summary type="html"><![CDATA[Application of deep neural networks to medical imaging tasks has in some
sense become commonplace. Still, a "thorn in the side" of the deep learning
movement is the argument that deep networks are prone to overfitting and are
thus unable to generalize well when datasets are small (as is common in medical
imaging tasks). One way to bolster confidence is to provide mathematical
guarantees, or bounds, on network performance after training which explicitly
quantify the possibility of overfitting. In this work, we explore recent
advances using the PAC-Bayesian framework to provide bounds on generalization
error for large (stochastic) networks. While previous efforts focus on
classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we
apply these techniques to both classification and segmentation in a smaller
medical imagining dataset: the ISIC 2018 challenge set. We observe the
resultant bounds are competitive compared to a simpler baseline, while also
being more explainable and alleviating the need for holdout sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1"&gt;Anthony Sicilia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1"&gt;Anastasia Sosnovskikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Seong Jae Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration. (arXiv:1911.09925v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.09925</id>
        <link href="http://arxiv.org/abs/1911.09925"/>
        <updated>2021-07-12T01:55:16.341Z</updated>
        <summary type="html"><![CDATA[DNN accelerators are often developed and evaluated in isolation without
considering the cross-stack, system-level effects in real-world environments.
This makes it difficult to appreciate the impact of System-on-Chip (SoC)
resource contention, OS overheads, and programming-stack inefficiencies on
overall performance/energy-efficiency. To address this challenge, we present
Gemmini, an open-source*, full-stack DNN accelerator generator. Gemmini
generates a wide design-space of efficient ASIC accelerators from a flexible
architectural template, together with flexible programming stacks and full SoCs
with shared resources that capture system-level effects. Gemmini-generated
accelerators have also been fabricated, delivering up to three
orders-of-magnitude speedups over high-performance CPUs on various DNN
benchmarks.

* https://github.com/ucb-bar/gemmini]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Genc_H/0/1/0/all/0/1"&gt;Hasan Genc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seah Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amid_A/0/1/0/all/0/1"&gt;Alon Amid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haj_Ali_A/0/1/0/all/0/1"&gt;Ameer Haj-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1"&gt;Vighnesh Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_P/0/1/0/all/0/1"&gt;Pranav Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jerry Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grubb_D/0/1/0/all/0/1"&gt;Daniel Grubb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liew_H/0/1/0/all/0/1"&gt;Harrison Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Howard Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_A/0/1/0/all/0/1"&gt;Albert Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1"&gt;Colin Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steffl_S/0/1/0/all/0/1"&gt;Samuel Steffl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1"&gt;John Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ragan_Kelley_J/0/1/0/all/0/1"&gt;Jonathan Ragan-Kelley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asanovic_K/0/1/0/all/0/1"&gt;Krste Asanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolic_B/0/1/0/all/0/1"&gt;Borivoje Nikolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yakun Sophia Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Mean Field Games and Mean Field Control with Applications to Finance. (arXiv:2107.04568v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2107.04568</id>
        <link href="http://arxiv.org/abs/2107.04568"/>
        <updated>2021-07-12T01:55:16.323Z</updated>
        <summary type="html"><![CDATA[Financial markets and more generally macro-economic models involve a large
number of individuals interacting through variables such as prices resulting
from the aggregate behavior of all the agents. Mean field games have been
introduced to study Nash equilibria for such problems in the limit when the
number of players is infinite. The theory has been extensively developed in the
past decade, using both analytical and probabilistic tools, and a wide range of
applications have been discovered, from economics to crowd motion. More
recently the interaction with machine learning has attracted a growing
interest. This aspect is particularly relevant to solve very large games with
complex structures, in high dimension or with common sources of randomness. In
this chapter, we review the literature on the interplay between mean field
games and deep learning, with a focus on three families of methods. A special
emphasis is given to financial applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Carmona_R/0/1/0/all/0/1"&gt;Ren&amp;#xe9; Carmona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Lauriere_M/0/1/0/all/0/1"&gt;Mathieu Lauri&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-Sample Analysis of Nonlinear Stochastic Approximation with Applications in Reinforcement Learning. (arXiv:1905.11425v6 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11425</id>
        <link href="http://arxiv.org/abs/1905.11425"/>
        <updated>2021-07-12T01:55:16.309Z</updated>
        <summary type="html"><![CDATA[Motivated by applications in reinforcement learning (RL), we study a
nonlinear stochastic approximation (SA) algorithm under Markovian noise, and
establish its finite-sample convergence bounds under various stepsizes.
Specifically, we show that when using constant stepsize (i.e.,
$\epsilon_k\equiv \epsilon$), the algorithm achieves exponential fast
convergence with asymptotic accuracy $\mathcal{O}(\epsilon\log(1/\epsilon))$.
When using diminishing stepsizes with appropriate decay rate, the algorithm
converges with rate $\mathcal{O}(\log(k)/k)$. Our proof is based on the
Lyapunov drift arguments, and to handle the Markovian noise, we exploit the
fast mixing of the underlying Markov chain. To demonstrate the generality of
our theoretical results on Markovian SA, we use it to derive the finite-sample
bounds of the popular $Q$-learning with linear function approximation
algorithm, under a condition on the behavior policy. Importantly, we do not
need to make the unrealistic assumption that the samples are i.i.d., and do not
require an additional projection step in the algorithm to maintain the
boundedness of the iterates. Numerical simulations corroborate our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zaiwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Doan_T/0/1/0/all/0/1"&gt;Thinh T. Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Clarke_J/0/1/0/all/0/1"&gt;John-Paul Clarke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Maguluri_S/0/1/0/all/0/1"&gt;Siva Theja Maguluri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Graph Dynamics with Reinforcement Learning and Graph Neural Networks. (arXiv:2010.05313v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05313</id>
        <link href="http://arxiv.org/abs/2010.05313"/>
        <updated>2021-07-12T01:55:16.299Z</updated>
        <summary type="html"><![CDATA[We consider the problem of controlling a partially-observed dynamic process
on a graph by a limited number of interventions. This problem naturally arises
in contexts such as scheduling virus tests to curb an epidemic; targeted
marketing in order to promote a product; and manually inspecting posts to
detect fake news spreading on social networks.

We formulate this setup as a sequential decision problem over a temporal
graph process. In face of an exponential state space, combinatorial action
space and partial observability, we design a novel tractable scheme to control
dynamical processes on temporal graphs. We successfully apply our approach to
two popular problems that fall into our framework: prioritizing which nodes
should be tested in order to curb the spread of an epidemic, and influence
maximization on a graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1"&gt;Eli A. Meirom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1"&gt;Haggai Maron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BrainNetGAN: Data augmentation of brain connectivity using generative adversarial network for dementia classification. (arXiv:2103.08494v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08494</id>
        <link href="http://arxiv.org/abs/2103.08494"/>
        <updated>2021-07-12T01:55:16.291Z</updated>
        <summary type="html"><![CDATA[Alzheimer's disease (AD) is the most common age-related dementia. It remains
a challenge to identify the individuals at risk of dementia for precise
management. Brain MRI offers a noninvasive biomarker to detect brain aging.
Previous evidence shows that the brain structural change detected by diffusion
MRI is associated with dementia. Mounting studies has conceptualised the brain
as a complex network, which has shown the utility of this approach in
characterising various neurological and psychiatric disorders. Therefore, the
structural connectivity shows promise in dementia classification. The proposed
BrainNetGAN is a generative adversarial network variant to augment the brain
structural connectivity matrices for binary dementia classification tasks.
Structural connectivity matrices between separated brain regions are
constructed using tractography on diffusion MRI data. The BrainNetGAN model is
trained to generate fake brain connectivity matrices, which are expected to
reflect latent distribution of the real brain network data. Finally, a
convolutional neural network classifier is proposed for binary dementia
classification. Numerical results show that the binary classification
performance in the testing set was improved using the BrainNetGAN augmented
dataset. The proposed methodology allows quick synthesis of an arbitrary number
of augmented connectivity matrices and can be easily transferred to similar
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yiran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Schonlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v2 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05266</id>
        <link href="http://arxiv.org/abs/2012.05266"/>
        <updated>2021-07-12T01:55:16.284Z</updated>
        <summary type="html"><![CDATA[The exponential growth of devices and data at the edges of the Internet is
rising scalability and privacy concerns on approaches based exclusively on
remote cloud platforms. Data gravity, a fundamental concept in Fog Computing,
points towards decentralisation of computation for data analysis, as a viable
alternative to address those concerns. Decentralising AI tasks on several
cooperative devices means identifying the optimal set of locations or
Collection Points (CP for short) to use, in the continuum between full
centralisation (i.e., all data on a single device) and full decentralisation
(i.e., data on source locations). We propose an analytical framework able to
find the optimal operating point in this continuum, linking the accuracy of the
learning task with the corresponding network and computational cost for moving
data and running the distributed training at the CPs. We show through
simulations that the model accurately predicts the optimal trade-off, quite
often an intermediate point between full centralisation and full
decentralisation, showing also a significant cost saving w.r.t. both of them.
Finally, the analytical model admits closed-form or numeric solutions, making
it not only a performance evaluation instrument but also a design tool to
configure a given distributed learning task optimally before its deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1"&gt;Lorenzo Valerio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1"&gt;Andrea Passarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Marco Conti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold Density Estimation via Generalized Dequantization. (arXiv:2102.07143v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07143</id>
        <link href="http://arxiv.org/abs/2102.07143"/>
        <updated>2021-07-12T01:55:16.278Z</updated>
        <summary type="html"><![CDATA[Density estimation is an important technique for characterizing distributions
given observations. Much existing research on density estimation has focused on
cases wherein the data lies in a Euclidean space. However, some kinds of data
are not well-modeled by supposing that their underlying geometry is Euclidean.
Instead, it can be useful to model such data as lying on a {\it manifold} with
some known structure. For instance, some kinds of data may be known to lie on
the surface of a sphere. We study the problem of estimating densities on
manifolds. We propose a method, inspired by the literature on "dequantization,"
which we interpret through the lens of a coordinate transformation of an
ambient Euclidean space and a smooth manifold of interest. Using methods from
normalizing flows, we apply this method to the dequantization of smooth
manifold structures in order to model densities on the sphere, tori, and the
orthogonal group.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Brofos_J/0/1/0/all/0/1"&gt;James A. Brofos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lederman_R/0/1/0/all/0/1"&gt;Roy R. Lederman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drug-Target Interaction Prediction via an Ensemble of Weighted Nearest Neighbors with Interaction Recovery. (arXiv:2012.12325v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12325</id>
        <link href="http://arxiv.org/abs/2012.12325"/>
        <updated>2021-07-12T01:55:16.261Z</updated>
        <summary type="html"><![CDATA[Predicting drug-target interactions (DTI) via reliable computational methods
is an effective and efficient way to mitigate the enormous costs and time of
the drug discovery process. Structure-based drug similarities and
sequence-based target protein similarities are the commonly used information
for DTI prediction. Among numerous computational methods, neighborhood-based
chemogenomic approaches that leverage drug and target similarities to perform
predictions directly are simple but promising ones. However, existing
similarity-based methods need to be re-trained to predict interactions for any
new drugs or targets and cannot directly perform predictions for both new
drugs, new targets, and new drug-target pairs. Furthermore, a large amount of
missing (undetected) interactions in current DTI datasets hinders most DTI
prediction methods. To address these issues, we propose a new method denoted as
Weighted k-Nearest Neighbor with Interaction Recovery (WkNNIR). Not only can
WkNNIR estimate interactions of any new drugs and/or new targets without any
need of re-training, but it can also recover missing interactions (false
negatives). In addition, WkNNIR exploits local imbalance to promote the
influence of more reliable similarities on the interaction recovery and
prediction processes. We also propose a series of ensemble methods that employ
diverse sampling strategies and could be coupled with WkNNIR as well as any
other DTI prediction method to improve performance. Experimental results over
five benchmark datasets demonstrate the effectiveness of our approaches in
predicting drug-target interactions. Lastly, we confirm the practical
prediction ability of proposed methods to discover reliable interactions that
were not reported in the original benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1"&gt;Konstantinos Pliakos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1"&gt;Celine Vens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04589</id>
        <link href="http://arxiv.org/abs/2107.04589"/>
        <updated>2021-07-12T01:55:16.254Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have shown competitive performance on
image recognition while requiring less vision-specific inductive biases. In
this paper, we investigate if such observation can be extended to image
generation. To this end, we integrate the ViT architecture into generative
adversarial networks (GANs). We observe that existing regularization methods
for GANs interact poorly with self-attention, causing serious instability
during training. To resolve this issue, we introduce novel regularization
techniques for training GANs with ViTs. Empirically, our approach, named
ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2
on CIFAR-10, CelebA, and LSUN bedroom datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kwonjoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Huiwen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ce Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Bayesian Learning Rule. (arXiv:2107.04562v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04562</id>
        <link href="http://arxiv.org/abs/2107.04562"/>
        <updated>2021-07-12T01:55:16.247Z</updated>
        <summary type="html"><![CDATA[We show that many machine-learning algorithms are specific instances of a
single algorithm called the Bayesian learning rule. The rule, derived from
Bayesian principles, yields a wide-range of algorithms from fields such as
optimization, deep learning, and graphical models. This includes classical
algorithms such as ridge regression, Newton's method, and Kalman filter, as
well as modern deep-learning algorithms such as stochastic-gradient descent,
RMSprop, and Dropout. The key idea in deriving such algorithms is to
approximate the posterior using candidate distributions estimated by using
natural gradients. Different candidate distributions result in different
algorithms and further approximations to natural gradients give rise to
variants of those algorithms. Our work not only unifies, generalizes, and
improves existing algorithms, but also helps us design new ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rue_H/0/1/0/all/0/1"&gt;H&amp;#xe5;vard Rue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04512</id>
        <link href="http://arxiv.org/abs/2107.04512"/>
        <updated>2021-07-12T01:55:16.230Z</updated>
        <summary type="html"><![CDATA[One of the challenges in a task oriented natural language application like
the Google Assistant, Siri, or Alexa is to localize the output to many
languages. This paper explores doing this by applying machine translation to
the English output. Using machine translation is very scalable, as it can work
with any English output and can handle dynamic text, but otherwise the problem
is a poor fit. The required quality bar is close to perfection, the range of
sentences is extremely narrow, and the sentences are often very different than
the ones in the machine translation training data. This combination of
requirements is novel in the field of domain adaptation for machine
translation. We are able to reach the required quality bar by building on
existing ideas and adding new ones: finetuning on in-domain translations,
adding sentences from the Web, adding semantic annotations, and using automatic
error detection. The paper shares our approach and results, together with a
distillation model to serve the translation models at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Scott Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1"&gt;Cliff Brunk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Justin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1"&gt;Markus Freitag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1"&gt;Gagan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1"&gt;Sidharth Mudgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1"&gt;Chris Varano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-07-12T01:55:16.222Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02087</id>
        <link href="http://arxiv.org/abs/2106.02087"/>
        <updated>2021-07-12T01:55:16.215Z</updated>
        <summary type="html"><![CDATA[Applying machine learning to tasks that operate with code changes requires
their numerical representation. In this work, we propose an approach for
obtaining such representations during pre-training and evaluate them on two
different downstream tasks - applying changes to code and commit message
generation. During pre-training, the model learns to apply the given code
change in a correct way. This task requires only code changes themselves, which
makes it unsupervised. In the task of applying code changes, our model
outperforms baseline models by 5.9 percentage points in accuracy. As for the
commit message generation, our model demonstrated the same results as
supervised models trained for this specific task, which indicates that it can
encode code changes well and can be improved in the future by pre-training on a
larger dataset of easily gathered code changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1"&gt;Mikhail Pravilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1"&gt;Egor Bogomolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1"&gt;Yaroslav Golubev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1"&gt;Timofey Bryksin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03664</id>
        <link href="http://arxiv.org/abs/2103.03664"/>
        <updated>2021-07-12T01:55:16.208Z</updated>
        <summary type="html"><![CDATA[We introduce a neural network framework, utilizing adversarial learning to
partition an image into two cuts, with one cut falling into a reference
distribution provided by the user. This concept tackles the task of
unsupervised anomaly segmentation, which has attracted increasing attention in
recent years due to their broad applications in tasks with unlabelled data.
This Adversarial-based Selective Cutting network (ASC-Net) bridges the two
domains of cluster-based deep learning methods and adversarial-based
anomaly/novelty detection algorithms. We evaluate this unsupervised learning
model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and
MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN
family, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results shed light on building an unsupervised learning algorithm
using user-defined knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1"&gt;Raunak Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Stress Assessment from ECG in a Virtual Reality Environment using Multimodal Fusion. (arXiv:2107.04566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04566</id>
        <link href="http://arxiv.org/abs/2107.04566"/>
        <updated>2021-07-12T01:55:16.196Z</updated>
        <summary type="html"><![CDATA[ECG is an attractive option to assess stress in serious Virtual Reality (VR)
applications due to its non-invasive nature. However, the existing Machine
Learning (ML) models perform poorly. Moreover, existing studies only perform a
binary stress assessment, while to develop a more engaging biofeedback-based
application, multi-level assessment is necessary. Existing studies annotate and
classify a single experience (e.g. watching a VR video) to a single stress
level, which again prevents design of dynamic experiences where real-time
in-game stress assessment can be utilized. In this paper, we report our
findings on a new study on VR stress assessment, where three stress levels are
assessed. ECG data was collected from 9 users experiencing a VR roller coaster.
The VR experience was then manually labeled in 10-seconds segments to three
stress levels by three raters. We then propose a novel multimodal deep fusion
model utilizing spectrogram and 1D ECG that can provide a stress prediction
from just a 1-second window. Experimental results demonstrate that the proposed
model outperforms the classical HRV-based ML models (9% increase in accuracy)
and baseline deep learning models (2.5% increase in accuracy). We also report
results on the benchmark WESAD dataset to show the supremacy of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_Z/0/1/0/all/0/1"&gt;Zeeshan Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbani_S/0/1/0/all/0/1"&gt;Suha Rabbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Rehman Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishaque_S/0/1/0/all/0/1"&gt;Syem Ishaque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1"&gt;Sridhar Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline reinforcement learning with uncertainty for treatment strategies in sepsis. (arXiv:2107.04491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04491</id>
        <link href="http://arxiv.org/abs/2107.04491"/>
        <updated>2021-07-12T01:55:16.169Z</updated>
        <summary type="html"><![CDATA[Guideline-based treatment for sepsis and septic shock is difficult because
sepsis is a disparate range of life-threatening organ dysfunctions whose
pathophysiology is not fully understood. Early intervention in sepsis is
crucial for patient outcome, yet those interventions have adverse effects and
are frequently overadministered. Greater personalization is necessary, as no
single action is suitable for all patients. We present a novel application of
reinforcement learning in which we identify optimal recommendations for sepsis
treatment from data, estimate their confidence level, and identify treatment
options infrequently observed in training data. Rather than a single
recommendation, our method can present several treatment options. We examine
learned policies and discover that reinforcement learning is biased against
aggressive intervention due to the confounding relationship between mortality
and level of treatment received. We mitigate this bias using subspace learning,
and develop methodology that can yield more accurate learning policies across
healthcare applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ran Liu&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Greenstein_J/0/1/0/all/0/1"&gt;Joseph L. Greenstein&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fackler_J/0/1/0/all/0/1"&gt;James C. Fackler&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmann_J/0/1/0/all/0/1"&gt;Jules Bergmann&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bembea_M/0/1/0/all/0/1"&gt;Melania M. Bembea&lt;/a&gt; (3 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Winslow_R/0/1/0/all/0/1"&gt;Raimond L. Winslow&lt;/a&gt; (1 and 2) ((1) Institute for Computational Medicine, the Johns Hopkins University, (2) Department of Biomedical Engineering, the Johns Hopkins University School of Medicine and Whiting School of Engineering, (3) Department of Anesthesiology and Critical Care Medicine, the Johns Hopkins University, (4) Department of Pediatrics, the Johns Hopkins University School of Medicine)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Rank Saddle Free Newton: Scalable Stochastic Nonconvex Optimization. (arXiv:2002.02881v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02881</id>
        <link href="http://arxiv.org/abs/2002.02881"/>
        <updated>2021-07-12T01:55:16.163Z</updated>
        <summary type="html"><![CDATA[Newton methods have fallen out of favor for modern optimization problems
(e.g. deep learning) because of concerns about per-iteration computational
complexity. In this setting highly subsampled first order methods are
preferred. In this work we motivate the extension of Newton methods to the
highly stochastic regime, and argue for the use of the scalable low rank saddle
free Newton (LRSFN) method. In this setting, iterative updates are dominated by
stochastic noise, and stability of the method is key. In stability analysis, we
demonstrate that stochastic errors for Newton methods can be greatly amplified
by ill-conditioned matrix operators. The LRSFN algorithm mitigates this issue
by the use of Levenberg-Marquardt damping, but generally second order methods
with stochastic Hessian and gradient information may need to take small steps,
unlike in deterministic problems. Numerical results show that even under
restrictive step-length conditions, LRSFN can outperform popular first order
methods on nontrivial deep learning tasks in terms of generalizability for
equivalent computational work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+OLeary_Roseberry_T/0/1/0/all/0/1"&gt;Thomas O&amp;#x27;Leary-Roseberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Alger_N/0/1/0/all/0/1"&gt;Nick Alger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ghattas_O/0/1/0/all/0/1"&gt;Omar Ghattas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison of Contextual and Non-Contextual Preference Ranking for Set Addition Problems. (arXiv:2107.04438v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2107.04438</id>
        <link href="http://arxiv.org/abs/2107.04438"/>
        <updated>2021-07-12T01:55:16.118Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of evaluating the addition of elements to
a set. This problem is difficult, because it can, in the general case, not be
reduced to unconditional preferences between the choices. Therefore, we model
preferences based on the context of the decision. We discuss and compare two
different Siamese network architectures for this task: a twin network that
compares the two sets resulting after the addition, and a triplet network that
models the contribution of each candidate to the existing set. We evaluate the
two settings on a real-world task; learning human card preferences for deck
building in the collectible card game Magic: The Gathering. We show that the
triplet approach achieves a better result than the twin network and that both
outperform previous results on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertram_T/0/1/0/all/0/1"&gt;Timo Bertram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1"&gt;Martin M&amp;#xfc;ller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reduced Order Modelling and Efficient Temporal Evolution of Fluid Simulations. (arXiv:2107.04556v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2107.04556</id>
        <link href="http://arxiv.org/abs/2107.04556"/>
        <updated>2021-07-12T01:55:16.069Z</updated>
        <summary type="html"><![CDATA[Reduced Order Modelling (ROM) has been widely used to create lower order,
computationally inexpensive representations of higher-order dynamical systems.
Using these representations, ROMs can efficiently model flow fields while using
significantly lesser parameters. Conventional ROMs accomplish this by linearly
projecting higher-order manifolds to lower-dimensional space using
dimensionality reduction techniques such as Proper Orthogonal Decomposition
(POD). In this work, we develop a novel deep learning framework DL-ROM (Deep
Learning - Reduced Order Modelling) to create a neural network capable of
non-linear projections to reduced order states. We then use the learned reduced
state to efficiently predict future time steps of the simulation using 3D
Autoencoder and 3D U-Net based architectures. Our model DL-ROM is able to
create highly accurate reconstructions from the learned ROM and is thus able to
efficiently predict future time steps by temporally traversing in the learned
reduced state. All of this is achieved without ground truth supervision or
needing to iteratively solve the expensive Navier-Stokes(NS) equations thereby
resulting in massive computational savings. To test the effectiveness and
performance of our approach, we evaluate our implementation on five different
Computational Fluid Dynamics (CFD) datasets using reconstruction performance
and computational runtime metrics. DL-ROM can reduce the computational runtimes
of iterative solvers by nearly two orders of magnitude while maintaining an
acceptable error threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pant_P/0/1/0/all/0/1"&gt;Pranshu Pant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Doshi_R/0/1/0/all/0/1"&gt;Ruchit Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bahl_P/0/1/0/all/0/1"&gt;Pranav Bahl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Farimani_A/0/1/0/all/0/1"&gt;Amir Barati Farimani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BayesSimIG: Scalable Parameter Inference for Adaptive Domain Randomization with IsaacGym. (arXiv:2107.04527v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04527</id>
        <link href="http://arxiv.org/abs/2107.04527"/>
        <updated>2021-07-12T01:55:16.062Z</updated>
        <summary type="html"><![CDATA[BayesSim is a statistical technique for domain randomization in reinforcement
learning based on likelihood-free inference of simulation parameters. This
paper outlines BayesSimIG: a library that provides an implementation of
BayesSim integrated with the recently released NVIDIA IsaacGym. This
combination allows large-scale parameter inference with end-to-end GPU
acceleration. Both inference and simulation get GPU speedup, with support for
running more than 10K parallel simulation environments for complex robotics
tasks that can have more than 100 simulation parameters to estimate. BayesSimIG
provides an integration with TensorBoard to easily visualize slices of
high-dimensional posteriors. The library is built in a modular way to support
research experiments with novel ways to collect and process the trajectories
from the parallel IsaacGym environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antonova_R/0/1/0/all/0/1"&gt;Rika Antonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1"&gt;Fabio Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Possas_R/0/1/0/all/0/1"&gt;Rafael Possas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redescription Model Mining. (arXiv:2107.04462v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.04462</id>
        <link href="http://arxiv.org/abs/2107.04462"/>
        <updated>2021-07-12T01:55:16.054Z</updated>
        <summary type="html"><![CDATA[This paper introduces Redescription Model Mining, a novel approach to
identify interpretable patterns across two datasets that share only a subset of
attributes and have no common instances. In particular, Redescription Model
Mining aims to find pairs of describable data subsets -- one for each dataset
-- that induce similar exceptional models with respect to a prespecified model
class. To achieve this, we combine two previously separate research areas:
Exceptional Model Mining and Redescription Mining. For this new problem
setting, we develop interestingness measures to select promising patterns,
propose efficient algorithms, and demonstrate their potential on synthetic and
real-world data. Uncovered patterns can hint at common underlying phenomena
that manifest themselves across datasets, enabling the discovery of possible
associations between (combinations of) attributes that do not appear in the
same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stamm_F/0/1/0/all/0/1"&gt;Felix I. Stamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becker_M/0/1/0/all/0/1"&gt;Martin Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohmaier_M/0/1/0/all/0/1"&gt;Markus Strohmaier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lemmerich_F/0/1/0/all/0/1"&gt;Florian Lemmerich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotic Optimality of Conditioned Stochastic Gradient Descent. (arXiv:2006.02745v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02745</id>
        <link href="http://arxiv.org/abs/2006.02745"/>
        <updated>2021-07-12T01:55:16.032Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate a general class of stochastic gradient descent
(SGD) algorithms, called conditioned SGD, based on a preconditioning of the
gradient direction. Under some mild assumptions, namely the $L$-smoothness of
the non-convex objective function and some weak growth condition on the noise,
we establish the almost sure convergence and the asymptotic normality for a
broad class of conditioning matrices. In particular, when the conditioning
matrix is an estimate of the inverse Hessian at the optimal point, the
algorithm is proved to be asymptotically optimal. The benefits of this approach
are validated on simulated and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Leluc_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Leluc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Portier_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Portier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04570</id>
        <link href="http://arxiv.org/abs/2107.04570"/>
        <updated>2021-07-12T01:55:16.024Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, \ie it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Code used in our experiments is available in
https://github.com/MotasemAlfarra/ANCER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1"&gt;Francisco Eiras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1"&gt;Motasem Alfarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;M. Pawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet K. Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1"&gt;Adel Bibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-Node Attention for Community Evolution Prediction. (arXiv:2107.04522v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04522</id>
        <link href="http://arxiv.org/abs/2107.04522"/>
        <updated>2021-07-12T01:55:16.017Z</updated>
        <summary type="html"><![CDATA[Communities in social networks evolve over time as people enter and leave the
network and their activity behaviors shift. The task of predicting structural
changes in communities over time is known as community evolution prediction.
Existing work in this area has focused on the development of frameworks for
defining events while using traditional classification methods to perform the
actual prediction. We present a novel graph neural network for predicting
community evolution events from structural and temporal information. The model
(GNAN) includes a group-node attention component which enables support for
variable-sized inputs and learned representation of groups based on member and
neighbor node features. A comparative evaluation with standard baseline methods
is performed and we demonstrate that our model outperforms the baselines.
Additionally, we show the effects of network trends on model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Revelle_M/0/1/0/all/0/1"&gt;Matt Revelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1"&gt;Carlotta Domeniconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelman_B/0/1/0/all/0/1"&gt;Ben Gelman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bib2Auth: Deep Learning Approach for Author Disambiguation using Bibliographic Data. (arXiv:2107.04382v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2107.04382</id>
        <link href="http://arxiv.org/abs/2107.04382"/>
        <updated>2021-07-12T01:55:16.008Z</updated>
        <summary type="html"><![CDATA[Author name ambiguity remains a critical open problem in digital libraries
due to synonymy and homonymy of names. In this paper, we propose a novel
approach to link author names to their real-world entities by relying on their
co-authorship pattern and area of research. Our supervised deep learning model
identifies an author by capturing his/her relationship with his/her co-authors
and area of research, which is represented by the titles and sources of the
target author's publications. These attributes are encoded by their semantic
and symbolic representations. To this end, Bib2Auth uses ~ 22K bibliographic
records from the DBLP repository and is trained with each pair of co-authors.
The extensive experiments have proved the capability of the approach to
distinguish between authors sharing the same name and recognize authors with
different name variations. Bib2Auth has shown good performance on a relatively
large dataset, which qualifies it to be directly integrated into bibliographic
indices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahubali_N/0/1/0/all/0/1"&gt;Nagaraj Bahubali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1"&gt;Abinaya Thulsi Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Adarsh Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prasadand_S/0/1/0/all/0/1"&gt;Soniya Manchenahalli Gnanendra Prasadand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aralappa_S/0/1/0/all/0/1"&gt;Sriram Aralappa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning in the Teacher-Student Setup: Impact of Task Similarity. (arXiv:2107.04384v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04384</id>
        <link href="http://arxiv.org/abs/2107.04384"/>
        <updated>2021-07-12T01:55:16.002Z</updated>
        <summary type="html"><![CDATA[Continual learning-the ability to learn many tasks in sequence-is critical
for artificial learning systems. Yet standard training methods for deep
networks often suffer from catastrophic forgetting, where learning new tasks
erases knowledge of earlier tasks. While catastrophic forgetting labels the
problem, the theoretical reasons for interference between tasks remain unclear.
Here, we attempt to narrow this gap between theory and practice by studying
continual learning in the teacher-student setup. We extend previous analytical
work on two-layer networks in the teacher-student setup to multiple teachers.
Using each teacher to represent a different task, we investigate how the
relationship between teachers affects the amount of forgetting and transfer
exhibited by the student when the task switches. In line with recent work, we
find that when tasks depend on similar features, intermediate task similarity
leads to greatest forgetting. However, feature similarity is only one way in
which tasks may be related. The teacher-student approach allows us to
disentangle task similarity at the level of readouts (hidden-to-output weights)
and features (input-to-hidden weights). We find a complex interplay between
both types of similarity, initial transfer/forgetting rates, maximum
transfer/forgetting, and long-term transfer/forgetting. Together, these results
help illuminate the diverse factors contributing to catastrophic forgetting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sebastian Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goldt_S/0/1/0/all/0/1"&gt;Sebastian Goldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Saxe_A/0/1/0/all/0/1"&gt;Andrew Saxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Mixture Density Networks: Learning to Drive Safely from Collision Data. (arXiv:2107.04485v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04485</id>
        <link href="http://arxiv.org/abs/2107.04485"/>
        <updated>2021-07-12T01:55:15.981Z</updated>
        <summary type="html"><![CDATA[Imitation learning has been widely used to learn control policies for
autonomous driving based on pre-recorded data. However, imitation learning
based policies have been shown to be susceptible to compounding errors when
encountering states outside of the training distribution. Further, these agents
have been demonstrated to be easily exploitable by adversarial road users
aiming to create collisions. To overcome these shortcomings, we introduce
Adversarial Mixture Density Networks (AMDN), which learns two distributions
from separate datasets. The first is a distribution of safe actions learned
from a dataset of naturalistic human driving. The second is a distribution
representing unsafe actions likely to lead to collision, learned from a dataset
of collisions. During training, we leverage these two distributions to provide
an additional loss based on the similarity of the two distributions. By
penalising the safe action distribution based on its similarity to the unsafe
action distribution when training on the collision dataset, a more robust and
safe control policy is obtained. We demonstrate the proposed AMDN approach in a
vehicle following use-case, and evaluate under naturalistic and adversarial
testing environments. We show that despite its simplicity, AMDN provides
significant benefits for the safety of the learned control policy, when
compared to pure imitation learning or standard mixture density network
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuutti_S/0/1/0/all/0/1"&gt;Sampo Kuutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1"&gt;Saber Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04292</id>
        <link href="http://arxiv.org/abs/2107.04292"/>
        <updated>2021-07-12T01:55:15.975Z</updated>
        <summary type="html"><![CDATA[Many joint entity relation extraction models setup two separated label spaces
for the two sub-tasks (i.e., entity detection and relation classification). We
argue that this setting may hinder the information interaction between entities
and relations. In this work, we propose to eliminate the different treatment on
the two sub-tasks' label spaces. The input of our model is a table containing
all word pairs from a sentence. Entities and relations are represented by
squares and rectangles in the table. We apply a unified classifier to predict
each cell's label, which unifies the learning of two sub-tasks. For testing, an
effective (yet fast) approximate decoder is proposed for finding squares and
rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)
show that, using only half the number of parameters, our model achieves
competitive accuracy with the best extractor, and is faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changzhi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuanbin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression. (arXiv:2107.04497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04497</id>
        <link href="http://arxiv.org/abs/2107.04497"/>
        <updated>2021-07-12T01:55:15.968Z</updated>
        <summary type="html"><![CDATA[Heteroscedastic regression is the task of supervised learning where each
label is subject to noise from a different distribution. This noise can be
caused by the labelling process, and impacts negatively the performance of the
learning algorithm as it violates the i.i.d. assumptions. In many situations
however, the labelling process is able to estimate the variance of such
distribution for each label, which can be used as an additional information to
mitigate this impact. We adapt an inverse-variance weighted mean square error,
based on the Gauss-Markov theorem, for parameter optimization on neural
networks. We introduce Batch Inverse-Variance, a loss function which is robust
to near-ground truth samples, and allows to control the effective learning
rate. Our experimental results show that BIV improves significantly the
performance of the networks on two noisy datasets, compared to L2 loss,
inverse-variance weighting, as well as a filtering-based baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_V/0/1/0/all/0/1"&gt;Vincent Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khamies_W/0/1/0/all/0/1"&gt;Waleed Khamies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding surrogate explanations: the interplay between complexity, fidelity and coverage. (arXiv:2107.04309v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04309</id>
        <link href="http://arxiv.org/abs/2107.04309"/>
        <updated>2021-07-12T01:55:15.962Z</updated>
        <summary type="html"><![CDATA[This paper analyses the fundamental ingredients behind surrogate explanations
to provide a better understanding of their inner workings. We start our
exposition by considering global surrogates, describing the trade-off between
complexity of the surrogate and fidelity to the black-box being modelled. We
show that transitioning from global to local - reducing coverage - allows for
more favourable conditions on the Pareto frontier of fidelity-complexity of a
surrogate. We discuss the interplay between complexity, fidelity and coverage,
and consider how different user needs can lead to problem formulations where
these are either constraints or penalties. We also present experiments that
demonstrate how the local surrogate interpretability procedure can be made
interactive and lead to better explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poyiadzi_R/0/1/0/all/0/1"&gt;Rafael Poyiadzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1"&gt;Xavier Renard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1"&gt;Thibault Laugel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1"&gt;Marcin Detyniecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiaccurate Proxies for Downstream Fairness. (arXiv:2107.04423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04423</id>
        <link href="http://arxiv.org/abs/2107.04423"/>
        <updated>2021-07-12T01:55:15.955Z</updated>
        <summary type="html"><![CDATA[We study the problem of training a model that must obey demographic fairness
conditions when the sensitive features are not available at training time -- in
other words, how can we train a model to be fair by race when we don't have
data about race? We adopt a fairness pipeline perspective, in which an
"upstream" learner that does have access to the sensitive features will learn a
proxy model for these features from the other attributes. The goal of the proxy
is to allow a general "downstream" learner -- with minimal assumptions on their
prediction task -- to be able to use the proxy to train a model that is fair
with respect to the true sensitive features. We show that obeying multiaccuracy
constraints with respect to the downstream model class suffices for this
purpose, and provide sample- and oracle efficient-algorithms and generalization
bounds for learning such proxies. In general, multiaccuracy can be much easier
to satisfy than classification accuracy, and can be satisfied even when the
sensitive features are hard to predict.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diana_E/0/1/0/all/0/1"&gt;Emily Diana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gill_W/0/1/0/all/0/1"&gt;Wesley Gill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1"&gt;Michael Kearns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharifi_Malvajerdi_S/0/1/0/all/0/1"&gt;Saeed Sharifi-Malvajerdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A First Look at Class Incremental Learning in Deep Learning Mobile Traffic Classification. (arXiv:2107.04464v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2107.04464</id>
        <link href="http://arxiv.org/abs/2107.04464"/>
        <updated>2021-07-12T01:55:15.937Z</updated>
        <summary type="html"><![CDATA[The recent popularity growth of Deep Learning (DL) re-ignited the interest
towards traffic classification, with several studies demonstrating the accuracy
of DL-based classifiers to identify Internet applications' traffic. Even with
the aid of hardware accelerators (GPUs, TPUs), DL model training remains
expensive, and limits the ability to operate frequent model updates necessary
to fit to the ever evolving nature of Internet traffic, and mobile traffic in
particular. To address this pain point, in this work we explore Incremental
Learning (IL) techniques to add new classes to models without a full
retraining, hence speeding up model's updates cycle. We consider iCarl, a state
of the art IL method, and MIRAGE-2019, a public dataset with traffic from 40
Android apps, aiming to understand "if there is a case for incremental learning
in traffic classification". By dissecting iCarl internals, we discuss ways to
improve its design, contributing a revised version, namely iCarl+. Despite our
analysis reveals their infancy, IL techniques are a promising research area on
the roadmap towards automated DL-based traffic analysis systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bovenzi_G/0/1/0/all/0/1"&gt;Giampaolo Bovenzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lixuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finamore_A/0/1/0/all/0/1"&gt;Alessandro Finamore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aceto_G/0/1/0/all/0/1"&gt;Giuseppe Aceto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciuonzo_D/0/1/0/all/0/1"&gt;Domenico Ciuonzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pescape_A/0/1/0/all/0/1"&gt;Antonio Pescap&amp;#xe8;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1"&gt;Dario Rossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04435</id>
        <link href="http://arxiv.org/abs/2107.04435"/>
        <updated>2021-07-12T01:55:15.928Z</updated>
        <summary type="html"><![CDATA[Given the increasing threat of adversarial attacks on deep neural networks
(DNNs), research on efficient detection methods is more important than ever. In
this work, we take a closer look at adversarial attack detection based on the
class scores of an already trained classification model. We propose to train a
support vector machine (SVM) on the class scores to detect adversarial
examples. Our method is able to detect adversarial examples generated by
various attacks, and can be easily adopted to a plethora of deep classification
models. We show that our approach yields an improved detection rate compared to
an existing method, whilst being easy to implement. We perform an extensive
empirical analysis on different deep classification models, investigating
various state-of-the-art adversarial attacks. Moreover, we observe that our
proposed method is better at detecting a combination of adversarial attacks.
This work indicates the potential of detecting various adversarial attacks
simply by using the class scores of an already trained classification model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1"&gt;Felix Michels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1"&gt;Oliver De Candido&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Model Robustness with Latent Distribution Locally and Globally. (arXiv:2107.04401v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04401</id>
        <link href="http://arxiv.org/abs/2107.04401"/>
        <updated>2021-07-12T01:55:15.922Z</updated>
        <summary type="html"><![CDATA[In this work, we consider model robustness of deep neural networks against
adversarial attacks from a global manifold perspective. Leveraging both the
local and global latent information, we propose a novel adversarial training
method through robust optimization, and a tractable way to generate Latent
Manifold Adversarial Examples (LMAEs) via an adversarial game between a
discriminator and a classifier. The proposed adversarial training with latent
distribution (ATLD) method defends against adversarial attacks by crafting
LMAEs with the latent manifold in an unsupervised manner. ATLD preserves the
local and global information of latent manifold and promises improved
robustness against adversarial attacks. To verify the effectiveness of our
proposed method, we conduct extensive experiments over different datasets
(e.g., CIFAR-10, CIFAR-100, SVHN) with different adversarial attacks (e.g.,
PGD, CW), and show that our method substantially outperforms the
state-of-the-art (e.g., Feature Scattering) in adversarial robustness by a
large accuracy margin. The source codes are available at
https://github.com/LitterQ/ATLD-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhuang Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shufei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiufeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinping Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoder-driven Spiral Representation Learning for Gravitational Wave Surrogate Modelling. (arXiv:2107.04312v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04312</id>
        <link href="http://arxiv.org/abs/2107.04312"/>
        <updated>2021-07-12T01:55:15.914Z</updated>
        <summary type="html"><![CDATA[Recently, artificial neural networks have been gaining momentum in the field
of gravitational wave astronomy, for example in surrogate modelling of
computationally expensive waveform models for binary black hole inspiral and
merger. Surrogate modelling yields fast and accurate approximations of
gravitational waves and neural networks have been used in the final step of
interpolating the coefficients of the surrogate model for arbitrary waveforms
outside the training sample. We investigate the existence of underlying
structures in the empirical interpolation coefficients using autoencoders. We
demonstrate that when the coefficient space is compressed to only two
dimensions, a spiral structure appears, wherein the spiral angle is linearly
related to the mass ratio. Based on this finding, we design a spiral module
with learnable parameters, that is used as the first layer in a neural network,
which learns to map the input space to the coefficients. The spiral module is
evaluated on multiple neural network architectures and consistently achieves
better speed-accuracy trade-off than baseline models. A thorough experimental
study is conducted and the final result is a surrogate model which can evaluate
millions of input parameters in a single forward pass in under 1ms on a desktop
GPU, while the mismatch between the corresponding generated waveforms and the
ground-truth waveforms is better than the compared baseline methods. We
anticipate the existence of analogous underlying structures and corresponding
computational gains also in the case of spinning black hole binaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1"&gt;Paraskevi Nousi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkouli_S/0/1/0/all/0/1"&gt;Styliani-Christina Fragkouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1"&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosif_P/0/1/0/all/0/1"&gt;Panagiotis Iosif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Apostolatos_T/0/1/0/all/0/1"&gt;Theocharis Apostolatos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1"&gt;George Pappas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stergioulas_N/0/1/0/all/0/1"&gt;Nikolaos Stergioulas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1"&gt;Anastasios Tefas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model compression as constrained optimization, with application to neural nets. Part V: combining compressions. (arXiv:2107.04380v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04380</id>
        <link href="http://arxiv.org/abs/2107.04380"/>
        <updated>2021-07-12T01:55:15.906Z</updated>
        <summary type="html"><![CDATA[Model compression is generally performed by using quantization, low-rank
approximation or pruning, for which various algorithms have been researched in
recent years. One fundamental question is: what types of compression work
better for a given model? Or even better: can we improve by combining
compressions in a suitable way? We formulate this generally as a problem of
optimizing the loss but where the weights are constrained to equal an additive
combination of separately compressed parts; and we give an algorithm to learn
the corresponding parts' parameters. Experimentally with deep neural nets, we
observe that 1) we can find significantly better models in the
error-compression space, indicating that different compression types have
complementary benefits, and 2) the best type of combination depends exquisitely
on the type of neural net. For example, we can compress ResNets and AlexNet
using only 1 bit per weight without error degradation at the cost of adding a
few floating point weights. However, VGG nets can be better compressed by
combining low-rank with a few floating point weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_Perpinan_M/0/1/0/all/0/1"&gt;Miguel &amp;#xc1;. Carreira-Perpi&amp;#xf1;&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Idelbayev_Y/0/1/0/all/0/1"&gt;Yerlan Idelbayev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to choose an Explainability Method? Towards a Methodical Implementation of XAI in Practice. (arXiv:2107.04427v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04427</id>
        <link href="http://arxiv.org/abs/2107.04427"/>
        <updated>2021-07-12T01:55:15.889Z</updated>
        <summary type="html"><![CDATA[Explainability is becoming an important requirement for organizations that
make use of automated decision-making due to regulatory initiatives and a shift
in public awareness. Various and significantly different algorithmic methods to
provide this explainability have been introduced in the field, but the existing
literature in the machine learning community has paid little attention to the
stakeholder whose needs are rather studied in the human-computer interface
community. Therefore, organizations that want or need to provide this
explainability are confronted with the selection of an appropriate method for
their use case. In this paper, we argue there is a need for a methodology to
bridge the gap between stakeholder needs and explanation methods. We present
our ongoing work on creating this methodology to help data scientists in the
process of providing explainability to stakeholders. In particular, our
contributions include documents used to characterize XAI methods and user
requirements (shown in Appendix), which our methodology builds upon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vermeire_T/0/1/0/all/0/1"&gt;Tom Vermeire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laugel_T/0/1/0/all/0/1"&gt;Thibault Laugel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renard_X/0/1/0/all/0/1"&gt;Xavier Renard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martens_D/0/1/0/all/0/1"&gt;David Martens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1"&gt;Marcin Detyniecki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04239</id>
        <link href="http://arxiv.org/abs/2107.04239"/>
        <updated>2021-07-12T01:55:15.882Z</updated>
        <summary type="html"><![CDATA[Neural approaches have achieved state-of-the-art accuracy on machine
translation but suffer from the high cost of collecting large scale parallel
data. Thus, a lot of research has been conducted for neural machine translation
(NMT) with very limited parallel data, i.e., the low-resource setting. In this
paper, we provide a survey for low-resource NMT and classify related works into
three categories according to the auxiliary data they used: (1) exploiting
monolingual data of source and/or target languages, (2) exploiting data from
auxiliary languages, and (3) exploiting multi-modal data. We hope that our
survey can help researchers to better understand this field and inspire them to
design better algorithms, and help industry practitioners to choose appropriate
algorithms for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence analysis for gradient flows in the training of artificial neural networks with ReLU activation. (arXiv:2107.04479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04479</id>
        <link href="http://arxiv.org/abs/2107.04479"/>
        <updated>2021-07-12T01:55:15.876Z</updated>
        <summary type="html"><![CDATA[Gradient descent (GD) type optimization schemes are the standard methods to
train artificial neural networks (ANNs) with rectified linear unit (ReLU)
activation. Such schemes can be considered as discretizations of gradient flows
(GFs) associated to the training of ANNs with ReLU activation and most of the
key difficulties in the mathematical convergence analysis of GD type
optimization schemes in the training of ANNs with ReLU activation seem to be
already present in the dynamics of the corresponding GF differential equations.
It is the key subject of this work to analyze such GF differential equations in
the training of ANNs with ReLU activation and three layers (one input layer,
one hidden layer, and one output layer). In particular, in this article we
prove in the case where the target function is possibly multi-dimensional and
continuous and in the case where the probability distribution of the input data
is absolutely continuous with respect to the Lebesgue measure that the risk of
every bounded GF trajectory converges to the risk of a critical point. In
addition, in this article we show in the case of a 1-dimensional affine linear
target function and in the case where the probability distribution of the input
data coincides with the standard uniform distribution that the risk of every
bounded GF trajectory converges to zero if the initial risk is sufficiently
small. Finally, in the special situation where there is only one neuron on the
hidden layer (1-dimensional hidden layer) we strengthen the above named result
for affine linear target functions by proving that that the risk of every (not
necessarily bounded) GF trajectory converges to zero if the initial risk is
sufficiently small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riekert_A/0/1/0/all/0/1"&gt;Adrian Riekert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Trajectory Prediction with Structural Constraints. (arXiv:2107.04193v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2107.04193</id>
        <link href="http://arxiv.org/abs/2107.04193"/>
        <updated>2021-07-12T01:55:15.869Z</updated>
        <summary type="html"><![CDATA[This work addresses the problem of predicting the motion trajectories of
dynamic objects in the environment. Recent advances in predicting motion
patterns often rely on machine learning techniques to extrapolate motion
patterns from observed trajectories, with no mechanism to directly incorporate
known rules. We propose a novel framework, which combines probabilistic
learning and constrained trajectory optimisation. The learning component of our
framework provides a distribution over future motion trajectories conditioned
on observed past coordinates. This distribution is then used as a prior to a
constrained optimisation problem which enforces chance constraints on the
trajectory distribution. This results in constraint-compliant trajectory
distributions which closely resemble the prior. In particular, we focus our
investigation on collision constraints, such that extrapolated future
trajectory distributions conform to the environment structure. We empirically
demonstrate on real-world and simulated datasets the ability of our framework
to learn complex probabilistic motion trajectories for motion data, while
directly enforcing constraints to improve generalisability, producing more
robust and higher quality trajectory distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhi_W/0/1/0/all/0/1"&gt;Weiming Zhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ott_L/0/1/0/all/0/1"&gt;Lionel Ott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1"&gt;Fabio Ramos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04127</id>
        <link href="http://arxiv.org/abs/2107.04127"/>
        <updated>2021-07-12T01:55:15.862Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce our submission to the 2nd Affective Behavior
Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning
model on multi-databases to perform two tasks: seven basic facial expressions
prediction and valence-arousal estimation. Since these databases do not
contains labels for all the two tasks, we have applied the distillation
knowledge technique to train two networks: one teacher and one student model.
The student model will be trained using both ground truth labels and soft
labels derived from the pretrained teacher model. During the training, we add
one more task, which is the combination of the two mentioned tasks, for better
exploiting inter-task correlations. We also exploit the sharing videos between
the two tasks of the AffWild2 database that is used in the competition, to
further improve the performance of the network. Experiment results shows that
the network have achieved promising results on the validation set of the
AffWild2 database. Code and pretrained model are publicly available at
https://github.com/glmanhtu/multitask-abaw-2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1"&gt;Manh Tu Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1"&gt;Marie Beurton-Aimar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-12T01:55:15.855Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04240</id>
        <link href="http://arxiv.org/abs/2107.04240"/>
        <updated>2021-07-12T01:55:15.835Z</updated>
        <summary type="html"><![CDATA[In many applications of computer graphics, art and design, it is desirable
for a user to provide intuitive non-image input, such as text, sketch, stroke,
graph or layout, and have a computer system automatically generate
photo-realistic images that adhere to the input content. While classic works
that allow such automatic image content generation have followed a framework of
image retrieval and composition, recent advances in deep generative models such
as generative adversarial networks (GANs), variational autoencoders (VAEs), and
flow-based methods have enabled more powerful and versatile image generation
tasks. This paper reviews recent works for image synthesis given intuitive user
input, covering advances in input versatility, image generation methodology,
benchmark datasets, and evaluation metrics. This motivates new perspectives on
input representation and interactivity, cross pollination between major image
generation paradigms, and evaluation and comparison of generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specialists Outperform Generalists in Ensemble Classification. (arXiv:2107.04381v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04381</id>
        <link href="http://arxiv.org/abs/2107.04381"/>
        <updated>2021-07-12T01:55:15.827Z</updated>
        <summary type="html"><![CDATA[Consider an ensemble of $k$ individual classifiers whose accuracies are
known. Upon receiving a test point, each of the classifiers outputs a predicted
label and a confidence in its prediction for this particular test point. In
this paper, we address the question of whether we can determine the accuracy of
the ensemble. Surprisingly, even when classifiers are combined in the
statistically optimal way in this setting, the accuracy of the resulting
ensemble classifier cannot be computed from the accuracies of the individual
classifiers-as would be the case in the standard setting of confidence weighted
majority voting. We prove tight upper and lower bounds on the ensemble
accuracy. We explicitly construct the individual classifiers that attain the
upper and lower bounds: specialists and generalists. Our theoretical results
have very practical consequences: (1) If we use ensemble methods and have the
choice to construct our individual (independent) classifiers from scratch, then
we should aim for specialist classifiers rather than generalists. (2) Our
bounds can be used to determine how many classifiers are at least required to
achieve a desired ensemble accuracy. Finally, we improve our bounds by
considering the mutual information between the true label and the individual
classifier's output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meyen_S/0/1/0/all/0/1"&gt;Sascha Meyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goppert_F/0/1/0/all/0/1"&gt;Frieder G&amp;#xf6;ppert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alber_H/0/1/0/all/0/1"&gt;Helen Alber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luxburg_U/0/1/0/all/0/1"&gt;Ulrike von Luxburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franz_V/0/1/0/all/0/1"&gt;Volker H. Franz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDRLnet: A Physics-Informed Neural Network Library. (arXiv:2107.04320v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04320</id>
        <link href="http://arxiv.org/abs/2107.04320"/>
        <updated>2021-07-12T01:55:15.820Z</updated>
        <summary type="html"><![CDATA[Physics Informed Neural Network (PINN) is a scientific computing framework
used to solve both forward and inverse problems modeled by Partial Differential
Equations (PDEs). This paper introduces IDRLnet, a Python toolbox for modeling
and solving problems through PINN systematically. IDRLnet constructs the
framework for a wide range of PINN algorithms and applications. It provides a
structured way to incorporate geometric objects, data sources, artificial
neural networks, loss metrics, and optimizers within Python. Furthermore, it
provides functionality to solve noisy inverse problems, variational
minimization, and integral differential equations. New PINN variants can be
integrated into the framework easily. Source code, tutorials, and documentation
are available at \url{https://github.com/idrl-lab/idrlnet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weien Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiaoyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoqian Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[REX: Revisiting Budgeted Training with an Improved Schedule. (arXiv:2107.04197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04197</id>
        <link href="http://arxiv.org/abs/2107.04197"/>
        <updated>2021-07-12T01:55:15.807Z</updated>
        <summary type="html"><![CDATA[Deep learning practitioners often operate on a computational and monetary
budget. Thus, it is critical to design optimization algorithms that perform
well under any budget. The linear learning rate schedule is considered the best
budget-aware schedule, as it outperforms most other schedules in the low budget
regime. On the other hand, learning rate schedules -- such as the
\texttt{30-60-90} step schedule -- are known to achieve high performance when
the model can be trained for many epochs. Yet, it is often not known a priori
whether one's budget will be large or small; thus, the optimal choice of
learning rate schedule is made on a case-by-case basis. In this paper, we frame
the learning rate schedule selection problem as a combination of $i)$ selecting
a profile (i.e., the continuous function that models the learning rate
schedule), and $ii)$ choosing a sampling rate (i.e., how frequently the
learning rate is updated/sampled from this profile). We propose a novel profile
and sampling rate combination called the Reflected Exponential (REX) schedule,
which we evaluate across seven different experimental settings with both SGD
and Adam optimizers. REX outperforms the linear schedule in the low budget
regime, while matching or exceeding the performance of several state-of-the-art
learning rate schedules (linear, step, exponential, cosine, step decay on
plateau, and OneCycle) in both high and low budget regimes. Furthermore, REX
requires no added computation, storage, or hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;John Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training a Deep Neural Network via Policy Gradients for Blind Source Separation in Polyphonic Music Recordings. (arXiv:2107.04235v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04235</id>
        <link href="http://arxiv.org/abs/2107.04235"/>
        <updated>2021-07-12T01:55:15.800Z</updated>
        <summary type="html"><![CDATA[We propose a method for the blind separation of sounds of musical instruments
in audio signals. We describe the individual tones via a parametric model,
training a dictionary to capture the relative amplitudes of the harmonics. The
model parameters are predicted via a U-Net, which is a type of deep neural
network. The network is trained without ground truth information, based on the
difference between the model prediction and the individual STFT time frames.
Since some of the model parameters do not yield a useful backpropagation
gradient, we model them stochastically and employ the policy gradient instead.
To provide phase information and account for inaccuracies in the
dictionary-based representation, we also let the network output a direct
prediction, which we then use to resynthesize the audio signals for the
individual instruments. Due to the flexibility of the neural network,
inharmonicity can be incorporated seamlessly and no preprocessing of the input
spectra is required. Our algorithm yields high-quality separation results with
particularly low interference on a variety of different audio samples, both
acoustic and synthetic, provided that the sample contains enough data for the
training and that the spectral characteristics of the musical instruments are
sufficiently stable to be approximated by the dictionary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schulze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1"&gt;Johannes Leuschner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1"&gt;Emily J. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensitivity analysis in differentially private machine learning using hybrid automatic differentiation. (arXiv:2107.04265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04265</id>
        <link href="http://arxiv.org/abs/2107.04265"/>
        <updated>2021-07-12T01:55:15.780Z</updated>
        <summary type="html"><![CDATA[In recent years, formal methods of privacy protection such as differential
privacy (DP), capable of deployment to data-driven tasks such as machine
learning (ML), have emerged. Reconciling large-scale ML with the closed-form
reasoning required for the principled analysis of individual privacy loss
requires the introduction of new tools for automatic sensitivity analysis and
for tracking an individual's data and their features through the flow of
computation. For this purpose, we introduce a novel \textit{hybrid} automatic
differentiation (AD) system which combines the efficiency of reverse-mode AD
with an ability to obtain a closed-form expression for any given quantity in
the computational graph. This enables modelling the sensitivity of arbitrary
differentiable function compositions, such as the training of neural networks
on private data. We demonstrate our approach by analysing the individual DP
guarantees of statistical database queries. Moreover, we investigate the
application of our technique to the training of DP neural networks. Our
approach can enable the principled reasoning about privacy loss in the setting
of data processing, and further the development of automatic sensitivity
analysis and privacy budgeting systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_K/0/1/0/all/0/1"&gt;Kritika Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trask_A/0/1/0/all/0/1"&gt;Andrew Trask&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-07-12T01:55:15.773Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Exploration by Solving Early Terminated MDP. (arXiv:2107.04200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04200</id>
        <link href="http://arxiv.org/abs/2107.04200"/>
        <updated>2021-07-12T01:55:15.765Z</updated>
        <summary type="html"><![CDATA[Safe exploration is crucial for the real-world application of reinforcement
learning (RL). Previous works consider the safe exploration problem as
Constrained Markov Decision Process (CMDP), where the policies are being
optimized under constraints. However, when encountering any potential dangers,
human tends to stop immediately and rarely learns to behave safely in danger.
Motivated by human learning, we introduce a new approach to address safe RL
problems under the framework of Early Terminated MDP (ET-MDP). We first define
the ET-MDP as an unconstrained MDP with the same optimal value function as its
corresponding CMDP. An off-policy algorithm based on context models is then
proposed to solve the ET-MDP, which thereby solves the corresponding CMDP with
better asymptotic performance and improved learning efficiency. Experiments on
various CMDP tasks show a substantial improvement over previous methods that
directly solve CMDP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziping Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Meng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhenghao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiadong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04174</id>
        <link href="http://arxiv.org/abs/2107.04174"/>
        <updated>2021-07-12T01:55:15.758Z</updated>
        <summary type="html"><![CDATA[Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1"&gt;Jacob Donley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1"&gt;Vladimir Tourbabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung-Suk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1"&gt;Mark Broyles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1"&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1"&gt;Ravish Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04357</id>
        <link href="http://arxiv.org/abs/2107.04357"/>
        <updated>2021-07-12T01:55:15.751Z</updated>
        <summary type="html"><![CDATA[One of the major prerequisites for any deep learning approach is the
availability of large-scale training data. When dealing with scanned document
images in real world scenarios, the principal information of its content is
stored in the layout itself. In this work, we have proposed an automated deep
generative model using Graph Neural Networks (GNNs) to generate synthetic data
with highly variable and plausible document layouts that can be used to train
document interpretation systems, in this case, specially in digital mailroom
applications. It is also the first graph-based approach for document layout
generation task experimented on administrative document images, in this case,
invoices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sanket Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy structure learning from data that contains systematic missing values. (arXiv:2107.04184v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04184</id>
        <link href="http://arxiv.org/abs/2107.04184"/>
        <updated>2021-07-12T01:55:15.732Z</updated>
        <summary type="html"><![CDATA[Learning from data that contain missing values represents a common phenomenon
in many domains. Relatively few Bayesian Network structure learning algorithms
account for missing data, and those that do tend to rely on standard approaches
that assume missing data are missing at random, such as the
Expectation-Maximisation algorithm. Because missing data are often systematic,
there is a need for more pragmatic methods that can effectively deal with data
sets containing missing values not missing at random. The absence of approaches
that deal with systematic missing data impedes the application of BN structure
learning methods to real-world problems where missingness are not random. This
paper describes three variants of greedy search structure learning that utilise
pairwise deletion and inverse probability weighting to maximally leverage the
observed data and to limit potential bias caused by missing values. The first
two of the variants can be viewed as sub-versions of the third and best
performing variant, but are important in their own in illustrating the
successive improvements in learning accuracy. The empirical investigations show
that the proposed approach outperforms the commonly used and state-of-the-art
Structural EM algorithm, both in terms of learning accuracy and efficiency, as
well as both when data are missing at random and not at random.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constantinou_A/0/1/0/all/0/1"&gt;Anthony C. Constantinou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning over non-IID Data for Indoor Localization. (arXiv:2107.04189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04189</id>
        <link href="http://arxiv.org/abs/2107.04189"/>
        <updated>2021-07-12T01:55:15.716Z</updated>
        <summary type="html"><![CDATA[Localization and tracking of objects using data-driven methods is a popular
topic due to the complexity in characterizing the physics of wireless channel
propagation models. In these modeling approaches, data needs to be gathered to
accurately train models, at the same time that user's privacy is maintained. An
appealing scheme to cooperatively achieve these goals is known as Federated
Learning (FL). A challenge in FL schemes is the presence of non-independent and
identically distributed (non-IID) data, caused by unevenly exploration of
different areas. In this paper, we consider the use of recent FL schemes to
train a set of personalized models that are then optimally fused through
Bayesian rules, which makes it appropriate in the context of indoor
localization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1"&gt;Peng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1"&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junha Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Closas_P/0/1/0/all/0/1"&gt;Pau Closas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-path Convolutional Neural Networks Efficiently Improve Feature Extraction in Continuous Adventitious Lung Sound Detection. (arXiv:2107.04226v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04226</id>
        <link href="http://arxiv.org/abs/2107.04226"/>
        <updated>2021-07-12T01:55:15.710Z</updated>
        <summary type="html"><![CDATA[We previously established a large lung sound database, HF_Lung_V2 (Lung_V2).
We trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU)
networks for detecting inhalation, exhalation, continuous adventitious sound
(CAS) and discontinuous adventitious sound at the recording level on the basis
of Lung_V2. However, the performance of CAS detection was poor due to many
reasons, one of which is the highly diversified CAS patterns. To make the
original CNN-BiGRU model learn the CAS patterns more effectively and not cause
too much computing burden, three strategies involving minimal modifications of
the network architecture of the CNN layers were investigated: (1) making the
CNN layers a bit deeper by using the residual blocks, (2) making the CNN layers
a bit wider by increasing the number of CNN kernels, and (3) separating the
feature input into multiple paths (the model was denoted by Multi-path
CNN-BiGRU). The performance of CAS segment and event detection were evaluated.
Results showed that improvement in CAS detection was observed among all the
proposed architecture-modified models. The F1 score for CAS event detection of
the proposed models increased from 0.445 to 0.491-0.530, which was deemed
significant. However, the Multi-path CNN-BiGRU model outperformed the other
models in terms of the number of winning titles (five) in total nine evaluation
metrics. In addition, the Multi-path CNN-BiGRU model did not cause extra
computing burden (0.97-fold inference time) compared to the original CNN-BiGRU
model. Conclusively, the Multi-path CNN layers can efficiently improve the
effectiveness of feature extraction and subsequently result in better CAS
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On lattice-free boosted MMI training of HMM and CTC-based full-context ASR models. (arXiv:2107.04154v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2107.04154</id>
        <link href="http://arxiv.org/abs/2107.04154"/>
        <updated>2021-07-12T01:55:15.687Z</updated>
        <summary type="html"><![CDATA[Hybrid automatic speech recognition (ASR) models are typically sequentially
trained with CTC or LF-MMI criteria. However, they have vastly different
legacies and are usually implemented in different frameworks. In this paper, by
decoupling the concepts of modeling units and label topologies and building
proper numerator/denominator graphs accordingly, we establish a generalized
framework for hybrid acoustic modeling (AM). In this framework, we show that
LF-MMI is a powerful training criterion applicable to both limited-context and
full-context models, for wordpiece/mono-char/bi-char/chenone units, with both
HMM/CTC topologies. From this framework, we propose three novel training
schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with
different advantages in training performance, decoding efficiency and decoding
time-stamp accuracy. The advantages of different training schemes are evaluated
comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated
on two real world ASR tasks to show their effectiveness. Besides, we also show
bi-char(bc) HMM-MMI models can serve as better alignment models than
traditional non-neural GMM-HMMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaohui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manohar_V/0/1/0/all/0/1"&gt;Vimal Manohar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;David Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yangyang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singhal_N/0/1/0/all/0/1"&gt;Nayan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chan_J/0/1/0/all/0/1"&gt;Julian Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1"&gt;Fuchun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Mike Seltzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NDPNet: A novel non-linear data projection network for few-shot fine-grained image classification. (arXiv:2106.06988v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06988</id>
        <link href="http://arxiv.org/abs/2106.06988"/>
        <updated>2021-07-12T01:55:15.679Z</updated>
        <summary type="html"><![CDATA[Metric-based few-shot fine-grained image classification (FSFGIC) aims to
learn a transferable feature embedding network by estimating the similarities
between query images and support classes from very few examples. In this work,
we propose, for the first time, to introduce the non-linear data projection
concept into the design of FSFGIC architecture in order to address the limited
sample problem in few-shot learning and at the same time to increase the
discriminability of the model for fine-grained image classification.
Specifically, we first design a feature re-abstraction embedding network that
has the ability to not only obtain the required semantic features for effective
metric learning but also re-enhance such features with finer details from input
images. Then the descriptors of the query images and the support classes are
projected into different non-linear spaces in our proposed similarity metric
learning network to learn discriminative projection factors. This design can
effectively operate in the challenging and restricted condition of a FSFGIC
task for making the distance between the samples within the same class smaller
and the distance between samples from different classes larger and for reducing
the coupling relationship between samples from different categories.
Furthermore, a novel similarity measure based on the proposed non-linear data
project is presented for evaluating the relationships of feature information
between a query image and a support set. It is worth to note that our proposed
architecture can be easily embedded into any episodic training mechanisms for
end-to-end training from scratch. Extensive experiments on FSFGIC tasks
demonstrate the superiority of the proposed methods over the state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuefang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhe Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yongsheng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changming Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08079</id>
        <link href="http://arxiv.org/abs/2102.08079"/>
        <updated>2021-07-12T01:55:15.661Z</updated>
        <summary type="html"><![CDATA[In this study, we introduce a measure for machine perception, inspired by the
concept of Just Noticeable Difference (JND) of human perception. Based on this
measure, we suggest an adversarial image generation algorithm, which
iteratively distorts an image by an additive noise until the machine learning
model detects the change in the image by outputting a false label. The amount
of noise added to the original image is defined as the gradient of the cost
function of the machine learning model. This cost function explicitly minimizes
the amount of perturbation applied on the input image and it is regularized by
bounded range and total variation functions to assure perceptual similarity of
the adversarial image to the input. We evaluate the adversarial images
generated by our algorithm both qualitatively and quantitatively on CIFAR10,
ImageNet, and MS COCO datasets. Our experiments on image classification and
object detection tasks show that adversarial images generated by our method are
both more successful in deceiving the recognition/detection model and less
perturbed compared to the images generated by the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1"&gt;Adil Kaan Akan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1"&gt;Emre Akbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vural_F/0/1/0/all/0/1"&gt;Fatos T. Yarman Vural&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Gaussian Processes with Derivative Information Using Variational Inference. (arXiv:2107.04061v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04061</id>
        <link href="http://arxiv.org/abs/2107.04061"/>
        <updated>2021-07-12T01:55:15.652Z</updated>
        <summary type="html"><![CDATA[Gaussian processes with derivative information are useful in many settings
where derivative information is available, including numerous Bayesian
optimization and regression tasks that arise in the natural sciences.
Incorporating derivative observations, however, comes with a dominating
$O(N^3D^3)$ computational cost when training on $N$ points in $D$ input
dimensions. This is intractable for even moderately sized problems. While
recent work has addressed this intractability in the low-$D$ setting, the
high-$N$, high-$D$ setting is still unexplored and of great value, particularly
as machine learning problems increasingly become high dimensional. In this
paper, we introduce methods to achieve fully scalable Gaussian process
regression with derivatives using variational inference. Analogous to the use
of inducing values to sparsify the labels of a training set, we introduce the
concept of inducing directional derivatives to sparsify the partial derivative
information of a training set. This enables us to construct a variational
posterior that incorporates derivative information but whose size depends
neither on the full dataset size $N$ nor the full dimensionality $D$. We
demonstrate the full scalability of our approach on a variety of tasks, ranging
from a high dimensional stellarator fusion regression task to training graph
convolutional neural networks on Pubmed using Bayesian optimization.
Surprisingly, we find that our approach can improve regression performance even
in settings where only label data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padidar_M/0/1/0/all/0/1"&gt;Misha Padidar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xinran Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Leo Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1"&gt;Jacob R. Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1"&gt;David Bindel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04050</id>
        <link href="http://arxiv.org/abs/2107.04050"/>
        <updated>2021-07-12T01:55:15.644Z</updated>
        <summary type="html"><![CDATA[Learning in multi-agent systems is highly challenging due to the inherent
complexity introduced by agents' interactions. We tackle systems with a huge
population of interacting agents (e.g., swarms) via Mean-Field Control (MFC).
MFC considers an asymptotically infinite population of identical agents that
aim to collaboratively maximize the collective reward. Specifically, we
consider the case of unknown system dynamics where the goal is to
simultaneously optimize for the rewards and learn from experience. We propose
an efficient model-based reinforcement learning algorithm
$\text{M}^3\text{-UCRL}$ that runs in episodes and provably solves this
problem. $\text{M}^3\text{-UCRL}$ uses upper-confidence bounds to balance
exploration and exploitation during policy learning. Our main theoretical
contributions are the first general regret bounds for model-based RL for MFC,
obtained via a novel mean-field type analysis. $\text{M}^3\text{-UCRL}$ can be
instantiated with different models such as neural networks or Gaussian
Processes, and effectively combined with neural network policy learning. We
empirically demonstrate the convergence of $\text{M}^3\text{-UCRL}$ on the
swarm motion problem of controlling an infinite population of agents seeking to
maximize location-dependent reward and avoid congested areas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pasztor_B/0/1/0/all/0/1"&gt;Barna Pasztor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bogunovic_I/0/1/0/all/0/1"&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychophysical Oriented Saliency Map Prediction Model. (arXiv:2011.04076v10 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04076</id>
        <link href="http://arxiv.org/abs/2011.04076"/>
        <updated>2021-07-12T01:55:15.637Z</updated>
        <summary type="html"><![CDATA[Visual attention is one of the most significant characteristics for selecting
and understanding the visual redundancy of the external world. Complex scenes
include enormous redundancy. The human vision system cannot process all
information simultaneously, due to the visual information bottleneck. The human
visual system mainly focuses on dominant parts of scenes, in order to reduce
the redundant input of visual information. This is commonly known as visual
attention prediction or visual saliency map prediction. This paper proposes a
new psychophysical saliency prediction architecture, WECSF, inspired by
multi-channel model of visual cortex functioning in humans. The model consists
of opponent color channels, a wavelet transform and wavelet energy map, and a
contrast sensitivity function for extracting low-level image features and
providing maximum approximation to the human visual system. In this paper, the
proposed model is evaluated using several data sets, including the MIT1003,
MIT300, TORONTO, SID4VAM, and UCF Sports data sets, in order to demonstrate its
efficiency. We also quantitatively and qualitatively compare the saliency
prediction performance with that of other state-of-the-art models. Our model
achieved stable and very good performance. Additionally, Fourier and
spectral-inspired saliency prediction models outperformed other
state-of-the-art non-neural networks (and even deep neural network) models on
psychophysical synthetic images. Finally, the proposed model can also be
applied to spatial-temporal saliency prediction and achieved superior
performance in the evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTGAN: Training GANs with Vision Transformers. (arXiv:2107.04589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04589</id>
        <link href="http://arxiv.org/abs/2107.04589"/>
        <updated>2021-07-12T01:55:15.629Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have shown competitive performance on
image recognition while requiring less vision-specific inductive biases. In
this paper, we investigate if such observation can be extended to image
generation. To this end, we integrate the ViT architecture into generative
adversarial networks (GANs). We observe that existing regularization methods
for GANs interact poorly with self-attention, causing serious instability
during training. To resolve this issue, we introduce novel regularization
techniques for training GANs with ViTs. Empirically, our approach, named
ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2
on CIFAR-10, CelebA, and LSUN bedroom datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kwonjoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Huiwen Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ce Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Cascaded Detection Tasks with Weakly-Supervised Domain Adaptation. (arXiv:2107.04523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04523</id>
        <link href="http://arxiv.org/abs/2107.04523"/>
        <updated>2021-07-12T01:55:15.609Z</updated>
        <summary type="html"><![CDATA[In order to handle the challenges of autonomous driving, deep learning has
proven to be crucial in tackling increasingly complex tasks, such as 3D
detection or instance segmentation. State-of-the-art approaches for image-based
detection tasks tackle this complexity by operating in a cascaded fashion: they
first extract a 2D bounding box based on which additional attributes, e.g.
instance masks, are inferred. While these methods perform well, a key challenge
remains the lack of accurate and cheap annotations for the growing variety of
tasks. Synthetic data presents a promising solution but, despite the effort in
domain adaptation research, the gap between synthetic and real data remains an
open problem. In this work, we propose a weakly supervised domain adaptation
setting which exploits the structure of cascaded detection tasks. In
particular, we learn to infer the attributes solely from the source domain
while leveraging 2D bounding boxes as weak labels in both domains to explain
the domain shift. We further encourage domain-invariant features through
class-wise feature alignment using ground-truth class information, which is not
available in the unsupervised setting. As our experiments demonstrate, the
approach is competitive with fully supervised settings while outperforming
unsupervised adaptation approaches by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hanselmann_N/0/1/0/all/0/1"&gt;Niklas Hanselmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1"&gt;Nick Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortelt_B/0/1/0/all/0/1"&gt;Benedikt Ortelt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensembles of Randomized NNs for Pattern-based Time Series Forecasting. (arXiv:2107.04091v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04091</id>
        <link href="http://arxiv.org/abs/2107.04091"/>
        <updated>2021-07-12T01:55:15.600Z</updated>
        <summary type="html"><![CDATA[In this work, we propose an ensemble forecasting approach based on randomized
neural networks. Improved randomized learning streamlines the fitting abilities
of individual learners by generating network parameters in accordance with the
data and target function features. A pattern-based representation of time
series makes the proposed approach suitable for forecasting time series with
multiple seasonality. We propose six strategies for controlling the diversity
of ensemble members. Case studies conducted on four real-world forecasting
problems verified the effectiveness and superior performance of the proposed
ensemble forecasting approach. It outperformed statistical models as well as
state-of-the-art machine learning models in terms of forecasting accuracy. The
proposed approach has several advantages: fast and easy training, simple
architecture, ease of implementation, high accuracy and the ability to deal
with nonstationarity and multiple seasonality in time series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Grzegorz Dudek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelka_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Pe&amp;#x142;ka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MCMC Variational Inference via Uncorrected Hamiltonian Annealing. (arXiv:2107.04150v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04150</id>
        <link href="http://arxiv.org/abs/2107.04150"/>
        <updated>2021-07-12T01:55:15.591Z</updated>
        <summary type="html"><![CDATA[Given an unnormalized target distribution we want to obtain approximate
samples from it and a tight lower bound on its (log) normalization constant log
Z. Annealed Importance Sampling (AIS) with Hamiltonian MCMC is a powerful
method that can be used to do this. Its main drawback is that it uses
non-differentiable transition kernels, which makes tuning its many parameters
hard. We propose a framework to use an AIS-like procedure with Uncorrected
Hamiltonian MCMC, called Uncorrected Hamiltonian Annealing. Our method leads to
tight and differentiable lower bounds on log Z. We show empirically that our
method yields better performances than other competing approaches, and that the
ability to tune its parameters using reparameterization gradients may lead to
large performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geffner_T/0/1/0/all/0/1"&gt;Tomas Geffner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domke_J/0/1/0/all/0/1"&gt;Justin Domke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of 2D vs. 3D U-Net Organ Segmentation in abdominal 3D CT images. (arXiv:2107.04062v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04062</id>
        <link href="http://arxiv.org/abs/2107.04062"/>
        <updated>2021-07-12T01:55:15.578Z</updated>
        <summary type="html"><![CDATA[A two-step concept for 3D segmentation on 5 abdominal organs inside
volumetric CT images is presented. First each relevant organ's volume of
interest is extracted as bounding box. The extracted volume acts as input for a
second stage, wherein two compared U-Nets with different architectural
dimensions re-construct an organ segmentation as label mask. In this work, we
focus on comparing 2D U-Nets vs. 3D U-Net counterparts. Our initial results
indicate Dice improvements of about 6\% at maximum. In this study to our
surprise, liver and kidneys for instance were tackled significantly better
using the faster and GPU-memory saving 2D U-Nets. For other abdominal key
organs, there were no significant differences, but we observe highly
significant advantages for the 2D U-Net in terms of GPU computational efforts
for all organs under study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zettler_N/0/1/0/all/0/1"&gt;Nico Zettler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mastmeyer_A/0/1/0/all/0/1"&gt;Andre Mastmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Challenges of Open World Recognitionunder Shifting Visual Domains. (arXiv:2107.04461v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04461</id>
        <link href="http://arxiv.org/abs/2107.04461"/>
        <updated>2021-07-12T01:55:15.571Z</updated>
        <summary type="html"><![CDATA[Robotic visual systems operating in the wild must act in unconstrained
scenarios, under different environmental conditions while facing a variety of
semantic concepts, including unknown ones. To this end, recent works tried to
empower visual object recognition methods with the capability to i) detect
unseen concepts and ii) extended their knowledge over time, as images of new
semantic classes arrive. This setting, called Open World Recognition (OWR), has
the goal to produce systems capable of breaking the semantic limits present in
the initial training set. However, this training set imposes to the system not
only its own semantic limits, but also environmental ones, due to its bias
toward certain acquisition conditions that do not necessarily reflect the high
variability of the real-world. This discrepancy between training and test
distribution is called domain-shift. This work investigates whether OWR
algorithms are effective under domain-shift, presenting the first benchmark
setup for assessing fairly the performances of OWR algorithms, with and without
domain-shift. We then use this benchmark to conduct analyses in various
scenarios, showing how existing OWR algorithms indeed suffer a severe
performance degradation when train and test distributions differ. Our analysis
shows that this degradation is only slightly mitigated by coupling OWR with
domain generalization techniques, indicating that the mere plug-and-play of
existing algorithms is not enough to recognize new and unknown categories in
unseen domains. Our results clearly point toward open issues and future
research directions, that need to be investigated for building robot visual
systems able to function reliably under these challenging yet very real
conditions. Code available at
https://github.com/DarioFontanel/OWR-VisualDomains]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1"&gt;Dario Fontanel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1"&gt;Fabio Cermelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Form Follow Function? An Empirical Exploration of the Impact of Deep Neural Network Architecture Design on Hardware-Specific Acceleration. (arXiv:2107.04144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04144</id>
        <link href="http://arxiv.org/abs/2107.04144"/>
        <updated>2021-07-12T01:55:15.547Z</updated>
        <summary type="html"><![CDATA[The fine-grained relationship between form and function with respect to deep
neural network architecture design and hardware-specific acceleration is one
area that is not well studied in the research literature, with form often
dictated by accuracy as opposed to hardware function. In this study, a
comprehensive empirical exploration is conducted to investigate the impact of
deep neural network architecture design on the degree of inference speedup that
can be achieved via hardware-specific acceleration. More specifically, we
empirically study the impact of a variety of commonly used macro-architecture
design patterns across different architectural depths through the lens of
OpenVINO microprocessor-specific and GPU-specific acceleration. Experimental
results showed that while leveraging hardware-specific acceleration achieved an
average inference speed-up of 380%, the degree of inference speed-up varied
drastically depending on the macro-architecture design pattern, with the
greatest speedup achieved on the depthwise bottleneck convolution design
pattern at 550%. Furthermore, we conduct an in-depth exploration of the
correlation between FLOPs requirement, level 3 cache efficacy, and network
latency with increasing architectural depth and width. Finally, we analyze the
inference time reductions using hardware-specific acceleration when compared to
native deep learning frameworks across a wide variety of hand-crafted deep
convolutional neural network architecture designs as well as ones found via
neural architecture search strategies. We found that the DARTS-derived
architecture to benefit from the greatest improvement from hardware-specific
software acceleration (1200%) while the depthwise bottleneck convolution-based
MobileNet-V2 to have the lowest overall inference time of around 2.4 ms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1"&gt;Ellick Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[White-Box Cartoonization Using An Extended GAN Framework. (arXiv:2107.04551v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04551</id>
        <link href="http://arxiv.org/abs/2107.04551"/>
        <updated>2021-07-12T01:55:15.539Z</updated>
        <summary type="html"><![CDATA[In the present study, we propose to implement a new framework for estimating
generative models via an adversarial process to extend an existing GAN
framework and develop a white-box controllable image cartoonization, which can
generate high-quality cartooned images/videos from real-world photos and
videos. The learning purposes of our system are based on three distinct
representations: surface representation, structure representation, and texture
representation. The surface representation refers to the smooth surface of the
images. The structure representation relates to the sparse colour blocks and
compresses generic content. The texture representation shows the texture,
curves, and features in cartoon images. Generative Adversarial Network (GAN)
framework decomposes the images into different representations and learns from
them to generate cartoon images. This decomposition makes the framework more
controllable and flexible which allows users to make changes based on the
required output. This approach overcomes any previous system in terms of
maintaining clarity, colours, textures, shapes of images yet showing the
characteristics of cartoon images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1"&gt;Amey Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizvi_H/0/1/0/all/0/1"&gt;Hasan Rizvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1"&gt;Mega Satish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D RegNet: Deep Learning Model for COVID-19 Diagnosis on Chest CT Image. (arXiv:2107.04055v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04055</id>
        <link href="http://arxiv.org/abs/2107.04055"/>
        <updated>2021-07-12T01:55:15.532Z</updated>
        <summary type="html"><![CDATA[In this paper, a 3D-RegNet-based neural network is proposed for diagnosing
the physical condition of patients with coronavirus (Covid-19) infection. In
the application of clinical medicine, lung CT images are utilized by
practitioners to determine whether a patient is infected with coronavirus.
However, there are some laybacks can be considered regarding to this diagnostic
method, such as time consuming and low accuracy. As a relatively large organ of
human body, important spatial features would be lost if the lungs were
diagnosed utilizing two dimensional slice image. Therefore, in this paper, a
deep learning model with 3D image was designed. The 3D image as input data was
comprised of two-dimensional pulmonary image sequence and from which relevant
coronavirus infection 3D features were extracted and classified. The results
show that the test set of the 3D model, the result: f1 score of 0.8379 and AUC
value of 0.8807 have been achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haibo Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-Centric Representation Learning for Video Question Answering. (arXiv:2104.05166v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05166</id>
        <link href="http://arxiv.org/abs/2104.05166"/>
        <updated>2021-07-12T01:55:15.524Z</updated>
        <summary type="html"><![CDATA[Video question answering (Video QA) presents a powerful testbed for
human-like intelligent behaviors. The task demands new capabilities to
integrate video processing, language understanding, binding abstract linguistic
concepts to concrete visual artifacts, and deliberative reasoning over
spacetime. Neural networks offer a promising approach to reach this potential
through learning from examples rather than handcrafting features and rules.
However, neural networks are predominantly feature-based - they map data to
unstructured vectorial representation and thus can fall into the trap of
exploiting shortcuts through surface statistics instead of true systematic
reasoning seen in symbolic systems. To tackle this issue, we advocate for
object-centric representation as a basis for constructing spatio-temporal
structures from videos, essentially bridging the semantic gap between low-level
pattern recognition and high-level symbolic algebra. To this end, we propose a
new query-guided representation framework to turn a video into an evolving
relational graph of objects, whose features and interactions are dynamically
and conditionally inferred. The object lives are then summarized into resumes,
lending naturally for deliberative relational reasoning that produces an answer
to the query. The framework is evaluated on major Video QA datasets,
demonstrating clear benefits of the object-centric approach to video reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_L/0/1/0/all/0/1"&gt;Long Hoang Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Thao Minh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1"&gt;Vuong Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification. (arXiv:1908.00862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00862</id>
        <link href="http://arxiv.org/abs/1908.00862"/>
        <updated>2021-07-12T01:55:15.517Z</updated>
        <summary type="html"><![CDATA[In person re-identification (Re-ID), supervised methods usually need a large
amount of expensive label information, while unsupervised ones are still unable
to deliver satisfactory identification performance. In this paper, we introduce
a novel person Re-ID task called unsupervised cross-camera person Re-ID, which
only needs the within-camera (intra-camera) label information but not
cross-camera (inter-camera) labels which are more expensive to obtain. In
real-world applications, the intra-camera label information can be easily
captured by tracking algorithms or few manual annotations. In this situation,
the main challenge becomes the distribution discrepancy across different camera
views, caused by the various body pose, occlusion, image resolution,
illumination conditions, and background noises in different cameras. To address
this situation, we propose a novel Adversarial Camera Alignment Network (ACAN)
for unsupervised cross-camera person Re-ID. It consists of the camera-alignment
task and the supervised within-camera learning task. To achieve the camera
alignment, we develop a Multi-Camera Adversarial Learning (MCAL) to map images
of different cameras into a shared subspace. Particularly, we investigate two
different schemes, including the existing GRL (i.e., gradient reversal layer)
scheme and the proposed scheme called "other camera equiprobability" (OCE), to
conduct the multi-camera adversarial task. Based on this shared subspace, we
then leverage the within-camera labels to train the network. Extensive
experiments on five large-scale datasets demonstrate the superiority of ACAN
over multiple state-of-the-art unsupervised methods that take advantage of
labeled source domains and generated images by GAN-based models. In particular,
we verify that the proposed multi-camera adversarial task does contribute to
the significant improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1"&gt;Jing Huo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xin Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect Adversarial Examples Based on Class Scores. (arXiv:2107.04435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04435</id>
        <link href="http://arxiv.org/abs/2107.04435"/>
        <updated>2021-07-12T01:55:15.509Z</updated>
        <summary type="html"><![CDATA[Given the increasing threat of adversarial attacks on deep neural networks
(DNNs), research on efficient detection methods is more important than ever. In
this work, we take a closer look at adversarial attack detection based on the
class scores of an already trained classification model. We propose to train a
support vector machine (SVM) on the class scores to detect adversarial
examples. Our method is able to detect adversarial examples generated by
various attacks, and can be easily adopted to a plethora of deep classification
models. We show that our approach yields an improved detection rate compared to
an existing method, whilst being easy to implement. We perform an extensive
empirical analysis on different deep classification models, investigating
various state-of-the-art adversarial attacks. Moreover, we observe that our
proposed method is better at detecting a combination of adversarial attacks.
This work indicates the potential of detecting various adversarial attacks
simply by using the class scores of an already trained classification model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michels_F/0/1/0/all/0/1"&gt;Felix Michels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Candido_O/0/1/0/all/0/1"&gt;Oliver De Candido&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-07-12T01:55:15.486Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the development set,
the achieved CCC is 0.469 for valence and 0.649 for arousal, which
significantly outperforms the baseline method with the corresponding CCC of
0.210 and 0.230 for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many Objective Bayesian Optimization. (arXiv:2107.04126v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2107.04126</id>
        <link href="http://arxiv.org/abs/2107.04126"/>
        <updated>2021-07-12T01:55:15.479Z</updated>
        <summary type="html"><![CDATA[Some real problems require the evaluation of expensive and noisy objective
functions. Moreover, the analytical expression of these objective functions may
be unknown. These functions are known as black-boxes, for example, estimating
the generalization error of a machine learning algorithm and computing its
prediction time in terms of its hyper-parameters. Multi-objective Bayesian
optimization (MOBO) is a set of methods that has been successfully applied for
the simultaneous optimization of black-boxes. Concretely, BO methods rely on a
probabilistic model of the objective functions, typically a Gaussian process.
This model generates a predictive distribution of the objectives. However, MOBO
methods have problems when the number of objectives in a multi-objective
optimization problem are 3 or more, which is the many objective setting. In
particular, the BO process is more costly as more objectives are considered,
computing the quality of the solution via the hyper-volume is also more costly
and, most importantly, we have to evaluate every objective function, wasting
expensive computational, economic or other resources. However, as more
objectives are involved in the optimization problem, it is highly probable that
some of them are redundant and not add information about the problem solution.
A measure that represents how similar are GP predictive distributions is
proposed. We also propose a many objective Bayesian optimization algorithm that
uses this metric to determine whether two objectives are redundant. The
algorithm stops evaluating one of them if the similarity is found, saving
resources and not hurting the performance of the multi-objective BO algorithm.
We show empirical evidence in a set of toy, synthetic, benchmark and real
experiments that GPs predictive distributions of the effectiveness of the
metric and the algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Martin_L/0/1/0/all/0/1"&gt;Lucia Asencio Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garrido_Merchan_E/0/1/0/all/0/1"&gt;Eduardo C. Garrido-Merch&amp;#xe1;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Registration with Explicit Reorientation of Diffusion-Weighted Images. (arXiv:1905.12056v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12056</id>
        <link href="http://arxiv.org/abs/1905.12056"/>
        <updated>2021-07-12T01:55:15.471Z</updated>
        <summary type="html"><![CDATA[We present an information-theoretic approach to the registration of images
with directional information, and especially for diffusion-Weighted Images
(DWI), with explicit optimization over the directional scale. We call it
Locally Orderless Registration with Directions (LORD). We focus on normalized
mutual information as a robust information-theoretic similarity measure for
DWI. The framework is an extension of the LOR-DWI density-based hierarchical
scale-space model that varies and optimizes the integration, spatial,
directional, and intensity scales. As affine transformations are insufficient
for inter-subject registration, we extend the model to non-rigid deformations.
We illustrate that the proposed model deforms orientation distribution
functions (ODFs) correctly and is capable of handling the classic complex
challenges in DWI-registrations, such as the registration of fiber-crossings
along with kissing, fanning, and interleaving fibers. Our experimental results
clearly illustrate a novel promising regularizing effect, that comes from the
nonlinear orientation-based cost function. We show the properties of the
different image scales and, we show that including orientational information in
our model makes the model better at retrieving deformations in contrast to
standard scalar-based registration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jensen_H/0/1/0/all/0/1"&gt;Henrik Gr&amp;#xf8;nholt Jensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lauze_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Lauze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13677</id>
        <link href="http://arxiv.org/abs/2105.13677"/>
        <updated>2021-07-12T01:55:15.452Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior-Guided Multi-View 3D Head Reconstruction. (arXiv:2107.04277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04277</id>
        <link href="http://arxiv.org/abs/2107.04277"/>
        <updated>2021-07-12T01:55:15.445Z</updated>
        <summary type="html"><![CDATA[Recovering a 3D head model including the complete face and hair regions is
still a challenging problem in computer vision and graphics. In this paper, we
consider this problem with a few multi-view portrait images as input. Previous
multi-view stereo methods, either based on the optimization strategies or deep
learning techniques, suffer from low-frequency geometric structures such as
unclear head structures and inaccurate reconstruction in hair regions. To
tackle this problem, we propose a prior-guided implicit neural rendering
network. Specifically, we model the head geometry with a learnable signed
distance field (SDF) and optimize it via an implicit differentiable renderer
with the guidance of some human head priors, including the facial prior
knowledge, head semantic segmentation information and 2D hair orientation maps.
The utilization of these priors can improve the reconstruction accuracy and
robustness, leading to a high-quality integrated 3D head model. Extensive
ablation studies and comparisons with state-of-the-art methods demonstrate that
our method could produce high-fidelity 3D head geometries with the guidance of
these priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xueying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yudong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhongqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Juyong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics. (arXiv:2107.04092v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2107.04092</id>
        <link href="http://arxiv.org/abs/2107.04092"/>
        <updated>2021-07-12T01:55:15.437Z</updated>
        <summary type="html"><![CDATA[We present two novel optimizations that accelerate clock-based spiking neural
network (SNN) simulators. The first one targets spike timing dependent
plasticity (STDP). It combines lazy- with event-driven plasticity and
efficiently facilitates the computation of pre- and post-synaptic spikes using
bitfields and integer intrinsics. It offers higher bandwidth than event-driven
plasticity alone and achieves a 1.5x-2x speedup over our closest competitor.
The second optimization targets spike delivery. We partition our graph
representation in a way that bounds the number of neurons that need be updated
at any given time which allows us to perform said update in shared memory
instead of global memory. This is 2x-2.5x faster than our closest competitor.
Both optimizations represent the final evolutionary stages of years of
iteration on STDP and spike delivery inside "Spice" (/spaIk/), our state of the
art SNN simulator. The proposed optimizations are not exclusive to our graph
representation or pipeline but are applicable to a multitude of simulator
designs. We evaluate our performance on three well-established models and
compare ourselves against three other state of the art simulators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bautembach_D/0/1/0/all/0/1"&gt;Dennis Bautembach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1"&gt;Iason Oikonomidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1"&gt;Antonis Argyros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Awareness Attention for Few-Shot Object Detection. (arXiv:2102.12152v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12152</id>
        <link href="http://arxiv.org/abs/2102.12152"/>
        <updated>2021-07-12T01:55:15.430Z</updated>
        <summary type="html"><![CDATA[While recent progress has significantly boosted few-shot classification (FSC)
performance, few-shot object detection (FSOD) remains challenging for modern
learning systems. Existing FSOD systems follow FSC approaches, ignoring the
issues of spatial misalignment and vagueness in class representations, and
consequently result in low performance. Observing this, we propose a novel
Dual-Awareness Attention (DAnA) mechanism that can adaptively generate
query-position-aware (QPA) support features and guide the detection networks
precisely. The generated QPA features represent local information of a support
image conditioned on a given region of the query. By taking the spatial
relationships across different images into consideration, our approach
conspicuously outperforms previous FSOD methods (+6.9 AP relatively) and
achieves remarkable results even under a challenging cross-dataset evaluation
setting. Furthermore, the proposed DAnA component is flexible and adaptable to
multiple existing object detection frameworks. By equipping DAnA, conventional
object detection models, Faster R-CNN and RetinaNet, which are not designed
explicitly for few-shot learning, reach state-of-the-art performance in FSOD
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tung-I Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yueh-Cheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yu-Hsiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeh_J/0/1/0/all/0/1"&gt;Jia-Fong Yeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Winston H. Hsu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fedlearn-Algo: A flexible open-source privacy-preserving machine learning platform. (arXiv:2107.04129v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04129</id>
        <link href="http://arxiv.org/abs/2107.04129"/>
        <updated>2021-07-12T01:55:15.423Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Fedlearn-Algo, an open-source privacy preserving
machine learning platform. We use this platform to demonstrate our research and
development results on privacy preserving machine learning algorithms. As the
first batch of novel FL algorithm examples, we release vertical federated
kernel binary classification model and vertical federated random forest model.
They have been tested to be more efficient than existing vertical federated
learning models in our practice. Besides the novel FL algorithm examples, we
also release a machine communication module. The uniform data transfer
interface supports transfering widely used data formats between machines. We
will maintain this platform by adding more functional modules and algorithm
examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chaowei Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiazhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1"&gt;Tao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1"&gt;Huasong Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Houpu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heng_H/0/1/0/all/0/1"&gt;Huang Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1"&gt;Peng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liefeng Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CMRNet: Camera to LiDAR-Map Registration. (arXiv:1906.10109v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.10109</id>
        <link href="http://arxiv.org/abs/1906.10109"/>
        <updated>2021-07-12T01:55:15.415Z</updated>
        <summary type="html"><![CDATA[In this paper we present CMRNet, a realtime approach based on a Convolutional
Neural Network to localize an RGB image of a scene in a map built from LiDAR
data. Our network is not trained in the working area, i.e. CMRNet does not
learn the map. Instead it learns to match an image to the map. We validate our
approach on the KITTI dataset, processing each frame independently without any
tracking procedure. CMRNet achieves 0.27m and 1.07deg median localization
accuracy on the sequence 00 of the odometry dataset, starting from a rough pose
estimate displaced up to 3.5m and 17deg. To the best of our knowledge this is
the first CNN-based approach that learns to match images from a monocular
camera to a given, preexisting 3D LiDAR-map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1"&gt;Daniele Cattaneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1"&gt;Matteo Vaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1"&gt;Augusto Luis Ballardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1"&gt;Simone Fontana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1"&gt;Domenico Giorgio Sorrenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1"&gt;Wolfram Burgard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Negative Transfer. (arXiv:2009.00909v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00909</id>
        <link href="http://arxiv.org/abs/2009.00909"/>
        <updated>2021-07-12T01:55:15.408Z</updated>
        <summary type="html"><![CDATA[Transfer learning (TL) tries to utilize data or knowledge from one or more
source domains to facilitate the learning in a target domain. It is
particularly useful when the target domain has few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., the source domain
data/knowledge cause reduced learning performance in the target domain, has
been a long-standing and challenging problem in TL. Various approaches to
handle NT have been proposed in the literature. However, this filed lacks a
systematic survey on the formalization of NT, their factors and the algorithms
that handle NT. This paper proposes to fill this gap. First, the definition of
negative transfer is considered and a taxonomy of the factors are discussed.
Then, near fifty representative approaches for handling NT are categorized and
reviewed, from four perspectives: secure transfer, domain similarity
estimation, distant transfer and negative transfer mitigation. NT in related
fields, e.g., multi-task learning, lifelong learning, and adversarial attacks
are also discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lingfei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02924</id>
        <link href="http://arxiv.org/abs/2012.02924"/>
        <updated>2021-07-12T01:55:15.395Z</updated>
        <summary type="html"><![CDATA[We present iGibson, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson integrates several key features to facilitate the
study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson is open-source, equipped with comprehensive examples and documentation.
For more information, visit our project website:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chengshu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Linxi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1"&gt;Claudia P&amp;#xe9;rez-D&amp;#x27;Arpino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1"&gt;Shyamal Buch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1"&gt;Sanjana Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1"&gt;Lyne P. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1"&gt;Micael E. Tchapmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1"&gt;Kent Vainio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Stuttering Identification: Review, Challenges & Future Directions. (arXiv:2107.04057v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04057</id>
        <link href="http://arxiv.org/abs/2107.04057"/>
        <updated>2021-07-12T01:55:15.363Z</updated>
        <summary type="html"><![CDATA[Stuttering is a speech disorder during which the flow of speech is
interrupted by involuntary pauses and repetition of sounds. Stuttering
identification is an interesting interdisciplinary domain research problem
which involves pathology, psychology, acoustics, and signal processing that
makes it hard and complicated to detect. Recent developments in machine and
deep learning have dramatically revolutionized speech domain, however minimal
attention has been given to stuttering identification. This work fills the gap
by trying to bring researchers together from interdisciplinary fields. In this
paper, we review comprehensively acoustic features, statistical and deep
learning based stuttering/disfluency classification methods. We also present
several challenges and possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheikh_S/0/1/0/all/0/1"&gt;Shakeel Ahmad Sheikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirsch_F/0/1/0/all/0/1"&gt;Fabrice Hirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouni_S/0/1/0/all/0/1"&gt;Slim Ouni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASC-Net : Adversarial-based Selective Network for Unsupervised Anomaly Segmentation. (arXiv:2103.03664v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03664</id>
        <link href="http://arxiv.org/abs/2103.03664"/>
        <updated>2021-07-12T01:55:15.344Z</updated>
        <summary type="html"><![CDATA[We introduce a neural network framework, utilizing adversarial learning to
partition an image into two cuts, with one cut falling into a reference
distribution provided by the user. This concept tackles the task of
unsupervised anomaly segmentation, which has attracted increasing attention in
recent years due to their broad applications in tasks with unlabelled data.
This Adversarial-based Selective Cutting network (ASC-Net) bridges the two
domains of cluster-based deep learning methods and adversarial-based
anomaly/novelty detection algorithms. We evaluate this unsupervised learning
model on BraTS brain tumor segmentation, LiTS liver lesion segmentation, and
MS-SEG2015 segmentation tasks. Compared to existing methods like the AnoGAN
family, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results shed light on building an unsupervised learning algorithm
using user-defined knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1"&gt;Raunak Dey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yi Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MutualEyeContact: A conversation analysis tool with focus on eye contact. (arXiv:2107.04476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04476</id>
        <link href="http://arxiv.org/abs/2107.04476"/>
        <updated>2021-07-12T01:55:15.326Z</updated>
        <summary type="html"><![CDATA[Eye contact between individuals is particularly important for understanding
human behaviour. To further investigate the importance of eye contact in social
interactions, portable eye tracking technology seems to be a natural choice.
However, the analysis of available data can become quite complex. Scientists
need data that is calculated quickly and accurately. Additionally, the relevant
data must be automatically separated to save time. In this work, we propose a
tool called MutualEyeContact which excels in those tasks and can help
scientists to understand the importance of (mutual) eye contact in social
interactions. We combine state-of-the-art eye tracking with face recognition
based on machine learning and provide a tool for analysis and visualization of
social interaction sessions. This work is a joint collaboration of computer
scientists and cognitive scientists. It combines the fields of social and
behavioural science with computer vision and deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1"&gt;Alexander Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isomura_T/0/1/0/all/0/1"&gt;Tomoko Isomura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1"&gt;Gerd Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1"&gt;Katsumi Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activated Gradients for Deep Neural Networks. (arXiv:2107.04228v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04228</id>
        <link href="http://arxiv.org/abs/2107.04228"/>
        <updated>2021-07-12T01:55:15.306Z</updated>
        <summary type="html"><![CDATA[Deep neural networks often suffer from poor performance or even training
failure due to the ill-conditioned problem, the vanishing/exploding gradient
problem, and the saddle point problem. In this paper, a novel method by acting
the gradient activation function (GAF) on the gradient is proposed to handle
these challenges. Intuitively, the GAF enlarges the tiny gradients and
restricts the large gradient. Theoretically, this paper gives conditions that
the GAF needs to meet, and on this basis, proves that the GAF alleviates the
problems mentioned above. In addition, this paper proves that the convergence
rate of SGD with the GAF is faster than that without the GAF under some
assumptions. Furthermore, experiments on CIFAR, ImageNet, and PASCAL visual
object classes confirm the GAF's effectiveness. The experimental results also
demonstrate that the proposed method is able to be adopted in various deep
neural networks to improve their performance. The source code is publicly
available at
https://github.com/LongJin-lab/Activated-Gradients-for-Deep-Neural-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liangming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiaohao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Long Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1"&gt;Mingsheng Shang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution. (arXiv:2006.11385v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11385</id>
        <link href="http://arxiv.org/abs/2006.11385"/>
        <updated>2021-07-12T01:55:15.297Z</updated>
        <summary type="html"><![CDATA[We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in Medical Imaging. (arXiv:2104.05600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05600</id>
        <link href="http://arxiv.org/abs/2104.05600"/>
        <updated>2021-07-12T01:55:15.289Z</updated>
        <summary type="html"><![CDATA[Application of deep neural networks to medical imaging tasks has in some
sense become commonplace. Still, a "thorn in the side" of the deep learning
movement is the argument that deep networks are prone to overfitting and are
thus unable to generalize well when datasets are small (as is common in medical
imaging tasks). One way to bolster confidence is to provide mathematical
guarantees, or bounds, on network performance after training which explicitly
quantify the possibility of overfitting. In this work, we explore recent
advances using the PAC-Bayesian framework to provide bounds on generalization
error for large (stochastic) networks. While previous efforts focus on
classification in larger natural image datasets (e.g., MNIST and CIFAR-10), we
apply these techniques to both classification and segmentation in a smaller
medical imagining dataset: the ISIC 2018 challenge set. We observe the
resultant bounds are competitive compared to a simpler baseline, while also
being more explainable and alleviating the need for holdout sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1"&gt;Anthony Sicilia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovskikh_A/0/1/0/all/0/1"&gt;Anastasia Sosnovskikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Seong Jae Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutually-aware Sub-Graphs Differentiable Architecture Search. (arXiv:2107.04324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04324</id>
        <link href="http://arxiv.org/abs/2107.04324"/>
        <updated>2021-07-12T01:55:15.269Z</updated>
        <summary type="html"><![CDATA[Differentiable architecture search is prevalent in the field of NAS because
of its simplicity and efficiency, where two paradigms, multi-path algorithms
and single-path methods, are dominated. Multi-path framework (e.g. DARTS) is
intuitive but suffers from memory usage and training collapse. Single-path
methods (e.g.GDAS and ProxylessNAS) mitigate the memory issue and shrink the
gap between searching and evaluation but sacrifice the performance. In this
paper, we propose a conceptually simple yet efficient method to bridge these
two paradigms, referred as Mutually-aware Sub-Graphs Differentiable
Architecture Search (MSG-DAS). The core of our framework is a differentiable
Gumbel-TopK sampler that produces multiple mutually exclusive single-path
sub-graphs. To alleviate the severer skip-connect issue brought by multiple
sub-graphs setting, we propose a Dropblock-Identity module to stabilize the
optimization. To make best use of the available models (super-net and
sub-graphs), we introduce a memory-efficient super-net guidance distillation to
improve training. The proposed framework strikes a balance between flexible
memory usage and searching quality. We demonstrate the effectiveness of our
methods on ImageNet and CIFAR10, where the searched models show a comparable
performance as the most recent approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1"&gt;Haoxian Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Sheng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1"&gt;Yujie Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weilin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Icon Annotation For Mobile Applications. (arXiv:2107.04452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04452</id>
        <link href="http://arxiv.org/abs/2107.04452"/>
        <updated>2021-07-12T01:55:15.243Z</updated>
        <summary type="html"><![CDATA[Annotating user interfaces (UIs) that involves localization and
classification of meaningful UI elements on a screen is a critical step for
many mobile applications such as screen readers and voice control of devices.
Annotating object icons, such as menu, search, and arrow backward, is
especially challenging due to the lack of explicit labels on screens, their
similarity to pictures, and their diverse shapes. Existing studies either use
view hierarchy or pixel based methods to tackle the task. Pixel based
approaches are more popular as view hierarchy features on mobile platforms are
often incomplete or inaccurate, however it leaves out instructional information
in the view hierarchy such as resource-ids or content descriptions. We propose
a novel deep learning based multi-modal approach that combines the benefits of
both pixel and view hierarchy features as well as leverages the
state-of-the-art object detection techniques. In order to demonstrate the
utility provided, we create a high quality UI dataset by manually annotating
the most commonly used 29 icons in Rico, a large scale mobile design dataset
consisting of 72k UI screenshots. The experimental results indicate the
effectiveness of our multi-modal approach. Our model not only outperforms a
widely used object classification baseline but also pixel based object
detection models. Our study sheds light on how to combine view hierarchy with
pixel features for annotating UI elements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Ying Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Compositional Convolutional Neural Networks. (arXiv:2107.04474v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04474</id>
        <link href="http://arxiv.org/abs/2107.04474"/>
        <updated>2021-07-12T01:55:15.236Z</updated>
        <summary type="html"><![CDATA[The reasonable definition of semantic interpretability presents the core
challenge in explainable AI. This paper proposes a method to modify a
traditional convolutional neural network (CNN) into an interpretable
compositional CNN, in order to learn filters that encode meaningful visual
patterns in intermediate convolutional layers. In a compositional CNN, each
filter is supposed to consistently represent a specific compositional object
part or image region with a clear meaning. The compositional CNN learns from
image labels for classification without any annotations of parts or regions for
supervision. Our method can be broadly applied to different types of CNNs.
Experiments have demonstrated the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wen Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhihua Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shikun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiaqi Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Ping Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modal Attention for MRI and Ultrasound Volume Registration. (arXiv:2107.04548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04548</id>
        <link href="http://arxiv.org/abs/2107.04548"/>
        <updated>2021-07-12T01:55:15.229Z</updated>
        <summary type="html"><![CDATA[Prostate cancer biopsy benefits from accurate fusion of transrectal
ultrasound (TRUS) and magnetic resonance (MR) images. In the past few years,
convolutional neural networks (CNNs) have been proved powerful in extracting
image features crucial for image registration. However, challenging
applications and recent advances in computer vision suggest that CNNs are quite
limited in its ability to understand spatial correspondence between features, a
task in which the self-attention mechanism excels. This paper aims to develop a
self-attention mechanism specifically for cross-modal image registration. Our
proposed cross-modal attention block effectively maps each of the features in
one volume to all features in the corresponding volume. Our experimental
results demonstrate that a CNN network designed with the cross-modal attention
block embedded outperforms an advanced CNN network 10 times of its size. We
also incorporated visualization techniques to improve the interpretability of
our network. The source code of our work is available at
https://github.com/DIAL-RPI/Attention-Reg .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xinrui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hengtao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuanang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hanqing Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Sheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_B/0/1/0/all/0/1"&gt;Bradford J. Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1"&gt;Pingkun Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability. (arXiv:2006.14512v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14512</id>
        <link href="http://arxiv.org/abs/2006.14512"/>
        <updated>2021-07-12T01:55:15.215Z</updated>
        <summary type="html"><![CDATA[Knowledge transferability, or transfer learning, has been widely adopted to
allow a pre-trained model in the source domain to be effectively adapted to
downstream tasks in the target domain. It is thus important to explore and
understand the factors affecting knowledge transferability. In this paper, as
the first work, we analyze and demonstrate the connections between knowledge
transferability and another important phenomenon--adversarial transferability,
\emph{i.e.}, adversarial examples generated against one model can be
transferred to attack other models. Our theoretical studies show that
adversarial transferability indicates knowledge transferability and vice versa.
Moreover, based on the theoretical insights, we propose two practical
adversarial transferability metrics to characterize this process, serving as
bidirectional indicators between adversarial and knowledge transferability. We
conduct extensive experiments for different scenarios on diverse datasets,
showing a positive correlation between adversarial transferability and
knowledge transferability. Our findings will shed light on future research
about effective knowledge transfer learning and adversarial transferability
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jacky Y. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04510</id>
        <link href="http://arxiv.org/abs/2107.04510"/>
        <updated>2021-07-12T01:55:15.207Z</updated>
        <summary type="html"><![CDATA[Video quality measurement plays a critical role in the development of video
processing applications. In this paper, we show how popular quality metrics
VMAF and its tuning-resistant version VMAF NEG can be artificially increased by
video preprocessing. We propose a pipeline for tuning parameters of processing
algorithms that allows increasing VMAF by up to 218.8%. A subjective comparison
of preprocessed videos showed that with the majority of methods visual quality
drops down or stays unchanged. We show that VMAF NEG scores can also be
increased by some preprocessing methods by up to 23.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1"&gt;Maksim Siniukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors. (arXiv:2107.04517v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04517</id>
        <link href="http://arxiv.org/abs/2107.04517"/>
        <updated>2021-07-12T01:55:15.179Z</updated>
        <summary type="html"><![CDATA[Reliable epistemic uncertainty estimation is an essential component for
backend applications of deep object detectors in safety-critical environments.
Modern network architectures tend to give poorly calibrated confidences with
limited predictive power. Here, we introduce novel gradient-based uncertainty
metrics and investigate them for different object detection architectures.
Experiments on the MS COCO, PASCAL VOC and the KITTI dataset show significant
improvements in true positive / false positive discrimination and prediction of
intersection over union as compared to network confidence. We also find
improvement over Monte-Carlo dropout uncertainty metrics and further
significant boosts by aggregating different sources of uncertainty metrics.The
resulting uncertainty models generate well-calibrated confidences in all
instances. Furthermore, we implement our uncertainty quantification models into
object detection pipelines as a means to discern true against false
predictions, replacing the ordinary score-threshold-based decision rule. In our
experiments, we achieve a significant boost in detection performance in terms
of mean average precision. With respect to computational complexity, we find
that computing gradient uncertainty metrics results in floating point operation
counts similar to those of Monte-Carlo dropout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1"&gt;Tobias Riedlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1"&gt;Matthias Rottmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1"&gt;Marius Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1"&gt;Hanno Gottschalk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score refinement for confidence-based 3D multi-object tracking. (arXiv:2107.04327v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04327</id>
        <link href="http://arxiv.org/abs/2107.04327"/>
        <updated>2021-07-12T01:55:15.158Z</updated>
        <summary type="html"><![CDATA[Multi-object tracking is a critical component in autonomous navigation, as it
provides valuable information for decision-making. Many researchers tackled the
3D multi-object tracking task by filtering out the frame-by-frame 3D
detections; however, their focus was mainly on finding useful features or
proper matching metrics. Our work focuses on a neglected part of the tracking
system: score refinement and tracklet termination. We show that manipulating
the scores depending on time consistency while terminating the tracklets
depending on the tracklet score improves tracking results. We do this by
increasing the matched tracklets' score with score update functions and
decreasing the unmatched tracklets' score. Compared to count-based methods, our
method consistently produces better AMOTA and MOTA scores when utilizing
various detectors and filtering algorithms on different datasets. The
improvements in AMOTA score went up to 1.83 and 2.96 in MOTA. We also used our
method as a late-fusion ensembling method, and it performed better than
voting-based ensemble methods by a solid margin. It achieved an AMOTA score of
67.6 on nuScenes test evaluation, which is comparable to other state-of-the-art
trackers. Code is publicly available at:
\url{https://github.com/cogsys-tuebingen/CBMOT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1"&gt;Nuri Benbarka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroder_J/0/1/0/all/0/1"&gt;Jona Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hoechst Is All You Need: LymphocyteClassification with Deep Learning. (arXiv:2107.04388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04388</id>
        <link href="http://arxiv.org/abs/2107.04388"/>
        <updated>2021-07-12T01:55:15.150Z</updated>
        <summary type="html"><![CDATA[Multiplex immunofluorescence and immunohistochemistry benefit patients by
allowing cancer pathologists to identify several proteins expressed on the
surface of cells, enabling cell classification, better understanding of the
tumour micro-environment, more accurate diagnoses, prognoses, and tailored
immunotherapy based on the immune status of individual patients. However, they
are expensive and time consuming processes which require complex staining and
imaging techniques by expert technicians. Hoechst staining is much cheaper and
easier to perform, but is not typically used in this case as it binds to DNA
rather than to the proteins targeted by immunofluorescent techniques, and it
was not previously thought possible to differentiate cells expressing these
proteins based only on DNA morphology. In this work we show otherwise, training
a deep convolutional neural network to identify cells expressing three proteins
(T lymphocyte markers CD3 and CD8, and the B lymphocyte marker CD20) with
greater than 90% precision and recall, from Hoechst 33342 stained tissue only.
Our model learns previously unknown morphological features associated with
expression of these proteins which can be used to accurately differentiate
lymphocyte subtypes for use in key prognostic metrics such as assessment of
immune cell infiltration,and thereby predict and improve patient outcomes
without the need for costly multiplex immunofluorescence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_J/0/1/0/all/0/1"&gt;Jessica Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Um_I/0/1/0/all/0/1"&gt;In Hwa Um&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1"&gt;David J Harrison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04281</id>
        <link href="http://arxiv.org/abs/2107.04281"/>
        <updated>2021-07-12T01:55:15.142Z</updated>
        <summary type="html"><![CDATA[Image inpainting aims to restore the missing regions and make the recovery
results identical to the originally complete image, which is different from the
common generative task emphasizing the naturalness of generated images.
Nevertheless, existing works usually regard it as a pure generation problem and
employ cutting-edge generative techniques to address it. The generative
networks fill the main missing parts with realistic contents but usually
distort the local structures. In this paper, we formulate image inpainting as a
mix of two problems, i.e., predictive filtering and deep generation. Predictive
filtering is good at preserving local structures and removing artifacts but
falls short to complete the large missing regions. The deep generative network
can fill the numerous missing pixels based on the understanding of the whole
scene but hardly restores the details identical to the original ones. To make
use of their respective advantages, we propose the joint predictive filtering
and generative network (JPGNet) that contains three branches: predictive
filtering & uncertainty network (PFUNet), deep generative network, and
uncertainty-aware fusion network (UAFNet). The PFUNet can adaptively predict
pixel-wise kernels for filtering-based inpainting according to the input image
and output an uncertainty map. This map indicates the pixels should be
processed by filtering or generative networks, which is further fed to the
UAFNet for a smart combination between filtering and generative results. Note
that, our method as a novel framework for the image inpainting problem can
benefit any existing generation-based methods. We validate our method on three
public datasets, i.e., Dunhuang, Places2, and CelebA, and demonstrate that our
method can enhance three state-of-the-art generative methods (i.e., StructFlow,
EdgeConnect, and RFRNet) significantly with the slightly extra time cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qing Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1"&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hongkai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+wang_S/0/1/0/all/0/1"&gt;Song wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Segmentation on Multiple Visual Domains. (arXiv:2107.04326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04326</id>
        <link href="http://arxiv.org/abs/2107.04326"/>
        <updated>2021-07-12T01:55:15.133Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation models only perform well on the domain they are trained
on and datasets for training are scarce and often have a small label-spaces,
because the pixel level annotations required are expensive to make. Thus
training models on multiple existing domains is desired to increase the output
label-space. Current research shows that there is potential to improve accuracy
across datasets by using multi-domain training, but this has not yet been
successfully extended to datasets of three different non-overlapping domains
without manual labelling. In this paper a method for this is proposed for the
datasets Cityscapes, SUIM and SUN RGB-D, by creating a label-space that spans
all classes of the datasets. Duplicate classes are merged and discrepant
granularity is solved by keeping classes separate. Results show that accuracy
of the multi-domain model has higher accuracy than all baseline models
together, if hardware performance is equalized, as resources are not limitless,
showing that models benefit from additional data even from domains that have
nothing in common.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naber_F/0/1/0/all/0/1"&gt;Floris Naber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Farthest Point Sampling in Point-Wise Analysis. (arXiv:2107.04291v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04291</id>
        <link href="http://arxiv.org/abs/2107.04291"/>
        <updated>2021-07-12T01:55:15.108Z</updated>
        <summary type="html"><![CDATA[Sampling, grouping, and aggregation are three important components in the
multi-scale analysis of point clouds. In this paper, we present a novel
data-driven sampler learning strategy for point-wise analysis tasks. Unlike the
widely used sampling technique, Farthest Point Sampling (FPS), we propose to
learn sampling and downstream applications jointly. Our key insight is that
uniform sampling methods like FPS are not always optimal for different tasks:
sampling more points around boundary areas can make the point-wise
classification easier for segmentation. Towards the end, we propose a novel
sampler learning strategy that learns sampling point displacement supervised by
task-related ground truth information and can be trained jointly with the
underlying tasks. We further demonstrate our methods in various point-wise
analysis architectures, including semantic part segmentation, point cloud
completion, and keypoint detection. Our experiments show that jointly learning
of the sampler and task brings remarkable improvement over previous baseline
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yiqun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lichang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haibin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chongyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hepatocellular Carcinoma Segmentation fromDigital Subtraction Angiography Videos usingLearnable Temporal Difference. (arXiv:2107.04306v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04306</id>
        <link href="http://arxiv.org/abs/2107.04306"/>
        <updated>2021-07-12T01:55:15.101Z</updated>
        <summary type="html"><![CDATA[Automatic segmentation of hepatocellular carcinoma (HCC)in Digital
Subtraction Angiography (DSA) videos can assist radiologistsin efficient
diagnosis of HCC and accurate evaluation of tumors in clinical practice. Few
studies have investigated HCC segmentation from DSAvideos. It shows great
challenging due to motion artifacts in filming, ambiguous boundaries of tumor
regions and high similarity in imaging toother anatomical tissues. In this
paper, we raise the problem of HCCsegmentation in DSA videos, and build our own
DSA dataset. We alsopropose a novel segmentation network called DSA-LTDNet,
including asegmentation sub-network, a temporal difference learning (TDL)
moduleand a liver region segmentation (LRS) sub-network for providing
additional guidance. DSA-LTDNet is preferable for learning the latent
motioninformation from DSA videos proactively and boosting segmentation
performance. All of experiments are conducted on our self-collected
dataset.Experimental results show that DSA-LTDNet increases the DICE scoreby
nearly 4% compared to the U-Net baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenting Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yicheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changmiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuixing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior Analysis. (arXiv:2107.04225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04225</id>
        <link href="http://arxiv.org/abs/2107.04225"/>
        <updated>2021-07-12T01:55:15.089Z</updated>
        <summary type="html"><![CDATA[Affective Behavior Analysis is an important part in human?computer
interaction. Existing successful affective behavior analysis method such as
TSAV[9] suffer from challenge of incomplete labeled datasets. To boost its
performance, this paper presents a multi-task mean teacher model for
semi?supervised Affective Behavior Analysis to learn from missing labels and
exploring the learning of multiple correlated task simultaneously. To be
specific, we first utilize TSAV as baseline model to simultaneously recognize
the three tasks. We have modified the preprocessing method of rendering mask to
provide better semantics information. After that, we extended TSAV model to
semi-supervised model using mean teacher, which allow it to be benefited from
unlabeled data. Experimental results on validation datasets show that our
method achieves better performance than TSAV model, which verifies that the
proposed network can effectively learn additional unlabeled data to boost the
affective behavior analysis performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lingfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shisen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIFE: A Generalizable Autodidactic Pipeline for 3D OCT-A Vessel Segmentation. (arXiv:2107.04282v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04282</id>
        <link href="http://arxiv.org/abs/2107.04282"/>
        <updated>2021-07-12T01:55:15.082Z</updated>
        <summary type="html"><![CDATA[Optical coherence tomography (OCT) is a non-invasive imaging technique widely
used for ophthalmology. It can be extended to OCT angiography (OCT-A), which
reveals the retinal vasculature with improved contrast. Recent deep learning
algorithms produced promising vascular segmentation results; however, 3D
retinal vessel segmentation remains difficult due to the lack of manually
annotated training data. We propose a learning-based method that is only
supervised by a self-synthesized modality named local intensity fusion (LIF).
LIF is a capillary-enhanced volume computed directly from the input OCT-A. We
then construct the local intensity fusion encoder (LIFE) to map a given OCT-A
volume and its LIF counterpart to a shared latent space. The latent space of
LIFE has the same dimensions as the input data and it contains features common
to both modalities. By binarizing this latent space, we obtain a volumetric
vessel segmentation. Our method is evaluated in a human fovea OCT-A and three
zebrafish OCT-A volumes with manual labels. It yields a Dice score of 0.7736 on
human data and 0.8594 +/- 0.0275 on zebrafish data, a dramatic improvement over
existing unsupervised algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dewei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1"&gt;Can Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Larson_K/0/1/0/all/0/1"&gt;Kathleen E. Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1"&gt;Ipek Oguz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CASPIANET++: A Multidimensional Channel-Spatial Asymmetric Attention Network with Noisy Student Curriculum Learning Paradigm for Brain Tumor Segmentation. (arXiv:2107.04099v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04099</id>
        <link href="http://arxiv.org/abs/2107.04099"/>
        <updated>2021-07-12T01:55:15.075Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have been used quite successfully for
semantic segmentation of brain tumors. However, current CNNs and attention
mechanisms are stochastic in nature and neglect the morphological indicators
used by radiologists to manually annotate regions of interest. In this paper,
we introduce a channel and spatial wise asymmetric attention (CASPIAN) by
leveraging the inherent structure of tumors to detect regions of saliency. To
demonstrate the efficacy of our proposed layer, we integrate this into a
well-established convolutional neural network (CNN) architecture to achieve
higher Dice scores, with less GPU resources. Also, we investigate the inclusion
of auxiliary multiscale and multiplanar attention branches to increase the
spatial context crucial in semantic segmentation tasks. The resulting
architecture is the new CASPIANET++, which achieves Dice Scores of 91.19% whole
tumor, 87.6% for tumor core and 81.03% for enhancing tumor. Furthermore, driven
by the scarcity of brain tumor data, we investigate the Noisy Student method
for segmentation tasks. Our new Noisy Student Curriculum Learning paradigm,
which infuses noise incrementally to increase the complexity of the training
images exposed to the network, further boosts the enhancing tumor region to
81.53%. Additional validation performed on the BraTS2020 data shows that the
Noisy Student Curriculum Learning method works well without any additional
training or finetuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liew_A/0/1/0/all/0/1"&gt;Andrea Liew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chun Cheng Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_B/0/1/0/all/0/1"&gt;Boon Leong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1"&gt;Maxine Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Association based Grouping for Form Structure Extraction. (arXiv:2107.04396v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04396</id>
        <link href="http://arxiv.org/abs/2107.04396"/>
        <updated>2021-07-12T01:55:15.055Z</updated>
        <summary type="html"><![CDATA[Document structure extraction has been a widely researched area for decades.
Recent work in this direction has been deep learning-based, mostly focusing on
extracting structure using fully convolution NN through semantic segmentation.
In this work, we present a novel multi-modal approach for form structure
extraction. Given simple elements such as textruns and widgets, we extract
higher-order structures such as TextBlocks, Text Fields, Choice Fields, and
Choice Groups, which are essential for information collection in forms. To
achieve this, we obtain a local image patch around each low-level element
(reference) by identifying candidate elements closest to it. We process textual
and spatial representation of candidates sequentially through a BiLSTM to
obtain context-aware representations and fuse them with image patch features
obtained by processing it through a CNN. Subsequently, the sequential decoder
takes this fused feature vector to predict the association type between
reference and candidates. These predicted associations are utilized to
determine larger structures through connected components analysis. Experimental
results show the effectiveness of our approach achieving a recall of 90.29%,
73.80%, 83.12%, and 52.72% for the above structures, respectively,
outperforming semantic segmentation baselines significantly. We show the
efficacy of our method through ablations, comparing it against using individual
modalities. We also introduce our new rich human-annotated Forms Dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mausoom Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1"&gt;Hiresh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics forcalibrated predictive uncertainty. (arXiv:2107.04296v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-07-12T01:55:15.047Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator. (arXiv:2107.04286v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04286</id>
        <link href="http://arxiv.org/abs/2107.04286"/>
        <updated>2021-07-12T01:55:15.039Z</updated>
        <summary type="html"><![CDATA[The ability to perceive the environments in different ways is essential to
robotic research. This involves the analysis of both 2D and 3D data sources. We
present a large scale urban scene dataset associated with a handy simulator
based on Unreal Engine 4 and AirSim, which consists of both man-made and
real-world reconstruction scenes in different scales, referred to as
UrbanScene3D. Unlike previous works that purely based on 2D information or
man-made 3D CAD models, UrbanScene3D contains both compact man-made models and
detailed real-world models reconstructed by aerial images. Each building has
been manually extracted from the entire scene model and then has been assigned
with a unique label, forming an instance segmentation map. The provided 3D
ground-truth textured models with instance segmentation labels in UrbanScene3D
allow users to obtain all kinds of data they would like to have: instance
segmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in
both visible and invisible places, etc. In addition, with the help of AirSim,
users can also simulate the robots (cars/drones)to test a variety of autonomous
tasks in the proposed city environment. Please refer to our paper and
website(https://vcc.tech/UrbanScene3D/) for further details and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuyou Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Generative Modelling for Document Layout Generation. (arXiv:2107.04357v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04357</id>
        <link href="http://arxiv.org/abs/2107.04357"/>
        <updated>2021-07-12T01:55:15.032Z</updated>
        <summary type="html"><![CDATA[One of the major prerequisites for any deep learning approach is the
availability of large-scale training data. When dealing with scanned document
images in real world scenarios, the principal information of its content is
stored in the layout itself. In this work, we have proposed an automated deep
generative model using Graph Neural Networks (GNNs) to generate synthetic data
with highly variable and plausible document layouts that can be used to train
document interpretation systems, in this case, specially in digital mailroom
applications. It is also the first graph-based approach for document layout
generation task experimented on administrative document images, in this case,
invoices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sanket Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riba_P/0/1/0/all/0/1"&gt;Pau Riba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1"&gt;Josep Llad&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1"&gt;Umapada Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Pixel-Matching for Video Object Segmentation. (arXiv:2107.04279v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04279</id>
        <link href="http://arxiv.org/abs/2107.04279"/>
        <updated>2021-07-12T01:55:15.025Z</updated>
        <summary type="html"><![CDATA[Video object segmentation, aiming to segment the foreground objects given the
annotation of the first frame, has been attracting increasing attentions. Many
state-of-the-art approaches have achieved great performance by relying on
online model updating or mask-propagation techniques. However, most online
models require high computational cost due to model fine-tuning during
inference. Most mask-propagation based models are faster but with relatively
low performance due to failure to adapt to object appearance variation. In this
paper, we are aiming to design a new model to make a good balance between speed
and performance. We propose a model, called NPMCA-net, which directly localizes
foreground objects based on mask-propagation and non-local technique by
matching pixels in reference and target frames. Since we bring in information
of both first and previous frames, our network is robust to large object
appearance variation, and can better adapt to occlusions. Extensive experiments
show that our approach can achieve a new state-of-the-art performance with a
fast speed at the same time (86.5% IoU on DAVIS-2016 and 72.2% IoU on
DAVIS-2017, with speed of 0.11s per frame) under the same level comparison.
Source code is available at https://github.com/siyueyu/NPMCA-net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Siyue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;BingFeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eng Gee Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Matrix Decomposition for Deep Convolutional Neural Networks Compression. (arXiv:2107.04386v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04386</id>
        <link href="http://arxiv.org/abs/2107.04386"/>
        <updated>2021-07-12T01:55:15.018Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) with a large number of parameters
requires huge computational resources, which has limited the application of
CNNs on resources constrained appliances. Decomposition-based methods,
therefore, have been utilized to compress CNNs in recent years. However, since
the compression factor and performance are negatively correlated, the
state-of-the-art works either suffer from severe performance degradation or
have limited low compression factors. To overcome these problems, unlike
previous works compressing layers separately, we propose to compress CNNs and
alleviate performance degradation via joint matrix decomposition. The idea is
inspired by the fact that there are lots of repeated modules in CNNs, and by
projecting weights with the same structures into the same subspace, networks
can be further compressed and even accelerated. In particular, three joint
matrix decomposition schemes are developed, and the corresponding optimization
approaches based on Singular Values Decomposition are proposed. Extensive
experiments are conducted across three challenging compact CNNs and 3 benchmark
data sets to demonstrate the superior performance of our proposed algorithms.
As a result, our methods can compress the size of ResNet-34 by 22x with
slighter accuracy degradation compared with several state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaowu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jihao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Weize Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning models for benign and malign Ocular Tumor Growth Estimation. (arXiv:2107.04220v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2107.04220</id>
        <link href="http://arxiv.org/abs/2107.04220"/>
        <updated>2021-07-12T01:55:15.000Z</updated>
        <summary type="html"><![CDATA[Relatively abundant availability of medical imaging data has provided
significant support in the development and testing of Neural Network based
image processing methods. Clinicians often face issues in selecting suitable
image processing algorithm for medical imaging data. A strategy for the
selection of a proper model is presented here. The training data set comprises
optical coherence tomography (OCT) and angiography (OCT-A) images of 50 mice
eyes with more than 100 days follow-up. The data contains images from treated
and untreated mouse eyes. Four deep learning variants are tested for automatic
(a) differentiation of tumor region with healthy retinal layer and (b)
segmentation of 3D ocular tumor volumes. Exhaustive sensitivity analysis of
deep learning models is performed with respect to the number of training and
testing images using 8 eight performance indices to study accuracy,
reliability/reproducibility, and speed. U-net with UVgg16 is best for malign
tumor data set with treatment (having considerable variation) and U-net with
Inception backbone for benign tumor data (with minor variation). Loss value and
root mean square error (R.M.S.E.) are found most and least sensitive
performance indices, respectively. The performance (via indices) is found to be
exponentially improving regarding a number of training images. The segmented
OCT-Angiography data shows that neovascularization drives the tumor volume.
Image analysis shows that photodynamic imaging-assisted tumor treatment
protocol is transforming an aggressively growing tumor into a cyst. An
empirical expression is obtained to help medical professionals to choose a
particular model given the number of images and types of characteristics. We
recommend that the presented exercise should be taken as standard practice
before employing a particular deep learning model for biomedical image
analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goswami_M/0/1/0/all/0/1"&gt;Mayank Goswami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effectiveness of State-of-the-Art Super Resolution Algorithms in Surveillance Environment. (arXiv:2107.04133v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04133</id>
        <link href="http://arxiv.org/abs/2107.04133"/>
        <updated>2021-07-12T01:55:14.993Z</updated>
        <summary type="html"><![CDATA[Image Super Resolution (SR) finds applications in areas where images need to
be closely inspected by the observer to extract enhanced information. One such
focused application is an offline forensic analysis of surveillance feeds. Due
to the limitations of camera hardware, camera pose, limited bandwidth, varying
illumination conditions, and occlusions, the quality of the surveillance feed
is significantly degraded at times, thereby compromising monitoring of
behavior, activities, and other sporadic information in the scene. For the
proposed research work, we have inspected the effectiveness of four
conventional yet effective SR algorithms and three deep learning-based SR
algorithms to seek the finest method that executes well in a surveillance
environment with limited training data op-tions. These algorithms generate an
enhanced resolution output image from a sin-gle low-resolution (LR) input
image. For performance analysis, a subset of 220 images from six surveillance
datasets has been used, consisting of individuals with varying distances from
the camera, changing illumination conditions, and complex backgrounds. The
performance of these algorithms has been evaluated and compared using both
qualitative and quantitative metrics. These SR algo-rithms have also been
compared based on face detection accuracy. By analyzing and comparing the
performance of all the algorithms, a Convolutional Neural Network (CNN) based
SR technique using an external dictionary proved to be best by achieving robust
face detection accuracy and scoring optimal quantitative metric results under
different surveillance conditions. This is because the CNN layers progressively
learn more complex features using an external dictionary.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1"&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1"&gt;Ammar Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1"&gt;Ansar Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raza_R/0/1/0/all/0/1"&gt;Rana Hammad Raza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Action Unit Detection with Joint Adaptive Attention and Graph Relation. (arXiv:2107.04389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04389</id>
        <link href="http://arxiv.org/abs/2107.04389"/>
        <updated>2021-07-12T01:55:14.983Z</updated>
        <summary type="html"><![CDATA[This paper describes an approach to the facial action unit (AU) detection. In
this work, we present our submission to the Field Affective Behavior Analysis
(ABAW) 2021 competition. The proposed method uses the pre-trained JAA model as
the feature extractor, and extracts global features, face alignment features
and AU local features on the basis of multi-scale features. We take the AU
local features as the input of the graph convolution to further consider the
correlation between AU, and finally use the fused features to classify AU. The
detected accuracy was evaluated by 0.5*accuracy + 0.5*F1. Our model achieves
0.674 on the challenging Aff-Wild2 database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chenggong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Juan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weilong Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Ruomeng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhilei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelet Transform-assisted Adaptive Generative Modeling for Colorization. (arXiv:2107.04261v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04261</id>
        <link href="http://arxiv.org/abs/2107.04261"/>
        <updated>2021-07-12T01:55:14.977Z</updated>
        <summary type="html"><![CDATA[Unsupervised deep learning has recently demonstrated the promise to produce
high-quality samples. While it has tremendous potential to promote the image
colorization task, the performance is limited owing to the manifold hypothesis
in machine learning. This study presents a novel scheme that exploiting the
score-based generative model in wavelet domain to address the issue. By taking
advantage of the multi-scale and multi-channel representation via wavelet
transform, the proposed model learns the priors from stacked wavelet
coefficient components, thus learns the image characteristics under coarse and
detail frequency spectrums jointly and effectively. Moreover, such a highly
flexible generative model without adversarial optimization can execute
colorization tasks better under dual consistency terms in wavelet domain,
namely data-consistency and structure-consistency. Specifically, in the
training phase, a set of multi-channel tensors consisting of wavelet
coefficients are used as the input to train the network by denoising score
matching. In the test phase, samples are iteratively generated via annealed
Langevin dynamics with data and structure consistencies. Experiments
demonstrated remarkable improvements of the proposed model on colorization
quality, particularly on colorization robustness and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wanyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zichen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiegen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unity Perception: Generate Synthetic Data for Computer Vision. (arXiv:2107.04259v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04259</id>
        <link href="http://arxiv.org/abs/2107.04259"/>
        <updated>2021-07-12T01:55:14.970Z</updated>
        <summary type="html"><![CDATA[We introduce the Unity Perception package which aims to simplify and
accelerate the process of generating synthetic datasets for computer vision
tasks by offering an easy-to-use and highly customizable toolset. This
open-source package extends the Unity Editor and engine components to generate
perfectly annotated examples for several common computer vision tasks.
Additionally, it offers an extensible Randomization framework that lets the
user quickly construct and configure randomized simulation parameters in order
to introduce variation into the generated datasets. We provide an overview of
the provided tools and how they work, and demonstrate the value of the
generated synthetic datasets by training a 2D object detection model. The model
trained with mostly synthetic data outperforms the model trained using only
real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1"&gt;Steve Borkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1"&gt;Adam Crespi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1"&gt;Saurav Dhakad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1"&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1"&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1"&gt;You-Cyuan Jhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamalzadeh_M/0/1/0/all/0/1"&gt;Mohsen Kamalzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_S/0/1/0/all/0/1"&gt;Steven Leal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1"&gt;Pete Parisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_C/0/1/0/all/0/1"&gt;Cesar Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1"&gt;Wesley Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaman_A/0/1/0/all/0/1"&gt;Alex Thaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warren_S/0/1/0/all/0/1"&gt;Samuel Warren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_N/0/1/0/all/0/1"&gt;Nupur Yadav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic and Geometric Unfolding of StyleGAN Latent Space. (arXiv:2107.04481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04481</id>
        <link href="http://arxiv.org/abs/2107.04481"/>
        <updated>2021-07-12T01:55:14.951Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have proven to be surprisingly
efficient for image editing by inverting and manipulating the latent code
corresponding to a natural image. This property emerges from the disentangled
nature of the latent space. In this paper, we identify two geometric
limitations of such latent space: (a) euclidean distances differ from image
perceptual distance, and (b) disentanglement is not optimal and facial
attribute separation using linear model is a limiting hypothesis. We thus
propose a new method to learn a proxy latent representation using normalizing
flows to remedy these limitations, and show that this leads to a more efficient
space for face image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1"&gt;Mustafa Shukor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1"&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1"&gt;Pierre Hellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Multi-database Emotion Recognition. (arXiv:2107.04127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04127</id>
        <link href="http://arxiv.org/abs/2107.04127"/>
        <updated>2021-07-12T01:55:14.944Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce our submission to the 2nd Affective Behavior
Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning
model on multi-databases to perform two tasks: seven basic facial expressions
prediction and valence-arousal estimation. Since these databases do not
contains labels for all the two tasks, we have applied the distillation
knowledge technique to train two networks: one teacher and one student model.
The student model will be trained using both ground truth labels and soft
labels derived from the pretrained teacher model. During the training, we add
one more task, which is the combination of the two mentioned tasks, for better
exploiting inter-task correlations. We also exploit the sharing videos between
the two tasks of the AffWild2 database that is used in the competition, to
further improve the performance of the network. Experiment results shows that
the network have achieved promising results on the validation set of the
AffWild2 database. Code and pretrained model are publicly available at
https://github.com/glmanhtu/multitask-abaw-2021]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1"&gt;Manh Tu Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beurton_Aimar_M/0/1/0/all/0/1"&gt;Marie Beurton-Aimar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-modal and Multi-task Learning Method for Action Unit and Expression Recognition. (arXiv:2107.04187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04187</id>
        <link href="http://arxiv.org/abs/2107.04187"/>
        <updated>2021-07-12T01:55:14.938Z</updated>
        <summary type="html"><![CDATA[Analyzing human affect is vital for human-computer interaction systems. Most
methods are developed in restricted scenarios which are not practical for
in-the-wild settings. The Affective Behavior Analysis in-the-wild (ABAW) 2021
Contest provides a benchmark for this in-the-wild problem. In this paper, we
introduce a multi-modal and multi-task learning method by using both visual and
audio information. We use both AU and expression annotations to train the model
and apply a sequence model to further extract associations between video
frames. We achieve an AU score of 0.712 and an expression score of 0.477 on the
validation set. These results demonstrate the effectiveness of our approach in
improving model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yue Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tianqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chao Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guoqiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dropout Discriminator for Domain Adaptation. (arXiv:2107.04231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04231</id>
        <link href="http://arxiv.org/abs/2107.04231"/>
        <updated>2021-07-12T01:55:14.929Z</updated>
        <summary type="html"><![CDATA[Adaptation of a classifier to new domains is one of the challenging problems
in machine learning. This has been addressed using many deep and non-deep
learning based methods. Among the methodologies used, that of adversarial
learning is widely applied to solve many deep learning problems along with
domain adaptation. These methods are based on a discriminator that ensures
source and target distributions are close. However, here we suggest that rather
than using a point estimate obtaining by a single discriminator, it would be
useful if a distribution based on ensembles of discriminators could be used to
bridge this gap. This could be achieved using multiple classifiers or using
traditional ensemble methods. In contrast, we suggest that a Monte Carlo
dropout based ensemble discriminator could suffice to obtain the distribution
based discriminator. Specifically, we propose a curriculum based dropout
discriminator that gradually increases the variance of the sample based
distribution and the corresponding reverse gradients are used to align the
source and target feature representations. An ensemble of discriminators helps
the model to learn the data distribution efficiently. It also provides a better
gradient estimates to train the feature extractor. The detailed results and
thorough ablation analysis show that our model outperforms state-of-the-art
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1"&gt;Vinod K Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subramanian_V/0/1/0/all/0/1"&gt;Venkatesh K Subramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1"&gt;Vinay P. Namboodiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB Stream Is Enough for Temporal Action Detection. (arXiv:2107.04362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04362</id>
        <link href="http://arxiv.org/abs/2107.04362"/>
        <updated>2021-07-12T01:55:14.920Z</updated>
        <summary type="html"><![CDATA[State-of-the-art temporal action detectors to date are based on two-stream
input including RGB frames and optical flow. Although combining RGB frames and
optical flow boosts performance significantly, optical flow is a hand-designed
representation which not only requires heavy computation, but also makes it
methodologically unsatisfactory that two-stream methods are often not learned
end-to-end jointly with the flow. In this paper, we argue that optical flow is
dispensable in high-accuracy temporal action detection and image level data
augmentation (ILDA) is the key solution to avoid performance degradation when
optical flow is removed. To evaluate the effectiveness of ILDA, we design a
simple yet efficient one-stage temporal action detector based on single RGB
stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has
comparable accuracy with all existing state-of-the-art two-stream detectors
while surpassing the inference speed of previous methods by a large margin and
the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is
available at \url{https://github.com/Media-Smart/vedatad}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongxiang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yichao Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04240</id>
        <link href="http://arxiv.org/abs/2107.04240"/>
        <updated>2021-07-12T01:55:14.900Z</updated>
        <summary type="html"><![CDATA[In many applications of computer graphics, art and design, it is desirable
for a user to provide intuitive non-image input, such as text, sketch, stroke,
graph or layout, and have a computer system automatically generate
photo-realistic images that adhere to the input content. While classic works
that allow such automatic image content generation have followed a framework of
image retrieval and composition, recent advances in deep generative models such
as generative adversarial networks (GANs), variational autoencoders (VAEs), and
flow-based methods have enabled more powerful and versatile image generation
tasks. This paper reviews recent works for image synthesis given intuitive user
input, covering advances in input versatility, image generation methodology,
benchmark datasets, and evaluation metrics. This motivates new perspectives on
input representation and interactivity, cross pollination between major image
generation paradigms, and evaluation and comparison of generation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1"&gt;Yuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emotion Recognition with Incomplete Labels Using Modified Multi-task Learning Technique. (arXiv:2107.04192v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2107.04192</id>
        <link href="http://arxiv.org/abs/2107.04192"/>
        <updated>2021-07-12T01:55:14.891Z</updated>
        <summary type="html"><![CDATA[The task of predicting affective information in the wild such as seven basic
emotions or action units from human faces has gradually become more interesting
due to the accessibility and availability of massive annotated datasets. In
this study, we propose a method that utilizes the association between seven
basic emotions and twelve action units from the AffWild2 dataset. The method
based on the architecture of ResNet50 involves the multi-task learning
technique for the incomplete labels of the two tasks. By combining the
knowledge for two correlated tasks, both performances are improved by a large
margin compared to those with the model employing only one kind of label.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thinh_P/0/1/0/all/0/1"&gt;Phan Tran Dac Thinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1"&gt;Hoang Manh Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Soo-Hyung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Guee-Sang Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2107.04174</id>
        <link href="http://arxiv.org/abs/2107.04174"/>
        <updated>2021-07-12T01:55:14.881Z</updated>
        <summary type="html"><![CDATA[Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1"&gt;Jacob Donley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tourbabin_V/0/1/0/all/0/1"&gt;Vladimir Tourbabin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung-Suk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broyles_M/0/1/0/all/0/1"&gt;Mark Broyles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1"&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_R/0/1/0/all/0/1"&gt;Ravish Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text-to-Text Pre-Training for Data-to-Text Tasks. (arXiv:2005.10433v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10433</id>
        <link href="http://arxiv.org/abs/2005.10433"/>
        <updated>2021-07-12T01:55:14.803Z</updated>
        <summary type="html"><![CDATA[We study the pre-train + fine-tune strategy for data-to-text tasks. Our
experiments indicate that text-to-text pre-training in the form of T5, enables
simple, end-to-end transformer based models to outperform pipelined neural
architectures tailored for data-to-text generation, as well as alternative
language model based pre-training techniques such as BERT and GPT-2.
Importantly, T5 pre-training leads to better generalization, as evidenced by
large improvements on out-of-domain test sets. We hope our work serves as a
useful baseline for future research, as transfer learning becomes ever more
prevalent for data-to-text tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Abhinav Rastogi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Robust Deep Ensemble Classifier for Figurative Language Detection. (arXiv:2107.04372v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04372</id>
        <link href="http://arxiv.org/abs/2107.04372"/>
        <updated>2021-07-12T01:55:14.789Z</updated>
        <summary type="html"><![CDATA[Recognition and classification of Figurative Language (FL) is an open problem
of Sentiment Analysis in the broader field of Natural Language Processing (NLP)
due to the contradictory meaning contained in phrases with metaphorical
content. The problem itself contains three interrelated FL recognition tasks:
sarcasm, irony and metaphor which, in the present paper, are dealt with
advanced Deep Learning (DL) techniques. First, we introduce a data
prepossessing framework towards efficient data representation formats so that
to optimize the respective inputs to the DL models. In addition, special
features are extracted in order to characterize the syntactic, expressive,
emotional and temper content reflected in the respective social media text
references. These features aim to capture aspects of the social network user's
writing method. Finally, features are fed to a robust, Deep Ensemble Soft
Classifier (DESC) which is based on the combination of different DL techniques.
Using three different benchmark datasets (one of them containing various FL
forms) we conclude that the DESC model achieves a very good performance, worthy
of comparison with relevant methodologies and state-of-the-art technologies in
the challenging field of FL recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1"&gt;Rolandos Alexandros Potamias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siolas_G/0/1/0/all/0/1"&gt;Georgios Siolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stafylopatis_A/0/1/0/all/0/1"&gt;Andreas - Georgios Stafylopatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Summary-Oriented Question Generation for Informational Queries. (arXiv:2010.09692v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09692</id>
        <link href="http://arxiv.org/abs/2010.09692"/>
        <updated>2021-07-12T01:55:14.669Z</updated>
        <summary type="html"><![CDATA[Users frequently ask simple factoid questions for question answering (QA)
systems, attenuating the impact of myriad recent works that support more
complex questions. Prompting users with automatically generated suggested
questions (SQs) can improve user understanding of QA system capabilities and
thus facilitate more effective use. We aim to produce self-explanatory
questions that focus on main document topics and are answerable with variable
length passages as appropriate. We satisfy these requirements by using a
BERT-based Pointer-Generator Network trained on the Natural Questions (NQ)
dataset. Our model shows SOTA performance of SQ generation on the NQ dataset
(20.1 BLEU-4). We further apply our model on out-of-domain news articles,
evaluating with a QA system due to the lack of gold questions and demonstrate
that our model produces better SQs for news articles -- with further
confirmation via a human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1"&gt;Xusen Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Li Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1"&gt;Kevin Small&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021. (arXiv:2107.00279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00279</id>
        <link href="http://arxiv.org/abs/2107.00279"/>
        <updated>2021-07-12T01:55:14.661Z</updated>
        <summary type="html"><![CDATA[This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous
Speech Translation task. We proposed a novel simultaneous translation model,
Cross Attention Augmented Transducer (CAAT), which extends conventional RNN-T
to sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous
translation. Experiments on speech-to-text (S2T) and text-to-text (T2T)
simultaneous translation tasks shows CAAT achieves better quality-latency
trade-offs compared to \textit{wait-k}, one of the previous state-of-the-art
approaches. Based on CAAT architecture and data augmentation, we build S2T and
T2T simultaneous translation systems in this evaluation campaign. Compared to
last year's optimal systems, our S2T simultaneous translation system improves
by an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous
translation system improves by an average of 4.6 BLEU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Mengge Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuchen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1"&gt;Lirong Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAN-NTM: Topic Attention Networks for Neural Topic Modeling. (arXiv:2012.01524v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01524</id>
        <link href="http://arxiv.org/abs/2012.01524"/>
        <updated>2021-07-12T01:55:14.653Z</updated>
        <summary type="html"><![CDATA[Topic models have been widely used to learn text representations and gain
insight into document corpora. To perform topic discovery, most existing neural
models either take document bag-of-words (BoW) or sequence of tokens as input
followed by variational inference and BoW reconstruction to learn topic-word
distribution. However, leveraging topic-word distribution for learning better
features during document encoding has not been explored much. To this end, we
develop a framework TAN-NTM, which processes document as a sequence of tokens
through a LSTM whose contextual outputs are attended in a topic-aware manner.
We propose a novel attention mechanism which factors in topic-word distribution
to enable the model to attend on relevant words that convey topic related cues.
The output of topic attention module is then used to carry out variational
inference. We perform extensive ablations and experiments resulting in ~9-15
percentage improvement over score of existing SOTA topic models in NPMI
coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity
and AGNews. Further, we show that our method learns better latent
document-topic features compared to existing topic models through improvement
on two downstream tasks: document classification and topic guided keyphrase
generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panwar_M/0/1/0/all/0/1"&gt;Madhur Panwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shailabh_S/0/1/0/all/0/1"&gt;Shashank Shailabh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1"&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1"&gt;Balaji Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Byte2Speech Models for Scalable Low-resource Speech Synthesis. (arXiv:2103.03541v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03541</id>
        <link href="http://arxiv.org/abs/2103.03541"/>
        <updated>2021-07-12T01:55:14.639Z</updated>
        <summary type="html"><![CDATA[To scale neural speech synthesis to various real-world languages, we present
a multilingual end-to-end framework that maps byte inputs to spectrograms, thus
allowing arbitrary input scripts. Besides strong results on 40+ languages, the
framework demonstrates capabilities to adapt to new languages under extreme
low-resource and even few-shot scenarios of merely 40s transcribed recording,
without the need of per-language resources like lexicon, extra corpus,
auxiliary models, or linguistic expertise, thus ensuring scalability. While it
retains satisfactory intelligibility and naturalness matching rich-resource
models. Exhaustive comparative and ablation studies are performed to reveal the
potential of the framework for low-resource languages. Furthermore, we propose
a novel method to extract language-specific sub-networks in a multilingual
model for a better understanding of its mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1"&gt;Mutian He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingzhou Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1"&gt;Frank K. Soong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-interpretable Convolutional Neural Networks for Text Classification. (arXiv:2105.08589v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08589</id>
        <link href="http://arxiv.org/abs/2105.08589"/>
        <updated>2021-07-12T01:55:14.612Z</updated>
        <summary type="html"><![CDATA[Deep learning models for natural language processing (NLP) are inherently
complex and often viewed as black box in nature. This paper develops an
approach for interpreting convolutional neural networks for text classification
problems by exploiting the local-linear models inherent in ReLU-DNNs. The CNN
model combines the word embedding through convolutional layers, filters them
using max-pooling, and optimizes using a ReLU-DNN for classification. To get an
overall self-interpretable model, the system of local linear models from the
ReLU DNN are mapped back through the max-pool filter to the appropriate
n-grams. Our results on experimental datasets demonstrate that our proposed
technique produce parsimonious models that are self-interpretable and have
comparable performance with respect to a more complex CNN model. We also study
the impact of the complexity of the convolutional layers and the classification
layers on the model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rahul Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02810</id>
        <link href="http://arxiv.org/abs/2102.02810"/>
        <updated>2021-07-12T01:55:14.598Z</updated>
        <summary type="html"><![CDATA[Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.

Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Rebuffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberti_M/0/1/0/all/0/1"&gt;Marco Roberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1"&gt;Laure Soulier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1"&gt;Geoffrey Scoutheeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cancelliere_R/0/1/0/all/0/1"&gt;Rossella Cancelliere&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1"&gt;Patrick Gallinari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHAP values for Explaining CNN-based Text Classification Models. (arXiv:2008.11825v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11825</id>
        <link href="http://arxiv.org/abs/2008.11825"/>
        <updated>2021-07-12T01:55:14.588Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly used in natural language processing
(NLP) models. However, the need to interpret and explain the results from
complex algorithms are limiting their widespread adoption in regulated
industries such as banking. There has been recent work on interpretability of
machine learning algorithms with structured data. But there are only limited
techniques for NLP applications where the problem is more challenging due to
the size of the vocabulary, high-dimensional nature, and the need to consider
textual coherence and language structure. This paper develops a methodology to
compute SHAP values for local explainability of CNN-based text classification
models. The approach is also extended to compute global scores to assess the
importance of features. The results are illustrated on sentiment analysis of
Amazon Electronic Review data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_T/0/1/0/all/0/1"&gt;Tarun Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_V/0/1/0/all/0/1"&gt;Vijayan N. Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Deep Neural Networks Predict Data Correlations from Column Names?. (arXiv:2107.04553v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2107.04553</id>
        <link href="http://arxiv.org/abs/2107.04553"/>
        <updated>2021-07-12T01:55:14.578Z</updated>
        <summary type="html"><![CDATA[For humans, it is often possible to predict data correlations from column
names. We conduct experiments to find out whether deep neural networks can
learn to do the same. If so, e.g., it would open up the possibility of tuning
tools that use NLP analysis on schema elements to prioritize their efforts for
correlation detection.

We analyze correlations for around 120,000 column pairs, taken from around
4,000 data sets. We try to predict correlations, based on column names alone.
For predictions, we exploit pre-trained language models, based on the recently
proposed Transformer architecture. We consider different types of correlations,
multiple prediction methods, and various prediction scenarios. We study the
impact of factors such as column name length or the amount of training data on
prediction accuracy. Altogether, we find that deep neural networks can predict
correlations with a relatively high accuracy in many scenarios (e.g., with an
accuracy of 95% for long column names).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1"&gt;Immanuel Trummer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT. (arXiv:2107.04374v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04374</id>
        <link href="http://arxiv.org/abs/2107.04374"/>
        <updated>2021-07-12T01:55:14.570Z</updated>
        <summary type="html"><![CDATA[The availability of biomedical text data and advances in natural language
processing (NLP) have made new applications in biomedical NLP possible.
Language models trained or fine tuned using domain specific corpora can
outperform general models, but work to date in biomedical NLP has been limited
in terms of corpora and tasks. We present BioALBERT, a domain-specific
adaptation of A Lite Bidirectional Encoder Representations from Transformers
(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical
(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark
datasets. Experiments show that BioALBERT outperforms the state of the art on
named entity recognition (+11.09% BLURB score improvement), relation extraction
(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document
classification (+0.62% F1-score), and question answering (+2.83% BLURB score).
It represents a new state of the art in 17 out of 20 benchmark datasets. By
making BioALBERT models and data available, our aim is to help the biomedical
NLP community avoid computational costs of training and establish a new set of
baselines for future efforts across a broad range of biomedical NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1"&gt;Usman Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1"&gt;Adam G. Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khushi_M/0/1/0/all/0/1"&gt;Matloob Khushi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinman Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Survey of Text Worlds as Embodied Natural Language Environments. (arXiv:2107.04132v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04132</id>
        <link href="http://arxiv.org/abs/2107.04132"/>
        <updated>2021-07-12T01:55:14.562Z</updated>
        <summary type="html"><![CDATA[Text Worlds are virtual environments for embodied agents that, unlike 2D or
3D environments, are rendered exclusively using textual descriptions. These
environments offer an alternative to higher-fidelity 3D environments due to
their low barrier to entry, providing the ability to study semantics,
compositional inference, and other high-level tasks with rich high-level action
spaces while controlling for perceptual input. This systematic survey outlines
recent developments in tooling, environments, and agent modeling for Text
Worlds, while examining recent trends in knowledge graphs, common sense
reasoning, transfer learning of Text World performance to higher-fidelity
environments, as well as near-term development targets that, once achieved,
make Text Worlds an attractive general research paradigm for natural language
processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1"&gt;Peter A Jansen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Low-Resource Neural Machine Translation. (arXiv:2107.04239v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04239</id>
        <link href="http://arxiv.org/abs/2107.04239"/>
        <updated>2021-07-12T01:55:14.525Z</updated>
        <summary type="html"><![CDATA[Neural approaches have achieved state-of-the-art accuracy on machine
translation but suffer from the high cost of collecting large scale parallel
data. Thus, a lot of research has been conducted for neural machine translation
(NMT) with very limited parallel data, i.e., the low-resource setting. In this
paper, we provide a survey for low-resource NMT and classify related works into
three categories according to the auxiliary data they used: (1) exploiting
monolingual data of source and/or target languages, (2) exploiting data from
auxiliary languages, and (3) exploiting multi-modal data. We hope that our
survey can help researchers to better understand this field and inspire them to
design better algorithms, and help industry practitioners to choose appropriate
algorithms for their applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Renqian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural content-aware collaborative filtering for cold-start music recommendation. (arXiv:2102.12369v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12369</id>
        <link href="http://arxiv.org/abs/2102.12369"/>
        <updated>2021-07-12T01:55:14.512Z</updated>
        <summary type="html"><![CDATA[State-of-the-art music recommender systems are based on collaborative
filtering, which builds upon learning similarities between users and songs from
the available listening data. These approaches inherently face the cold-start
problem, as they cannot recommend novel songs with no listening history.
Content-aware recommendation addresses this issue by incorporating content
information about the songs on top of collaborative filtering. However, methods
falling in this category rely on a shallow user/item interaction that
originates from a matrix factorization framework. In this work, we introduce
neural content-aware collaborative filtering, a unified framework which
alleviates these limits, and extends the recently introduced neural
collaborative filtering to its content-aware counterpart. We propose a
generative model which leverages deep learning for both extracting content
information from low-level acoustic features and for modeling the interaction
between users and songs embeddings. The deep content feature extractor can
either directly predict the item embedding, or serve as a regularization prior,
yielding two variants (strict} and relaxed) of our model. Experimental results
show that the proposed method reaches state-of-the-art results for a cold-start
music recommendation task. We notably observe that exploiting deep neural
networks for learning refined user/item interactions outperforms approaches
using a more simple interaction model in a content-aware framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Magron_P/0/1/0/all/0/1"&gt;Paul Magron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment. (arXiv:2107.04268v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04268</id>
        <link href="http://arxiv.org/abs/2107.04268"/>
        <updated>2021-07-12T01:55:14.501Z</updated>
        <summary type="html"><![CDATA[Deep learning models for automatic readability assessment generally discard
linguistic features traditionally used in machine learning models for the task.
We propose to incorporate linguistic features into neural network models by
learning syntactic dense embeddings based on linguistic features. To cope with
the relationships between the features, we form a correlation graph among
features and use it to learn their embeddings so that similar features will be
represented by similar embeddings. Experiments with six data sets of two
proficiency levels demonstrate that our proposed methodology can complement
BERT-only model to achieve significantly better performances for automatic
readability assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xinying Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hanwu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1"&gt;Jian-Yun Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Dawei Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04082</id>
        <link href="http://arxiv.org/abs/2107.04082"/>
        <updated>2021-07-12T01:55:14.471Z</updated>
        <summary type="html"><![CDATA[Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 25 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1"&gt;Diptanu Gon Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kritika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1"&gt;Assaf Sela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Levi Graph AMR Parser using Heterogeneous Attention. (arXiv:2107.04152v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04152</id>
        <link href="http://arxiv.org/abs/2107.04152"/>
        <updated>2021-07-12T01:55:14.460Z</updated>
        <summary type="html"><![CDATA[Coupled with biaffine decoders, transformers have been effectively adapted to
text-to-graph transduction and achieved state-of-the-art performance on AMR
parsing. Many prior works, however, rely on the biaffine decoder for either or
both arc and label predictions although most features used by the decoder may
be learned by the transformer already. This paper presents a novel approach to
AMR parsing by combining heterogeneous data (tokens, concepts, labels) as one
input to a transformer to learn attention, and use only attention matrices from
the transformer to predict all elements in AMR graphs (concepts, arcs, labels).
Although our models use significantly fewer parameters than the previous
state-of-the-art graph parser, they show similar or better accuracy on AMR 2.0
and 3.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Han He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jinho D. Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Models for Answer Verification in Question Answering Systems. (arXiv:2107.04217v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04217</id>
        <link href="http://arxiv.org/abs/2107.04217"/>
        <updated>2021-07-12T01:55:14.451Z</updated>
        <summary type="html"><![CDATA[This paper studies joint models for selecting correct answer sentences among
the top $k$ provided by answer sentence selection (AS2) modules, which are core
components of retrieval-based Question Answering (QA) systems. Our work shows
that a critical step to effectively exploit an answer set regards modeling the
interrelated information between pair of answers. For this purpose, we build a
three-way multi-classifier, which decides if an answer supports, refutes, or is
neutral with respect to another one. More specifically, our neural architecture
integrates a state-of-the-art AS2 model with the multi-classifier, and a joint
layer connecting all components. We tested our models on WikiQA, TREC-QA, and a
real-world dataset. The results show that our models obtain the new state of
the art in AS2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Thuy Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1"&gt;Alessandro Moschitti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01175</id>
        <link href="http://arxiv.org/abs/2107.01175"/>
        <updated>2021-07-12T01:55:14.429Z</updated>
        <summary type="html"><![CDATA[We propose an audio-visual spatial-temporal deep neural network with: (1) a
visual block containing a pretrained 2D-CNN followed by a temporal
convolutional network (TCN); (2) an aural block containing several parallel
TCNs; and (3) a leader-follower attentive fusion block combining the
audio-visual information. The TCN with large history coverage enables our model
to exploit spatial-temporal information within a much larger window length
(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36
or 48). The fusion block emphasizes the visual modality while exploits the
noisy aural modality using the inter-modality attention mechanism. To make full
use of the data and alleviate over-fitting, cross-validation is carried out on
the training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. On the development set,
the achieved CCC is 0.469 for valence and 0.649 for arousal, which
significantly outperforms the baseline method with the corresponding CCC of
0.210 and 0.230 for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Su Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Ziquan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation. (arXiv:2107.04212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2107.04212</id>
        <link href="http://arxiv.org/abs/2107.04212"/>
        <updated>2021-07-12T01:55:14.406Z</updated>
        <summary type="html"><![CDATA[Content moderation is often performed by a collaboration between humans and
machine learning models. However, it is not well understood how to design the
collaborative process so as to maximize the combined moderator-model system
performance. This work presents a rigorous study of this problem, focusing on
an approach that incorporates model uncertainty into the collaborative process.
First, we introduce principled metrics to describe the performance of the
collaborative system under capacity constraints on the human moderator,
quantifying how efficiently the combined system utilizes human decisions. Using
these metrics, we conduct a large benchmark study evaluating the performance of
state-of-the-art uncertainty models under different collaborative review
strategies. We find that an uncertainty-based strategy consistently outperforms
the widely used strategy based on toxicity scores, and moreover that the choice
of review strategy drastically changes the overall system performance. Our
results demonstrate the importance of rigorous metrics for understanding and
developing effective moderator-model systems for content moderation, as well as
the utility of uncertainty estimation in this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kivlichan_I/0/1/0/all/0/1"&gt;Ian D. Kivlichan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasserman_L/0/1/0/all/0/1"&gt;Lucy Vasserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Translation to Localize Task Oriented NLG Output. (arXiv:2107.04512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04512</id>
        <link href="http://arxiv.org/abs/2107.04512"/>
        <updated>2021-07-12T01:55:14.390Z</updated>
        <summary type="html"><![CDATA[One of the challenges in a task oriented natural language application like
the Google Assistant, Siri, or Alexa is to localize the output to many
languages. This paper explores doing this by applying machine translation to
the English output. Using machine translation is very scalable, as it can work
with any English output and can handle dynamic text, but otherwise the problem
is a poor fit. The required quality bar is close to perfection, the range of
sentences is extremely narrow, and the sentences are often very different than
the ones in the machine translation training data. This combination of
requirements is novel in the field of domain adaptation for machine
translation. We are able to reach the required quality bar by building on
existing ideas and adding new ones: finetuning on in-domain translations,
adding sentences from the Web, adding semantic annotations, and using automatic
error detection. The paper shares our approach and results, together with a
distillation model to serve the translation models at scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Scott Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunk_C/0/1/0/all/0/1"&gt;Cliff Brunk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Justin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1"&gt;Markus Freitag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_G/0/1/0/all/0/1"&gt;Gagan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1"&gt;Sidharth Mudgal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varano_C/0/1/0/all/0/1"&gt;Chris Varano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniRE: A Unified Label Space for Entity Relation Extraction. (arXiv:2107.04292v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2107.04292</id>
        <link href="http://arxiv.org/abs/2107.04292"/>
        <updated>2021-07-12T01:55:14.376Z</updated>
        <summary type="html"><![CDATA[Many joint entity relation extraction models setup two separated label spaces
for the two sub-tasks (i.e., entity detection and relation classification). We
argue that this setting may hinder the information interaction between entities
and relations. In this work, we propose to eliminate the different treatment on
the two sub-tasks' label spaces. The input of our model is a table containing
all word pairs from a sentence. Entities and relations are represented by
squares and rectangles in the table. We apply a unified classifier to predict
each cell's label, which unifies the learning of two sub-tasks. For testing, an
effective (yet fast) approximate decoder is proposed for finding squares and
rectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)
show that, using only half the number of parameters, our model achieves
competitive accuracy with the best extractor, and is faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yijun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changzhi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuanbin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00846</id>
        <link href="http://arxiv.org/abs/2107.00846"/>
        <updated>2021-07-12T01:55:14.284Z</updated>
        <summary type="html"><![CDATA[For present e-commerce platforms, session-based recommender systems are
developed to predict users' preference for next-item recommendation. Although a
session can usually reflect a user's current preference, a local shift of the
user's intention within the session may still exist. Specifically, the
interactions that take place in the early positions within a session generally
indicate the user's initial intention, while later interactions are more likely
to represent the latest intention. Such positional information has been rarely
considered in existing methods, which restricts their ability to capture the
significance of interactions at different positions. To thoroughly exploit the
positional information within a session, a theoretical framework is developed
in this paper to provide an in-depth analysis of the positional information. We
formally define the properties of forward-awareness and backward-awareness to
evaluate the ability of positional encoding schemes in capturing the initial
and the latest intention. According to our analysis, existing positional
encoding schemes are generally forward-aware only, which can hardly represent
the dynamics of the intention in a session. To enhance the positional encoding
scheme for the session-based recommendation, a dual positional encoding (DPE)
is proposed to account for both forward-awareness and backward-awareness. Based
on DPE, we propose a novel Positional Recommender (PosRec) model with a
well-designed Position-aware Gated Graph Neural Network module to fully exploit
the positional information for session-based recommendation tasks. Extensive
experiments are conducted on two e-commerce benchmark datasets, Yoochoose and
Diginetica and the experimental results show the superiority of the PosRec by
comparing it with the state-of-the-art session-based recommender models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00852</id>
        <link href="http://arxiv.org/abs/2107.00852"/>
        <updated>2021-07-12T01:55:14.243Z</updated>
        <summary type="html"><![CDATA[Different from the traditional recommender system, the session-based
recommender system introduces the concept of the session, i.e., a sequence of
interactions between a user and multiple items within a period, to preserve the
user's recent interest. The existing work on the session-based recommender
system mainly relies on mining sequential patterns within individual sessions,
which are not expressive enough to capture more complicated dependency
relationships among items. In addition, it does not consider the cross-session
information due to the anonymity of the session data, where the linkage between
different sessions is prevented. In this paper, we solve these problems with
the graph neural networks technique. First, each session is represented as a
graph rather than a linear sequence structure, based on which a novel Full
Graph Neural Network (FGNN) is proposed to learn complicated item dependency.
To exploit and incorporate cross-session information in the individual
session's representation learning, we further construct a Broadly Connected
Session (BCS) graph to link different sessions and a novel Mask-Readout
function to improve session embedding based on the BCS graph. Extensive
experiments have been conducted on two e-commerce benchmark datasets, i.e.,
Yoochoose and Diginetica, and the experimental results demonstrate the
superiority of our proposal through comparisons with state-of-the-art
session-based recommender models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation. (arXiv:2007.02747v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02747</id>
        <link href="http://arxiv.org/abs/2007.02747"/>
        <updated>2021-07-12T01:55:14.054Z</updated>
        <summary type="html"><![CDATA[Streaming session-based recommendation (SSR) is a challenging task that
requires the recommender system to do the session-based recommendation (SR) in
the streaming scenario. In the real-world applications of e-commerce and social
media, a sequence of user-item interactions generated within a certain period
are grouped as a session, and these sessions consecutively arrive in the form
of streams. Most of the recent SR research has focused on the static setting
where the training data is first acquired and then used to train a
session-based recommender model. They need several epochs of training over the
whole dataset, which is infeasible in the streaming setting. Besides, they can
hardly well capture long-term user interests because of the neglect or the
simple usage of the user information. Although some streaming recommendation
strategies have been proposed recently, they are designed for streams of
individual interactions rather than streams of sessions. In this paper, we
propose a Global Attributed Graph (GAG) neural network model with a Wasserstein
reservoir for the SSR problem. On one hand, when a new session arrives, a
session graph with a global attribute is constructed based on the current
session and its associate user. Thus, the GAG can take both the global
attribute and the current session into consideration to learn more
comprehensive representations of the session and the user, yielding a better
performance in the recommendation. On the other hand, for the adaptation to the
streaming session scenario, a Wasserstein reservoir is proposed to help
preserve a representative sketch of the historical data. Extensive experiments
on two real-world datasets have been conducted to verify the superiority of the
GAG model compared with the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[News Recommender System: A review of recent progress, challenges, and opportunities. (arXiv:2009.04964v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04964</id>
        <link href="http://arxiv.org/abs/2009.04964"/>
        <updated>2021-07-12T01:55:14.040Z</updated>
        <summary type="html"><![CDATA[Nowadays, more and more news readers tend to read news online where they have
access to millions of news articles from multiple sources. In order to help
users to find the right and relevant content, news recommender systems (NRS)
are developed to relieve the information overload problem and suggest news
items that users might be interested in. In this paper, we highlight the major
challenges faced by the news recommendation domain and identify the possible
solutions from the state-of-the-art. Due to the rapid growth of building
recommender systems using deep learning models, we divide our discussion in two
parts. In the first part, we present an overview of the conventional
recommendation solutions, datasets, evaluation criteria beyond accuracy and
recommendation platforms being used in NRS. In the second part, we explain the
deep learning-based recommendation solutions applied in NRS. Different from
previous surveys, we also study the effects of news recommendations on user
behavior and try to suggest the possible remedies to mitigate these effects. By
providing the state-of-the-art knowledge, this survey can help researchers and
practical professionals in their understanding of developments in news
recommendation algorithms. It also sheds light on potential new directions]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1"&gt;Shaina Raza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Chen Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hacking VMAF and VMAF NEG: metrics vulnerability to different preprocessing. (arXiv:2107.04510v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2107.04510</id>
        <link href="http://arxiv.org/abs/2107.04510"/>
        <updated>2021-07-12T01:55:14.002Z</updated>
        <summary type="html"><![CDATA[Video quality measurement plays a critical role in the development of video
processing applications. In this paper, we show how popular quality metrics
VMAF and its tuning-resistant version VMAF NEG can be artificially increased by
video preprocessing. We propose a pipeline for tuning parameters of processing
algorithms that allows increasing VMAF by up to 218.8%. A subjective comparison
of preprocessed videos showed that with the majority of methods visual quality
drops down or stays unchanged. We show that VMAF NEG scores can also be
increased by some preprocessing methods by up to 23.6%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siniukov_M/0/1/0/all/0/1"&gt;Maksim Siniukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1"&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulikov_D/0/1/0/all/0/1"&gt;Dmitriy Kulikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1"&gt;Dmitriy Vatolin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks. (arXiv:1911.11942v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.11942</id>
        <link href="http://arxiv.org/abs/1911.11942"/>
        <updated>2021-07-12T01:55:13.979Z</updated>
        <summary type="html"><![CDATA[Predicting a user's preference in a short anonymous interaction session
instead of long-term history is a challenging problem in the real-life
session-based recommendation, e.g., e-commerce and media stream. Recent
research of the session-based recommender system mainly focuses on sequential
patterns by utilizing the attention mechanism, which is straightforward for the
session's natural sequence sorted by time. However, the user's preference is
much more complicated than a solely consecutive time pattern in the transition
of item choices. In this paper, therefore, we study the item transition pattern
by constructing a session graph and propose a novel model which collaboratively
considers the sequence order and the latent order in the session graph for a
session-based recommender system. We formulate the next item recommendation
within the session as a graph classification problem. Specifically, we propose
a weighted attention graph layer and a Readout function to learn embeddings of
items and sessions for the next item recommendation. Extensive experiments have
been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica,
and the experimental results show that our model outperforms other
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1"&gt;Ruihong Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crowd Sensing and Living Lab Outdoor Experimentation Made Easy. (arXiv:2107.04117v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2107.04117</id>
        <link href="http://arxiv.org/abs/2107.04117"/>
        <updated>2021-07-12T01:55:13.946Z</updated>
        <summary type="html"><![CDATA[Outdoor `living lab' experimentation using pervasive computing provides new
opportunities: higher realism, external validity and large-scale
socio-spatio-temporal observations. However, experimentation `in the wild' is
highly complex and costly. Noise, biases, privacy concerns to comply with
standards of ethical review boards, remote moderation, control of experimental
conditions and equipment perplex the collection of high-quality data for causal
inference. This article introduces Smart Agora, a novel open-source software
platform for rigorous systematic outdoor experimentation. Without writing a
single line of code, highly complex experimental scenarios are visually
designed and automatically deployed to smart phones. Novel geolocated survey
and sensor data are collected subject of participants verifying desired
experimental conditions, for instance. their presence at certain urban spots.
This new approach drastically improves the quality and purposefulness of crowd
sensing, tailored to conditions that confirm/reject hypotheses. The features
that support this innovative functionality and the broad spectrum of its
applicability are demonstrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1"&gt;Evangelos Pournaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghulam_A/0/1/0/all/0/1"&gt;Atif Nabi Ghulam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunz_R/0/1/0/all/0/1"&gt;Renato Kunz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanggli_R/0/1/0/all/0/1"&gt;Regula H&amp;#xe4;nggli&lt;/a&gt;</name>
        </author>
    </entry>
</feed>