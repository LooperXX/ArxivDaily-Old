<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-25T00:47:02.980Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12027</id>
        <link href="http://arxiv.org/abs/2106.12027"/>
        <updated>2021-06-24T01:51:45.769Z</updated>
        <summary type="html"><![CDATA[Atomic clauses are fundamental text units for understanding complex
sentences. Identifying the atomic sentences within complex sentences is
important for applications such as summarization, argument mining, discourse
analysis, discourse parsing, and question answering. Previous work mainly
relies on rule-based methods dependent on parsing. We propose a new task to
decompose each complex sentence into simple sentences derived from the tensed
clauses in the source, and a novel problem formulation as a graph edit task.
Our neural model learns to Accept, Break, Copy or Drop elements of a graph that
combines word adjacency and grammatical dependencies. The full processing
pipeline includes modules for graph construction, graph editing, and sentence
generation from the output graph. We introduce DeSSE, a new dataset designed to
train and evaluate complex sentence decomposition, and MinWiki, a subset of
MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on
MinWiki. On DeSSE, which has a more even balance of complex sentence types, our
model achieves higher accuracy on the number of atomic sentences than an
encoder-decoder baseline. Results include a detailed error analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanjun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1"&gt;Ting-hao&lt;/a&gt; (Kenneth) &lt;a href="http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1"&gt;Rebecca J. Passonneau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-06-24T01:51:45.764Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12026</id>
        <link href="http://arxiv.org/abs/2106.12026"/>
        <updated>2021-06-24T01:51:45.758Z</updated>
        <summary type="html"><![CDATA[Many learning-based 3D shape semantic segmentation methods assign labels to
shape atoms (e.g. points in a point cloud or faces in a mesh) with a
single-pass approach trained in an end-to-end fashion. Such methods achieve
impressive performance but require large amounts of labeled training data. This
paradigm entangles two separable subproblems: (1) decomposing a shape into
regions and (2) assigning semantic labels to these regions. We claim that
disentangling these subproblems reduces the labeled data burden: (1) region
decomposition requires no semantic labels and could be performed in an
unsupervised fashion, and (2) labeling shape regions instead of atoms results
in a smaller search space and should be learnable with less labeled training
data. In this paper, we investigate this second claim by presenting the
Neurally-Guided Shape Parser (NGSP), a method that learns how to assign
semantic labels to regions of an over-segmented 3D shape. We solve this problem
via MAP inference, modeling the posterior probability of a labeling assignment
conditioned on an input shape. We employ a Monte Carlo importance sampling
approach guided by a neural proposal network, a search-based approach made
feasible by assuming the input shape is decomposed into discrete regions. We
evaluate NGSP on the task of hierarchical semantic segmentation on manufactured
3D shapes from PartNet. We find that NGSP delivers significant performance
improvements over baselines that learn to label shape atoms and then aggregate
predictions for each shape region, especially in low-data regimes. Finally, we
demonstrate that NGSP is robust to region granularity, as it maintains strong
segmentation performance even as the regions undergo significant corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1"&gt;Rana Hanocka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10490</id>
        <link href="http://arxiv.org/abs/2102.10490"/>
        <updated>2021-06-24T01:51:45.753Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) often trains and evaluates a large number of
architectures. Recent predictor-based NAS approaches attempt to address such
heavy computation costs with two key steps: sampling some
architecture-performance pairs and fitting a proxy accuracy predictor. Given
limited samples, these predictors, however, are far from accurate to locate top
architectures due to the difficulty of fitting the huge search space. This
paper reflects on a simple yet crucial question: if our final goal is to find
the best architecture, do we really need to model the whole space well?. We
propose a paradigm shift from fitting the whole architecture space using one
strong predictor, to progressively fitting a search path towards the
high-performance sub-space through a set of weaker predictors. As a key
property of the proposed weak predictors, their probabilities of sampling
better architectures keep increasing. Hence we only sample a few well-performed
architectures guided by the previously learned predictor and estimate a new
better weak predictor. This embarrassingly easy framework produces
coarse-to-fine iteration to refine the ranking of sampling space gradually.
Extensive experiments demonstrate that our method costs fewer samples to find
top-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as
achieves the state-of-the-art ImageNet performance on the NASNet search space.
In particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,
WeakNAS outperforms all of them with notable margins, e.g., requiring at least
7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also
absorb them for further performance boost. We further strike the new SOTA
result of 81.3% in the ImageNet MobileNet Search Space. The code is available
at https://github.com/VITA-Group/WeakNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junru Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ye Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Effect Bandits. (arXiv:2106.12200v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12200</id>
        <link href="http://arxiv.org/abs/2106.12200"/>
        <updated>2021-06-24T01:51:45.748Z</updated>
        <summary type="html"><![CDATA[This paper studies regret minimization in multi-armed bandits, a classical
online learning problem. To develop more statistically-efficient algorithms, we
propose to use the assumption of a random-effect model. In this model, the mean
rewards of arms are drawn independently from an unknown distribution, whose
parameters we estimate. We provide an estimator of the arm means in this model
and also analyze its uncertainty. Based on these results, we design a UCB
algorithm, which we call ReUCB. We analyze ReUCB and prove a Bayes regret bound
on its $n$-round regret, which matches an existing lower bound. Our experiments
show that ReUCB can outperform Thompson sampling in various scenarios, without
assuming that the prior distribution of arm means is known.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation. (arXiv:2105.01593v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01593</id>
        <link href="http://arxiv.org/abs/2105.01593"/>
        <updated>2021-06-24T01:51:45.741Z</updated>
        <summary type="html"><![CDATA[We propose two algorithms that use linear function approximation (LFA) for
stochastic shortest path (SSP) and bound their regret over $K$ episodes. When
all stationary policies are proper, our first algorithm obtains sublinear
regret ($K^{3/4}$), is computationally efficient, and uses stationary policies.
This is the first LFA algorithm with these three properties, to the best of our
knowledge. Our second algorithm improves the regret to $\sqrt{K}$ when the
feature vectors satisfy certain assumptions. Both algorithms are special cases
of a more general one, which has $\sqrt{K}$ regret for general features given
access to a certain computation oracle. These algorithms and regret bounds are
the first for SSP with function approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vial_D/0/1/0/all/0/1"&gt;Daniel Vial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1"&gt;Advait Parulekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Innovations Autoencoder and its Application in Real-Time Anomaly Detection. (arXiv:2106.12382v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12382</id>
        <link href="http://arxiv.org/abs/2106.12382"/>
        <updated>2021-06-24T01:51:45.726Z</updated>
        <summary type="html"><![CDATA[An innovations sequence of a time series is a sequence of independent and
identically distributed random variables with which the original time series
has a causal representation. The innovation at a time is statistically
independent of the prior history of the time series. As such, it represents the
new information contained at present but not in the past. Because of its simple
probability structure, an innovations sequence is the most efficient signature
of the original. Unlike the principle or independent analysis (PCA/ICA)
representations, an innovations sequence preserves not only the complete
statistical properties but also the temporal order of the original time series.
An long-standing open problem is to find a computationally tractable way to
extract an innovations sequence of non-Gaussian processes. This paper presents
a deep learning approach, referred to as Innovations Autoencoder (IAE), that
extracts innovations sequences using a causal convolutional neural network. An
application of IAE to nonparametric anomaly detection with unknown anomaly and
anomaly-free models is also presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Data Selection for Improved Energy Efficiency of Federated Edge Learning. (arXiv:2106.12561v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12561</id>
        <link href="http://arxiv.org/abs/2106.12561"/>
        <updated>2021-06-24T01:51:45.721Z</updated>
        <summary type="html"><![CDATA[In Federated edge learning (FEEL), energy-constrained devices at the network
edge consume significant energy when training and uploading their local machine
learning models, leading to a decrease in their lifetime. This work proposes
novel solutions for energy-efficient FEEL by jointly considering local training
data, available computation, and communications resources, and deadline
constraints of FEEL rounds to reduce energy consumption. This paper considers a
system model where the edge server is equipped with multiple antennas employing
beamforming techniques to communicate with the local users through orthogonal
channels. Specifically, we consider a problem that aims to find the optimal
user's resources, including the fine-grained selection of relevant training
samples, bandwidth, transmission power, beamforming weights, and processing
speed with the goal of minimizing the total energy consumption given a deadline
constraint on the communication rounds of FEEL. Then, we devise tractable
solutions by first proposing a novel fine-grained training algorithm that
excludes less relevant training samples and effectively chooses only the
samples that improve the model's performance. After that, we derive closed-form
solutions, followed by a Golden-Section-based iterative algorithm to find the
optimal computation and communication resources that minimize energy
consumption. Experiments using MNIST and CIFAR-10 datasets demonstrate that our
proposed algorithms considerably outperform the state-of-the-art solutions as
energy consumption decreases by 79% for MNIST and 73% for CIFAR-10 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albaseer_A/0/1/0/all/0/1"&gt;Abdullatif Albaseer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdallah_M/0/1/0/all/0/1"&gt;Mohamed Abdallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1"&gt;Ala Al-Fuqaha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12378</id>
        <link href="http://arxiv.org/abs/2106.12378"/>
        <updated>2021-06-24T01:51:45.707Z</updated>
        <summary type="html"><![CDATA[Transformers recently are adapted from the community of natural language
processing as a promising substitute of convolution-based neural networks for
visual learning tasks. However, its supremacy degenerates given an insufficient
amount of training data (e.g., ImageNet). To make it into practical utility, we
propose a novel distillation-based method to train vision transformers. Unlike
previous works, where merely heavy convolution-based teachers are provided, we
introduce lightweight teachers with different architectural inductive biases
(e.g., convolution and involution) to co-advise the student transformer. The
key is that teachers with different inductive biases attain different knowledge
despite that they are trained on the same dataset, and such different knowledge
compounds and boosts the student's performance during distillation. Equipped
with this cross inductive bias distillation method, our vision transformers
(termed as CivT) outperform all previous transformers of the same architecture
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1"&gt;Tianyu Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closed-Form, Provable, and Robust PCA via Leverage Statistics and Innovation Search. (arXiv:2106.12190v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12190</id>
        <link href="http://arxiv.org/abs/2106.12190"/>
        <updated>2021-06-24T01:51:45.703Z</updated>
        <summary type="html"><![CDATA[The idea of Innovation Search, which was initially proposed for data
clustering, was recently used for outlier detection. In the application of
Innovation Search for outlier detection, the directions of innovation were
utilized to measure the innovation of the data points. We study the Innovation
Values computed by the Innovation Search algorithm under a quadratic cost
function and it is proved that Innovation Values with the new cost function are
equivalent to Leverage Scores. This interesting connection is utilized to
establish several theoretical guarantees for a Leverage Score based robust PCA
method and to design a new robust PCA method. The theoretical results include
performance guarantees with different models for the distribution of outliers
and the distribution of inliers. In addition, we demonstrate the robustness of
the algorithms against the presence of noise. The numerical and theoretical
studies indicate that while the presented approach is fast and closed-form, it
can outperform most of the existing algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rahmani_M/0/1/0/all/0/1"&gt;Mostafa Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12066</id>
        <link href="http://arxiv.org/abs/2106.12066"/>
        <updated>2021-06-24T01:51:45.698Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning is one of the key problems in natural language
processing, but the relative scarcity of labeled data holds back the progress
for languages other than English. Pretrained cross-lingual models are a source
of powerful language-agnostic representations, yet their inherent reasoning
capabilities are still actively studied. In this work, we design a simple
approach to commonsense reasoning which trains a linear classifier with weights
of multi-head attention as features. To evaluate this approach, we create a
multilingual Winograd Schema corpus by processing several datasets from prior
work within a standardized pipeline and measure cross-lingual generalization
ability in terms of out-of-sample performance. The method performs
competitively with recent supervised and unsupervised approaches for
commonsense reasoning, even when applied to other languages in a zero-shot
manner. Also, we demonstrate that most of the performance is given by the same
small subset of attention heads for all studied languages, which provides
evidence of universal reasoning capabilities in multilingual encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Quantized Gradient Methods. (arXiv:2002.02508v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02508</id>
        <link href="http://arxiv.org/abs/2002.02508"/>
        <updated>2021-06-24T01:51:45.693Z</updated>
        <summary type="html"><![CDATA[This paper considers quantized distributed optimization algorithms in the
parameter server framework of distributed training. We introduce the principle
we call Differential Quantization (DQ) that prescribes that the past
quantization errors should be compensated in such a way as to direct the
descent trajectory of a quantized algorithm towards that of its unquantized
counterpart. Assuming that the objective function is smooth and strongly
convex, we prove that in the limit of large problem dimension, Differentially
Quantized Gradient Descent (DQ-GD) attains a linear contraction factor of
$\max\{\sigma_{\mathrm{GD}}, 2^{-R}\}$, where $\sigma_{\mathrm{GD}}$ is the
contraction factor of unquantized gradient descent (GD). Thus at any
$R\geq\log_2 1 /\sigma_{\mathrm{GD}}$ bits, the contraction factor of DQ-GD is
the same as that of unquantized GD, i.e., there is no loss due to quantization.
We show a converse demonstrating that no quantized gradient descent algorithm
can converge faster than $\max\{\sigma_{\mathrm{GD}}, 2^{-R}\}$. In contrast,
naively quantized GD where the worker directly quantizes the gradient barely
attains $\sigma_{\mathrm{GD}} + 2^{-R}$. The principle of differential
quantization continues to apply to gradient methods with momentum such as
Nesterov's accelerated gradient descent, and Polyak's heavy ball method. For
these algorithms as well, if the rate is above a certain threshold, there is no
loss in contraction factor obtained by the differentially quantized algorithm
compared to its unquantized counterpart, and furthermore, the differentially
quantized heavy ball method attains the optimal contraction achievable among
all (even unquantized) gradient methods. Experimental results on both simulated
and real-world least-squares problems validate our theoretical analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostina_V/0/1/0/all/0/1"&gt;Victoria Kostina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08271</id>
        <link href="http://arxiv.org/abs/2003.08271"/>
        <updated>2021-06-24T01:51:45.687Z</updated>
        <summary type="html"><![CDATA[Recently, the emergence of pre-trained models (PTMs) has brought natural
language processing (NLP) to a new era. In this survey, we provide a
comprehensive review of PTMs for NLP. We first briefly introduce language
representation learning and its research progress. Then we systematically
categorize existing PTMs based on a taxonomy with four perspectives. Next, we
describe how to adapt the knowledge of PTMs to the downstream tasks. Finally,
we outline some potential directions of PTMs for future research. This survey
is purposed to be a hands-on guide for understanding, using, and developing
PTMs for various NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yige Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1"&gt;Ning Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training. (arXiv:2106.12089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12089</id>
        <link href="http://arxiv.org/abs/2106.12089"/>
        <updated>2021-06-24T01:51:45.682Z</updated>
        <summary type="html"><![CDATA[Recurrent Neural Networks (RNNs), more specifically their Long Short-Term
Memory (LSTM) variants, have been widely used as a deep learning tool for
tackling sequence-based learning tasks in text and speech. Training of such
LSTM applications is computationally intensive due to the recurrent nature of
hidden state computation that repeats for each time step. While sparsity in
Deep Neural Nets has been widely seen as an opportunity for reducing
computation time in both training and inference phases, the usage of non-ReLU
activation in LSTM RNNs renders the opportunities for such dynamic sparsity
associated with neuron activation and gradient values to be limited or
non-existent. In this work, we identify dropout induced sparsity for LSTMs as a
suitable mode of computation reduction. Dropout is a widely used regularization
mechanism, which randomly drops computed neuron values during each iteration of
training. We propose to structure dropout patterns, by dropping out the same
set of physical neurons within a batch, resulting in column (row) level hidden
state sparsity, which are well amenable to computation reduction at run-time in
general-purpose SIMD hardware as well as systolic arrays. We conduct our
experiments for three representative NLP tasks: language modelling on the PTB
dataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi
datasets, and named entity recognition sequence labelling using the CoNLL-2003
shared task. We demonstrate that our proposed approach can be used to translate
dropout-based computation reduction into reduced training time, with
improvement ranging from 1.23x to 1.64x, without sacrificing the target metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarma_A/0/1/0/all/0/1"&gt;Anup Sarma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sonali Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huaipan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Mahmut T Kandemir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_C/0/1/0/all/0/1"&gt;Chita R Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IQ-Learn: Inverse soft-Q Learning for Imitation. (arXiv:2106.12142v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12142</id>
        <link href="http://arxiv.org/abs/2106.12142"/>
        <updated>2021-06-24T01:51:45.677Z</updated>
        <summary type="html"><![CDATA[In many sequential decision-making problems (e.g., robotics control, game
playing, sequential prediction), human or expert data is available containing
useful information about the task. However, imitation learning (IL) from a
small amount of expert data can be challenging in high-dimensional environments
with complex dynamics. Behavioral cloning is a simple method that is widely
used due to its simplicity of implementation and stable convergence but doesn't
utilize any information involving the environment's dynamics. Many existing
methods that exploit dynamics information are difficult to train in practice
due to an adversarial optimization process over reward and policy approximators
or biased, high variance gradient estimators. We introduce a method for
dynamics-aware IL which avoids adversarial training by learning a single
Q-function, implicitly representing both reward and policy. On standard
benchmarks, the implicitly learned rewards show a high positive correlation
with the ground-truth rewards, illustrating our method can also be used for
inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning
(IQ-Learn) obtains state-of-the-art results in offline and online imitation
learning settings, surpassing existing methods both in the number of required
environment interactions and scalability in high-dimensional spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1"&gt;Divyansh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Shuvam Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cundy_C/0/1/0/all/0/1"&gt;Chris Cundy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1"&gt;Jiaming Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound. (arXiv:2106.12535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12535</id>
        <link href="http://arxiv.org/abs/2106.12535"/>
        <updated>2021-06-24T01:51:45.672Z</updated>
        <summary type="html"><![CDATA[We investigate a stochastic counterpart of majority votes over finite
ensembles of classifiers, and study its generalization properties. While our
approach holds for arbitrary distributions, we instantiate it with Dirichlet
distributions: this allows for a closed-form and differentiable expression for
the expected risk, which then turns the generalization bound into a tractable
training objective. The resulting stochastic majority vote learning algorithm
achieves state-of-the-art accuracy and benefits from (non-vacuous) tight
generalization bounds, in a series of numerical experiments when compared to
competing algorithms which also minimize PAC-Bayes objectives -- both with
uninformed (data-independent) and informed (data-dependent) priors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zantedeschi_V/0/1/0/all/0/1"&gt;Valentina Zantedeschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viallard_P/0/1/0/all/0/1"&gt;Paul Viallard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morvant_E/0/1/0/all/0/1"&gt;Emilie Morvant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Emonet_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Emonet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habrard_A/0/1/0/all/0/1"&gt;Amaury Habrard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1"&gt;Pascal Germain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1"&gt;Benjamin Guedj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12340</id>
        <link href="http://arxiv.org/abs/2106.12340"/>
        <updated>2021-06-24T01:51:45.656Z</updated>
        <summary type="html"><![CDATA[In today's academic publishing model, especially in Computer Science,
conferences commonly constitute the main platforms for releasing the latest
peer-reviewed advancements in their respective fields. However, choosing a
suitable academic venue for publishing one's research can represent a
challenging task considering the plethora of available conferences,
particularly for those at the start of their academic careers, or for those
seeking to publish outside of their usual domain. In this paper, we propose
GraphConfRec, a conference recommender system which combines SciGraph and graph
neural networks, to infer suggestions based not only on title and abstract, but
also on co-authorship and citation relationships. GraphConfRec achieves a
recall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention
network-based recommendation model. A user study with 25 subjects supports the
positive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1"&gt;Andreea Iana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1"&gt;Heiko Paulheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Leads and Who Follows in Strategic Classification?. (arXiv:2106.12529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12529</id>
        <link href="http://arxiv.org/abs/2106.12529"/>
        <updated>2021-06-24T01:51:45.642Z</updated>
        <summary type="html"><![CDATA[As predictive models are deployed into the real world, they must increasingly
contend with strategic behavior. A growing body of work on strategic
classification treats this problem as a Stackelberg game: the decision-maker
"leads" in the game by deploying a model, and the strategic agents "follow" by
playing their best response to the deployed model. Importantly, in this
framing, the burden of learning is placed solely on the decision-maker, while
the agents' best responses are implicitly treated as instantaneous. In this
work, we argue that the order of play in strategic classification is
fundamentally determined by the relative frequencies at which the
decision-maker and the agents adapt to each other's actions. In particular, by
generalizing the standard model to allow both players to learn over time, we
show that a decision-maker that makes updates faster than the agents can
reverse the order of play, meaning that the agents lead and the decision-maker
follows. We observe in standard learning settings that such a role reversal can
be desirable for both the decision-maker and the strategic agents. Finally, we
show that a decision-maker with the freedom to choose their update frequency
can induce learning dynamics that converge to Stackelberg equilibria with
either order of play.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1"&gt;Tijana Zrnic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_E/0/1/0/all/0/1"&gt;Eric Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sastry_S/0/1/0/all/0/1"&gt;S. Shankar Sastry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection. (arXiv:2102.05311v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05311</id>
        <link href="http://arxiv.org/abs/2102.05311"/>
        <updated>2021-06-24T01:51:45.620Z</updated>
        <summary type="html"><![CDATA[We investigate the adversarial robustness of CNNs from the perspective of
channel-wise activations. By comparing \textit{non-robust} (normally trained)
and \textit{robustified} (adversarially trained) models, we observe that
adversarial training (AT) robustifies CNNs by aligning the channel-wise
activations of adversarial data with those of their natural counterparts.
However, the channels that are \textit{negatively-relevant} (NR) to predictions
are still over-activated when processing adversarial data. Besides, we also
observe that AT does not result in similar robustness for all classes. For the
robust classes, channels with larger activation magnitudes are usually more
\textit{positively-relevant} (PR) to predictions, but this alignment does not
hold for the non-robust classes. Given these observations, we hypothesize that
suppressing NR channels and aligning PR ones with their relevances further
enhances the robustness of CNNs under AT. To examine this hypothesis, we
introduce a novel mechanism, i.e., \underline{C}hannel-wise
\underline{I}mportance-based \underline{F}eature \underline{S}election (CIFS).
The CIFS manipulates channels' activations of certain layers by generating
non-negative multipliers to these channels based on their relevances to
predictions. Extensive experiments on benchmark datasets including CIFAR10 and
SVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying
CNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hanshu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HILONet: Hierarchical Imitation Learning from Non-Aligned Observations. (arXiv:2011.02671v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02671</id>
        <link href="http://arxiv.org/abs/2011.02671"/>
        <updated>2021-06-24T01:51:45.607Z</updated>
        <summary type="html"><![CDATA[It is challenging learning from demonstrated observation-only trajectories in
a non-time-aligned environment because most imitation learning methods aim to
imitate experts by following the demonstration step-by-step. However, aligned
demonstrations are seldom obtainable in real-world scenarios. In this work, we
propose a new imitation learning approach called Hierarchical Imitation
Learning from Observation(HILONet), which adopts a hierarchical structure to
choose feasible sub-goals from demonstrated observations dynamically. Our
method can solve all kinds of tasks by achieving these sub-goals, whether it
has a single goal position or not. We also present three different ways to
increase sample efficiency in the hierarchical structure. We conduct extensive
experiments using several environments. The results show the improvement in
both performance and learning efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shanqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Junjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenzhou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Licheng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12442</id>
        <link href="http://arxiv.org/abs/2106.12442"/>
        <updated>2021-06-24T01:51:45.580Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of pedestrian and bicyclist paths is integral to the
development of reliable autonomous vehicles in dense urban environments. The
interactions between vehicle and pedestrian or bicyclist have a significant
impact on the trajectories of traffic participants e.g. stopping or turning to
avoid collisions. Although recent datasets and trajectory prediction approaches
have fostered the development of autonomous vehicles yet the amount of
vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,
we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In
particular, our dataset caters more diverse and complex interactions in dense
urban scenarios compared to the existing datasets. To address the challenges in
predicting future trajectories with dense interactions, we develop a joint
inference model that learns an expressive multi-modal shared latent space
across agents in the urban scene. This enables our Joint-$\beta$-cVAE approach
to better model the distribution of future trajectories. We achieve state of
the art results on the nuScenes and Euro-PVI datasets demonstrating the
importance of capturing interactions between ego-vehicle and pedestrians
(bicyclists) for accurate predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1"&gt;Mario Fritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1"&gt;Bernt Schiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample-Optimal PAC Learning of Halfspaces with Malicious Noise. (arXiv:2102.06247v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06247</id>
        <link href="http://arxiv.org/abs/2102.06247"/>
        <updated>2021-06-24T01:51:45.014Z</updated>
        <summary type="html"><![CDATA[We study efficient PAC learning of homogeneous halfspaces in $\mathbb{R}^d$
in the presence of malicious noise of Valiant~(1985). This is a challenging
noise model and only until recently has near-optimal noise tolerance bound been
established under the mild condition that the unlabeled data distribution is
isotropic log-concave. However, it remains unsettled how to obtain the optimal
sample complexity simultaneously. In this work, we present a new analysis for
the algorithm of Awasthi~et~al.~(2017) and show that it essentially achieves
the near-optimal sample complexity bound of $\tilde{O}(d)$, improving the best
known result of $\tilde{O}(d^2)$. Our main ingredient is a novel incorporation
of a matrix Chernoff-type inequality to bound the spectrum of an empirical
covariance matrix for well-behaved distributions, in conjunction with a careful
exploration of the localization schemes of Awasthi~et~al.~(2017). We further
extend the algorithm and analysis to the more general and stronger nasty noise
model of Bshouty~et~al.~(2002), showing that it is still possible to achieve
near-optimal noise tolerance and sample complexity in polynomial time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Certified Robust Training with Short Warmup. (arXiv:2103.17268v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17268</id>
        <link href="http://arxiv.org/abs/2103.17268"/>
        <updated>2021-06-24T01:51:45.008Z</updated>
        <summary type="html"><![CDATA[Recently, bound propagation based certified robust training methods have been
proposed for training neural networks with certifiable robustness guarantees.
Despite that state-of-the-art (SOTA) methods including interval bound
propagation (IBP) and CROWN-IBP have per-batch training complexity similar to
standard neural network training, they usually use a long warmup schedule with
hundreds or thousands epochs to reach SOTA performance and are thus still
costly. In this paper, we identify two important issues in existing methods,
namely exploded bounds at initialization, and the imbalance in ReLU activation
states. These two issues make certified training difficult and unstable, and
thereby long warmup schedules were needed in prior works. To mitigate these
issues and conduct certified training with shorter warmup, we propose three
improvements: 1) We derive a new weight initialization method for IBP training;
2) We propose to fully add Batch Normalization (BN) to each layer in the model,
since we find BN can reduce the imbalance in ReLU activation states; 3) We also
design regularization to explicitly tighten certified bounds and balance ReLU
activation states. In our experiments, we are able to obtain 65.03% verified
error on CIFAR-10 ($\epsilon=\frac{8}{255}$) and 82.36% verified error on
TinyImageNet ($\epsilon=\frac{1}{255}$) using very short training schedules
(160 and 80 total epochs, respectively), outperforming literature SOTA trained
with hundreds or thousands epochs under the same network architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhouxing Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yihan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-24T01:51:44.984Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Data Subset Selection for Regression with Controlled Generalization Error. (arXiv:2106.12491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12491</id>
        <link href="http://arxiv.org/abs/2106.12491"/>
        <updated>2021-06-24T01:51:44.978Z</updated>
        <summary type="html"><![CDATA[Data subset selection from a large number of training instances has been a
successful approach toward efficient and cost-effective machine learning.
However, models trained on a smaller subset may show poor generalization
ability. In this paper, our goal is to design an algorithm for selecting a
subset of the training data, so that the model can be trained quickly, without
significantly sacrificing on accuracy. More specifically, we focus on data
subset selection for L2 regularized regression problems and provide a novel
problem formulation which seeks to minimize the training loss with respect to
both the trainable parameters and the subset of training data, subject to error
bounds on the validation set. We tackle this problem using several technical
innovations. First, we represent this problem with simplified constraints using
the dual of the original training problem and show that the objective of this
new representation is a monotone and alpha-submodular function, for a wide
variety of modeling choices. Such properties lead us to develop SELCON, an
efficient majorization-minimization algorithm for data subset selection, that
admits an approximation guarantee even when the training provides an imperfect
estimate of the trained model. Finally, our experiments on several datasets
show that SELCON trades off accuracy and efficiency more effectively than the
current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1"&gt;Durga Sivasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal training of variational quantum algorithms without barren plateaus. (arXiv:2104.14543v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14543</id>
        <link href="http://arxiv.org/abs/2104.14543"/>
        <updated>2021-06-24T01:51:44.972Z</updated>
        <summary type="html"><![CDATA[Variational quantum algorithms (VQAs) promise efficient use of near-term
quantum computers. However, training VQAs often requires an extensive amount of
time and suffers from the barren plateau problem where the magnitude of the
gradients vanishes with increasing number of qubits. Here, we show how to
optimally train VQAs for learning quantum states. Parameterized quantum
circuits can form Gaussian kernels, which we use to derive adaptive learning
rates for gradient ascent. We introduce the generalized quantum natural
gradient that features stability and optimized movement in parameter space.
Both methods together outperform other optimization routines in training VQAs.
Our methods also excel at numerically optimizing driving protocols for quantum
control problems. The gradients of the VQA do not vanish when the fidelity
between the initial state and the state to be learned is bounded from below. We
identify a VQA for quantum simulation with such a constraint that thus can be
trained free of barren plateaus. Finally, we propose the application of
Gaussian kernels for quantum machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Haug_T/0/1/0/all/0/1"&gt;Tobias Haug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_M/0/1/0/all/0/1"&gt;M.S. Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weisfeiler and Lehman Go Cellular: CW Networks. (arXiv:2106.12575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12575</id>
        <link href="http://arxiv.org/abs/2106.12575"/>
        <updated>2021-06-24T01:51:44.967Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are limited in their expressive power, struggle
with long-range interactions and lack a principled way to model higher-order
structures. These problems can be attributed to the strong coupling between the
computational graph and the input graph structure. The recently proposed
Message Passing Simplicial Networks naturally decouple these elements by
performing message passing on the clique complex of the graph. Nevertheless,
these models are severely constrained by the rigid combinatorial structure of
Simplicial Complexes (SCs). In this work, we extend recent theoretical results
on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs
and graphs. We show that this generalisation provides a powerful set of graph
``lifting'' transformations, each leading to a unique hierarchical message
passing procedure. The resulting methods, which we collectively call CW
Networks (CWNs), are strictly more powerful than the WL test and, in certain
cases, not less powerful than the 3-WL test. In particular, we demonstrate the
effectiveness of one such scheme, based on rings, when applied to molecular
graph problems. The proposed architecture benefits from provably larger
expressivity than commonly used GNNs, principled modelling of higher-order
signals and from compressing the distances between nodes. We demonstrate that
our model achieves state-of-the-art results on a variety of molecular datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bodnar_C/0/1/0/all/0/1"&gt;Cristian Bodnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasca_F/0/1/0/all/0/1"&gt;Fabrizio Frasca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otter_N/0/1/0/all/0/1"&gt;Nina Otter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_M/0/1/0/all/0/1"&gt;Michael Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12566</id>
        <link href="http://arxiv.org/abs/2106.12566"/>
        <updated>2021-06-24T01:51:44.952Z</updated>
        <summary type="html"><![CDATA[The attention module, which is a crucial component in Transformer, cannot
scale efficiently to long sequences due to its quadratic complexity. Many works
focus on approximating the dot-then-exponentiate softmax function in the
original attention, leading to sub-quadratic or even linear-complexity
Transformer architectures. However, we show that these methods cannot be
applied to more powerful attention modules that go beyond the
dot-then-exponentiate style, e.g., Transformers with relative positional
encoding (RPE). Since in many state-of-the-art models, relative positional
encoding is used as default, designing efficient Transformers that can
incorporate RPE is appealing. In this paper, we propose a novel way to
accelerate attention calculation for Transformers with RPE on top of the
kernelized attention. Based upon the observation that relative positional
encoding forms a Toeplitz matrix, we mathematically show that kernelized
attention with RPE can be calculated efficiently using Fast Fourier Transform
(FFT). With FFT, our method achieves $\mathcal{O}(n\log n)$ time complexity.
Interestingly, we further demonstrate that properly using relative positional
encoding can mitigate the training instability problem of vanilla kernelized
attention. On a wide range of tasks, we empirically show that our models can be
trained from scratch without any optimization issues. The learned model
performs better than many efficient Transformer variants and is faster than
standard Transformer in the long-sequence regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shanda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Dinglan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessment of the influence of features on a classification problem: an application to COVID-19 patients. (arXiv:2104.14958v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14958</id>
        <link href="http://arxiv.org/abs/2104.14958"/>
        <updated>2021-06-24T01:51:44.947Z</updated>
        <summary type="html"><![CDATA[This paper deals with an important subject in classification problems
addressed by machine learning techniques: the evaluation of the influence of
each of the features on the classification of individuals. Specifically, a
measure of that influence is introduced using the Shapley value of cooperative
games. In addition, an axiomatic characterisation of the proposed measure is
provided based on properties of efficiency and balanced contributions.
Furthermore, some experiments have been designed in order to validate the
appropriate performance of such measure. Finally, the methodology introduced is
applied to a sample of COVID-19 patients to study the influence of certain
demographic or risk factors on various events of interest related to the
evolution of the disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Davila_Pena_L/0/1/0/all/0/1"&gt;L. Davila-Pena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garcia_Jurado_I/0/1/0/all/0/1"&gt;Ignacio Garc&amp;#xed;a-Jurado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casas_Mendez_B/0/1/0/all/0/1"&gt;B. Casas-M&amp;#xe9;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel multi-classifier information fusion based on Dempster-Shafer theory: application to vibration-based fault detection. (arXiv:2012.02481v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02481</id>
        <link href="http://arxiv.org/abs/2012.02481"/>
        <updated>2021-06-24T01:51:44.942Z</updated>
        <summary type="html"><![CDATA[Achieving a high prediction rate is a crucial task in fault detection.
Although various classification procedures are available, none of them can give
high accuracy in all applications. Therefore, in this paper, a novel
multi-classifier fusion approach is developed to boost the performance of the
individual classifiers. This is acquired by using Dempster-Shafer theory (DST).
However, in cases with conflicting evidences, the DST may give
counter-intuitive results. In this regard, a preprocessing technique based on a
new metric is devised in order to measure and mitigate the conflict between the
evidences. To evaluate and validate the effectiveness of the proposed approach,
the method is applied to 15 benchmarks datasets from UCI and KEEL. Further, it
is applied for classifying polycrystalline Nickel alloy first-stage turbine
blades based on their broadband vibrational response. Through statistical
analysis with different noise levels, and by comparing with four
state-of-the-art fusion techniques, it is shown that that the proposed method
improves the classification accuracy and outperforms the individual
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yaghoubi_V/0/1/0/all/0/1"&gt;Vahid Yaghoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Liangliang Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paepegem_W/0/1/0/all/0/1"&gt;Wim Van Paepegem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersemans_M/0/1/0/all/0/1"&gt;Mathias Kersemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06392</id>
        <link href="http://arxiv.org/abs/2104.06392"/>
        <updated>2021-06-24T01:51:44.936Z</updated>
        <summary type="html"><![CDATA[A popular way to create detailed yet easily controllable 3D shapes is via
procedural modeling, i.e. generating geometry using programs. Such programs
consist of a series of instructions along with their associated parameter
values. To fully realize the benefits of this representation, a shape program
should be compact and only expose degrees of freedom that allow for meaningful
manipulation of output geometry. One way to achieve this goal is to design
higher-level macro operators that, when executed, expand into a series of
commands from the base shape modeling language. However, manually authoring
such macros, much like shape programs themselves, is difficult and largely
restricted to domain experts. In this paper, we present ShapeMOD, an algorithm
for automatically discovering macros that are useful across large datasets of
3D shape programs. ShapeMOD operates on shape programs expressed in an
imperative, statement-based language. It is designed to discover macros that
make programs more compact by minimizing the number of function calls and free
parameters required to represent an input shape collection. We run ShapeMOD on
multiple collections of programs expressed in a domain-specific language for 3D
shape structures. We show that it automatically discovers a concise set of
macros that abstract out common structural and parametric patterns that
generalize over large shape collections. We also demonstrate that the macros
found by ShapeMOD improve performance on downstream tasks including shape
generative modeling and inferring programs from point clouds. Finally, we
conduct a user study that indicates that ShapeMOD's discovered macros make
interactive shape editing more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1"&gt;David Charatan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-06-24T01:51:44.930Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise. (arXiv:2012.10793v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10793</id>
        <link href="http://arxiv.org/abs/2012.10793"/>
        <updated>2021-06-24T01:51:44.890Z</updated>
        <summary type="html"><![CDATA[We study {\em online} active learning of homogeneous halfspaces in
$\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy
label is constrained to be at most $\nu$. Our main contribution is a
Perceptron-like online active learning algorithm that runs in polynomial time,
and under the conditions that the marginal distribution is isotropic
log-concave and $\nu = \Omega(\epsilon)$, where $\epsilon \in (0, 1)$ is the
target error rate, our algorithm PAC learns the underlying halfspace with
near-optimal label complexity of $\tilde{O}\big(d \cdot
polylog(\frac{1}{\epsilon})\big)$ and sample complexity of
$\tilde{O}\big(\frac{d}{\epsilon} \big)$. Prior to this work, existing online
algorithms designed for tolerating the adversarial noise are subject to either
label complexity polynomial in $\frac{1}{\epsilon}$, or suboptimal noise
tolerance, or restrictive marginal distributions. With the additional prior
knowledge that the underlying halfspace is $s$-sparse, we obtain
attribute-efficient label complexity of $\tilde{O}\big( s \cdot polylog(d,
\frac{1}{\epsilon}) \big)$ and sample complexity of
$\tilde{O}\big(\frac{s}{\epsilon} \cdot polylog(d) \big)$. As an immediate
corollary, we show that under the agnostic model where no assumption is made on
the noise rate $\nu$, our active learner achieves an error rate of $O(OPT) +
\epsilon$ with the same running time and label and sample complexity, where
$OPT$ is the best possible error rate achievable by any homogeneous halfspace.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the Evolution of Parametric Drivers of High-End Sea-Level Hazards. (arXiv:2106.12041v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12041</id>
        <link href="http://arxiv.org/abs/2106.12041"/>
        <updated>2021-06-24T01:51:44.883Z</updated>
        <summary type="html"><![CDATA[Climate models are critical tools for developing strategies to manage the
risks posed by sea-level rise to coastal communities. While these models are
necessary for understanding climate risks, there is a level of uncertainty
inherent in each parameter in the models. This model parametric uncertainty
leads to uncertainty in future climate risks. Consequently, there is a need to
understand how those parameter uncertainties impact our assessment of future
climate risks and the efficacy of strategies to manage them. Here, we use
random forests to examine the parametric drivers of future climate risk and how
the relative importances of those drivers change over time. We find that the
equilibrium climate sensitivity and a factor that scales the effect of aerosols
on radiative forcing are consistently the most important climate model
parametric uncertainties throughout the 2020 to 2150 interval for both low and
high radiative forcing scenarios. The near-term hazards of high-end sea-level
rise are driven primarily by thermal expansion, while the longer-term hazards
are associated with mass loss from the Antarctic and Greenland ice sheets. Our
results highlight the practical importance of considering time-evolving
parametric uncertainties when developing strategies to manage future climate
risks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Hough_A/0/1/0/all/0/1"&gt;Alana Hough&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tony E. Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14754</id>
        <link href="http://arxiv.org/abs/2104.14754"/>
        <updated>2021-06-24T01:51:44.878Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) synthesize realistic images from
random latent vectors. Although manipulating the latent vectors controls the
synthesized outputs, editing real images with GANs suffers from i)
time-consuming optimization for projecting real images to the latent vectors,
ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the
intermediate latent space has spatial dimensions, and a spatially variant
modulation replaces AdaIN. It makes the embedding through an encoder more
accurate than existing optimization-based methods while maintaining the
properties of GANs. Experimental results demonstrate that our method
significantly outperforms state-of-the-art models in various image manipulation
tasks such as local editing and image interpolation. Last but not least,
conventional editing methods on GANs are still valid on our StyleMapGAN. Source
code is available at https://github.com/naver-ai/StyleMapGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-06-24T01:51:44.872Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to complicated inverse
Fisher-matrix computations. We address this issue for optimization, inference,
and search problems by using \emph{local-parameter coordinates}. Our method
generalizes an existing evolutionary-strategy method, recovers Newton and
Riemannian-gradient methods as special cases, and also yields new tractable
natural-gradient algorithms for learning flexible covariance structures of
Gaussian and Wishart-based distributions via \emph{matrix groups}. We show
results on a range of applications on deep learning, variational inference, and
evolution strategies. Our work opens a new direction for scalable structured
geometric methods via local parameterizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12380</id>
        <link href="http://arxiv.org/abs/2008.12380"/>
        <updated>2021-06-24T01:51:44.867Z</updated>
        <summary type="html"><![CDATA[Fluorescence microscopy allows for a detailed inspection of cells, cellular
networks, and anatomical landmarks by staining with a variety of
carefully-selected markers visualized as color channels. Quantitative
characterization of structures in acquired images often relies on automatic
image analysis methods. Despite the success of deep learning methods in other
vision applications, their potential for fluorescence image analysis remains
underexploited. One reason lies in the considerable workload required to train
accurate models, which are normally specific for a given combination of
markers, and therefore applicable to a very restricted number of experimental
settings. We herein propose Marker Sampling and Excite, a neural network
approach with a modality sampling strategy and a novel attention module that
together enable (i) flexible training with heterogeneous datasets with
combinations of markers and (ii) successful utility of learned models on
arbitrary subsets of markers prospectively. We show that our single neural
network solution performs comparably to an upper bound scenario where an
ensemble of many networks is na\"ively trained for each possible marker
combination separately. In addition, we demonstrate the feasibility of this
framework in high-throughput biological analysis by revising a recent
quantitative characterization of bone marrow vasculature in 3D confocal
microscopy datasets and further confirm the validity of our approach on an
additional, significantly different dataset of microvessels in fetal liver
tissues. Not only can our work substantially ameliorate the use of deep
learning in fluorescence microscopy analysis, but it can also be utilized in
other fields with incomplete data acquisitions and missing modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1"&gt;Alvaro Gomariz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1"&gt;Tiziano Portenier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1"&gt;Patrick M. Helbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1"&gt;Stephan Isringhausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1"&gt;Ute Suessbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Nombela-Arrieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1"&gt;Orcun Goksel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bregman Gradient Policy Optimization. (arXiv:2106.12112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12112</id>
        <link href="http://arxiv.org/abs/2106.12112"/>
        <updated>2021-06-24T01:51:44.852Z</updated>
        <summary type="html"><![CDATA[In this paper, we design a novel Bregman gradient policy optimization
framework for reinforcement learning based on Bregman divergences and momentum
techniques. Specifically, we propose a Bregman gradient policy optimization
(BGPO) algorithm based on the basic momentum technique and mirror descent
iteration. At the same time, we present an accelerated Bregman gradient policy
optimization (VR-BGPO) algorithm based on a momentum variance-reduced
technique. Moreover, we introduce a convergence analysis framework for our
Bregman gradient policy optimization under the nonconvex setting. Specifically,
we prove that BGPO achieves the sample complexity of $\tilde{O}(\epsilon^{-4})$
for finding $\epsilon$-stationary point only requiring one trajectory at each
iteration, and VR-BGPO reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point, which
also only requires one trajectory at each iteration. In particular, by using
different Bregman divergences, our methods unify many existing policy
optimization algorithms and their new variants such as the existing
(variance-reduced) policy gradient algorithms and (variance-reduced) natural
policy gradient algorithms. Extensive experimental results on multiple
reinforcement learning tasks demonstrate the efficiency of our new algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shangqian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks. (arXiv:2012.08740v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08740</id>
        <link href="http://arxiv.org/abs/2012.08740"/>
        <updated>2021-06-24T01:51:44.835Z</updated>
        <summary type="html"><![CDATA[We study the problem of clustering nodes in a dynamic graph, where the
connections between nodes and nodes' cluster memberships may change over time,
e.g., due to community migration. We first propose a dynamic stochastic block
model that captures these changes, and a simple decay-based clustering
algorithm that clusters nodes based on weighted connections between them, where
the weight decreases at a fixed rate over time. This decay rate can then be
interpreted as signifying the importance of including historical connection
information in the clustering. However, the optimal decay rate may differ for
clusters with different rates of turnover. We characterize the optimal decay
rate for each cluster and propose a clustering method that achieves almost
exact recovery of the true clusters. We then demonstrate the efficacy of our
clustering algorithm with optimized decay rates on simulated graph data.
Recurrent neural networks (RNNs), a popular algorithm for sequence learning,
use a similar decay-based method, and we use this insight to propose two new
RNN-GCN (graph convolutional network) architectures for semi-supervised graph
clustering. We finally demonstrate that the proposed architectures perform well
on real data compared to state-of-the-art graph clustering algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuhang Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1"&gt;Carlee Joe-Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented Tensor Decomposition with Stochastic Optimization. (arXiv:2106.07900v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07900</id>
        <link href="http://arxiv.org/abs/2106.07900"/>
        <updated>2021-06-24T01:51:44.828Z</updated>
        <summary type="html"><![CDATA[Tensor decompositions are powerful tools for dimensionality reduction and
feature interpretation of multidimensional data such as signals. Existing
tensor decomposition objectives (e.g., Frobenius norm) are designed for fitting
raw data under statistical assumptions, which may not align with downstream
classification tasks. Also, real-world tensor data are usually high-ordered and
have large dimensions with millions or billions of entries. Thus, it is
expensive to decompose the whole tensor with traditional algorithms. In
practice, raw tensor data also contains redundant information while data
augmentation techniques may be used to smooth out noise in samples. This paper
addresses the above challenges by proposing augmented tensor decomposition
(ATD), which effectively incorporates data augmentations to boost downstream
classification. To reduce the memory footprint of the decomposition, we propose
a stochastic algorithm that updates the factor matrices in a batch fashion. We
evaluate ATD on multiple signal datasets. It shows comparable or better
performance (e.g., up to 15% in accuracy) over self-supervised and autoencoder
baselines with less than 5% of model parameters, achieves 0.6% ~ 1.3% accuracy
gain over other tensor-based baselines, and reduces the memory footprint by 9X
when compared to standard tensor decomposition algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Westover_M/0/1/0/all/0/1"&gt;M Brandon Westover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04806</id>
        <link href="http://arxiv.org/abs/2009.04806"/>
        <updated>2021-06-24T01:51:44.821Z</updated>
        <summary type="html"><![CDATA[Sketch drawings capture the salient information of visual concepts. Previous
work has shown that neural networks are capable of producing sketches of
natural objects drawn from a small number of classes. While earlier approaches
focus on generation quality or retrieval, we explore properties of image
representations learned by training a model to produce sketches of images. We
show that this generative, class-agnostic model produces informative embeddings
of images from novel examples, classes, and even novel datasets in a few-shot
setting. Additionally, we find that these learned representations exhibit
interesting structure and compositionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alexander Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard S. Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANMEX: One-vs-One Attributions Guided by GAN-based Counterfactual Explanation Baselines. (arXiv:2011.06015v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06015</id>
        <link href="http://arxiv.org/abs/2011.06015"/>
        <updated>2021-06-24T01:51:44.816Z</updated>
        <summary type="html"><![CDATA[Attribution methods have been shown as promising approaches for identifying
key features that led to learned model predictions. While most existing
attribution methods rely on a baseline input for performing feature
perturbations, limited research has been conducted to address the baseline
selection issues. Poor choices of baselines limit the ability of one-vs-one
(1-vs-1) explanations for multi-class classifiers, which means the attribution
methods were not able to explain why an input belongs to its original class but
not the other specified target class. 1-vs-1 explanation is crucial when
certain classes are more similar than others, e.g. two bird types among
multiple animals, by focusing on key differentiating features rather than
shared features across classes. In this paper, we present GAN-based Model
EXplainability (GANMEX), a novel approach applying Generative Adversarial
Networks (GAN) by incorporating the to-be-explained classifier as part of the
adversarial networks. Our approach effectively selects the counterfactual
baseline as the closest realistic sample belong to the target class, which
allows attribution methods to provide true 1-vs-1 explanations. We showed that
GANMEX baselines improved the saliency maps and led to stronger performance on
perturbation-based evaluation metrics over the existing baselines. Existing
attribution results are known for being insensitive to model randomization, and
we demonstrated that GANMEX baselines led to better outcome under the cascading
randomization of the model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shih_S/0/1/0/all/0/1"&gt;Sheng-Min Shih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tien_P/0/1/0/all/0/1"&gt;Pin-Ju Tien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1"&gt;Zohar Karnin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03206</id>
        <link href="http://arxiv.org/abs/2103.03206"/>
        <updated>2021-06-24T01:51:44.801Z</updated>
        <summary type="html"><![CDATA[Biological systems perceive the world by simultaneously processing
high-dimensional inputs from modalities as diverse as vision, audition, touch,
proprioception, etc. The perception models used in deep learning on the other
hand are designed for individual modalities, often relying on domain-specific
assumptions such as the local grid structures exploited by virtually all
existing vision models. These priors introduce helpful inductive biases, but
also lock models to individual modalities. In this paper we introduce the
Perceiver - a model that builds upon Transformers and hence makes few
architectural assumptions about the relationship between its inputs, but that
also scales to hundreds of thousands of inputs, like ConvNets. The model
leverages an asymmetric attention mechanism to iteratively distill inputs into
a tight latent bottleneck, allowing it to scale to handle very large inputs. We
show that this architecture is competitive with or outperforms strong,
specialized models on classification tasks across various modalities: images,
point clouds, audio, video, and video+audio. The Perceiver obtains performance
comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly
attending to 50,000 pixels. It is also competitive in all modalities in
AudioSet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1"&gt;Felix Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data. (arXiv:2010.11354v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11354</id>
        <link href="http://arxiv.org/abs/2010.11354"/>
        <updated>2021-06-24T01:51:44.795Z</updated>
        <summary type="html"><![CDATA[Methods that sparsify a network at initialization are important in practice
because they greatly improve the efficiency of both learning and inference. Our
work is based on a recently proposed decomposition of the Neural Tangent Kernel
(NTK) that has decoupled the dynamics of the training process into a
data-dependent component and an architecture-dependent kernel - the latter
referred to as Path Kernel. That work has shown how to design sparse neural
networks for faster convergence, without any training data, using the
Synflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in
terms of convergence, for a given network density, it results in sub-networks
with "bottleneck" (narrow) layers - leading to poor performance as compared to
other data-agnostic methods that use the same number of parameters. Then we
propose a new method to construct sparse networks, without any training data,
referred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic
network formation method based on biased random walks that only depends on the
initial weights. It has similar path kernel properties as Synflow-L2 but it
generates much wider layers, resulting in better generalization and
performance. PHEW achieves significant improvements over the data-independent
SynFlow and SynFlow-L2 methods at a wide range of network densities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1"&gt;Shreyas Malakarjun Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dovrolis_C/0/1/0/all/0/1"&gt;Constantine Dovrolis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing. (arXiv:2104.14754v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14754</id>
        <link href="http://arxiv.org/abs/2104.14754"/>
        <updated>2021-06-24T01:51:44.790Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) synthesize realistic images from
random latent vectors. Although manipulating the latent vectors controls the
synthesized outputs, editing real images with GANs suffers from i)
time-consuming optimization for projecting real images to the latent vectors,
ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the
intermediate latent space has spatial dimensions, and a spatially variant
modulation replaces AdaIN. It makes the embedding through an encoder more
accurate than existing optimization-based methods while maintaining the
properties of GANs. Experimental results demonstrate that our method
significantly outperforms state-of-the-art models in various image manipulation
tasks such as local editing and image interpolation. Last but not least,
conventional editing methods on GANs are still valid on our StyleMapGAN. Source
code is available at https://github.com/naver-ai/StyleMapGAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yunjey Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1"&gt;Sungjoo Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1"&gt;Youngjung Uh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taming GANs with Lookahead-Minmax. (arXiv:2006.14567v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14567</id>
        <link href="http://arxiv.org/abs/2006.14567"/>
        <updated>2021-06-24T01:51:44.784Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks are notoriously challenging to train. The
underlying minmax optimization is highly susceptible to the variance of the
stochastic gradient and the rotational component of the associated game vector
field. To tackle these challenges, we propose the Lookahead algorithm for
minmax optimization, originally developed for single objective minimization
only. The backtracking step of our Lookahead-minmax naturally handles the
rotational game dynamics, a property which was identified to be key for
enabling gradient ascent descent methods to converge on challenging examples
often analyzed in the literature. Moreover, it implicitly handles high variance
without using large mini-batches, known to be essential for reaching state of
the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and
ImageNet demonstrate a clear advantage of combining Lookahead-minmax with Adam
or extragradient, in terms of performance and improved stability, for
negligible memory and computational cost. Using 30-fold fewer parameters and
16-fold smaller minibatches we outperform the reported performance of the
class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the
class labels, bringing state-of-the-art GAN training within reach of common
computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chavdarova_T/0/1/0/all/0/1"&gt;Tatjana Chavdarova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pagliardini_M/0/1/0/all/0/1"&gt;Matteo Pagliardini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fleuret_F/0/1/0/all/0/1"&gt;Francois Fleuret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09258</id>
        <link href="http://arxiv.org/abs/2101.09258"/>
        <updated>2021-06-24T01:51:44.779Z</updated>
        <summary type="html"><![CDATA[Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based models
can be tractably computed through a connection to continuous normalizing flows,
but log-likelihood is not directly optimized by the weighted combination of
score matching losses. We show that for a specific weighting scheme, the
objective upper bounds the negative log-likelihood, thus enabling approximate
maximum likelihood training of score-based models. We empirically observe that
maximum likelihood training consistently improves the likelihood of score-based
models across multiple datasets, stochastic processes, and model architectures.
Our best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on
CIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1"&gt;Conor Durkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"&gt;Iain Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06392</id>
        <link href="http://arxiv.org/abs/2104.06392"/>
        <updated>2021-06-24T01:51:44.766Z</updated>
        <summary type="html"><![CDATA[A popular way to create detailed yet easily controllable 3D shapes is via
procedural modeling, i.e. generating geometry using programs. Such programs
consist of a series of instructions along with their associated parameter
values. To fully realize the benefits of this representation, a shape program
should be compact and only expose degrees of freedom that allow for meaningful
manipulation of output geometry. One way to achieve this goal is to design
higher-level macro operators that, when executed, expand into a series of
commands from the base shape modeling language. However, manually authoring
such macros, much like shape programs themselves, is difficult and largely
restricted to domain experts. In this paper, we present ShapeMOD, an algorithm
for automatically discovering macros that are useful across large datasets of
3D shape programs. ShapeMOD operates on shape programs expressed in an
imperative, statement-based language. It is designed to discover macros that
make programs more compact by minimizing the number of function calls and free
parameters required to represent an input shape collection. We run ShapeMOD on
multiple collections of programs expressed in a domain-specific language for 3D
shape structures. We show that it automatically discovers a concise set of
macros that abstract out common structural and parametric patterns that
generalize over large shape collections. We also demonstrate that the macros
found by ShapeMOD improve performance on downstream tasks including shape
generative modeling and inferring programs from point clouds. Finally, we
conduct a user study that indicates that ShapeMOD's discovered macros make
interactive shape editing more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1"&gt;David Charatan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-06-24T01:51:44.760Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIGL: Securing Software Installations Through Deep Graph Learning. (arXiv:2008.11533v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11533</id>
        <link href="http://arxiv.org/abs/2008.11533"/>
        <updated>2021-06-24T01:51:44.754Z</updated>
        <summary type="html"><![CDATA[Many users implicitly assume that software can only be exploited after it is
installed. However, recent supply-chain attacks demonstrate that application
integrity must be ensured during installation itself. We introduce SIGL, a new
tool for detecting malicious behavior during software installation. SIGL
collects traces of system call activity, building a data provenance graph that
it analyzes using a novel autoencoder architecture with a graph long short-term
memory network (graph LSTM) for the encoder and a standard multilayer
perceptron for the decoder. SIGL flags suspicious installations as well as the
specific installation-time processes that are likely to be malicious. Using a
test corpus of 625 malicious installers containing real-world malware, we
demonstrate that SIGL has a detection accuracy of 96%, outperforming similar
systems from industry and academia by up to 87% in precision and recall and 45%
in accuracy. We also demonstrate that SIGL can pinpoint the processes most
likely to have triggered malicious behavior, works on different audit platforms
and operating systems, and is robust to training data contamination and
adversarial attack. It can be used with application-specific models, even in
the presence of new software versions, as well as application-agnostic
meta-models that encompass a wide range of applications and installers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xueyuan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasquier_T/0/1/0/all/0/1"&gt;Thomas Pasquier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Ding Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_J/0/1/0/all/0/1"&gt;Junghwan Rhee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mickens_J/0/1/0/all/0/1"&gt;James Mickens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1"&gt;Margo Seltzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Entropic Out-of-Distribution Detection using Isometric Distances and the Minimum Distance Score. (arXiv:2105.14399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14399</id>
        <link href="http://arxiv.org/abs/2105.14399"/>
        <updated>2021-06-24T01:51:44.749Z</updated>
        <summary type="html"><![CDATA[Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all the previously
mentioned drawbacks). The entropic out-of-distribution detection solution
comprises the IsoMax loss for training and the entropic score for
out-of-distribution detection. The IsoMax loss works as a SoftMax loss drop-in
replacement because swapping the SoftMax loss with the IsoMax loss requires no
changes in the model's architecture or training procedures/hyperparameters. In
this paper, we propose to perform what we call an isometrization of the
distances used in the IsoMax loss. Additionally, we propose to replace the
entropic score with the minimum distance score. Our experiments showed that
these simple modifications increase out-of-distribution detection performance
while keeping the solution seamless.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Thompson Sampling. (arXiv:2102.06129v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06129</id>
        <link href="http://arxiv.org/abs/2102.06129"/>
        <updated>2021-06-24T01:51:44.744Z</updated>
        <summary type="html"><![CDATA[Efficient exploration in bandits is a fundamental online learning problem. We
propose a variant of Thompson sampling that learns to explore better as it
interacts with bandit instances drawn from an unknown prior. The algorithm
meta-learns the prior and thus we call it MetaTS. We propose several efficient
implementations of MetaTS and analyze it in Gaussian bandits. Our analysis
shows the benefit of meta-learning and is of a broader interest, because we
derive a novel prior-dependent Bayes regret bound for Thompson sampling. Our
theory is complemented by empirical evaluation, which shows that MetaTS quickly
adapts to the unknown prior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konobeev_M/0/1/0/all/0/1"&gt;Mikhail Konobeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chih-wei Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mladenov_M/0/1/0/all/0/1"&gt;Martin Mladenov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1"&gt;Craig Boutilier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering of check-in sequences using the mixture Markov chain process. (arXiv:2106.12039v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.12039</id>
        <link href="http://arxiv.org/abs/2106.12039"/>
        <updated>2021-06-24T01:51:44.729Z</updated>
        <summary type="html"><![CDATA[This work is devoted to the clustering of check-in sequences from a geosocial
network. We used the mixture Markov chain process as a mathematical model for
time-dependent types of data. For clustering, we adjusted the
Expectation-Maximization (EM) algorithm. As a result, we obtained highly
detailed communities (clusters) of users of the now defunct geosocial network,
Weeplaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmileva_E/0/1/0/all/0/1"&gt;Elena Shmileva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarzhan_V/0/1/0/all/0/1"&gt;Viktor Sarzhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A LightGBM based Forecasting of Dominant Wave Periods in Oceanic Waters. (arXiv:2105.08721v3 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08721</id>
        <link href="http://arxiv.org/abs/2105.08721"/>
        <updated>2021-06-24T01:51:44.724Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Light Gradient Boosting (LightGBM) to forecast
dominant wave periods in oceanic waters. First, we use the data collected from
CDIP buoys and apply various data filtering methods. The data filtering methods
allow us to obtain a high-quality dataset for training and validation purposes.
We then extract various wave-based features like wave heights, periods,
skewness, kurtosis, etc., and atmospheric features like humidity, pressure, and
air temperature for the buoys. Afterward, we train algorithms that use LightGBM
and Extra Trees through a hv-block cross-validation scheme to forecast dominant
wave periods for up to 30 days ahead. LightGBM has the R2 score of 0.94, 0.94,
and 0.94 for 1-day ahead, 15-day ahead, and 30-day ahead prediction. Similarly,
Extra Trees (ET) has an R2 score of 0.88, 0.86, and 0.85 for 1-day ahead,
15-day ahead, and 30 day ahead prediction. In case of the test dataset,
LightGBM has R2 score of 0.94, 0.94, and 0.94 for 1-day ahead, 15-day ahead and
30-day ahead prediction. ET has R2 score of 0.88, 0.86, and 0.85 for 1-day
ahead, 15-day ahead, and 30-day ahead prediction. A similar R2 score for both
training and the test dataset suggests that the machine learning models
developed in this paper are robust. Since the LightGBM algorithm outperforms ET
for all the windows tested, it is taken as the final algorithm. Note that the
performance of both methods does not decrease significantly as the forecast
horizon increases. Likewise, the proposed method outperforms the numerical
approaches included in this paper in the test dataset. For 1 day ahead
prediction, the proposed algorithm has SI, Bias, CC, and RMSE of 0.09, 0.00,
0.97, and 1.78 compared to 0.268, 0.40, 0.63, and 2.18 for the European Centre
for Medium-range Weather Forecasts (ECMWF) model, which outperforms all the
other methods in the test dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1"&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1"&gt;Mahdi Abdelguerfi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding simplicity: unsupervised discovery of features, patterns, and order parameters via shift-invariant variational autoencoders. (arXiv:2106.12472v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2106.12472</id>
        <link href="http://arxiv.org/abs/2106.12472"/>
        <updated>2021-06-24T01:51:44.718Z</updated>
        <summary type="html"><![CDATA[Recent advances in scanning tunneling and transmission electron microscopies
(STM and STEM) have allowed routine generation of large volumes of imaging data
containing information on the structure and functionality of materials. The
experimental data sets contain signatures of long-range phenomena such as
physical order parameter fields, polarization and strain gradients in STEM, or
standing electronic waves and carrier-mediated exchange interactions in STM,
all superimposed onto scanning system distortions and gradual changes of
contrast due to drift and/or mis-tilt effects. Correspondingly, while the human
eye can readily identify certain patterns in the images such as lattice
periodicities, repeating structural elements, or microstructures, their
automatic extraction and classification are highly non-trivial and universal
pathways to accomplish such analyses are absent. We pose that the most
distinctive elements of the patterns observed in STM and (S)TEM images are
similarity and (almost-) periodicity, behaviors stemming directly from the
parsimony of elementary atomic structures, superimposed on the gradual changes
reflective of order parameter distributions. However, the discovery of these
elements via global Fourier methods is non-trivial due to variability and lack
of ideal discrete translation symmetry. To address this problem, we develop
shift-invariant variational autoencoders (shift-VAE) that allow disentangling
characteristic repeating features in the images, their variations, and shifts
inevitable for random sampling of image space. Shift-VAEs balance the
uncertainty in the position of the object of interest with the uncertainty in
shape reconstruction. This approach is illustrated for model 1D data, and
further extended to synthetic and experimental STM and STEM 2D data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ziatdinov_M/0/1/0/all/0/1"&gt;Maxim Ziatdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wong_C/0/1/0/all/0/1"&gt;Chun Yin Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kalinin_S/0/1/0/all/0/1"&gt;Sergei V. Kalinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-06-24T01:51:44.713Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Speech Enhancement using Dynamical Variational Auto-Encoders. (arXiv:2106.12271v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.12271</id>
        <link href="http://arxiv.org/abs/2106.12271"/>
        <updated>2021-06-24T01:51:44.707Z</updated>
        <summary type="html"><![CDATA[Dynamical variational auto-encoders (DVAEs) are a class of deep generative
models with latent variables, dedicated to time series data modeling. DVAEs can
be considered as extensions of the variational autoencoder (VAE) that include
the modeling of temporal dependencies between successive observed and/or latent
vectors in data sequences. Previous work has shown the interest of DVAEs and
their better performance over the VAE for speech signals (spectrogram)
modeling. Independently, the VAE has been successfully applied to speech
enhancement in noise, in an unsupervised noise-agnostic set-up that does not
require the use of a parallel dataset of clean and noisy speech samples for
training, but only requires clean speech signals. In this paper, we extend
those works to DVAE-based single-channel unsupervised speech enhancement, hence
exploiting both speech signals unsupervised representation learning and
dynamics modeling. We propose an unsupervised speech enhancement algorithm
based on the most general form of DVAEs, that we then adapt to three specific
DVAE models to illustrate the versatility of the framework. More precisely, we
combine DVAE-based speech priors with a noise model based on nonnegative matrix
factorization, and we derive a variational expectation-maximization (VEM)
algorithm to perform speech enhancement. Experimental results show that the
proposed approach based on DVAEs outperforms its VAE counterpart and a
supervised speech enhancement baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bie_X/0/1/0/all/0/1"&gt;Xiaoyu Bie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leglaive_S/0/1/0/all/0/1"&gt;Simon Leglaive&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1"&gt;Xavier Alameda-Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1"&gt;Laurent Girin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDR Environment Map Estimation for Real-Time Augmented Reality. (arXiv:2011.10687v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10687</id>
        <link href="http://arxiv.org/abs/2011.10687"/>
        <updated>2021-06-24T01:51:44.693Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somanath_G/0/1/0/all/0/1"&gt;Gowri Somanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1"&gt;Daniel Kurz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Based on One Monocular Video Delivers Highly Valid and Reliable Gait Parameters. (arXiv:2008.08045v5 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08045</id>
        <link href="http://arxiv.org/abs/2008.08045"/>
        <updated>2021-06-24T01:51:44.688Z</updated>
        <summary type="html"><![CDATA[Despite its paramount importance for manifold use cases (e.g., in the health
care industry, sports, rehabilitation and fitness assessment), sufficiently
valid and reliable gait parameter measurement is still limited to high-tech
gait laboratories mostly. Here, we demonstrate the excellent validity and
test-retest repeatability of a novel gait assessment system which is built upon
modern convolutional neural networks to extract three-dimensional skeleton
joints from monocular frontal-view videos of walking humans. The validity study
is based on a comparison to the GAITRite pressure-sensitive walkway system. All
measured gait parameters (gait speed, cadence, step length and step time)
showed excellent concurrent validity for multiple walk trials at normal and
fast gait speeds. The test-retest-repeatability is on the same level as the
GAITRite system. In conclusion, we are convinced that our results can pave the
way for cost, space and operationally effective gait analysis in broad
mainstream applications. Most sensor-based systems are costly, must be operated
by extensively trained personnel (e.g., motion capture systems) or - even if
not quite as costly - still possess considerable complexity (e.g., wearable
sensors). In contrast, a video sufficient for the assessment method presented
here can be obtained by anyone, without much training, via a smartphone camera.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Azhand_D/0/1/0/all/0/1"&gt;Dr. Arash Azhand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rabe_D/0/1/0/all/0/1"&gt;Dr. Sophie Rabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Muller_D/0/1/0/all/0/1"&gt;Dr. Swantje M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sattler_I/0/1/0/all/0/1"&gt;Igor Sattler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steinert_D/0/1/0/all/0/1"&gt;Dr. Anika Steinert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-24T01:51:44.683Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10898</id>
        <link href="http://arxiv.org/abs/2106.10898"/>
        <updated>2021-06-24T01:51:44.678Z</updated>
        <summary type="html"><![CDATA[Multi-armed bandits (MAB) provide a principled online learning approach to
attain the balance between exploration and exploitation. Due to the superior
performance and low feedback learning without the learning to act in multiple
situations, Multi-armed Bandits drawing widespread attention in applications
ranging such as recommender systems. Likewise, within the recommender system,
collaborative filtering (CF) is arguably the earliest and most influential
method in the recommender system. Crucially, new users and an ever-changing
pool of recommended items are the challenges that recommender systems need to
address. For collaborative filtering, the classical method is training the
model offline, then perform the online testing, but this approach can no longer
handle the dynamic changes in user preferences which is the so-called cold
start. So how to effectively recommend items to users in the absence of
effective information? To address the aforementioned problems, a multi-armed
bandit based collaborative filtering recommender system has been proposed,
named BanditMF. BanditMF is designed to address two challenges in the
multi-armed bandits algorithm and collaborative filtering: (1) how to solve the
cold start problem for collaborative filtering under the condition of scarcity
of valid information, (2) how to solve the sub-optimal problem of bandit
algorithms in strong social relations domains caused by independently
estimating unknown parameters associated with each user and ignoring
correlations between users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenghao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-time Collective Prediction. (arXiv:2106.12012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12012</id>
        <link href="http://arxiv.org/abs/2106.12012"/>
        <updated>2021-06-24T01:51:44.673Z</updated>
        <summary type="html"><![CDATA[An increasingly common setting in machine learning involves multiple parties,
each with their own data, who want to jointly make predictions on future test
points. Agents wish to benefit from the collective expertise of the full set of
agents to make better predictions than they would individually, but may not be
willing to release their data or model parameters. In this work, we explore a
decentralized mechanism to make collective predictions at test time, leveraging
each agent's pre-trained model without relying on external validation, model
retraining, or data pooling. Our approach takes inspiration from the literature
in social science on human consensus-making. We analyze our mechanism
theoretically, showing that it converges to inverse meansquared-error (MSE)
weighting in the large-sample limit. To compute error bars on the collective
predictions we propose a decentralized Jackknife procedure that evaluates the
sensitivity of our mechanism to a single agent's prediction. Empirically, we
demonstrate that our scheme effectively combines models with differing quality
across the input space. The proposed consensus prediction achieves significant
gains over classical model averaging, and even outperforms weighted averaging
schemes that have access to additional validation data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendler_Dunner_C/0/1/0/all/0/1"&gt;Celestine Mendler-D&amp;#xfc;nner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Outdoor Localization Using Radio Maps: A Deep Learning Approach. (arXiv:2106.12556v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12556</id>
        <link href="http://arxiv.org/abs/2106.12556"/>
        <updated>2021-06-24T01:51:44.668Z</updated>
        <summary type="html"><![CDATA[This paper deals with the problem of localization in a cellular network in a
dense urban scenario. Global Navigation Satellite Systems typically perform
poorly in urban environments, where the likelihood of line-of-sight conditions
between the devices and the satellites is low, and thus alternative
localization methods are required for good accuracy. We present a deep learning
method for localization, based merely on pathloss, which does not require any
increase in computation complexity at the user devices with respect to the
device standard operations, unlike methods that rely on time of arrival or
angle of arrival information. In a wireless network, user devices scan the base
station beacon slots and identify the few strongest base station signals for
handover and user-base station association purposes. In the proposed method,
the user to be localized simply reports such received signal strengths to a
central processing unit, which may be located in the cloud. For each base
station we have good approximation of the pathloss at every location in a dense
grid in the map. This approximation is provided by RadioUNet, a deep
learning-based simulator of pathloss functions in urban environment, that we
have previously proposed and published. Using the estimated pathloss radio maps
of all base stations and the corresponding reported signal strengths, the
proposed deep learning algorithm can extract a very accurate localization of
the user. The proposed method, called LocUNet, enjoys high robustness to
inaccuracies in the estimated radio maps. We demonstrate this by numerical
experiments, which obtain state-of-the-art results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yapar_C/0/1/0/all/0/1"&gt;&amp;#xc7;a&amp;#x11f;kan Yapar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1"&gt;Ron Levie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1"&gt;Gitta Kutyniok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caire_G/0/1/0/all/0/1"&gt;Giuseppe Caire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenML-Python: an extensible Python API for OpenML. (arXiv:1911.02490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.02490</id>
        <link href="http://arxiv.org/abs/1911.02490"/>
        <updated>2021-06-24T01:51:44.653Z</updated>
        <summary type="html"><![CDATA[OpenML is an online platform for open science collaboration in machine
learning, used to share datasets and results of machine learning experiments.
In this paper we introduce OpenML-Python, a client API for Python, opening up
the OpenML platform for a wide range of Python-based tools. It provides easy
access to all datasets, tasks and experiments on OpenML from within Python. It
also provides functionality to conduct machine learning experiments, upload the
results to OpenML, and reproduce results which are stored on OpenML.
Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to
easily integrate other machine learning libraries written in Python into the
OpenML ecosystem. Source code and documentation is available at
https://github.com/openml/openml-python/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feurer_M/0/1/0/all/0/1"&gt;Matthias Feurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1"&gt;Jan N. van Rijn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadra_A/0/1/0/all/0/1"&gt;Arlind Kadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gijsbers_P/0/1/0/all/0/1"&gt;Pieter Gijsbers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1"&gt;Neeratyoy Mallik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sahithya Ravi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_A/0/1/0/all/0/1"&gt;Andreas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1"&gt;Joaquin Vanschoren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1"&gt;Frank Hutter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-06-24T01:51:44.648Z</updated>
        <summary type="html"><![CDATA[Deep generative models have recently demonstrated the ability to synthesize
photorealistic images of human faces with novel identities. A key challenge to
the wide applicability of such techniques is to provide independent control
over semantically meaningful parameters: appearance, head pose, face shape, and
facial expressions. In this paper, we propose VariTex - to the best of our
knowledge the first method that learns a variational latent feature space of
neural face textures, which allows sampling of novel identities. We combine
this generative model with a parametric face model and gain explicit control
over head pose and facial expressions. To generate images of complete human
heads, we propose an additive decoder that generates plausible additional
details such as hair. A novel training scheme enforces a pose independent
latent space and in consequence, allows learning of a one-to-many mapping
between latent codes and pose-conditioned exterior regions. The resulting
method can generate geometrically consistent images of novel identities
allowing fine-grained control over head pose, face shape, and facial
expressions, facilitating a broad range of downstream tasks, like sampling
novel identities, re-posing, expression transfer, and more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Supervised Constrastive Learning. (arXiv:2102.08817v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08817</id>
        <link href="http://arxiv.org/abs/2102.08817"/>
        <updated>2021-06-24T01:51:44.642Z</updated>
        <summary type="html"><![CDATA[Minimizing cross-entropy over the softmax scores of a linear map composed
with a high-capacity encoder is arguably the most popular choice for training
neural networks on supervised learning tasks. However, recent works show that
one can directly optimize the encoder instead, to obtain equally (or even more)
discriminative representations via a supervised variant of a contrastive
objective. In this work, we address the question whether there are fundamental
differences in the sought-for representation geometry in the output space of
the encoder at minimal loss. Specifically, we prove, under mild assumptions,
that both losses attain their minimum once the representations of each class
collapse to the vertices of a regular simplex, inscribed in a hypersphere. We
provide empirical evidence that this configuration is attained in practice and
that reaching a close-to-optimal state typically indicates good generalization
performance. Yet, the two losses show remarkably different optimization
behavior. The number of iterations required to perfectly fit to data scales
superlinearly with the amount of randomly flipped labels for the supervised
contrastive loss. This is in contrast to the approximately linear scaling
previously reported for networks trained with cross-entropy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Graf_F/0/1/0/all/0/1"&gt;Florian Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hofer_C/0/1/0/all/0/1"&gt;Christoph D. Hofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Niethammer_M/0/1/0/all/0/1"&gt;Marc Niethammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Social Learning via Multi-agent Reinforcement Learning. (arXiv:2010.00581v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00581</id>
        <link href="http://arxiv.org/abs/2010.00581"/>
        <updated>2021-06-24T01:51:44.637Z</updated>
        <summary type="html"><![CDATA[Social learning is a key component of human and animal intelligence. By
taking cues from the behavior of experts in their environment, social learners
can acquire sophisticated behavior and rapidly adapt to new circumstances. This
paper investigates whether independent reinforcement learning (RL) agents in a
multi-agent environment can learn to use social learning to improve their
performance. We find that in most circumstances, vanilla model-free RL agents
do not use social learning. We analyze the reasons for this deficiency, and
show that by imposing constraints on the training environment and introducing a
model-based auxiliary loss we are able to obtain generalized social learning
policies which enable agents to: i) discover complex skills that are not
learned from single-agent training, and ii) adapt online to novel environments
by taking cues from experts present in the new environment. In contrast, agents
trained with model-free RL or imitation learning generalize poorly and do not
succeed in the transfer tasks. By mixing multi-agent and solo training, we can
obtain agents that use social learning to gain skills that they can deploy when
alone, even out-performing agents trained alone from the start.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1"&gt;Kamal Ndousse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eck_D/0/1/0/all/0/1"&gt;Douglas Eck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1"&gt;Natasha Jaques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations. (arXiv:2003.06085v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06085</id>
        <link href="http://arxiv.org/abs/2003.06085"/>
        <updated>2021-06-24T01:51:44.623Z</updated>
        <summary type="html"><![CDATA[Imitation learning is an effective and safe technique to train robot policies
in the real world because it does not depend on an expensive random exploration
process. However, due to the lack of exploration, learning policies that
generalize beyond the demonstrated behaviors is still an open challenge. We
present a novel imitation learning framework to enable robots to 1) learn
complex real world manipulation tasks efficiently from a small number of human
demonstrations, and 2) synthesize new behaviors not contained in the collected
demonstrations. Our key insight is that multi-task domains often present a
latent structure, where demonstrated trajectories for different tasks intersect
at common regions of the state space. We present Generalization Through
Imitation (GTI), a two-stage offline imitation learning algorithm that exploits
this intersecting structure to train goal-directed policies that generalize to
unseen start and goal state combinations. In the first stage of GTI, we train a
stochastic policy that leverages trajectory intersections to have the capacity
to compose behaviors from different demonstration trajectories together. In the
second stage of GTI, we collect a small set of rollouts from the unconditioned
stochastic policy of the first stage, and train a goal-directed agent to
generalize to novel start and goal configurations. We validate GTI in both
simulated domains and a challenging long-horizon robotic manipulation domain in
the real world. Additional results and videos are available at
https://sites.google.com/view/gti2020/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1"&gt;Ajay Mandlekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Mart&amp;#xed;n-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Attributions and Counterfactual Explanations Can Be Manipulated. (arXiv:2106.12563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12563</id>
        <link href="http://arxiv.org/abs/2106.12563"/>
        <updated>2021-06-24T01:51:44.617Z</updated>
        <summary type="html"><![CDATA[As machine learning models are increasingly used in critical decision-making
settings (e.g., healthcare, finance), there has been a growing emphasis on
developing methods to explain model predictions. Such \textit{explanations} are
used to understand and establish trust in models and are vital components in
machine learning pipelines. Though explanations are a critical piece in these
systems, there is little understanding about how they are vulnerable to
manipulation by adversaries. In this paper, we discuss how two broad classes of
explanations are vulnerable to manipulation. We demonstrate how adversaries can
design biased models that manipulate model agnostic feature attribution methods
(e.g., LIME \& SHAP) and counterfactual explanations that hill-climb during the
counterfactual search (e.g., Wachter's Algorithm \& DiCE) into
\textit{concealing} the model's biases. These vulnerabilities allow an
adversary to deploy a biased model, yet explanations will not reveal this bias,
thereby deceiving stakeholders into trusting the model. We evaluate the
manipulations on real world data sets, including COMPAS and Communities \&
Crime, and find explanations can be manipulated in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Hima Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performance and Complexity Analysis of bi-directional Recurrent Neural Network Models vs. Volterra Nonlinear Equalizers in Digital Coherent Systems. (arXiv:2103.03832v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03832</id>
        <link href="http://arxiv.org/abs/2103.03832"/>
        <updated>2021-06-24T01:51:44.591Z</updated>
        <summary type="html"><![CDATA[We investigate the complexity and performance of recurrent neural network
(RNN) models as post-processing units for the compensation of fibre
nonlinearities in digital coherent systems carrying polarization multiplexed
16-QAM and 32-QAM signals. We evaluate three bi-directional RNN models, namely
the bi-LSTM, bi-GRU and bi-Vanilla-RNN and show that all of them are promising
nonlinearity compensators especially in dispersion unmanaged systems. Our
simulations show that during inference the three models provide similar
compensation performance, therefore in real-life systems the simplest scheme
based on Vanilla-RNN units should be preferred. We compare bi-Vanilla-RNN with
Volterra nonlinear equalizers and exhibit its superiority both in terms of
performance and complexity, thus highlighting that RNN processing is a very
promising pathway for the upgrade of long-haul optical communication systems
utilizing coherent detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deligiannidis_S/0/1/0/all/0/1"&gt;Stavros Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mesaritakis_C/0/1/0/all/0/1"&gt;Charis Mesaritakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bogris_A/0/1/0/all/0/1"&gt;Adonis Bogris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02869</id>
        <link href="http://arxiv.org/abs/2104.02869"/>
        <updated>2021-06-24T01:51:44.578Z</updated>
        <summary type="html"><![CDATA[Visual explanation methods have an important role in the prognosis of the
patients where the annotated data is limited or unavailable. There have been
several attempts to use gradient-based attribution methods to localize
pathology from medical scans without using segmentation labels. This research
direction has been impeded by the lack of robustness and reliability. These
methods are highly sensitive to the network parameters. In this study, we
introduce a robust visual explanation method to address this problem for
medical applications. We provide an innovative visual explanation algorithm for
general purpose and as an example application, we demonstrate its effectiveness
for quantifying lesions in the lungs caused by the Covid-19 with high accuracy
and robustness without using dense segmentation labels. This approach overcomes
the drawbacks of commonly used Grad-CAM and its extended versions. The premise
behind our proposed strategy is that the information flow is minimized while
ensuring the classifier prediction stays similar. Our findings indicate that
the bottleneck condition provides a more stable severity estimation than the
similar attribution methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ugur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1"&gt;Ismail Irmakci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1"&gt;Elif Keles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1"&gt;Ahmet Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1"&gt;Concetto Spampinato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1"&gt;Sachin Jambawalikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1"&gt;Evrim Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1"&gt;Ulas Bagci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Universal Adversarial Attacks: A Few Bad Actors Ruin Graph Learning Models. (arXiv:2002.04784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04784</id>
        <link href="http://arxiv.org/abs/2002.04784"/>
        <updated>2021-06-24T01:51:44.572Z</updated>
        <summary type="html"><![CDATA[Deep neural networks, while generalize well, are known to be sensitive to
small adversarial perturbations. This phenomenon poses severe security threat
and calls for in-depth investigation of the robustness of deep learning models.
With the emergence of neural networks for graph structured data, similar
investigations are urged to understand their robustness. It has been found that
adversarially perturbing the graph structure and/or node features may result in
a significant degradation of the model performance. In this work, we show from
a different angle that such fragility similarly occurs if the graph contains a
few bad-actor nodes, which compromise a trained graph neural network through
flipping the connections to any targeted victim. Worse, the bad actors found
for one graph model severely compromise other models as well. We call the bad
actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them.
Thorough empirical investigations suggest an interesting finding that the
anchor nodes often belong to the same class; and they also corroborate the
intuitive trade-off between the number of anchor nodes and the attack success
rate. For the dataset Cora which contains 2708 nodes, as few as six anchor
nodes will result in an attack success rate higher than 80\% for GCN and other
three models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiao Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Optimization Kernel: Towards Robust Deep Learning. (arXiv:2106.06097v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06097</id>
        <link href="http://arxiv.org/abs/2106.06097"/>
        <updated>2021-06-24T01:51:44.565Z</updated>
        <summary type="html"><![CDATA[Recent studies show a close connection between neural networks (NN) and
kernel methods. However, most of these analyses (e.g., NTK) focus on the
influence of (infinite) width instead of the depth of NN models. There remains
a gap between theory and practical network designs that benefit from the depth.
This paper first proposes a novel kernel family named Neural Optimization
Kernel (NOK). Our kernel is defined as the inner product between two $T$-step
updated functionals in RKHS w.r.t. a regularized optimization problem.
Theoretically, we proved the monotonic descent property of our update rule for
both convex and non-convex problems, and a $O(1/T)$ convergence rate of our
updates for convex problems. Moreover, we propose a data-dependent structured
approximation of our NOK, which builds the connection between training deep NNs
and kernel methods associated with NOK. The resultant computational graph is a
ResNet-type finite width NN. Our structured approximation preserved the
monotonic descent property and $O(1/T)$ convergence rate. Namely, a $T$-layer
NN performs $T$-step monotonic descent updates. Notably, we show our
$T$-layered structured NN with ReLU maintains a $O(1/T)$ convergence rate
w.r.t. a convex regularized problem, which explains the success of ReLU on
training deep NN from a NN architecture optimization perspective. For the
unsupervised learning and the shared parameter case, we show the equivalence of
training structured NN with GD and performing functional gradient descent in
RKHS associated with a fixed (data-dependent) NOK at an infinity-width regime.
For finite NOKs, we prove generalization bounds. Remarkably, we show that
overparameterized deep NN (NOK) can increase the expressive power to reduce
empirical risk and reduce the generalization bound at the same time. Extensive
experiments verify the robustness of our structured NOK blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lyu_Y/0/1/0/all/0/1"&gt;Yueming Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Information Obfuscation for Split Inference of Neural Networks. (arXiv:2104.11413v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11413</id>
        <link href="http://arxiv.org/abs/2104.11413"/>
        <updated>2021-06-24T01:51:44.521Z</updated>
        <summary type="html"><![CDATA[Splitting network computations between the edge device and a server enables
low edge-compute inference of neural networks but might expose sensitive
information about the test query to the server. To address this problem,
existing techniques train the model to minimize information leakage for a given
set of sensitive attributes. In practice, however, the test queries might
contain attributes that are not foreseen during training. We propose instead an
unsupervised obfuscation method to discard the information irrelevant to the
main task. We formulate the problem via an information theoretical framework
and derive an analytical solution for a given distortion to the model output.
In our method, the edge device runs the model up to a split layer determined
based on its computational capacity. It then obfuscates the obtained feature
vector based on the first layer of the server model by removing the components
in the null space as well as the low-energy components of the remaining signal.
Our experimental results show that our method outperforms existing techniques
in removing the information of the irrelevant attributes and maintaining the
accuracy on the target label. We also show that our method reduces the
communication cost and incurs only a small computational overhead.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1"&gt;Mohammad Samragh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1"&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triastcyn_A/0/1/0/all/0/1"&gt;Aleksei Triastcyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azarian_K/0/1/0/all/0/1"&gt;Kambiz Azarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1"&gt;Joseph Soriaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1"&gt;Farinaz Koushanfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep ReLU Networks Preserve Expected Length. (arXiv:2102.10492v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10492</id>
        <link href="http://arxiv.org/abs/2102.10492"/>
        <updated>2021-06-24T01:51:44.505Z</updated>
        <summary type="html"><![CDATA[Assessing the complexity of functions computed by a neural network helps us
understand how the network will learn and generalize. One natural measure of
complexity is how the network distorts length - if the network takes a
unit-length curve as input, what is the length of the resulting curve of
outputs? It has been widely believed that this length grows exponentially in
network depth. We prove that in fact this is not the case: the expected length
distortion does not grow with depth, and indeed shrinks slightly, for ReLU
networks with standard random initialization. We also generalize this result by
proving upper bounds both for higher moments of the length distortion and for
the distortion of higher-dimensional volumes. These theoretical results are
corroborated by our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jeong_R/0/1/0/all/0/1"&gt;Ryan Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rolnick_D/0/1/0/all/0/1"&gt;David Rolnick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09123</id>
        <link href="http://arxiv.org/abs/2104.09123"/>
        <updated>2021-06-24T01:51:44.491Z</updated>
        <summary type="html"><![CDATA[While common image object detection tasks focus on bounding boxes or
segmentation masks as object representations, we consider the problem of
finding objects based on four arbitrary vertices. We propose a novel model,
named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet
and uses similar algorithms and ideas. It is designated for applications
requiring high-accuracy detection of regularly shaped objects, which is the
case in the logistics use-case of packaging structure recognition. We evaluate
our model on our specific real-world dataset for this use-case. Baselined
against a previous solution, consisting of a Mask R-CNN model and suitable
post-processing steps, TetraPackNet achieves superior results (9% higher in
accuracy) in the sub-task of four-corner based transport unit side detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1"&gt;Laura D&amp;#xf6;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1"&gt;Felix Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1"&gt;Alexander Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1"&gt;Martin Pouls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning in weakly nonlinear systems: A Case study on Significant wave heights. (arXiv:2105.08583v2 [physics.ao-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08583</id>
        <link href="http://arxiv.org/abs/2105.08583"/>
        <updated>2021-06-24T01:51:44.477Z</updated>
        <summary type="html"><![CDATA[This paper proposes a machine learning method based on the Extra Trees (ET)
algorithm for forecasting Significant Wave Heights in oceanic waters. To derive
multiple features from the CDIP buoys, which make point measurements, we first
nowcast various parameters and then forecast them at 30-min intervals. The
proposed algorithm has Scatter Index (SI), Bias, Correlation Coefficient, Root
Mean Squared Error (RMSE) of 0.130, -0.002, 0.97, and 0.14, respectively, for
one day ahead prediction and 0.110, -0.001, 0.98, and 0.122, respectively, for
14-day ahead prediction on the testing dataset. While other state-of-the-art
methods can only forecast up to 120 hours ahead, we extend it further to 14
days. Our proposed setup includes spectral features, hv-block cross-validation,
and stringent QC criteria. The proposed algorithm performs significantly better
than the state-of-the-art methods commonly used for significant wave height
forecasting for one-day ahead prediction. Moreover, the improved performance of
the proposed machine learning method compared to the numerical methods shows
that this performance can be extended to even longer periods allowing for early
prediction of significant wave heights in oceanic waters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Pokhrel_P/0/1/0/all/0/1"&gt;Pujan Pokhrel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoque_M/0/1/0/all/0/1"&gt;Md Tamjidul Hoque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Abdelguerfi_M/0/1/0/all/0/1"&gt;Mahdi Abdelguerfi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perceiver: General Perception with Iterative Attention. (arXiv:2103.03206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03206</id>
        <link href="http://arxiv.org/abs/2103.03206"/>
        <updated>2021-06-24T01:51:44.471Z</updated>
        <summary type="html"><![CDATA[Biological systems perceive the world by simultaneously processing
high-dimensional inputs from modalities as diverse as vision, audition, touch,
proprioception, etc. The perception models used in deep learning on the other
hand are designed for individual modalities, often relying on domain-specific
assumptions such as the local grid structures exploited by virtually all
existing vision models. These priors introduce helpful inductive biases, but
also lock models to individual modalities. In this paper we introduce the
Perceiver - a model that builds upon Transformers and hence makes few
architectural assumptions about the relationship between its inputs, but that
also scales to hundreds of thousands of inputs, like ConvNets. The model
leverages an asymmetric attention mechanism to iteratively distill inputs into
a tight latent bottleneck, allowing it to scale to handle very large inputs. We
show that this architecture is competitive with or outperforms strong,
specialized models on classification tasks across various modalities: images,
point clouds, audio, video, and video+audio. The Perceiver obtains performance
comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly
attending to 50,000 pixels. It is also competitive in all modalities in
AudioSet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1"&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimeno_F/0/1/0/all/0/1"&gt;Felix Gimeno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brock_A/0/1/0/all/0/1"&gt;Andrew Brock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Joao Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Query Release Through Adaptive Projection. (arXiv:2103.06641v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06641</id>
        <link href="http://arxiv.org/abs/2103.06641"/>
        <updated>2021-06-24T01:51:44.466Z</updated>
        <summary type="html"><![CDATA[We propose, implement, and evaluate a new algorithm for releasing answers to
very large numbers of statistical queries like $k$-way marginals, subject to
differential privacy. Our algorithm makes adaptive use of a continuous
relaxation of the Projection Mechanism, which answers queries on the private
dataset using simple perturbation, and then attempts to find the synthetic
dataset that most closely matches the noisy answers. We use a continuous
relaxation of the synthetic dataset domain which makes the projection loss
differentiable, and allows us to use efficient ML optimization techniques and
tooling. Rather than answering all queries up front, we make judicious use of
our privacy budget by iteratively and adaptively finding queries for which our
(relaxed) synthetic data has high error, and then repeating the projection. We
perform extensive experimental evaluations across a range of parameters and
datasets, and find that our method outperforms existing algorithms in many
cases, especially when the privacy budget is small or the query class is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aydore_S/0/1/0/all/0/1"&gt;Sergul Aydore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_W/0/1/0/all/0/1"&gt;William Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1"&gt;Michael Kearns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melis_L/0/1/0/all/0/1"&gt;Luca Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1"&gt;Aaron Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siva_A/0/1/0/all/0/1"&gt;Ankit Siva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an efficient approach for the nonconvex $\ell_p$-ball projection: algorithm and analysis. (arXiv:2101.01350v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01350</id>
        <link href="http://arxiv.org/abs/2101.01350"/>
        <updated>2021-06-24T01:51:44.448Z</updated>
        <summary type="html"><![CDATA[This paper primarily focuses on computing the Euclidean projection of a
vector onto the $\ell_{p}$ ball in which $p\in(0,1)$. Such a problem emerges as
the core building block in statistical machine learning and signal processing
tasks because of its ability to promote sparsity. However, efficient numerical
algorithms for finding the projections are still not available, particularly in
large-scale optimization. To meet this challenge, we first derive the
first-order necessary optimality conditions of this problem using Fr\'echet
normal cone. Based on this characterization, we develop a novel numerical
approach for computing the stationary point through solving a sequence of
projections onto the reweighted $\ell_{1}$-balls. This method is practically
simple to implement and computationally efficient. Moreover, the proposed
algorithm is shown to converge uniquely under mild conditions and has a
worst-case $O(1/\sqrt{k})$ convergence rate. Numerical experiments demonstrate
the efficiency of our proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiangyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiashan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Model Signal-Awareness via Prediction-Preserving Input Minimization. (arXiv:2011.14934v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14934</id>
        <link href="http://arxiv.org/abs/2011.14934"/>
        <updated>2021-06-24T01:51:44.442Z</updated>
        <summary type="html"><![CDATA[This work explores the signal awareness of AI models for source code
understanding. Using a software vulnerability detection use case, we evaluate
the models' ability to capture the correct vulnerability signals to produce
their predictions. Our prediction-preserving input minimization (P2IM) approach
systematically reduces the original source code to a minimal snippet which a
model needs to maintain its prediction. The model's reliance on incorrect
signals is then uncovered when the vulnerability in the original code is
missing in the minimal snippet, both of which the model however predicts as
being vulnerable. We measure the signal awareness of models using a new metric
we propose- Signal-aware Recall (SAR). We apply P2IM on three different neural
network architectures across multiple datasets. The results show a sharp drop
in the model's Recall from the high 90s to sub-60s with the new metric,
highlighting that the models are presumably picking up a lot of noise or
dataset nuances while learning their vulnerability detection logic. Although
the drop in model performance may be perceived as an adversarial attack, but
this isn't P2IM's objective. The idea is rather to uncover the signal-awareness
of a black-box model in a data-driven manner via controlled queries. SAR's
purpose is to measure the impact of task-agnostic model training, and not to
suggest a shortcoming in the Recall metric. The expectation, in fact, is for
SAR to match Recall in the ideal scenario where the model truly captures
task-specific signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suneja_S/0/1/0/all/0/1"&gt;Sahil Suneja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yunhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yufan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laredo_J/0/1/0/all/0/1"&gt;Jim Laredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morari_A/0/1/0/all/0/1"&gt;Alessandro Morari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrating the Lee-Carter and the Poisson Lee-Carter models via Neural Networks. (arXiv:2106.12312v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12312</id>
        <link href="http://arxiv.org/abs/2106.12312"/>
        <updated>2021-06-24T01:51:44.436Z</updated>
        <summary type="html"><![CDATA[This paper introduces a neural network approach for fitting the Lee-Carter
and the Poisson Lee-Carter model on multiple populations. We develop some
neural networks that replicate the structure of the individual LC models and
allow their joint fitting by analysing the mortality data of all the considered
populations simultaneously. The neural network architecture is specifically
designed to calibrate each individual model using all available information
instead of using a population-specific subset of data as in the traditional
estimation schemes. A large set of numerical experiments performed on all the
countries of the Human Mortality Database (HMD) shows the effectiveness of our
approach. In particular, the resulting parameter estimates appear smooth and
less sensitive to the random fluctuations often present in the mortality rates'
data, especially for low-population countries. In addition, the forecasting
performance results significantly improved as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scognamiglio_S/0/1/0/all/0/1"&gt;Salvatore Scognamiglio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-hoc Uncertainty Calibration for Domain Drift Scenarios. (arXiv:2012.10988v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10988</id>
        <link href="http://arxiv.org/abs/2012.10988"/>
        <updated>2021-06-24T01:51:44.429Z</updated>
        <summary type="html"><![CDATA[We address the problem of uncertainty calibration. While standard deep neural
networks typically yield uncalibrated predictions, calibrated confidence scores
that are representative of the true likelihood of a prediction can be achieved
using post-hoc calibration methods. However, to date the focus of these
approaches has been on in-domain calibration. Our contribution is two-fold.
First, we show that existing post-hoc calibration methods yield highly
over-confident predictions under domain shift. Second, we introduce a simple
strategy where perturbations are applied to samples in the validation set
before performing the post-hoc calibration step. In extensive experiments, we
demonstrate that this perturbation step results in substantially better
calibration under domain shift on a wide range of architectures and modelling
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomani_C/0/1/0/all/0/1"&gt;Christian Tomani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruber_S/0/1/0/all/0/1"&gt;Sebastian Gruber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_M/0/1/0/all/0/1"&gt;Muhammed Ebrar Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buettner_F/0/1/0/all/0/1"&gt;Florian Buettner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret-optimal measurement-feedback control. (arXiv:2011.12785v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12785</id>
        <link href="http://arxiv.org/abs/2011.12785"/>
        <updated>2021-06-24T01:51:44.423Z</updated>
        <summary type="html"><![CDATA[We consider measurement-feedback control in linear dynamical systems from the
perspective of regret minimization. Unlike most prior work in this area, we
focus on the problem of designing an online controller which competes with the
optimal dynamic sequence of control actions selected in hindsight, instead of
the best controller in some specific class of controllers. This formulation of
regret is attractive when the environment changes over time and no single
controller achieves good performance over the entire time horizon. We show that
in the measurement-feedback setting, unlike in the full-information setting,
there is no single offline controller which outperforms every other offline
controller on every disturbance, and propose a new $H_2$-optimal offline
controller as a benchmark for the online controller to compete against. We show
that the corresponding regret-optimal online controller can be found via a
novel reduction to the classical Nehari problem from robust control and present
a tight data-dependent bound on its regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Goel_G/0/1/0/all/0/1"&gt;Gautam Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Model Adaptation Using Domain Agnostic Internal Distributions. (arXiv:2007.00197v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00197</id>
        <link href="http://arxiv.org/abs/2007.00197"/>
        <updated>2021-06-24T01:51:44.407Z</updated>
        <summary type="html"><![CDATA[We develop an algorithm for sequential adaptation of a classifier that is
trained for a source domain to generalize in an unannotated target domain. We
consider that the model has been trained on the source domain annotated data
and then it needs to be adapted using the target domain unannotated data when
the source domain data is not accessible. We align the distributions of the
source and the target domains in a discriminative embedding space via an
intermediate internal distribution. This distribution is estimated using the
source data representations in the embedding. We conduct experiments on four
benchmarks to demonstrate the method is effective and compares favorably
against existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1"&gt;Aram Galstyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-24T01:51:44.402Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Neural Radiance Caching for Path Tracing. (arXiv:2106.12372v1 [cs.GR])]]></title>
        <id>http://arxiv.org/abs/2106.12372</id>
        <link href="http://arxiv.org/abs/2106.12372"/>
        <updated>2021-06-24T01:51:44.397Z</updated>
        <summary type="html"><![CDATA[We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1"&gt;Thomas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rousselle_F/0/1/0/all/0/1"&gt;Fabrice Rousselle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novak_J/0/1/0/all/0/1"&gt;Jan Nov&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1"&gt;Alexander Keller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Posterior Meta-Replay for Continual Learning. (arXiv:2103.01133v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01133</id>
        <link href="http://arxiv.org/abs/2103.01133"/>
        <updated>2021-06-24T01:51:44.391Z</updated>
        <summary type="html"><![CDATA[Learning a sequence of tasks without access to i.i.d. observations is a
widely studied form of continual learning (CL) that remains challenging. In
principle, Bayesian learning directly applies to this setting, since recursive
and one-off Bayesian updates yield the same result. In practice, however,
recursive updating often leads to poor trade-off solutions across tasks because
approximate inference is necessary for most models of interest. Here, we
describe an alternative Bayesian approach where task-conditioned parameter
distributions are continually inferred from data. We offer a practical deep
learning implementation of our framework based on probabilistic
task-conditioned hypernetworks, an approach we term "posterior meta-replay".
Experiments on standard benchmarks show that our probabilistic hypernetworks
compress sequences of posterior parameter distributions with virtually no
forgetting. We obtain considerable performance gains compared to existing
Bayesian CL methods, and identify task inference as our major limiting factor.
This limitation has several causes that are independent of the considered
sequential setting, opening up new avenues for progress in CL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henning_C/0/1/0/all/0/1"&gt;Christian Henning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cervera_M/0/1/0/all/0/1"&gt;Maria R. Cervera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oswald_J/0/1/0/all/0/1"&gt;Johannes von Oswald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Traber_R/0/1/0/all/0/1"&gt;Regina Traber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ehret_B/0/1/0/all/0/1"&gt;Benjamin Ehret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1"&gt;Seijin Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacramento_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Sacramento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1"&gt;Benjamin F. Grewe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11561</id>
        <link href="http://arxiv.org/abs/2003.11561"/>
        <updated>2021-06-24T01:51:44.385Z</updated>
        <summary type="html"><![CDATA[The objective of this paper is to develop predictive models to classify
Brazilian legal proceedings in three possible classes of status: (i) archived
proceedings, (ii) active proceedings, and (iii) suspended proceedings. This
problem's resolution is intended to assist public and private institutions in
managing large portfolios of legal proceedings, providing gains in scale and
efficiency. In this paper, legal proceedings are made up of sequences of short
texts called "motions." We combined several natural language processing (NLP)
and machine learning techniques to solve the problem. Although working with
Portuguese NLP, which can be challenging due to lack of resources, our
approaches performed remarkably well in the classification task, achieving
maximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93
(weighted). Furthermore, we could extract and interpret the patterns learned by
one of our models besides quantifying how those patterns relate to the
classification task. The interpretability step is important among machine
learning legal applications and gives us an exciting insight into how black-box
models make decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1"&gt;Itamar Ciochetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1"&gt;Emerson Bertolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Cal: Well-controlled Post-hoc Calibration by Ranking. (arXiv:2105.04290v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04290</id>
        <link href="http://arxiv.org/abs/2105.04290"/>
        <updated>2021-06-24T01:51:44.369Z</updated>
        <summary type="html"><![CDATA[In many applications, it is desirable that a classifier not only makes
accurate predictions, but also outputs calibrated posterior probabilities.
However, many existing classifiers, especially deep neural network classifiers,
tend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a
model by learning a calibration map. Existing approaches mostly focus on
constructing calibration maps with low calibration errors, however, this
quality is inadequate for a calibrator being useful. In this paper, we
introduce two constraints that are worth consideration in designing a
calibration map for post-hoc calibration. Then we present Meta-Cal, which is
built from a base calibrator and a ranking model. Under some mild assumptions,
two high-probability bounds are given with respect to these constraints.
Empirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular
network architectures show our proposed method significantly outperforms the
current state of the art for post-hoc multi-class classification calibration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingchen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blaschko_M/0/1/0/all/0/1"&gt;Matthew B. Blaschko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MG-DVD: A Real-time Framework for Malware Variant Detection Based on Dynamic Heterogeneous Graph Learning. (arXiv:2106.12288v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12288</id>
        <link href="http://arxiv.org/abs/2106.12288"/>
        <updated>2021-06-24T01:51:44.363Z</updated>
        <summary type="html"><![CDATA[Detecting the newly emerging malware variants in real time is crucial for
mitigating cyber risks and proactively blocking intrusions. In this paper, we
propose MG-DVD, a novel detection framework based on dynamic heterogeneous
graph learning, to detect malware variants in real time. Particularly, MG-DVD
first models the fine-grained execution event streams of malware variants into
dynamic heterogeneous graphs and investigates real-world meta-graphs between
malware objects, which can effectively characterize more discriminative
malicious evolutionary patterns between malware and their variants. Then,
MG-DVD presents two dynamic walk-based heterogeneous graph learning methods to
learn more comprehensive representations of malware variants, which
significantly reduces the cost of the entire graph retraining. As a result,
MG-DVD is equipped with the ability to detect malware variants in real time,
and it presents better interpretability by introducing meaningful meta-graphs.
Comprehensive experiments on large-scale samples prove that our proposed MG-DVD
outperforms state-of-the-art methods in detecting malware variants in terms of
effectiveness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_M/0/1/0/all/0/1"&gt;Ming Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu-Dong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12300</id>
        <link href="http://arxiv.org/abs/2106.12300"/>
        <updated>2021-06-24T01:51:44.358Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has become an active and promising distributed
machine learning paradigm. As a result of statistical heterogeneity, recent
studies clearly show that the performance of popular FL methods (e.g., FedAvg)
deteriorates dramatically due to the client drift caused by local updates. This
paper proposes a novel Federated Learning algorithm (called IGFL), which
leverages both Individual and Group behaviors to mimic distribution, thereby
improving the ability to deal with heterogeneity. Unlike existing FL methods,
our IGFL can be applied to both client and server optimization. As a
by-product, we propose a new attention-based federated learning in the server
optimization of IGFL. To the best of our knowledge, this is the first time to
incorporate attention mechanisms into federated optimization. We conduct
extensive experiments and show that IGFL can significantly improve the
performance of existing federated learning methods. Especially when the
distributions of data among individuals are diverse, IGFL can improve the
classification accuracy by about 13% compared with prior baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05748</id>
        <link href="http://arxiv.org/abs/2106.05748"/>
        <updated>2021-06-24T01:51:44.353Z</updated>
        <summary type="html"><![CDATA[Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chao Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1"&gt;Justin Dulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1"&gt;Gregory Rolwes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1"&gt;Duke Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1"&gt;Nadia Shakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stiff Neural Ordinary Differential Equations. (arXiv:2103.15341v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15341</id>
        <link href="http://arxiv.org/abs/2103.15341"/>
        <updated>2021-06-24T01:51:44.347Z</updated>
        <summary type="html"><![CDATA[Neural Ordinary Differential Equations (ODE) are a promising approach to
learn dynamic models from time-series data in science and engineering
applications. This work aims at learning Neural ODE for stiff systems, which
are usually raised from chemical kinetic modeling in chemical and biological
systems. We first show the challenges of learning neural ODE in the classical
stiff ODE systems of Robertson's problem and propose techniques to mitigate the
challenges associated with scale separations in stiff systems. We then present
successful demonstrations in stiff systems of Robertson's problem and an air
pollution problem. The demonstrations show that the usage of deep networks with
rectified activations, proper scaling of the network outputs as well as loss
functions, and stabilized gradient calculations are the key techniques enabling
the learning of stiff neural ODE. The success of learning stiff neural ODE
opens up possibilities of using neural ODEs in applications with widely varying
time-scales, like chemical dynamics in energy conversion, environmental
engineering, and the life sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kim_S/0/1/0/all/0/1"&gt;Suyong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ji_W/0/1/0/all/0/1"&gt;Weiqi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Deng_S/0/1/0/all/0/1"&gt;Sili Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yingbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rackauckas_C/0/1/0/all/0/1"&gt;Christopher Rackauckas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays. (arXiv:2106.12261v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12261</id>
        <link href="http://arxiv.org/abs/2106.12261"/>
        <updated>2021-06-24T01:51:44.341Z</updated>
        <summary type="html"><![CDATA[We consider stochastic convex optimization problems, where several machines
act asynchronously in parallel while sharing a common memory. We propose a
robust training method for the constrained setting and derive non asymptotic
convergence guarantees that do not depend on prior knowledge of update delays,
objective smoothness, and gradient variance. Conversely, existing methods for
this setting crucially rely on this prior knowledge, which render them
unsuitable for essentially all shared-resources computational environments,
such as clouds and data centers. Concretely, existing approaches are unable to
accommodate changes in the delays which result from dynamic allocation of the
machines, while our method implicitly adapts to such changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aviv_R/0/1/0/all/0/1"&gt;Rotem Zamir Aviv&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hakimi_I/0/1/0/all/0/1"&gt;Ido Hakimi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_A/0/1/0/all/0/1"&gt;Assaf Schuster&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1"&gt;Kfir Y. Levy&lt;/a&gt; (1 and 3) ((1) Department of Electrical and Computer Engineering, Technion, (2) Department of Computer Science, Technion, (3) A Viterbi Fellow)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks. (arXiv:2102.06385v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06385</id>
        <link href="http://arxiv.org/abs/2102.06385"/>
        <updated>2021-06-24T01:51:44.326Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the bandits with knapsacks (BwK) problem and develop
a primal-dual based algorithm that achieves a problem-dependent logarithmic
regret bound. The BwK problem extends the multi-arm bandit (MAB) problem to
model the resource consumption associated with playing each arm, and the
existing BwK literature has been mainly focused on deriving asymptotically
optimal distribution-free regret bounds. We first study the primal and dual
linear programs underlying the BwK problem. From this primal-dual perspective,
we discover symmetry between arms and knapsacks, and then propose a new notion
of sub-optimality measure for the BwK problem. The sub-optimality measure
highlights the important role of knapsacks in determining algorithm regret and
inspires the design of our two-phase algorithm. In the first phase, the
algorithm identifies the optimal arms and the binding knapsacks, and in the
second phase, it exhausts the binding knapsacks via playing the optimal arms
through an adaptive procedure. Our regret upper bound involves the proposed
sub-optimality measure and it has a logarithmic dependence on length of horizon
$T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the
number of knapsacks). To the best of our knowledge, this is the first
problem-dependent logarithmic regret bound for solving the general BwK problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaocheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chunlin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinyu Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Hoeffding- and Bernstein-type Concentration Inequalities. (arXiv:2102.06304v4 [math.PR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06304</id>
        <link href="http://arxiv.org/abs/2102.06304"/>
        <updated>2021-06-24T01:51:44.319Z</updated>
        <summary type="html"><![CDATA[We prove concentration inequalities for functions of independent random
variables {under} sub-gaussian and sub-exponential conditions. The utility of
the inequalities is demonstrated by an extension of the now classical method of
Rademacher complexities to Lipschitz function classes and unbounded
sub-exponential distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Andreas Maurer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Divergences of Variational Inference. (arXiv:2007.02912v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02912</id>
        <link href="http://arxiv.org/abs/2007.02912"/>
        <updated>2021-06-24T01:51:44.314Z</updated>
        <summary type="html"><![CDATA[Variational inference (VI) plays an essential role in approximate Bayesian
inference due to its computational efficiency and broad applicability. Crucial
to the performance of VI is the selection of the associated divergence measure,
as VI approximates the intractable distribution by minimizing this divergence.
In this paper we propose a meta-learning algorithm to learn the divergence
metric suited for the task of interest, automating the design of VI methods. In
addition, we learn the initialization of the variational parameters without
additional cost when our method is deployed in the few-shot learning scenarios.
We demonstrate our approach outperforms standard VI on Gaussian mixture
distribution approximation, Bayesian neural network regression, image
generation with variational autoencoders and recommender systems with a partial
variational autoencoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devlin_S/0/1/0/all/0/1"&gt;Sam Devlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cheng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals. (arXiv:2103.11972v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11972</id>
        <link href="http://arxiv.org/abs/2103.11972"/>
        <updated>2021-06-24T01:51:44.308Z</updated>
        <summary type="html"><![CDATA[There has been a recent resurgence of interest in explainable artificial
intelligence (XAI) that aims to reduce the opaqueness of AI-based
decision-making systems, allowing humans to scrutinize and trust them. Prior
work in this context has focused on the attribution of responsibility for an
algorithm's decisions to its inputs wherein responsibility is typically
approached as a purely associational concept. In this paper, we propose a
principled causality-based approach for explaining black-box decision-making
systems that addresses limitations of existing methods in XAI. At the core of
our framework lies probabilistic contrastive counterfactuals, a concept that
can be traced back to philosophical, cognitive, and social foundations of
theories on how humans generate and select explanations. We show how such
counterfactuals can quantify the direct and indirect influences of a variable
on decisions made by an algorithm, and provide actionable recourse for
individuals negatively affected by the algorithm's decision. Unlike prior work,
our system, LEWIS: (1)can compute provably effective explanations and recourse
at local, global and contextual levels (2)is designed to work with users with
varying levels of background knowledge of the underlying causal model and
(3)makes no assumptions about the internals of an algorithmic system except for
the availability of its input-output data. We empirically evaluate LEWIS on
three real-world datasets and show that it generates human-understandable
explanations that improve upon state-of-the-art approaches in XAI, including
the popular LIME and SHAP. Experiments on synthetic data further demonstrate
the correctness of LEWIS's explanations and the scalability of its recourse
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galhotra_S/0/1/0/all/0/1"&gt;Sainyam Galhotra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradhan_R/0/1/0/all/0/1"&gt;Romila Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_B/0/1/0/all/0/1"&gt;Babak Salimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Acyclicity Reasoning for Bayesian Network Structure Learning with Constraint Programming. (arXiv:2106.12269v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.12269</id>
        <link href="http://arxiv.org/abs/2106.12269"/>
        <updated>2021-06-24T01:51:44.302Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are probabilistic graphical models with a wide range of
application areas including gene regulatory networks inference, risk analysis
and image processing. Learning the structure of a Bayesian network (BNSL) from
discrete data is known to be an NP-hard task with a superexponential search
space of directed acyclic graphs. In this work, we propose a new polynomial
time algorithm for discovering a subset of all possible cluster cuts, a greedy
algorithm for approximately solving the resulting linear program, and a
generalised arc consistency algorithm for the acyclicity constraint. We embed
these in the constraint programmingbased branch-and-bound solver CPBayes and
show that, despite being suboptimal, they improve performance by orders of
magnitude. The resulting solver also compares favourably with GOBNILP, a
state-of-the-art solver for the BNSL problem which solves an NP-hard problem to
discover each cut and solves the linear program exactly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trosser_F/0/1/0/all/0/1"&gt;Fulya Tr&amp;#xf6;sser&lt;/a&gt; (MIAT INRA), &lt;a href="http://arxiv.org/find/cs/1/au:+Givry_S/0/1/0/all/0/1"&gt;Simon de Givry&lt;/a&gt; (MIAT INRA), &lt;a href="http://arxiv.org/find/cs/1/au:+Katsirelos_G/0/1/0/all/0/1"&gt;George Katsirelos&lt;/a&gt; (MIA-Paris)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tilting the playing field: Dynamical loss functions for machine learning. (arXiv:2102.03793v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03793</id>
        <link href="http://arxiv.org/abs/2102.03793"/>
        <updated>2021-06-24T01:51:44.286Z</updated>
        <summary type="html"><![CDATA[We show that learning can be improved by using loss functions that evolve
cyclically during training to emphasize one class at a time. In
underparameterized networks, such dynamical loss functions can lead to
successful training for networks that fail to find a deep minima of the
standard cross-entropy loss. In overparameterized networks, dynamical loss
functions can lead to better generalization. Improvement arises from the
interplay of the changing loss landscape with the dynamics of the system as it
evolves to minimize the loss. In particular, as the loss function oscillates,
instabilities develop in the form of bifurcation cascades, which we study using
the Hessian and Neural Tangent Kernel. Valleys in the landscape widen and
deepen, and then narrow and rise as the loss landscape changes during a cycle.
As the landscape narrows, the learning rate becomes too large and the network
becomes unstable and bounces around the valley. This process ultimately pushes
the system into deeper and wider regions of the loss landscape and is
characterized by decreasing eigenvalues of the Hessian. This results in better
regularized models with improved generalization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1"&gt;Miguel Ruiz-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Ge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenholz_S/0/1/0/all/0/1"&gt;Samuel S. Schoenholz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andrea J. Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07991</id>
        <link href="http://arxiv.org/abs/2006.07991"/>
        <updated>2021-06-24T01:51:44.281Z</updated>
        <summary type="html"><![CDATA[The goal of this work is to characterize the representational impact that
foveation operations have for machine vision systems, inspired by the foveated
human visual system, which has higher acuity at the center of gaze and
texture-like encoding in the periphery. To do so, we introduce models
consisting of a first-stage \textit{fixed} image transform followed by a
second-stage \textit{learnable} convolutional neural network, and we varied the
first stage component. The primary model has a foveated-textural input stage,
which we compare to a model with foveated-blurred input and a model with
spatially-uniform blurred input (both matched for perceptual compression), and
a final reference model with minimal input-based compression. We find that: 1)
the foveated-texture model shows similar scene classification accuracy as the
reference model despite its compressed input, with greater i.i.d.
generalization than the other models; 2) the foveated-texture model has greater
sensitivity to high-spatial frequency information and greater robustness to
occlusion, w.r.t the comparison models; 3) both the foveated systems, show a
stronger center image-bias relative to the spatially-uniform systems even with
a weight sharing constraint. Critically, these results are preserved over
different classical CNN architectures throughout their learning dynamics.
Altogether, this suggests that foveation with peripheral texture-based
computations yields an efficient, distinct, and robust representational format
of scene information, and provides symbiotic computational insight into the
representational consequences that texture-based peripheral encoding may have
for processing in the human visual system, while also potentially inspiring the
next generation of computer vision models via spatially-adaptive computation.
Code + Data available here: https://github.com/ArturoDeza/EmergentProperties]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1"&gt;Arturo Deza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1"&gt;Talia Konkle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Health and Wellbeing for Shift Workers Using Job-role Based Deep Neural Network. (arXiv:2106.12081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12081</id>
        <link href="http://arxiv.org/abs/2106.12081"/>
        <updated>2021-06-24T01:51:44.275Z</updated>
        <summary type="html"><![CDATA[Shift workers who are essential contributors to our society, face high risks
of poor health and wellbeing. To help with their problems, we collected and
analyzed physiological and behavioral wearable sensor data from shift working
nurses and doctors, as well as their behavioral questionnaire data and their
self-reported daily health and wellbeing labels, including alertness,
happiness, energy, health, and stress. We found the similarities and
differences between the responses of nurses and doctors. According to the
differences in self-reported health and wellbeing labels between nurses and
doctors, and the correlations among their labels, we proposed a job-role based
multitask and multilabel deep learning model, where we modeled physiological
and behavioral data for nurses and doctors simultaneously to predict
participants' next day's multidimensional self-reported health and wellbeing
status. Our model showed significantly better performances than baseline models
and previous state-of-the-art models in the evaluations of binary/3-class
classification and regression prediction tasks. We also found features related
to heart rate, sleep, and work shift contributed to shift workers' health and
wellbeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_A/0/1/0/all/0/1"&gt;Asami Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1"&gt;Ryota Sakamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimaoka_M/0/1/0/all/0/1"&gt;Motomu Shimaoka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Representational Power of Graph Autoencoder. (arXiv:2106.12005v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12005</id>
        <link href="http://arxiv.org/abs/2106.12005"/>
        <updated>2021-06-24T01:51:44.270Z</updated>
        <summary type="html"><![CDATA[While representation learning has yielded a great success on many graph
learning tasks, there is little understanding behind the structures that are
being captured by these embeddings. For example, we wonder if the topological
features, such as the Triangle Count, the Degree of the node and other
centrality measures are concretely encoded in the embeddings. Furthermore, we
ask if the presence of these structures in the embeddings is necessary for a
better performance on the downstream tasks, such as clustering and
classification. To address these questions, we conduct an extensive empirical
study over three classes of unsupervised graph embedding models and seven
different variants of Graph Autoencoders. Our results show that five
topological features: the Degree, the Local Clustering Score, the Betweenness
Centrality, the Eigenvector Centrality, and Triangle Count are concretely
preserved in the first layer of the graph autoencoder that employs the SUM
aggregation rule, under the condition that the model preserves the second-order
proximity. We supplement further evidence for the presence of these features by
revealing a hierarchy in the distribution of the topological features in the
embeddings of the aforementioned model. We also show that a model with such
properties can outperform other models on certain downstream tasks, especially
when the preserved features are relevant to the task at hand. Finally, we
evaluate the suitability of our findings through a test case study related to
social influence prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_M/0/1/0/all/0/1"&gt;Maroun Haddad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouguessa_M/0/1/0/all/0/1"&gt;Mohamed Bouguessa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher Model Fingerprinting Attacks Against Transfer Learning. (arXiv:2106.12478v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12478</id>
        <link href="http://arxiv.org/abs/2106.12478"/>
        <updated>2021-06-24T01:51:44.264Z</updated>
        <summary type="html"><![CDATA[Transfer learning has become a common solution to address training data
scarcity in practice. It trains a specified student model by reusing or
fine-tuning early layers of a well-trained teacher model that is usually
publicly available. However, besides utility improvement, the transferred
public knowledge also brings potential threats to model confidentiality, and
even further raises other security and privacy issues.

In this paper, we present the first comprehensive investigation of the
teacher model exposure threat in the transfer learning context, aiming to gain
a deeper insight into the tension between public knowledge and model
confidentiality. To this end, we propose a teacher model fingerprinting attack
to infer the origin of a student model, i.e., the teacher model it transfers
from. Specifically, we propose a novel optimization-based method to carefully
generate queries to probe the student model to realize our attack. Unlike
existing model reverse engineering approaches, our proposed fingerprinting
method neither relies on fine-grained model outputs, e.g., posteriors, nor
auxiliary information of the model architecture or training dataset. We
systematically evaluate the effectiveness of our proposed attack. The empirical
results demonstrate that our attack can accurately identify the model origin
with few probing queries. Moreover, we show that the proposed attack can serve
as a stepping stone to facilitating other attacks against machine learning
models, such as model stealing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yufei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12511</id>
        <link href="http://arxiv.org/abs/2106.12511"/>
        <updated>2021-06-24T01:51:44.249Z</updated>
        <summary type="html"><![CDATA[Left ventricular hypertrophy (LVH) results from chronic remodeling caused by
a broad range of systemic and cardiovascular disease including hypertension,
aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early
detection and characterization of LVH can significantly impact patient care but
is limited by under-recognition of hypertrophy, measurement error and
variability, and difficulty differentiating etiologies of LVH. To overcome this
challenge, we present EchoNet-LVH - a deep learning workflow that automatically
quantifies ventricular hypertrophy with precision equal to human experts and
predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model
accurately measures intraventricular wall thickness (mean absolute error [MAE]
1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI
2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and
classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic
cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets
from independent domestic and international healthcare systems, EchoNet-LVH
accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)
and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy
(AUC 0.89) on the domestic external validation site. Leveraging measurements
across multiple heart beats, our model can more accurately identify subtle
changes in LV geometry and its causal etiologies. Compared to human experts,
EchoNet-LVH is fully automated, allowing for reproducible, precise
measurements, and lays the foundation for precision diagnosis of cardiac
hypertrophy. As a resource to promote further innovation, we also make publicly
available a large dataset of 23,212 annotated echocardiogram videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1"&gt;Grant Duffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Paul P Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Neal Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1"&gt;Bryan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1"&gt;Alan C. Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1"&gt;Matthew J. Shun-Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1"&gt;Kevin M. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1"&gt;Joseph Ebinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1"&gt;Florian Rader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David H. Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1"&gt;Ingela Schnittger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1"&gt;Euan A. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Y. Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jignesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1"&gt;Ronald Witteles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Susan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1"&gt;David Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LogME: Practical Assessment of Pre-trained Models for Transfer Learning. (arXiv:2102.11005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11005</id>
        <link href="http://arxiv.org/abs/2102.11005"/>
        <updated>2021-06-24T01:51:44.244Z</updated>
        <summary type="html"><![CDATA[This paper studies task adaptive pre-trained model selection, an
underexplored problem of assessing pre-trained models for the target task and
select best ones from the model zoo \emph{without fine-tuning}. A few pilot
works addressed the problem in transferring supervised pre-trained models to
classification tasks, but they cannot handle emerging unsupervised pre-trained
models or regression tasks. In pursuit of a practical assessment method, we
propose to estimate the maximum value of label evidence given features
extracted by pre-trained models. Unlike the maximum likelihood, the maximum
evidence is \emph{immune to over-fitting}, while its expensive computation can
be dramatically reduced by our carefully designed algorithm. The Logarithm of
Maximum Evidence (LogME) can be used to assess pre-trained models for transfer
learning: a pre-trained model with a high LogME value is likely to have good
transfer performance. LogME is \emph{fast, accurate, and general},
characterizing itself as the first practical method for assessing pre-trained
models. Compared with brute-force fine-tuning, LogME brings at most
$3000\times$ speedup in wall-clock time and requires only $1\%$ memory
footprint. It outperforms prior methods by a large margin in their setting and
is applicable to new settings. It is general enough for diverse pre-trained
models (supervised pre-trained and unsupervised pre-trained), downstream tasks
(classification and regression), and modalities (vision and language). Code is
available at this repository:
\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1"&gt;Kaichao You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise. (arXiv:2102.04297v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04297</id>
        <link href="http://arxiv.org/abs/2102.04297"/>
        <updated>2021-06-24T01:51:44.238Z</updated>
        <summary type="html"><![CDATA[The empirical success of deep learning is often attributed to SGD's
mysterious ability to avoid sharp local minima in the loss landscape, as sharp
minima are known to lead to poor generalization. Recently, empirical evidence
of heavy-tailed gradient noise was reported in many deep learning tasks, and it
was shown in \c{S}im\c{s}ekli (2019a,b) that SGD can escape sharp local minima
under the presence of such heavy-tailed gradient noise, providing a partial
solution to the mystery. In this work, we analyze a popular variant of SGD
where gradients are truncated above a fixed threshold. We show that it achieves
a stronger notion of avoiding sharp minima: it can effectively eliminate sharp
local minima entirely from its training trajectory. We characterize the
dynamics of truncated SGD driven by heavy-tailed noises. First, we show that
the truncation threshold and width of the attraction field dictate the order of
the first exit time from the associated local minimum. Moreover, when the
objective function satisfies appropriate structural conditions, we prove that
as the learning rate decreases, the dynamics of heavy-tailed truncated SGD
closely resemble those of a continuous-time Markov chain that never visits any
sharp minima. Real data experiments on deep learning confirm our theoretical
prediction that heavy-tailed SGD with gradient clipping finds a "flatter" local
minima and achieves better generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;Sewoong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1"&gt;Chang-Han Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12569</id>
        <link href="http://arxiv.org/abs/2106.12569"/>
        <updated>2021-06-24T01:51:44.232Z</updated>
        <summary type="html"><![CDATA[Binarized Neural Networks (BNNs) have the potential to revolutionize the way
that deep learning is carried out in edge computing platforms. However, the
effectiveness of interpretability methods on these networks has not been
assessed.

In this paper, we compare the performance of several widely used saliency
map-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when
applied to Binarized or Full Precision Neural Networks (FPNNs). We found that
the basic Gradient method produces very similar-looking maps for both types of
network. However, SmoothGrad produces significantly noisier maps for BNNs.
GradCAM also produces saliency maps which differ between network types, with
some of the BNNs having seemingly nonsensical explanations. We comment on
possible reasons for these differences in explanations and present it as an
example of why interpretability techniques should be tested on a wider range of
network types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1"&gt;Amy Widdicombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon J. Julier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Learning of Tensor Network Structures. (arXiv:2008.05437v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05437</id>
        <link href="http://arxiv.org/abs/2008.05437"/>
        <updated>2021-06-24T01:51:44.226Z</updated>
        <summary type="html"><![CDATA[Tensor Networks (TN) offer a powerful framework to efficiently represent very
high-dimensional objects. TN have recently shown their potential for machine
learning applications and offer a unifying view of common tensor decomposition
models such as Tucker, tensor train (TT) and tensor ring (TR). However,
identifying the best tensor network structure from data for a given task is
challenging. In this work, we leverage the TN formalism to develop a generic
and efficient adaptive algorithm to jointly learn the structure and the
parameters of a TN from data. Our method is based on a simple greedy approach
starting from a rank one tensor and successively identifying the most promising
tensor network edges for small rank increments. Our algorithm can adaptively
identify TN structures with small number of parameters that effectively
optimize any differentiable objective function. Experiments on tensor
decomposition, tensor completion and model compression tasks demonstrate the
effectiveness of the proposed algorithm. In particular, our method outperforms
the state-of-the-art evolutionary topology search [Li and Sun, 2020] for tensor
decomposition of images (while being orders of magnitude faster) and finds
efficient tensor network structures to compress neural networks outperforming
popular TT based approaches [Novikov et al., 2015].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hashemizadeh_M/0/1/0/all/0/1"&gt;Meraj Hashemizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Michelle Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiband VAE: Latent Space Partitioning for Knowledge Consolidation in Continual Learning. (arXiv:2106.12196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12196</id>
        <link href="http://arxiv.org/abs/2106.12196"/>
        <updated>2021-06-24T01:51:44.211Z</updated>
        <summary type="html"><![CDATA[We propose a new method for unsupervised continual knowledge consolidation in
generative models that relies on the partitioning of Variational Autoencoder's
latent space. Acquiring knowledge about new data samples without forgetting
previous ones is a critical problem of continual learning. Currently proposed
methods achieve this goal by extending the existing model while constraining
its behavior not to degrade on the past data, which does not exploit the full
potential of relations within the entire training dataset. In this work, we
identify this limitation and posit the goal of continual learning as a
knowledge accumulation task. We solve it by continuously re-aligning latent
space partitions that we call bands which are representations of samples seen
in different tasks, driven by the similarity of the information they contain.
In addition, we introduce a simple yet effective method for controlled
forgetting of past data that improves the quality of reconstructions encoded in
latent bands and a latent space disentanglement technique that improves
knowledge consolidation. On top of the standard continual learning evaluation
benchmarks, we evaluate our method on a new knowledge consolidation scenario
and show that the proposed approach outperforms state-of-the-art by up to
twofold across all testing scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1"&gt;Kamil Deja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wawrzynski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Wawrzy&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1"&gt;Daniel Marczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1"&gt;Wojciech Masarczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Explainable Representations of Malware Behavior. (arXiv:2106.12328v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12328</id>
        <link href="http://arxiv.org/abs/2106.12328"/>
        <updated>2021-06-24T01:51:44.205Z</updated>
        <summary type="html"><![CDATA[We address the problems of identifying malware in network telemetry logs and
providing \emph{indicators of compromise} -- comprehensible explanations of
behavioral patterns that identify the threat. In our system, an array of
specialized detectors abstracts network-flow data into comprehensible
\emph{network events} in a first step. We develop a neural network that
processes this sequence of events and identifies specific threats, malware
families and broad categories of malware. We then use the
\emph{integrated-gradients} method to highlight events that jointly constitute
the characteristic behavioral pattern of the threat. We compare network
architectures based on CNNs, LSTMs, and transformers, and explore the efficacy
of unsupervised pre-training experimentally on large-scale telemetry data. We
demonstrate how this system detects njRAT and other malware based on behavioral
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prasse_P/0/1/0/all/0/1"&gt;Paul Prasse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brabec_J/0/1/0/all/0/1"&gt;Jan Brabec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohout_J/0/1/0/all/0/1"&gt;Jan Kohout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Martin Kopp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajer_L/0/1/0/all/0/1"&gt;Lukas Bajer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheffer_T/0/1/0/all/0/1"&gt;Tobias Scheffer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should You Go Deeper? Optimizing Convolutional Neural Network Architectures without Training by Receptive Field Analysis. (arXiv:2106.12307v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12307</id>
        <link href="http://arxiv.org/abs/2106.12307"/>
        <updated>2021-06-24T01:51:44.186Z</updated>
        <summary type="html"><![CDATA[Applying artificial neural networks (ANN) to specific tasks, researchers,
programmers, and other specialists usually overshot the number of convolutional
layers in their designs. By implication, these ANNs hold too many parameters,
which needed unnecessarily trained without impacting the result. The features,
a convolutional layer can process, are strictly limited by its receptive field.
By layer-wise analyzing the expansion of the receptive fields, we can reliably
predict sequences of layers that will not contribute qualitatively to the
inference in thegiven ANN architecture. Based on these analyses, we propose
design strategies to resolve these inefficiencies, optimizing the
explainability and the computational performance of ANNs. Since neither the
strategies nor the analysis requires training of the actual model, these
insights allow for a very efficient design process of ANNs architectures which
might be automated in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1"&gt;Mats L. Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoning_J/0/1/0/all/0/1"&gt;Julius Sch&amp;#xf6;ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krumnack_U/0/1/0/all/0/1"&gt;Ulf Krumnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12562</id>
        <link href="http://arxiv.org/abs/2106.12562"/>
        <updated>2021-06-24T01:51:44.178Z</updated>
        <summary type="html"><![CDATA[We introduce feature alignment, a technique for obtaining approximate
reversibility in artificial neural networks. By means of feature extraction, we
can train a neural network to learn an estimated map for its reverse process
from outputs to inputs. Combined with variational autoencoders, we can generate
new samples from the same statistics as the training data. Improvements of the
results are obtained by using concepts from generative adversarial networks.
Finally, we show that the technique can be modified for training neural
networks locally, saving computational memory resources. Applying these
techniques, we report results for three vision generative tasks: MNIST,
CIFAR-10, and celebA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1"&gt;Tiago de Souza Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1"&gt;Jonas Maziero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03802</id>
        <link href="http://arxiv.org/abs/2010.03802"/>
        <updated>2021-06-24T01:51:44.123Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to the problem of text style transfer. Unlike
previous approaches requiring style-labeled training data, our method makes use
of readily-available unlabeled text by relying on the implicit connection in
style between adjacent sentences, and uses labeled data only at inference time.
We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to
extract a style vector from text and use it to condition the decoder to perform
style transfer. As our label-free training results in a style vector space
encoding many facets of style, we recast transfers as "targeted restyling"
vector operations that adjust specific attributes of the input while preserving
others. We demonstrate that training on unlabeled Amazon reviews data results
in a model that is competitive on sentiment transfer, even compared to models
trained fully on labeled data. Furthermore, applying our novel method to a
diverse corpus of unlabeled web text results in a single model capable of
transferring along multiple dimensions of style (dialect, emotiveness,
formality, politeness, sentiment) despite no additional training and using only
a handful of exemplars at inference time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1"&gt;Parker Riley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mandy Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Girish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1"&gt;David Uthus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sampling with Mirrored Stein Operators. (arXiv:2106.12506v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12506</id>
        <link href="http://arxiv.org/abs/2106.12506"/>
        <updated>2021-06-24T01:51:44.116Z</updated>
        <summary type="html"><![CDATA[We introduce a new family of particle evolution samplers suitable for
constrained domains and non-Euclidean geometries. Stein Variational Mirror
Descent and Mirrored Stein Variational Gradient Descent minimize the
Kullback-Leibler (KL) divergence to constrained target distributions by
evolving particles in a dual space defined by a mirror map. Stein Variational
Natural Gradient exploits non-Euclidean geometry to more efficiently minimize
the KL divergence to unconstrained targets. We derive these samplers from a new
class of mirrored Stein operators and adaptive kernels developed in this work.
We demonstrate that these new samplers yield accurate approximations to
distributions on the simplex, deliver valid confidence intervals in
post-selection inference, and converge more rapidly than prior methods in
large-scale unconstrained posterior inference. Finally, we establish the
convergence of our new procedures under verifiable conditions on the target
distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Phish in a Haystack: A Pipeline for Phishing Classification on Certificate Transparency Logs. (arXiv:2106.12343v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12343</id>
        <link href="http://arxiv.org/abs/2106.12343"/>
        <updated>2021-06-24T01:51:44.111Z</updated>
        <summary type="html"><![CDATA[Current popular phishing prevention techniques mainly utilize reactive
blocklists, which leave a ``window of opportunity'' for attackers during which
victims are unprotected. One possible approach to shorten this window aims to
detect phishing attacks earlier, during website preparation, by monitoring
Certificate Transparency (CT) logs. Previous attempts to work with CT log data
for phishing classification exist, however they lack evaluations on actual CT
log data. In this paper, we present a pipeline that facilitates such
evaluations by addressing a number of problems when working with CT log data.
The pipeline includes dataset creation, training, and past or live
classification of CT logs. Its modular structure makes it possible to easily
exchange classifiers or verification sources to support ground truth labeling
efforts and classifier comparisons. We test the pipeline on a number of new and
existing classifiers, and find a general potential to improve classifiers for
this scenario in the future. We publish the source code of the pipeline and the
used datasets along with this paper
(https://gitlab.com/rwth-itsec/ctl-pipeline), thus making future research in
this direction more accessible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1"&gt;Arthur Drichel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drury_V/0/1/0/all/0/1"&gt;Vincent Drury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1"&gt;Justus von Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1"&gt;Ulrike Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combination of Convolutional Neural Network and Gated Recurrent Unit for Energy Aware Resource Allocation. (arXiv:2106.12178v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12178</id>
        <link href="http://arxiv.org/abs/2106.12178"/>
        <updated>2021-06-24T01:51:44.106Z</updated>
        <summary type="html"><![CDATA[Cloud computing service models have experienced rapid growth and inefficient
resource usage is known as one of the greatest causes of high energy
consumption in cloud data centers. Resource allocation in cloud data centers
aiming to reduce energy consumption has been conducted using live migration of
Virtual Machines (VMs) and their consolidation into the small number of
Physical Machines (PMs). However, the selection of the appropriate VM for
migration is an important challenge. To solve this issue, VMs can be classified
according to the pattern of user requests into sensitive or insensitive classes
to latency, and thereafter suitable VMs can be selected for migration. In this
paper, the combination of Convolution Neural Network (CNN) and Gated Recurrent
Unit (GRU) is utilized for the classification of VMs in the Microsoft Azure
dataset. Due to the fact the majority of VMs in this dataset are labeled as
insensitive to latency, migration of more VMs in this group not only reduces
energy consumption but also decreases the violation of Service Level Agreements
(SLA). Based on the empirical results, the proposed model obtained an accuracy
of 95.18which clearly demonstrates the superiority of our proposed model
compared to other existing models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodaverdian_Z/0/1/0/all/0/1"&gt;Zeinab Khodaverdian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadr_H/0/1/0/all/0/1"&gt;Hossein Sadr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edalatpanah_S/0/1/0/all/0/1"&gt;Seyed Ahmad Edalatpanah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solimandarabi_M/0/1/0/all/0/1"&gt;Mojdeh Nazari Solimandarabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12144</id>
        <link href="http://arxiv.org/abs/2106.12144"/>
        <updated>2021-06-24T01:51:44.101Z</updated>
        <summary type="html"><![CDATA[Conventional representation learning algorithms for knowledge graphs (KG) map
each entity to a unique embedding vector. Such a shallow lookup results in a
linear growth of memory consumption for storing the embedding matrix and incurs
high computational costs when working with real-world KGs. Drawing parallels
with subword tokenization commonly used in NLP, we explore the landscape of
more parameter-efficient node embedding strategies with possibly sublinear
memory requirements. To this end, we propose NodePiece, an anchor-based
approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of
subword/sub-entity units is constructed from anchor nodes in a graph with known
relation types. Given such a fixed-size vocabulary, it is possible to bootstrap
an encoding and embedding for any entity, including those unseen during
training. Experiments show that NodePiece performs competitively in node
classification, link prediction, and relation prediction tasks while retaining
less than 10% of explicit nodes in a graph as anchors and often having 10x
fewer parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiapeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1"&gt;Etienne Denis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret-optimal Estimation and Control. (arXiv:2106.12097v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12097</id>
        <link href="http://arxiv.org/abs/2106.12097"/>
        <updated>2021-06-24T01:51:44.085Z</updated>
        <summary type="html"><![CDATA[We consider estimation and control in linear time-varying dynamical systems
from the perspective of regret minimization. Unlike most prior work in this
area, we focus on the problem of designing causal estimators and controllers
which compete against a clairvoyant noncausal policy, instead of the best
policy selected in hindsight from some fixed parametric class. We show that the
regret-optimal estimator and regret-optimal controller can be derived in
state-space form using operator-theoretic techniques from robust control and
present tight,data-dependent bounds on the regret incurred by our algorithms in
terms of the energy of the disturbances. Our results can be viewed as extending
traditional robust estimation and control, which focuses on minimizing
worst-case cost, to minimizing worst-case regret. We propose regret-optimal
analogs of Model-Predictive Control (MPC) and the Extended KalmanFilter (EKF)
for systems with nonlinear dynamics and present numerical experiments which
show that our regret-optimal algorithms can significantly outperform standard
approaches to estimation and control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goel_G/0/1/0/all/0/1"&gt;Gautam Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassibi_B/0/1/0/all/0/1"&gt;Babak Hassibi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07849</id>
        <link href="http://arxiv.org/abs/2003.07849"/>
        <updated>2021-06-24T01:51:44.080Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have gained considerable attention
owing to their ability to reproduce images. However, they can recreate training
images faithfully despite image degradation in the form of blur, noise, and
compression, generating similarly degraded images. To solve this problem, the
recently proposed noise robust GAN (NR-GAN) provides a partial solution by
demonstrating the ability to learn a clean image generator directly from noisy
images using a two-generator model comprising image and noise generators.
However, its application is limited to noise, which is relatively easy to
decompose owing to its additive and reversible characteristics, and its
application to irreversible image degradation, in the form of blur,
compression, and combination of all, remains a challenge. To address these
problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that
can learn a clean image generator directly from degraded images without
knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or
quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator
model composed of image, blur-kernel, noise, and quality-factor generators.
However, in contrast to NR-GAN, to address irreversible characteristics, we
introduce masking architectures adjusting degradation strength values in a
data-driven manner using bypasses before and after degradation. Furthermore, to
suppress uncertainty caused by the combination of blur, noise, and compression,
we introduce adaptive consistency losses imposing consistency between
irreversible degradation processes according to the degradation strengths. We
demonstrate the effectiveness of BNCR-GAN through large-scale comparative
studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we
demonstrate the applicability of BNCR-GAN in image restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12534</id>
        <link href="http://arxiv.org/abs/2106.12534"/>
        <updated>2021-06-24T01:51:44.075Z</updated>
        <summary type="html"><![CDATA[Reflecting on the last few years, the biggest breakthroughs in deep
reinforcement learning (RL) have been in the discrete action domain. Robotic
manipulation, however, is inherently a continuous control environment, but
these continuous control reinforcement learning algorithms often depend on
actor-critic methods that are sample-inefficient and inherently difficult to
train, due to the joint optimisation of the actor and critic. To that end, we
explore how we can bring the stability of discrete action RL algorithms to the
robot manipulation domain. We extend the recently released ARM algorithm, by
replacing the continuous next-best pose agent with a discrete next-best pose
agent. Discretisation of rotation is trivial given its bounded nature, while
translation is inherently unbounded, making discretisation difficult. We
formulate the translation prediction as the voxel prediction problem by
discretising the 3D space; however, voxelisation of a large workspace is memory
intensive and would not work with a high density of voxels, crucial to
obtaining the resolution needed for robotic manipulation. We therefore propose
to apply this voxel prediction in a coarse-to-fine manner by gradually
increasing the resolution. In each step, we extract the highest valued voxel as
the predicted location, which is then used as the centre of the
higher-resolution voxelisation in the next step. This coarse-to-fine prediction
is applied over several steps, giving a near-lossless prediction of the
translation. We show that our new coarse-to-fine algorithm is able to
accomplish RLBench tasks much more efficiently than the continuous control
equivalent, and even train some real-world tasks, tabular rasa, in less than 7
minutes, with only 3 demonstrations. Moreover, we show that by moving to a
voxel representation, we are able to easily incorporate observations from
multiple cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Stephen James&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1"&gt;Kentaro Wada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1"&gt;Tristan Laidlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1"&gt;Andrew J. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning: Progress, Taxonomies and Opportunities. (arXiv:2106.12177v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12177</id>
        <link href="http://arxiv.org/abs/2106.12177"/>
        <updated>2021-06-24T01:51:44.069Z</updated>
        <summary type="html"><![CDATA[Imitation learning aims to extract knowledge from human experts'
demonstrations or artificially created agents in order to replicate their
behaviors. Its success has been demonstrated in areas such as video games,
autonomous driving, robotic simulations and object manipulation. However, this
replicating process could be problematic, such as the performance is highly
dependent on the demonstration quality, and most trained agents are limited to
perform well in task-specific environments. In this survey, we provide a
systematic review on imitation learning. We first introduce the background
knowledge from development history and preliminaries, followed by presenting
different taxonomies within Imitation Learning and key milestones of the field.
We then detail challenges in learning strategies and present research
opportunities with learning policy from suboptimal demonstration, voice
instructions and other associated optimization schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1"&gt;Sunny Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianlong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor Tsang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Learning Lagrange Policies for Multi-Action Restless Bandits. (arXiv:2106.12024v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12024</id>
        <link href="http://arxiv.org/abs/2106.12024"/>
        <updated>2021-06-24T01:51:44.053Z</updated>
        <summary type="html"><![CDATA[Multi-action restless multi-armed bandits (RMABs) are a powerful framework
for constrained resource allocation in which $N$ independent processes are
managed. However, previous work only study the offline setting where problem
dynamics are known. We address this restrictive assumption, designing the first
algorithms for learning good policies for Multi-action RMABs online using
combinations of Lagrangian relaxation and Q-learning. Our first approach,
MAIQL, extends a method for Q-learning the Whittle index in binary-action RMABs
to the multi-action setting. We derive a generalized update rule and
convergence proof and establish that, under standard assumptions, MAIQL
converges to the asymptotically optimal multi-action RMAB policy as
$t\rightarrow{}\infty$. However, MAIQL relies on learning Q-functions and
indexes on two timescales which leads to slow convergence and requires problem
structure to perform well. Thus, we design a second algorithm, LPQL, which
learns the well-performing and more general Lagrange policy for multi-action
RMABs by learning to minimize the Lagrange bound through a variant of
Q-learning. To ensure fast convergence, we take an approximation strategy that
enables learning on a single timescale, then give a guarantee relating the
approximation's precision to an upper bound of LPQL's return as
$t\rightarrow{}\infty$. Finally, we show that our approaches always outperform
baselines across multiple settings, including one derived from real-world
medication adherence data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killian_J/0/1/0/all/0/1"&gt;Jackson A. Killian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1"&gt;Arpita Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1"&gt;Sanket Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1"&gt;Milind Tambe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. (arXiv:2006.07805v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07805</id>
        <link href="http://arxiv.org/abs/2006.07805"/>
        <updated>2021-06-24T01:51:44.048Z</updated>
        <summary type="html"><![CDATA[The transition matrix, denoting the transition relationship from clean labels
to noisy labels, is essential to build statistically consistent classifiers in
label-noise learning. Existing methods for estimating the transition matrix
rely heavily on estimating the noisy class posterior. However, the estimation
error for noisy class posterior could be large due to the randomness of label
noise, which would lead the transition matrix to be poorly estimated.
Therefore, in this paper, we aim to solve this problem by exploiting the
divide-and-conquer paradigm. Specifically, we introduce an intermediate class
to avoid directly estimating the noisy class posterior. By this intermediate
class, the original transition matrix can then be factorized into the product
of two easy-to-estimate transition matrices. We term the proposed method the
dual-T estimator. Both theoretical analyses and empirical results illustrate
the effectiveness of the dual-T estimator for estimating transition matrices,
leading to better classification performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jiankang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Interpretable Residual Extragradient ISTA for Sparse Coding. (arXiv:2106.11970v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11970</id>
        <link href="http://arxiv.org/abs/2106.11970"/>
        <updated>2021-06-24T01:51:44.043Z</updated>
        <summary type="html"><![CDATA[Recently, the study on learned iterative shrinkage thresholding algorithm
(LISTA) has attracted increasing attentions. A large number of experiments as
well as some theories have proved the high efficiency of LISTA for solving
sparse coding problems. However, existing LISTA methods are all serial
connection. To address this issue, we propose a novel extragradient based LISTA
(ELISTA), which has a residual structure and theoretical guarantees. In
particular, our algorithm can also provide the interpretability for Res-Net to
a certain extent. From a theoretical perspective, we prove that our method
attains linear convergence. In practice, extensive empirical results verify the
advantages of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[False perfection in machine prediction: Detecting and assessing circularity problems in machine learning. (arXiv:2106.12417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12417</id>
        <link href="http://arxiv.org/abs/2106.12417"/>
        <updated>2021-06-24T01:51:44.037Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms train models from patterns of input data and
target outputs, with the goal of predicting correct outputs for unseen test
inputs. Here we demonstrate a problem of machine learning in vital application
areas such as medical informatics or patent law that consists of the inclusion
of measurements on which target outputs are deterministically defined in the
representations of input data. This leads to perfect, but circular predictions
based on a machine reconstruction of the known target definition, but fails on
real-world data where the defining measurements may not or only incompletely be
available. We present a circularity test that shows, for given datasets and
black-box machine learning models, whether the target functional definition can
be reconstructed and has been used in training. We argue that a transfer of
research results to real-world applications requires to avoid circularity by
separating measurements that define target outcomes from data representations
in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hagmann_M/0/1/0/all/0/1"&gt;Michael Hagmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05218</id>
        <link href="http://arxiv.org/abs/2102.05218"/>
        <updated>2021-06-24T01:51:44.032Z</updated>
        <summary type="html"><![CDATA[The outbreak of COVID-19 Disease due to the novel coronavirus has caused a
shortage of medical resources. To aid and accelerate the diagnosis process,
automatic diagnosis of COVID-19 via deep learning models has recently been
explored by researchers across the world. While different data-driven deep
learning models have been developed to mitigate the diagnosis of COVID-19, the
data itself is still scarce due to patient privacy concerns. Federated Learning
(FL) is a natural solution because it allows different organizations to
cooperatively learn an effective deep learning model without sharing raw data.
However, recent studies show that FL still lacks privacy protection and may
cause data leakage. We investigate this challenging problem by proposing a
simple yet effective algorithm, named \textbf{F}ederated \textbf{L}earning
\textbf{o}n Medical Datasets using \textbf{P}artial Networks (FLOP), that
shares only a partial model between the server and clients. Extensive
experiments on benchmark data and real-world healthcare tasks show that our
approach achieves comparable or better performance while reducing the privacy
and security risks. Of particular interest, we conduct experiments on the
COVID-19 dataset and find that our FLOP algorithm can allow different hospitals
to collaboratively and effectively train a partially shared model without
sharing local patients' data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Weituo Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1"&gt;Gregory Spell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks. (arXiv:2106.12379v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12379</id>
        <link href="http://arxiv.org/abs/2106.12379"/>
        <updated>2021-06-24T01:51:44.026Z</updated>
        <summary type="html"><![CDATA[The increasing computational requirements of deep neural networks (DNNs) have
led to significant interest in obtaining DNN models that are sparse, yet
accurate. Recent work has investigated the even harder case of sparse training,
where the DNN weights are, for as much as possible, already sparse to reduce
computational costs during training.

Existing sparse training methods are mainly empirical and often have lower
accuracy relative to the dense baseline. In this paper, we present a general
approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs,
demonstrate convergence for a variant of the algorithm, and show that AC/DC
outperforms existing sparse training methods in accuracy at similar
computational budgets; at high sparsity levels, AC/DC even outperforms existing
methods that rely on accurate pre-trained dense models. An important property
of AC/DC is that it allows co-training of dense and sparse models, yielding
accurate sparse-dense model pairs at the end of the training process. This is
useful in practice, where compressed variants may be desirable for deployment
in resource-constrained settings without re-doing the entire training flow, and
also provides us with insights into the accuracy gap between dense and
compressed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1"&gt;Alexandra Peste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1"&gt;Eugenia Iofinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vladu_A/0/1/0/all/0/1"&gt;Adrian Vladu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1"&gt;Dan Alistarh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Approach to Fair Online Learning via Blackwell Approachability. (arXiv:2106.12242v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12242</id>
        <link href="http://arxiv.org/abs/2106.12242"/>
        <updated>2021-06-24T01:51:44.010Z</updated>
        <summary type="html"><![CDATA[We provide a setting and a general approach to fair online learning with
stochastic sensitive and non-sensitive contexts. The setting is a repeated game
between the Player and Nature, where at each stage both pick actions based on
the contexts. Inspired by the notion of unawareness, we assume that the Player
can only access the non-sensitive context before making a decision, while we
discuss both cases of Nature accessing the sensitive contexts and Nature
unaware of the sensitive contexts. Adapting Blackwell's approachability theory
to handle the case of an unknown contexts' distribution, we provide a general
necessary and sufficient condition for learning objectives to be compatible
with some fairness constraints. This condition is instantiated on (group-wise)
no-regret and (group-wise) calibration objectives, and on demographic parity as
an additional constraint. When the objective is not compatible with the
constraint, the provided framework permits to characterise the optimal
trade-off between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chzhen_E/0/1/0/all/0/1"&gt;Evgenii Chzhen&lt;/a&gt; (LMO, CELESTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Giraud_C/0/1/0/all/0/1"&gt;Christophe Giraud&lt;/a&gt; (LMO, CELESTE), &lt;a href="http://arxiv.org/find/cs/1/au:+Stoltz_G/0/1/0/all/0/1"&gt;Gilles Stoltz&lt;/a&gt; (LMO, CELESTE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Bayesian Multiscale Deep Learning Framework for Flows in Random Media. (arXiv:2103.09056v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09056</id>
        <link href="http://arxiv.org/abs/2103.09056"/>
        <updated>2021-06-24T01:51:44.005Z</updated>
        <summary type="html"><![CDATA[Fine-scale simulation of complex systems governed by multiscale partial
differential equations (PDEs) is computationally expensive and various
multiscale methods have been developed for addressing such problems. In
addition, it is challenging to develop accurate surrogate and uncertainty
quantification models for high-dimensional problems governed by stochastic
multiscale PDEs using limited training data. In this work to address these
challenges, we introduce a novel hybrid deep-learning and multiscale approach
for stochastic multiscale PDEs with limited training data. For demonstration
purposes, we focus on a porous media flow problem. We use an image-to-image
supervised deep learning model to learn the mapping between the input
permeability field and the multiscale basis functions. We introduce a Bayesian
approach to this hybrid framework to allow us to perform uncertainty
quantification and propagation tasks. The performance of this hybrid approach
is evaluated with varying intrinsic dimensionality of the permeability field.
Numerical results indicate that the hybrid network can efficiently predict well
for high-dimensional inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Padmanabha_G/0/1/0/all/0/1"&gt;Govinda Anantha Padmanabha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zabaras_N/0/1/0/all/0/1"&gt;Nicholas Zabaras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15207</id>
        <link href="http://arxiv.org/abs/2006.15207"/>
        <updated>2021-06-24T01:51:43.999Z</updated>
        <summary type="html"><![CDATA[Detecting out-of-distribution (OOD) inputs is critical for safely deploying
deep learning models in an open-world setting. However, existing OOD detection
solutions can be brittle in the open world, facing various types of adversarial
OOD inputs. While methods leveraging auxiliary OOD data have emerged, our
analysis on illuminative examples reveals a key insight that the majority of
auxiliary OOD examples may not meaningfully improve or even hurt the decision
boundary of the OOD detector, which is also observed in empirical results on
real data. In this paper, we provide a theoretically motivated method,
Adversarial Training with informative Outlier Mining (ATOM), which improves the
robustness of OOD detection. We show that, by mining informative auxiliary OOD
data, one can significantly improve OOD detection performance, and somewhat
surprisingly, generalize to unseen adversarial attacks. ATOM achieves
state-of-the-art performance under a broad family of classic and adversarial
OOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,
ATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,
surpassing the previous best baseline by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yingyu Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Deep Learning Hyperparameter Search for Robust Function Mapping to Polynomials with Noise. (arXiv:2106.12532v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12532</id>
        <link href="http://arxiv.org/abs/2106.12532"/>
        <updated>2021-06-24T01:51:43.993Z</updated>
        <summary type="html"><![CDATA[Advances in neural architecture search, as well as explainability and
interpretability of connectionist architectures, have been reported in the
recent literature. However, our understanding of how to design Bayesian Deep
Learning (BDL) hyperparameters, specifically, the depth, width and ensemble
size, for robust function mapping with uncertainty quantification, is still
emerging. This paper attempts to further our understanding by mapping Bayesian
connectionist representations to polynomials of different orders with varying
noise types and ratios. We examine the noise-contaminated polynomials to search
for the combination of hyperparameters that can extract the underlying
polynomial signals while quantifying uncertainties based on the noise
attributes. Specifically, we attempt to study the question that an appropriate
neural architecture and ensemble configuration can be found to detect a signal
of any n-th order polynomial contaminated with noise having different
distributions and signal-to-noise (SNR) ratios and varying noise attributes.
Our results suggest the possible existence of an optimal network depth as well
as an optimal number of ensembles for prediction skills and uncertainty
quantification, respectively. However, optimality is not discernible for width,
even though the performance gain reduces with increasing width at high values
of width. Our experiments and insights can be directional to understand
theoretical properties of BDL representations and to design practical
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harilal_N/0/1/0/all/0/1"&gt;Nidhin Harilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_U/0/1/0/all/0/1"&gt;Udit Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1"&gt;Auroop R. Ganguly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12183</id>
        <link href="http://arxiv.org/abs/2106.12183"/>
        <updated>2021-06-24T01:51:43.979Z</updated>
        <summary type="html"><![CDATA[One of the distinct features of this century has been the population of older
adults which has been on a constant rise. Elderly people have several needs and
requirements due to physical disabilities, cognitive issues, weakened memory
and disorganized behavior, that they face with increasing age. The extent of
these limitations also differs according to the varying diversities in elderly,
which include age, gender, background, experience, skills, knowledge and so on.
These varying needs and challenges with increasing age, limits abilities of
older adults to perform Activities of Daily Living (ADLs) in an independent
manner. To add to it, the shortage of caregivers creates a looming need for
technology-based services for elderly people, to assist them in performing
their daily routine tasks to sustain their independent living and active aging.
To address these needs, this work consists of making three major contributions
in this field. First, it provides a rather comprehensive review of assisted
living technologies aimed at helping elderly people to perform ADLs. Second,
the work discusses the challenges identified through this review, that
currently exist in the context of implementation of assisted living services
for elderly care in Smart Homes and Smart Cities. Finally, the work also
outlines an approach for implementation, extension and integration of the
existing works in this field for development of a much-needed framework that
can provide personalized assistance and user-centered behavior interventions to
elderly as per their varying and ever-changing needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14152</id>
        <link href="http://arxiv.org/abs/2103.14152"/>
        <updated>2021-06-24T01:51:43.973Z</updated>
        <summary type="html"><![CDATA[End-to-end models with auto-regressive decoders have shown impressive results
for automatic speech recognition (ASR). These models formulate the
sequence-level probability as a product of the conditional probabilities of all
individual tokens given their histories. However, the performance of locally
normalised models can be sub-optimal because of factors such as exposure bias.
Consequently, the model distribution differs from the underlying data
distribution. In this paper, the residual energy-based model (R-EBM) is
proposed to complement the auto-regressive ASR model to close the gap between
the two distributions. Meanwhile, R-EBMs can also be regarded as
utterance-level confidence estimators, which may benefit many downstream tasks.
Experiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word
error rates (WERs) by 8.2%/6.7% while improving areas under precision-recall
curves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.
Furthermore, on a state-of-the-art model using self-supervised learning
(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence
estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liangliang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Algorithms for Individually Fair $k$-Clustering. (arXiv:2106.12150v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.12150</id>
        <link href="http://arxiv.org/abs/2106.12150"/>
        <updated>2021-06-24T01:51:43.969Z</updated>
        <summary type="html"><![CDATA[We study data clustering problems with $\ell_p$-norm objectives (e.g.
$k$-Median and $k$-Means) in the context of individual fairness. The dataset
consists of $n$ points, and we want to find $k$ centers such that (a) the
objective is minimized, while (b) respecting the individual fairness constraint
that every point $v$ has a center within a distance at most $r(v)$, where
$r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz
[FORC 2020] introduced this concept and designed a clustering algorithm with
provable (approximate) fairness and objective guarantees for the $\ell_\infty$
or $k$-Center objective. Mahabadi and Vakilian [ICML 2020] revisited this
problem to give a local-search algorithm for all $\ell_p$-norms. Empirically,
their algorithms outperform Jung et. al.'s by a large margin in terms of cost
(for $k$-Median and $k$-Means), but they incur a reasonable loss in fairness.
In this paper, our main contribution is to use Linear Programming (LP)
techniques to obtain better algorithms for this problem, both in theory and in
practice. We prove that by modifying known LP rounding techniques, one gets a
worst-case guarantee on the objective which is much better than in MV20, and
empirically, this objective is extremely close to the optimal. Furthermore, our
theoretical fairness guarantees are comparable with MV20 in theory, and
empirically, we obtain noticeably fairer solutions. Although solving the LP
{\em exactly} might be prohibitive, we demonstrate that in practice, a simple
sparsification technique drastically improves the run-time of our algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_D/0/1/0/all/0/1"&gt;Deeparnab Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negahbani_M/0/1/0/all/0/1"&gt;Maryam Negahbani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Compressed Sensing using Generative Models. (arXiv:2006.09461v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09461</id>
        <link href="http://arxiv.org/abs/2006.09461"/>
        <updated>2021-06-24T01:51:43.962Z</updated>
        <summary type="html"><![CDATA[The goal of compressed sensing is to estimate a high dimensional vector from
an underdetermined system of noisy linear equations. In analogy to classical
compressed sensing, here we assume a generative model as a prior, that is, we
assume the vector is represented by a deep generative model $G: \mathbb{R}^k
\rightarrow \mathbb{R}^n$. Classical recovery approaches such as empirical risk
minimization (ERM) are guaranteed to succeed when the measurement matrix is
sub-Gaussian. However, when the measurement matrix and measurements are
heavy-tailed or have outliers, recovery may fail dramatically. In this paper we
propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm
guarantees recovery for heavy-tailed data, even in the presence of outliers.
Theoretically, our results show our novel MOM-based algorithm enjoys the same
sample complexity guarantees as ERM under sub-Gaussian assumptions. Our
experiments validate both aspects of our claims: other algorithms are indeed
fragile and fail under heavy-tailed and/or corrupted data, while our approach
exhibits the predicted robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Caramanis_C/0/1/0/all/0/1"&gt;Constantine Caramanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oneshot Differentially Private Top-k Selection. (arXiv:2105.08233v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08233</id>
        <link href="http://arxiv.org/abs/2105.08233"/>
        <updated>2021-06-24T01:51:43.957Z</updated>
        <summary type="html"><![CDATA[Being able to efficiently and accurately select the top-$k$ elements with
differential privacy is an integral component of various private data analysis
tasks. In this paper, we present the oneshot Laplace mechanism, which
generalizes the well-known Report Noisy Max mechanism to reporting noisy
top-$k$ elements. We show that the oneshot Laplace mechanism with a noise level
of $\widetilde{O}(\sqrt{k}/\eps)$ is approximately differentially private.
Compared to the previous peeling approach of running Report Noisy Max $k$
times, the oneshot Laplace mechanism only adds noises and computes the top $k$
elements once, hence much more efficient for large $k$. In addition, our proof
of privacy relies on a novel coupling technique that bypasses the use of
composition theorems. Finally, we present a novel application of efficient
top-$k$ selection in the classical problem of ranking from pairwise
comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_G/0/1/0/all/0/1"&gt;Gang Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weijie J. Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phase Retrieval using Single-Instance Deep Generative Prior. (arXiv:2106.04812v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04812</id>
        <link href="http://arxiv.org/abs/2106.04812"/>
        <updated>2021-06-24T01:51:43.951Z</updated>
        <summary type="html"><![CDATA[Several deep learning methods for phase retrieval exist, but most of them
fail on realistic data without precise support information. We propose a novel
method based on single-instance deep generative prior that works well on
complex-valued crystal data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1"&gt;Kshitij Tayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manekar_R/0/1/0/all/0/1"&gt;Raunak Manekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1"&gt;Zhong Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;David Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1"&gt;Felix Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02526</id>
        <link href="http://arxiv.org/abs/2012.02526"/>
        <updated>2021-06-24T01:51:43.937Z</updated>
        <summary type="html"><![CDATA[The renaissance of artificial neural networks was catalysed by the success of
classification models, tagged by the community with the broader term supervised
learning. The extraordinary results gave rise to a hype loaded with ambitious
promises and overstatements. Soon the community realised that the success owed
much to the availability of thousands of labelled examples and supervised
learning went, for many, from glory to shame: Some criticised deep learning as
a whole and others proclaimed that the way forward had to be alternatives to
supervised learning: predictive, unsupervised, semi-supervised and, more
recently, self-supervised learning. However, all these seem brand names, rather
than actual categories of a theoretically grounded taxonomy. Moreover, the call
to banish supervised learning was motivated by the questionable claim that
humans learn with little or no supervision and are capable of robust
out-of-distribution generalisation. Here, we review insights about learning and
supervision in nature, revisit the notion that learning and generalisation are
not possible without supervision or inductive biases and argue that we will
make better progress if we just call it by its name.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12320</id>
        <link href="http://arxiv.org/abs/2106.12320"/>
        <updated>2021-06-24T01:51:43.932Z</updated>
        <summary type="html"><![CDATA[Automatic processing of bibliographic data becomes very important in digital
libraries, data science and machine learning due to its importance in keeping
pace with the significant increase of published papers every year from one side
and to the inherent challenges from the other side. This processing has several
aspects including but not limited to I) Automatic extraction of references from
PDF documents, II) Building an accurate citation graph, III) Author name
disambiguation, etc. Bibliographic data is heterogeneous by nature and occurs
in both structured (e.g. citation graph) and unstructured (e.g. publications)
formats. Therefore, it requires data science and machine learning techniques to
be processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on
Bibliographic Data Analysis and Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1"&gt;Silvio Peroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Learning with Radial Basis Function Networks. (arXiv:2103.08414v2 [cs.CE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08414</id>
        <link href="http://arxiv.org/abs/2103.08414"/>
        <updated>2021-06-24T01:51:43.926Z</updated>
        <summary type="html"><![CDATA[We investigate the benefits of feature selection, nonlinear modelling and
online learning when forecasting in financial time series. We consider the
sequential and continual learning sub-genres of online learning. The
experiments we conduct show that there is a benefit to online transfer
learning, in the form of radial basis function networks, beyond the sequential
updating of recursive least-squares models. We show that the radial basis
function networks, which make use of clustering algorithms to construct a
kernel Gram matrix, are more beneficial than treating each training vector as
separate basis functions, as occurs with kernel Ridge regression. We
demonstrate quantitative procedures to determine the very structure of the
radial basis function networks. Finally, we conduct experiments on the log
returns of financial time series and show that the online learning models,
particularly the radial basis function networks, are able to outperform a
random walk baseline, whereas the offline learning models struggle to do so.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Borrageiro_G/0/1/0/all/0/1"&gt;Gabriel Borrageiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firoozye_N/0/1/0/all/0/1"&gt;Nick Firoozye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barucca_P/0/1/0/all/0/1"&gt;Paolo Barucca&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12182</id>
        <link href="http://arxiv.org/abs/2106.12182"/>
        <updated>2021-06-24T01:51:43.921Z</updated>
        <summary type="html"><![CDATA[This work tackles the issue of fairness in the context of generative
procedures, such as image super-resolution, which entail different definitions
from the standard classification setting. Moreover, while traditional group
fairness definitions are typically defined with respect to specified protected
groups -- camouflaging the fact that these groupings are artificial and carry
historical and political motivations -- we emphasize that there are no ground
truth identities. For instance, should South and East Asians be viewed as a
single group or separate groups? Should we consider one race as a whole or
further split by gender? Choosing which groups are valid and who belongs in
them is an impossible dilemma and being ``fair'' with respect to Asians may
require being ``unfair'' with respect to South Asians. This motivates the
introduction of definitions that allow algorithms to be \emph{oblivious} to the
relevant groupings.

We define several intuitive notions of group fairness and study their
incompatibilities and trade-offs. We show that the natural extension of
demographic parity is strongly dependent on the grouping, and \emph{impossible}
to achieve obliviously. On the other hand, the conceptually new definition we
introduce, Conditional Proportional Representation, can be achieved obliviously
through Posterior Sampling. Our experiments validate our theoretical results
and achieve fair image reconstruction using state-of-the-art generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1"&gt;Jessica Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoftNER: Mining Knowledge Graphs From Cloud Incidents. (arXiv:2101.05961v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05961</id>
        <link href="http://arxiv.org/abs/2101.05961"/>
        <updated>2021-06-24T01:51:43.905Z</updated>
        <summary type="html"><![CDATA[The move from boxed products to services and the widespread adoption of cloud
computing has had a huge impact on the software development life cycle and
DevOps processes. Particularly, incident management has become critical for
developing and operating large-scale services. Prior work on incident
management has heavily focused on the challenges with incident triaging and
de-duplication. In this work, we address the fundamental problem of structured
knowledge extraction from service incidents. We have built SoftNER, a framework
for mining Knowledge Graphs from incident reports. First, we build a novel
multi-task learning based BiLSTM-CRF model which leverages not just the
semantic context but also the data-types for extracting factual information in
the form of named entities. Next, we present an approach to mine relations
between the named entities for automatically constructing knowledge graphs. We
have deployed SoftNER at Microsoft, a major cloud service provider and have
evaluated it on more than 2 months of cloud incidents. We show that the
unsupervised machine learning pipeline has a high precision of 0.96. Our
multi-task learning based deep learning model also outperforms the
state-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,
we are able to build accurate models for applications such as incident triaging
and recommending entities based on their relevance to incident titles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shetty_M/0/1/0/all/0/1"&gt;Manish Shetty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_C/0/1/0/all/0/1"&gt;Chetan Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sumit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1"&gt;Nikitha Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagappan_N/0/1/0/all/0/1"&gt;Nachiappan Nagappan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Feature-Complete Differentiable Physics for Articulated Rigid Bodies with Contact. (arXiv:2103.16021v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16021</id>
        <link href="http://arxiv.org/abs/2103.16021"/>
        <updated>2021-06-24T01:51:43.900Z</updated>
        <summary type="html"><![CDATA[We present a fast and feature-complete differentiable physics engine, Nimble
(nimblephysics.org), that supports Lagrangian dynamics and hard contact
constraints for articulated rigid body simulation. Our differentiable physics
engine offers a complete set of features that are typically only available in
non-differentiable physics simulators commonly used by robotics applications.
We solve contact constraints precisely using linear complementarity problems
(LCPs). We present efficient and novel analytical gradients through the LCP
formulation of inelastic contact that exploit the sparsity of the LCP solution.
We support complex contact geometry, and gradients approximating
continuous-time elastic collision. We also introduce a novel method to compute
complementarity-aware gradients that help downstream optimization tasks avoid
stalling in saddle points. We show that an implementation of this combination
in an existing physics engine (DART) is capable of a 87x single-core speedup
over finite-differencing in computing analytical Jacobians for a single
timestep, while preserving all the expressiveness of original DART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Werling_K/0/1/0/all/0/1"&gt;Keenon Werling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omens_D/0/1/0/all/0/1"&gt;Dalton Omens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jeongseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Exarchos_I/0/1/0/all/0/1"&gt;Ioannis Exarchos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;C. Karen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy choice in experiments with unknown interference. (arXiv:2011.08174v4 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08174</id>
        <link href="http://arxiv.org/abs/2011.08174"/>
        <updated>2021-06-24T01:51:43.894Z</updated>
        <summary type="html"><![CDATA[This paper discusses experimental design to estimate welfare-maximizing
policies. We consider a setting where units are organized into large, finitely
many independent clusters and interact over unobserved dimensions within each
cluster. The contribution of this paper is two-fold. First, we construct a test
for whether a welfare-improving treatment configuration exists and hence worth
learning by conducting a larger scale experiment. Second, we introduce an
adaptive randomization procedure to estimate welfare-maximizing individual
treatment allocation rules valid under unobserved interference. We derive
asymptotic properties of the marginal effects estimators and finite-sample
regret guarantees of the policy. Finally, we illustrate the method's advantage
in simulations calibrated to an existing experiment on information diffusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1"&gt;Davide Viviano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Anomaly Sequence Detection for High-Resolution Monitoring of Power Systems. (arXiv:2012.05163v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05163</id>
        <link href="http://arxiv.org/abs/2012.05163"/>
        <updated>2021-06-24T01:51:43.814Z</updated>
        <summary type="html"><![CDATA[A deep learning approach is proposed to detect data and system anomalies
using high-resolution continuous point-on-wave (CPOW) or phasor measurements.
Both the anomaly and anomaly-free measurement models are assumed to have
unknown temporal dependencies and probability distributions. Historical
training samples are assumed for the anomaly-free model, while no training
samples are available for the anomaly measurements. By transforming the
anomaly-free observations into uniform independent and identically distributed
sequences via a generative adversarial network, the proposed approach deploys a
uniformity test for anomaly detection at the sensor level. A distributed
detection scheme that combines sensor level detections at the control center is
also proposed that combines local detections to form more reliable detections.
Numerical results demonstrate significant improvement over the state-of-the-art
solutions for various bad-data cases using real and synthetic CPOW and PMU data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mestav_K/0/1/0/all/0/1"&gt;Kursat Rasim Mestav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12543</id>
        <link href="http://arxiv.org/abs/2106.12543"/>
        <updated>2021-06-24T01:51:43.808Z</updated>
        <summary type="html"><![CDATA[As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. Despite the widespread use of explainability
techniques, evaluating and comparing different feature attribution methods
remains challenging: evaluations ideally require human studies, and empirical
evaluation metrics are often computationally prohibitive on real-world
datasets. In this work, we address this issue by releasing XAI-Bench: a suite
of synthetic datasets along with a library for benchmarking feature attribution
algorithms. Unlike real-world datasets, synthetic datasets allow the efficient
computation of conditional expected values that are needed to evaluate
ground-truth Shapley values and other metrics. The synthetic datasets we
release offer a wide variety of parameters that can be configured to simulate
real-world data. We demonstrate the power of our library by benchmarking
popular explainability techniques across several evaluation metrics and
identifying failure modes for popular explainers. The efficiency of our library
will help bring new explainability methods from development to deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1"&gt;Sujay Khandagale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:43.802Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07477</id>
        <link href="http://arxiv.org/abs/2106.07477"/>
        <updated>2021-06-24T01:51:43.786Z</updated>
        <summary type="html"><![CDATA[Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that the token-mixing MLP is a variant of the depthwise convolution
with a global reception field and spatial-specific configuration. But the
global reception field and the spatial-specific property make token-mixing MLP
prone to over-fitting. In this paper, we propose a novel pure MLP architecture,
spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only
contains channel-mixing MLP. We utilize a spatial-shift operation for
communications between patches. It has a local reception field and is
spatial-agnostic. It is parameter-free and efficient for computation. The
proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when
training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent
performance as ViT on ImageNet-1K dataset with considerably simpler
architecture and fewer FLOPs and parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yunfeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12070</id>
        <link href="http://arxiv.org/abs/2106.12070"/>
        <updated>2021-06-24T01:51:43.781Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are behind many of the recent successes in machine
learning applications. However, these models can produce overconfident
decisions while encountering out-of-distribution (OOD) examples or making a
wrong prediction. This inconsistent predictive confidence limits the
integration of independently-trained learning models into a larger system. This
paper introduces separable concept learning framework to realistically measure
the performance of classifiers in presence of OOD examples. In this setup,
several instances of a classifier are trained on different parts of a partition
of the set of classes. Later, the performance of the combination of these
models is evaluated on a separate test set. Unlike current OOD detection
techniques, this framework does not require auxiliary OOD datasets and does not
separate classification from detection performance. Furthermore, we present a
new strong baseline for more consistent predictive confidence in deep models,
called fitted ensembles, where overconfident predictions are rectified by
transformed versions of the original classification task. Fitted ensembles can
naturally detect OOD examples without requiring auxiliary data by observing
contradicting predictions among its components. Experiments on MNIST, SVHN,
CIFAR-10/100, and ImageNet show fitted ensemble significantly outperform
conventional ensembles on OOD examples and are possible to scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Ankit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1"&gt;Kenneth O. Stanley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12570</id>
        <link href="http://arxiv.org/abs/2106.12570"/>
        <updated>2021-06-24T01:51:43.776Z</updated>
        <summary type="html"><![CDATA[Multimodal VAEs seek to model the joint distribution over heterogeneous data
(e.g.\ vision, language), whilst also capturing a shared representation across
such modalities. Prior work has typically combined information from the
modalities by reconciling idiosyncratic representations directly in the
recognition model through explicit products, mixtures, or other such
factorisations. Here we introduce a novel alternative, the MEME, that avoids
such explicit combinations by repurposing semi-supervised VAEs to combine
information between modalities implicitly through mutual supervision. This
formulation naturally allows learning from partially-observed data where some
modalities can be entirely missing -- something that most existing approaches
either cannot handle, or do so to a limited extent. We demonstrate that MEME
outperforms baselines on standard metrics across both partial and complete
observation schemes on the MNIST-SVHN (image--image) and CUB (image--text)
datasets. We also contrast the quality of the representations learnt by mutual
supervision against standard approaches and observe interesting trends in its
ability to capture relatedness between data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1"&gt;Tom Joy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuge Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1"&gt;Sebastian M. Schmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lagrangian dual framework for conservative neural network solutions of kinetic equations. (arXiv:2106.12147v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.12147</id>
        <link href="http://arxiv.org/abs/2106.12147"/>
        <updated>2021-06-24T01:51:43.769Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel conservative formulation for solving
kinetic equations via neural networks. More precisely, we formulate the
learning problem as a constrained optimization problem with constraints that
represent the physical conservation laws. The constraints are relaxed toward
the residual loss function by the Lagrangian duality. By imposing physical
conservation properties of the solution as constraints of the learning problem,
we demonstrate far more accurate approximations of the solutions in terms of
errors and the conservation laws, for the kinetic Fokker-Planck equation and
the homogeneous Boltzmann equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Hwang_H/0/1/0/all/0/1"&gt;Hyung Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Son_H/0/1/0/all/0/1"&gt;Hwijae Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Predictions in Neural ODEs: Identification and Interventions. (arXiv:2106.12430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12430</id>
        <link href="http://arxiv.org/abs/2106.12430"/>
        <updated>2021-06-24T01:51:43.754Z</updated>
        <summary type="html"><![CDATA[Spurred by tremendous success in pattern matching and prediction tasks,
researchers increasingly resort to machine learning to aid original scientific
discovery. Given large amounts of observational data about a system, can we
uncover the rules that govern its evolution? Solving this task holds the great
promise of fully understanding the causal interactions and being able to make
reliable predictions about the system's behavior under interventions. We take a
step towards answering this question for time-series data generated from
systems of ordinary differential equations (ODEs). While the governing ODEs
might not be identifiable from data alone, we show that combining simple
regularization schemes with flexible neural ODEs can robustly recover the
dynamics and causal structures from time-series data. Our results on a variety
of (non)-linear first and second order systems as well as real data validate
our method. We conclude by showing that we can also make accurate predictions
under interventions on variables or the system itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aliee_H/0/1/0/all/0/1"&gt;Hananeh Aliee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theis_F/0/1/0/all/0/1"&gt;Fabian J. Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1"&gt;Niki Kilbertus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12545</id>
        <link href="http://arxiv.org/abs/2106.12545"/>
        <updated>2021-06-24T01:51:43.748Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in
diabetic patients. DR is a microvascular disease that affects the eye retina,
which causes vessel blockage and therefore cuts the main source of nutrition
for the retina tissues. Treatment for this visual disorder is most effective
when it is detected in its earliest stages, as severe DR can result in
irreversible blindness. Nonetheless, DR identification requires the expertise
of Ophthalmologists which is often expensive and time-consuming. Therefore,
automatic detection systems were introduced aiming to facilitate the
identification process, making it available globally in a time and
cost-efficient manner. However, due to the limited reliable datasets and
medical records for this particular eye disease, the obtained predictions
accuracies were relatively unsatisfying for eye specialists to rely on them as
diagnostic systems. Thus, we explored an ensemble-based learning strategy,
merging a substantial selection of well-known classification algorithms in one
sophisticated diagnostic model. The proposed framework achieved the highest
accuracy rates among all other common classification algorithms in the area. 4
subdatasets were generated to contain the top 5 and top 10 features of the
Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies
of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original
dataset respectively. The results imply the impressive performance of the
subdataset, which significantly conduces to a less complex classification
process]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1"&gt;Israa Odeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1"&gt;Mouhammd Alkasassbeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1"&gt;Mohammad Alauthman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for improved global precipitation in numerical weather prediction systems. (arXiv:2106.12045v1 [physics.ao-ph])]]></title>
        <id>http://arxiv.org/abs/2106.12045</id>
        <link href="http://arxiv.org/abs/2106.12045"/>
        <updated>2021-06-24T01:51:43.743Z</updated>
        <summary type="html"><![CDATA[The formation of precipitation in state-of-the-art weather and climate models
is an important process. The understanding of its relationship with other
variables can lead to endless benefits, particularly for the world's monsoon
regions dependent on rainfall as a support for livelihood. Various factors play
a crucial role in the formation of rainfall, and those physical processes are
leading to significant biases in the operational weather forecasts. We use the
UNET architecture of a deep convolutional neural network with residual learning
as a proof of concept to learn global data-driven models of precipitation. The
models are trained on reanalysis datasets projected on the cubed-sphere
projection to minimize errors due to spherical distortion. The results are
compared with the operational dynamical model used by the India Meteorological
Department. The theoretical deep learning-based model shows doubling of the
grid point, as well as area averaged skill measured in Pearson correlation
coefficients relative to operational system. This study is a proof-of-concept
showing that residual learning-based UNET can unravel physical relationships to
target precipitation, and those physical constraints can be used in the
dynamical operational models towards improved precipitation forecasts. Our
results pave the way for the development of online, hybrid models in the
future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1"&gt;Manmeet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kumar_B/0/1/0/all/0/1"&gt;Bipin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1"&gt;Dev Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rao_S/0/1/0/all/0/1"&gt;Suryachandra Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Gill_S/0/1/0/all/0/1"&gt;Sukhpal Singh Gill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chattopadhyay_R/0/1/0/all/0/1"&gt;Rajib Chattopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nanjundiah_R/0/1/0/all/0/1"&gt;Ravi S Nanjundiah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes. (arXiv:2106.12091v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12091</id>
        <link href="http://arxiv.org/abs/2106.12091"/>
        <updated>2021-06-24T01:51:43.736Z</updated>
        <summary type="html"><![CDATA[Supercomputer FCFS-based scheduling policies result in many transient idle
nodes, a phenomenon that is only partially alleviated by backfill scheduling
methods that promote small jobs to run before large jobs. Here we describe how
to realize a novel use for these otherwise wasted resources, namely, deep
neural network (DNN) training. This important workload is easily organized as
many small fragments that can be configured dynamically to fit essentially any
node*time hole in a supercomputer's schedule. We describe how the task of
rescaling suitable DNN training tasks to fit dynamically changing holes can be
formulated as a deterministic mixed integer linear programming (MILP)-based
resource allocation algorithm, and show that this MILP problem can be solved
efficiently at run time. We show further how this MILP problem can be adapted
to optimize for administrator- or user-defined metrics. We validate our method
with supercomputer scheduler logs and different DNN training scenarios, and
demonstrate efficiencies of up to 93% compared with running the same training
tasks on dedicated nodes. Our method thus enables substantial supercomputer
resources to be allocated to DNN training with no impact on other applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengchun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kettimuthu_R/0/1/0/all/0/1"&gt;Rajkumar Kettimuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1"&gt;Michael E. Papka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1"&gt;Ian Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis. (arXiv:2104.02869v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02869</id>
        <link href="http://arxiv.org/abs/2104.02869"/>
        <updated>2021-06-24T01:51:43.731Z</updated>
        <summary type="html"><![CDATA[Visual explanation methods have an important role in the prognosis of the
patients where the annotated data is limited or unavailable. There have been
several attempts to use gradient-based attribution methods to localize
pathology from medical scans without using segmentation labels. This research
direction has been impeded by the lack of robustness and reliability. These
methods are highly sensitive to the network parameters. In this study, we
introduce a robust visual explanation method to address this problem for
medical applications. We provide an innovative visual explanation algorithm for
general purpose and as an example application, we demonstrate its effectiveness
for quantifying lesions in the lungs caused by the Covid-19 with high accuracy
and robustness without using dense segmentation labels. This approach overcomes
the drawbacks of commonly used Grad-CAM and its extended versions. The premise
behind our proposed strategy is that the information flow is minimized while
ensuring the classifier prediction stays similar. Our findings indicate that
the bottleneck condition provides a more stable severity estimation than the
similar attribution methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ugur Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1"&gt;Ismail Irmakci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Keles_E/0/1/0/all/0/1"&gt;Elif Keles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_A/0/1/0/all/0/1"&gt;Ahmet Topcu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Ziyue Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1"&gt;Concetto Spampinato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jambawalikar_S/0/1/0/all/0/1"&gt;Sachin Jambawalikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1"&gt;Evrim Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1"&gt;Baris Turkbey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1"&gt;Ulas Bagci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLOP: Federated Learning on Medical Datasets using Partial Networks. (arXiv:2102.05218v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05218</id>
        <link href="http://arxiv.org/abs/2102.05218"/>
        <updated>2021-06-24T01:51:43.725Z</updated>
        <summary type="html"><![CDATA[The outbreak of COVID-19 Disease due to the novel coronavirus has caused a
shortage of medical resources. To aid and accelerate the diagnosis process,
automatic diagnosis of COVID-19 via deep learning models has recently been
explored by researchers across the world. While different data-driven deep
learning models have been developed to mitigate the diagnosis of COVID-19, the
data itself is still scarce due to patient privacy concerns. Federated Learning
(FL) is a natural solution because it allows different organizations to
cooperatively learn an effective deep learning model without sharing raw data.
However, recent studies show that FL still lacks privacy protection and may
cause data leakage. We investigate this challenging problem by proposing a
simple yet effective algorithm, named \textbf{F}ederated \textbf{L}earning
\textbf{o}n Medical Datasets using \textbf{P}artial Networks (FLOP), that
shares only a partial model between the server and clients. Extensive
experiments on benchmark data and real-world healthcare tasks show that our
approach achieves comparable or better performance while reducing the privacy
and security risks. Of particular interest, we conduct experiments on the
COVID-19 dataset and find that our FLOP algorithm can allow different hospitals
to collaboratively and effectively train a partially shared model without
sharing local patients' data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1"&gt;Weituo Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spell_G/0/1/0/all/0/1"&gt;Gregory Spell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12499</id>
        <link href="http://arxiv.org/abs/2106.12499"/>
        <updated>2021-06-24T01:51:43.719Z</updated>
        <summary type="html"><![CDATA[Self-training based unsupervised domain adaptation (UDA) has shown great
potential to address the problem of domain shift, when applying a trained deep
learning model in a source domain to unlabeled target domains. However, while
the self-training UDA has demonstrated its effectiveness on discriminative
tasks, such as classification and segmentation, via the reliable pseudo-label
selection based on the softmax discrete histogram, the self-training UDA for
generative tasks, such as image synthesis, is not fully investigated. In this
work, we propose a novel generative self-training (GST) UDA framework with
continuous value prediction and regression objective for cross-domain image
synthesis. Specifically, we propose to filter the pseudo-label with an
uncertainty mask, and quantify the predictive confidence of generated images
with practical variational Bayes learning. The fast test-time adaptation is
achieved by a round-based alternative optimization scheme. We validated our
framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis
problem, where datasets in the source and target domains were acquired from
different scanners or centers. Extensive validations were carried out to verify
our framework against popular adversarial training UDA methods. Results show
that our GST, with tagged MRI of test subjects in new target domains, improved
the synthesis quality by a large margin, compared with the adversarial training
UDA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Jiachen Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1"&gt;Reese Timothy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure Domain Adaptation with Multiple Sources. (arXiv:2106.12124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12124</id>
        <link href="http://arxiv.org/abs/2106.12124"/>
        <updated>2021-06-24T01:51:43.713Z</updated>
        <summary type="html"><![CDATA[Multi-source unsupervised domain adaptation (MUDA) is a recently explored
learning framework, where the goal is to address the challenge of labeled data
scarcity in a target domain via transferring knowledge from multiple source
domains with annotated data. Since the source data is distributed, the privacy
of source domains' data can be a natural concern. We benefit from the idea of
domain alignment in an embedding space to address the privacy concern for MUDA.
Our method is based on aligning the sources and target distributions indirectly
via internally learned distributions, without communicating data samples
between domains. We justify our approach theoretically and perform extensive
experiments to demonstrate that our method is effective and compares favorably
against existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stan_S/0/1/0/all/0/1"&gt;Serban Stan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1"&gt;Mohammad Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S$^2$-MLP: Spatial-Shift MLP Architecture for Vision. (arXiv:2106.07477v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07477</id>
        <link href="http://arxiv.org/abs/2106.07477"/>
        <updated>2021-06-24T01:51:43.676Z</updated>
        <summary type="html"><![CDATA[Recently, visual Transformer (ViT) and its following works abandon the
convolution and exploit the self-attention operation, attaining a comparable or
even higher accuracy than CNNs. More recently, MLP-Mixer abandons both the
convolution and the self-attention operation, proposing an architecture
containing only MLP layers. To achieve cross-patch communications, it devises
an additional token-mixing MLP besides the channel-mixing MLP. It achieves
promising results when training on an extremely large-scale dataset. But it
cannot achieve as outstanding performance as its CNN and ViT counterparts when
training on medium-scale datasets such as ImageNet1K and ImageNet21K. The
performance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We
discover that the token-mixing MLP is a variant of the depthwise convolution
with a global reception field and spatial-specific configuration. But the
global reception field and the spatial-specific property make token-mixing MLP
prone to over-fitting. In this paper, we propose a novel pure MLP architecture,
spatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only
contains channel-mixing MLP. We utilize a spatial-shift operation for
communications between patches. It has a local reception field and is
spatial-agnostic. It is parameter-free and efficient for computation. The
proposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when
training on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent
performance as ViT on ImageNet-1K dataset with considerably simpler
architecture and fewer FLOPs and parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yunfeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Mingming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Ping Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[First Step Towards EXPLAINable DGA Multiclass Classification. (arXiv:2106.12336v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.12336</id>
        <link href="http://arxiv.org/abs/2106.12336"/>
        <updated>2021-06-24T01:51:43.669Z</updated>
        <summary type="html"><![CDATA[Numerous malware families rely on domain generation algorithms (DGAs) to
establish a connection to their command and control (C2) server. Counteracting
DGAs, several machine learning classifiers have been proposed enabling the
identification of the DGA that generated a specific domain name and thus
triggering targeted remediation measures. However, the proposed
state-of-the-art classifiers are based on deep learning models. The black box
nature of these makes it difficult to evaluate their reasoning. The resulting
lack of confidence makes the utilization of such models impracticable. In this
paper, we propose EXPLAIN, a feature-based and contextless DGA multiclass
classifier. We comparatively evaluate several combinations of feature sets and
hyperparameters for our approach against several state-of-the-art classifiers
in a unified setting on the same real-world data. Our classifier achieves
competitive results, is real-time capable, and its predictions are easier to
trace back to features than the predictions made by the DGA multiclass
classifiers proposed in related work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drichel_A/0/1/0/all/0/1"&gt;Arthur Drichel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faerber_N/0/1/0/all/0/1"&gt;Nils Faerber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_U/0/1/0/all/0/1"&gt;Ulrike Meyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Textural Bias Improves Robustness of Deep Segmentation Models. (arXiv:2011.15093v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15093</id>
        <link href="http://arxiv.org/abs/2011.15093"/>
        <updated>2021-06-24T01:51:43.661Z</updated>
        <summary type="html"><![CDATA[Despite advances in deep learning, robustness under domain shift remains a
major bottleneck in medical imaging settings. Findings on natural images
suggest that deep neural models can show a strong textural bias when carrying
out image classification tasks. In this thorough empirical study, we draw
inspiration from findings on natural images and investigate ways in which
addressing the textural bias phenomenon could bring up the robustness of deep
segmentation models when applied to three-dimensional (3D) medical data. To
achieve this, publicly available MRI scans from the Developing Human Connectome
Project are used to study ways in which simulating textural noise can help
train robust models in a complex semantic segmentation task. We contribute an
extensive empirical investigation consisting of 176 experiments and illustrate
how applying specific types of simulated textural noise prior to training can
lead to texture invariant models, resulting in improved robustness when
segmenting scans corrupted by previously unseen noise types and levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chai_S/0/1/0/all/0/1"&gt;Seoin Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fetit_A/0/1/0/all/0/1"&gt;Ahmed E. Fetit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12096</id>
        <link href="http://arxiv.org/abs/2106.12096"/>
        <updated>2021-06-24T01:51:43.652Z</updated>
        <summary type="html"><![CDATA[Many machine learning techniques incorporate identity-preserving
transformations into their models to generalize their performance to previously
unseen data. These transformations are typically selected from a set of
functions that are known to maintain the identity of an input when applied
(e.g., rotation, translation, flipping, and scaling). However, there are many
natural variations that cannot be labeled for supervision or defined through
examination of the data. As suggested by the manifold hypothesis, many of these
natural variations live on or near a low-dimensional, nonlinear manifold.
Several techniques represent manifold variations through a set of learned Lie
group operators that define directions of motion on the manifold. However
theses approaches are limited because they require transformation labels when
training their models and they lack a method for determining which regions of
the manifold are appropriate for applying each specific operator. We address
these limitations by introducing a learning strategy that does not require
transformation labels and developing a method that learns the local regions
where each operator is likely to be used while preserving the identity of
inputs. Experiments on MNIST and Fashion MNIST highlight our model's ability to
learn identity-preserving transformations on multi-class datasets.
Additionally, we train on CelebA to showcase our model's ability to learn
semantically meaningful transformations on complex datasets in an unsupervised
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Connor_M/0/1/0/all/0/1"&gt;Marissa Connor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallah_K/0/1/0/all/0/1"&gt;Kion Fallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rozell_C/0/1/0/all/0/1"&gt;Christopher Rozell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-06-24T01:51:43.645Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Practical & Unified Notation for Information-Theoretic Quantities in ML. (arXiv:2106.12062v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12062</id>
        <link href="http://arxiv.org/abs/2106.12062"/>
        <updated>2021-06-24T01:51:43.618Z</updated>
        <summary type="html"><![CDATA[Information theory is of importance to machine learning, but the notation for
information-theoretic quantities is sometimes opaque. The right notation can
convey valuable intuitions and concisely express new ideas. We propose such a
notation for machine learning users and expand it to include
information-theoretic quantities between events (outcomes) and random
variables. We apply this notation to a popular information-theoretic
acquisition function in Bayesian active learning which selects the most
informative (unlabelled) samples to be labelled by an expert. We demonstrate
the value of our notation when extending the acquisition function to the
core-set problem, which consists of selecting the most informative samples
\emph{given} the labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Canonical Correlation Analysis to Self-supervised Graph Neural Networks. (arXiv:2106.12484v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12484</id>
        <link href="http://arxiv.org/abs/2106.12484"/>
        <updated>2021-06-24T01:51:43.607Z</updated>
        <summary type="html"><![CDATA[We introduce a conceptually simple yet effective model for self-supervised
representation learning with graph data. It follows the previous methods that
generate two views of an input graph through data augmentation. However, unlike
contrastive methods that focus on instance-level discrimination, we optimize an
innovative feature-level objective inspired by classical Canonical Correlation
Analysis. Compared with other works, our approach requires none of the
parameterized mutual information estimator, additional projector, asymmetric
structures, and most importantly, negative samples which can be costly. We show
that the new objective essentially 1) aims at discarding augmentation-variant
information by learning invariant representations, and 2) can prevent
degenerated solutions by decorrelating features in different dimensions. Our
theoretical analysis further provides an understanding for the new objective
which can be equivalently seen as an instantiation of the Information
Bottleneck Principle under the self-supervised setting. Despite its simplicity,
our method performs competitively on seven public graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hengrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qitian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Consistency of Deep Convolutional Neural Networks. (arXiv:2106.12498v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12498</id>
        <link href="http://arxiv.org/abs/2106.12498"/>
        <updated>2021-06-24T01:51:43.593Z</updated>
        <summary type="html"><![CDATA[Compared with avid research activities of deep convolutional neural networks
(DCNNs) in practice, the study of theoretical behaviors of DCNNs lags heavily
behind. In particular, the universal consistency of DCNNs remains open. In this
paper, we prove that implementing empirical risk minimization on DCNNs with
expansive convolution (with zero-padding) is strongly universally consistent.
Motivated by the universal consistency, we conduct a series of experiments to
show that without any fully connected layers, DCNNs with expansive convolution
perform not worse than the widely used deep neural networks with hybrid
structure containing contracting (without zero-padding) convolution layers and
several fully connected layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shao-Bo Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaidong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Ding-Xuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-06-24T01:51:43.588Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pretrained vision models such as VGG16 and ResNet

Index Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain
Adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HAWQV3: Dyadic Neural Network Quantization. (arXiv:2011.10680v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10680</id>
        <link href="http://arxiv.org/abs/2011.10680"/>
        <updated>2021-06-24T01:51:43.582Z</updated>
        <summary type="html"><![CDATA[Current low-precision quantization algorithms often have the hidden cost of
conversion back and forth from floating point to quantized integer values. This
hidden cost limits the latency improvement realized by quantizing Neural
Networks. To address this, we present HAWQV3, a novel mixed-precision
integer-only quantization framework. The contributions of HAWQV3 are the
following: (i) An integer-only inference where the entire computational graph
is performed only with integer multiplication, addition, and bit shifting,
without any floating point operations or even integer division; (ii) A novel
hardware-aware mixed-precision quantization method where the bit-precision is
calculated by solving an integer linear programming problem that balances the
trade-off between model perturbation and other constraints, e.g., memory
footprint and latency; (iii) Direct hardware deployment and open source
contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving
an average speed up of $1.45\times$ for uniform 4-bit, as compared to uniform
8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed
methods on ResNet18/50 and InceptionV3, for various model compression levels
with/without mixed precision. For ResNet50, our INT8 quantization achieves an
accuracy of $77.58\%$, which is $2.68\%$ higher than prior integer-only work,
and our mixed-precision INT4/8 quantization can reduce INT8 latency by $23\%$
and still achieve $76.73\%$ accuracy. Our framework and the TVM implementation
have been open sourced.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhangcheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jiali Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_E/0/1/0/all/0/1"&gt;Eric Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Leyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qijing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[groupShapley: Efficient prediction explanation with Shapley values for feature groups. (arXiv:2106.12228v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12228</id>
        <link href="http://arxiv.org/abs/2106.12228"/>
        <updated>2021-06-24T01:51:43.567Z</updated>
        <summary type="html"><![CDATA[Shapley values has established itself as one of the most appropriate and
theoretically sound frameworks for explaining predictions from complex machine
learning models. The popularity of Shapley values in the explanation setting is
probably due to its unique theoretical properties. The main drawback with
Shapley values, however, is that its computational complexity grows
exponentially in the number of input features, making it unfeasible in many
real world situations where there could be hundreds or thousands of features.
Furthermore, with many (dependent) features, presenting/visualizing and
interpreting the computed Shapley values also becomes challenging. The present
paper introduces groupShapley: a conceptually simple approach for dealing with
the aforementioned bottlenecks. The idea is to group the features, for example
by type or dependence, and then compute and present Shapley values for these
groups instead of for all individual features. Reducing hundreds or thousands
of features to half a dozen or so, makes precise computations practically
feasible and the presentation and knowledge extraction greatly simplified. We
prove that under certain conditions, groupShapley is equivalent to summing the
feature-wise Shapley values within each feature group. Moreover, we provide a
simulation study exemplifying the differences when these conditions are not
met. We illustrate the usability of the approach in a real world car insurance
example, where groupShapley is used to provide simple and intuitive
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1"&gt;Martin Jullum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Redelmeier_A/0/1/0/all/0/1"&gt;Annabelle Redelmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aas_K/0/1/0/all/0/1"&gt;Kjersti Aas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12497</id>
        <link href="http://arxiv.org/abs/2106.12497"/>
        <updated>2021-06-24T01:51:43.562Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled and unseen target domain, which is
usually trained on data from both domains. Access to the source domain data at
the adaptation stage, however, is often limited, due to data storage or privacy
issues. To alleviate this, in this work, we target source free UDA for
segmentation, and propose to adapt an ``off-the-shelf" segmentation model
pre-trained in the source domain to the target domain, with an adaptive
batch-wise normalization statistics adaptation framework. Specifically, the
domain-specific low-order batch statistics, i.e., mean and variance, are
gradually adapted with an exponential momentum decay scheme, while the
consistency of domain shareable high-order batch statistics, i.e., scaling and
shifting parameters, is explicitly enforced by our optimization objective. The
transferability of each channel is adaptively measured first from which to
balance the contribution of each channel. Moreover, the proposed source free
UDA framework is orthogonal to unsupervised learning methods, e.g.,
self-entropy minimization, which can thus be simply added on top of our
framework. Extensive experiments on the BraTS 2018 database show that our
source free UDA framework outperformed existing source-relaxed UDA methods for
the cross-subtype UDA segmentation task and yielded comparable results for the
cross-modality UDA segmentation task, compared with a supervised UDA methods
with the source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Kernel and Neural Bandits. (arXiv:2106.12034v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12034</id>
        <link href="http://arxiv.org/abs/2106.12034"/>
        <updated>2021-06-24T01:51:43.556Z</updated>
        <summary type="html"><![CDATA[We study pure exploration in bandits, where the dimension of the feature
representation can be much larger than the number of arms. To overcome the
curse of dimensionality, we propose to adaptively embed the feature
representation of each arm into a lower-dimensional space and carefully deal
with the induced model misspecifications. Our approach is conceptually very
different from existing works that can either only handle low-dimensional
linear bandits or passively deal with model misspecifications. We showcase the
application of our approach to two pure exploration settings that were
previously under-studied: (1) the reward function belongs to a possibly
infinite-dimensional Reproducing Kernel Hilbert Space, and (2) the reward
function is nonlinear and can be approximated by neural networks. Our main
results provide sample complexity guarantees that only depend on the effective
dimension of the feature spaces in the kernel or neural representations.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate the efficacy of our methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ruoxi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12174</id>
        <link href="http://arxiv.org/abs/2106.12174"/>
        <updated>2021-06-24T01:51:43.551Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are transforming the world, as well as our healthcare
system. We propose a deep learning-based cough sound classification model that
can distinguish between children with healthy versus pathological coughs such
as asthma, upper respiratory tract infection (URTI), and lower respiratory
tract infection (LRTI). In order to train a deep neural network model, we
collected a new dataset of cough sounds, labelled with clinician's diagnosis.
The chosen model is a bidirectional long-short term memory network (BiLSTM)
based on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting
trained model when trained for classifying two classes of coughs -- healthy or
pathology (in general or belonging to a specific respiratory pathology),
reaches accuracy exceeding 84\% when classifying cough to the label provided by
the physicians' diagnosis. In order to classify subject's respiratory pathology
condition, results of multiple cough epochs per subject were combined. The
resulting prediction accuracy exceeds 91\% for all three respiratory
pathologies. However, when the model is trained to classify and discriminate
among the four classes of coughs, overall accuracy dropped: one class of
pathological coughs are often misclassified as other. However, if one consider
the healthy cough classified as healthy and pathological cough classified to
have some kind of pathologies, then the overall accuracy of four class model is
above 84\%. A longitudinal study of MFCC feature space when comparing
pathological and recovered coughs collected from the same subjects revealed the
fact that pathological cough irrespective of the underlying conditions occupy
the same feature space making it harder to differentiate only using MFCC
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1"&gt;Balamurali B T&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1"&gt;Hwan Ing Hee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Saumitra Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1"&gt;Oon Hoe Teoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Sung Shin Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Khai Pin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jer Ming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving. (arXiv:2106.12194v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12194</id>
        <link href="http://arxiv.org/abs/2106.12194"/>
        <updated>2021-06-24T01:51:43.546Z</updated>
        <summary type="html"><![CDATA[To further improve the learning efficiency and performance of reinforcement
learning (RL), in this paper we propose a novel uncertainty-aware model-based
RL (UA-MBRL) framework, and then implement and validate it in autonomous
driving under various task scenarios. First, an action-conditioned ensemble
model with the ability of uncertainty assessment is established as the virtual
environment model. Then, a novel uncertainty-aware model-based RL framework is
developed based on the adaptive truncation approach, providing virtual
interactions between the agent and environment model, and improving RL's
training efficiency and performance. The developed algorithms are then
implemented in end-to-end autonomous vehicle control tasks, validated and
compared with state-of-the-art methods under various driving scenarios. The
validation results suggest that the proposed UA-MBRL method surpasses the
existing model-based and model-free RL approaches, in terms of learning
efficiency and achieved performance. The results also demonstrate the good
ability of the proposed method with respect to the adaptiveness and robustness,
under various autonomous driving scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jingda Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1"&gt;Chen Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models. (arXiv:2106.12248v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.12248</id>
        <link href="http://arxiv.org/abs/2106.12248"/>
        <updated>2021-06-24T01:51:43.530Z</updated>
        <summary type="html"><![CDATA[Frequently, population studies feature pyramidally-organized data represented
using Hierarchical Bayesian Models (HBM) enriched with plates. These models can
become prohibitively large in settings such as neuroimaging, where a sample is
composed of a functional MRI signal measured on 64 thousand brain locations,
across 4 measurement sessions, and at least tens of subjects. Even a reduced
example on a specific cortical region of 300 brain locations features around 1
million parameters, hampering the usage of modern density estimation techniques
such as Simulation-Based Inference (SBI). To infer parameter posterior
distributions in this challenging class of problems, we designed a novel
methodology that automatically produces a variational family dual to a target
HBM. This variatonal family, represented as a neural network, consists in the
combination of an attention-based hierarchical encoder feeding summary
statistics to a set of normalizing flows. Our automatically-derived neural
network exploits exchangeability in the plate-enriched HBM and factorizes its
parameter space. The resulting architecture reduces by orders of magnitude its
parameterization with respect to that of a typical SBI representation, while
maintaining expressivity. Our method performs inference on the specified HBM in
an amortized setup: once trained, it can readily be applied to a new data
sample to compute the parameters' full posterior. We demonstrate the capability
of our method on simulated data, as well as a challenging high-dimensional
brain parcellation experiment. We also open up several questions that lie at
the intersection between SBI techniques and structured Variational Inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rouillard_L/0/1/0/all/0/1"&gt;Louis Rouillard&lt;/a&gt; (PARIETAL, Inria, CEA), &lt;a href="http://arxiv.org/find/cs/1/au:+Wassermann_D/0/1/0/all/0/1"&gt;Demian Wassermann&lt;/a&gt; (PARIETAL, Inria, CEA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Adversarial Neural Architecture Search. (arXiv:2105.09356v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09356</id>
        <link href="http://arxiv.org/abs/2105.09356"/>
        <updated>2021-06-24T01:51:43.523Z</updated>
        <summary type="html"><![CDATA[Despite the empirical success of neural architecture search (NAS) in deep
learning applications, the optimality, reproducibility and cost of NAS schemes
remain hard to assess. In this paper, we propose Generative Adversarial NAS
(GA-NAS) with theoretically provable convergence guarantees, promoting
stability and reproducibility in neural architecture search. Inspired by
importance sampling, GA-NAS iteratively fits a generator to previously
discovered top architectures, thus increasingly focusing on important parts of
a large search space. Furthermore, we propose an efficient adversarial learning
approach, where the generator is trained by reinforcement learning based on
rewards provided by a discriminator, thus being able to explore the search
space without evaluating a large number of architectures. Extensive experiments
show that GA-NAS beats the best published results under several cases on three
public NAS benchmarks. In the meantime, GA-NAS can handle ad-hoc search
constraints and search spaces. We show that GA-NAS can be used to improve
already optimized baselines found by other NAS methods, including EfficientNet
and ProxylessNAS, in terms of ImageNet accuracy or the number of parameters, in
their original search space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Seyed Saeed Changiz Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fred X. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salameh_M/0/1/0/all/0/1"&gt;Mohammad Salameh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1"&gt;Keith Mills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1"&gt;Shuo Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Biometric Identification of Sea Turtles (Chelonia mydas). (arXiv:1909.11277v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11277</id>
        <link href="http://arxiv.org/abs/1909.11277"/>
        <updated>2021-06-24T01:51:43.517Z</updated>
        <summary type="html"><![CDATA[Passive biometric identification enables wildlife monitoring with minimal
disturbance. Using a motion-activated camera placed at an elevated position and
facing downwards, we collected images of sea turtle carapace, each belonging to
one of sixteen Chelonia mydas juveniles. We then learned co-variant and robust
image descriptors from these images, enabling indexing and retrieval. In this
work, we presented several classification results of sea turtle carapaces using
the learned image descriptors. We found that a template-based descriptor, i.e.,
Histogram of Oriented Gradients (HOG) performed exceedingly better during
classification than keypoint-based descriptors. For our dataset, a
high-dimensional descriptor is a must due to the minimal gradient and color
information inside the carapace images. Using HOG, we obtained an average
classification accuracy of 65%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;Irwandi Hipiny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;Hamimah Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mujahid_A/0/1/0/all/0/1"&gt;Aazani Mujahid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yahya_N/0/1/0/all/0/1"&gt;Nurhartini Kamalia Yahya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Baseline for Batch Active Learning with Stochastic Acquisition Functions. (arXiv:2106.12059v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12059</id>
        <link href="http://arxiv.org/abs/2106.12059"/>
        <updated>2021-06-24T01:51:43.512Z</updated>
        <summary type="html"><![CDATA[In active learning, new labels are commonly acquired in batches. However,
common acquisition functions are only meant for one-sample acquisition rounds
at a time, and when their scores are used naively for batch acquisition, they
result in batches lacking diversity, which deteriorates performance. On the
other hand, state-of-the-art batch acquisition functions are costly to compute.
In this paper, we present a novel class of stochastic acquisition functions
that extend one-sample acquisition functions to the batch setting by observing
how one-sample acquisition scores change as additional samples are acquired and
modelling this difference for additional batch samples. We simply acquire new
samples by sampling from the pool set using a Gibbs distribution based on the
acquisition scores. Our acquisition functions are both vastly cheaper to
compute and out-perform other batch acquisition functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1"&gt;Sebastian Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Rate of Convergence of Variation-Constrained Deep Neural Networks. (arXiv:2106.12068v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12068</id>
        <link href="http://arxiv.org/abs/2106.12068"/>
        <updated>2021-06-24T01:51:43.507Z</updated>
        <summary type="html"><![CDATA[Multi-layer feedforward networks have been used to approximate a wide range
of nonlinear functions. An important and fundamental problem is to understand
the learnability of a network model through its statistical risk, or the
expected prediction error on future data. To the best of our knowledge, the
rate of convergence of neural networks shown by existing works is bounded by at
most the order of $n^{-1/4}$ for a sample size of $n$. In this paper, we show
that a class of variation-constrained neural networks, with arbitrary width,
can achieve near-parametric rate $n^{-1/2+\delta}$ for an arbitrarily small
positive constant $\delta$. It is equivalent to $n^{-1 +2\delta}$ under the
mean squared error. This rate is also observed by numerical experiments. The
result indicates that the neural function space needed for approximating smooth
functions may not be as large as what is often perceived. Our result also
provides insight to the phenomena that deep neural networks do not easily
suffer from overfitting when the number of neurons and learning parameters
rapidly grow with $n$ or even surpass $n$. We also discuss the rate of
convergence regarding other network parameters, including the input dimension,
network layer, and coefficient norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuantao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ParK: Sound and Efficient Kernel Ridge Regression by Feature Space Partitions. (arXiv:2106.12231v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.12231</id>
        <link href="http://arxiv.org/abs/2106.12231"/>
        <updated>2021-06-24T01:51:43.491Z</updated>
        <summary type="html"><![CDATA[We introduce ParK, a new large-scale solver for kernel ridge regression. Our
approach combines partitioning with random projections and iterative
optimization to reduce space and time complexity while provably maintaining the
same statistical accuracy. In particular, constructing suitable partitions
directly in the feature space rather than in the input space, we promote
orthogonality between the local estimators, thus ensuring that key quantities
such as local effective dimension and bias remain under control. We
characterize the statistical-computational tradeoff of our model, and
demonstrate the effectiveness of our method by numerical experiments on
large-scale datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Carratino_L/0/1/0/all/0/1"&gt;Luigi Carratino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vigogna_S/0/1/0/all/0/1"&gt;Stefano Vigogna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1"&gt;Daniele Calandriello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rosasco_L/0/1/0/all/0/1"&gt;Lorenzo Rosasco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14300</id>
        <link href="http://arxiv.org/abs/2105.14300"/>
        <updated>2021-06-24T01:51:43.486Z</updated>
        <summary type="html"><![CDATA[Most existing Visual Question Answering (VQA) systems tend to overly rely on
language bias and hence fail to reason from the visual clue. To address this
issue, we propose a novel Language-Prior Feedback (LPF) objective function, to
re-balance the proportion of each answer's loss value in the total VQA loss.
The LPF firstly calculates a modulating factor to determine the language bias
using a question-only branch. Then, the LPF assigns a self-adaptive weight to
each training sample in the training process. With this reweighting mechanism,
the LPF ensures that the total VQA loss can be reshaped to a more balanced
form. By this means, the samples that require certain visual information to
predict will be efficiently used during training. Our method is simple to
implement, model-agnostic, and end-to-end trainable. We conduct extensive
experiments and the results show that the LPF (1) brings a significant
improvement over various VQA models, (2) achieves competitive performance on
the bias-sensitive VQA-CP v2 benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haifeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiaying Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Permuted AdaIN: Reducing the Bias Towards Global Statistics in Image Classification. (arXiv:2010.05785v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05785</id>
        <link href="http://arxiv.org/abs/2010.05785"/>
        <updated>2021-06-24T01:51:43.481Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that convolutional neural network classifiers overly
rely on texture at the expense of shape cues. We make a similar but different
distinction between shape and local image cues, on the one hand, and global
image statistics, on the other. Our method, called Permuted Adaptive Instance
Normalization (pAdaIN), reduces the representation of global statistics in the
hidden layers of image classifiers. pAdaIN samples a random permutation $\pi$
that rearranges the samples in a given batch. Adaptive Instance Normalization
(AdaIN) is then applied between the activations of each (non-permuted) sample
$i$ and the corresponding activations of the sample $\pi(i)$, thus swapping
statistics between the samples of the batch. Since the global image statistics
are distorted, this swapping procedure causes the network to rely on cues, such
as shape or texture. By choosing the random permutation with probability $p$
and the identity permutation otherwise, one can control the effect's strength.

With the correct choice of $p$, fixed apriori for all experiments and
selected without considering test data, our method consistently outperforms
baselines in multiple settings. In image classification, our method improves on
both CIFAR100 and ImageNet using multiple architectures. In the setting of
robustness, our method improves on both ImageNet-C and Cifar-100-C for multiple
architectures. In the setting of domain adaptation and domain generalization,
our method achieves state of the art results on the transfer learning task from
GTAV to Cityscapes and on the PACS benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nuriel_O/0/1/0/all/0/1"&gt;Oren Nuriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1"&gt;Sagie Benaim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Gaussian Processes: A Survey. (arXiv:2106.12135v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12135</id>
        <link href="http://arxiv.org/abs/2106.12135"/>
        <updated>2021-06-24T01:51:43.474Z</updated>
        <summary type="html"><![CDATA[Gaussian processes are one of the dominant approaches in Bayesian learning.
Although the approach has been applied to numerous problems with great success,
it has a few fundamental limitations. Multiple methods in literature have
addressed these limitations. However, there has not been a comprehensive survey
of the topics as of yet. Most existing surveys focus on only one particular
variant of Gaussian processes and their derivatives. This survey details the
core motivations for using Gaussian processes, their mathematical formulations,
limitations, and research themes that have flourished over the years to address
said limitations. Furthermore, one particular research area is Deep Gaussian
Processes (DGPs), it has improved substantially in the past decade. The
significant publications that advanced the forefront of this research area are
outlined in their survey. Finally, a brief discussion on open problems and
research directions for future work is presented at the end.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jakkala_K/0/1/0/all/0/1"&gt;Kalvik Jakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchEmbedNet: Learning Novel Concepts by Imitating Drawings. (arXiv:2009.04806v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04806</id>
        <link href="http://arxiv.org/abs/2009.04806"/>
        <updated>2021-06-24T01:51:43.468Z</updated>
        <summary type="html"><![CDATA[Sketch drawings capture the salient information of visual concepts. Previous
work has shown that neural networks are capable of producing sketches of
natural objects drawn from a small number of classes. While earlier approaches
focus on generation quality or retrieval, we explore properties of image
representations learned by training a model to produce sketches of images. We
show that this generative, class-agnostic model produces informative embeddings
of images from novel examples, classes, and even novel datasets in a few-shot
setting. Additionally, we find that these learned representations exhibit
interesting structure and compositionality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Alexander Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mengye Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard S. Zemel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases. (arXiv:2104.09123v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09123</id>
        <link href="http://arxiv.org/abs/2104.09123"/>
        <updated>2021-06-24T01:51:43.462Z</updated>
        <summary type="html"><![CDATA[While common image object detection tasks focus on bounding boxes or
segmentation masks as object representations, we consider the problem of
finding objects based on four arbitrary vertices. We propose a novel model,
named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet
and uses similar algorithms and ideas. It is designated for applications
requiring high-accuracy detection of regularly shaped objects, which is the
case in the logistics use-case of packaging structure recognition. We evaluate
our model on our specific real-world dataset for this use-case. Baselined
against a previous solution, consisting of a Mask R-CNN model and suitable
post-processing steps, TetraPackNet achieves superior results (9% higher in
accuracy) in the sub-task of four-corner based transport unit side detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dorr_L/0/1/0/all/0/1"&gt;Laura D&amp;#xf6;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brandt_F/0/1/0/all/0/1"&gt;Felix Brandt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumann_A/0/1/0/all/0/1"&gt;Alexander Naumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouls_M/0/1/0/all/0/1"&gt;Martin Pouls&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ShaRF: Shape-conditioned Radiance Fields from a Single View. (arXiv:2102.08860v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08860</id>
        <link href="http://arxiv.org/abs/2102.08860"/>
        <updated>2021-06-24T01:51:43.447Z</updated>
        <summary type="html"><![CDATA[We present a method for estimating neural scenes representations of objects
given only a single image. The core of our method is the estimation of a
geometric scaffold for the object and its use as a guide for the reconstruction
of the underlying radiance field. Our formulation is based on a generative
process that first maps a latent code to a voxelized shape, and then renders it
to an image, with the object appearance being controlled by a second latent
code. During inference, we optimize both the latent codes and the networks to
fit a test image of a new object. The explicit disentanglement of shape and
appearance allows our model to be fine-tuned given a single image. We can then
render new views in a geometrically consistent manner and they represent
faithfully the input object. Additionally, our method is able to generalize to
images outside of the training domain (more realistic renderings and even real
photographs). Finally, the inferred geometric scaffold is itself an accurate
estimate of the object's 3D shape. We demonstrate in several experiments the
effectiveness of our approach in both synthetic and real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1"&gt;Konstantinos Rematas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1"&gt;Ricardo Martin-Brualla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Linear Regression under Distribution Shift. (arXiv:2106.12108v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12108</id>
        <link href="http://arxiv.org/abs/2106.12108"/>
        <updated>2021-06-24T01:51:43.442Z</updated>
        <summary type="html"><![CDATA[Transfer learning is essential when sufficient data comes from the source
domain, with scarce labeled data from the target domain. We develop estimators
that achieve minimax linear risk for linear regression problems under
distribution shift. Our algorithms cover different transfer learning settings
including covariate shift and model shift. We also consider when data are
generated from either linear or general nonlinear models. We show that linear
minimax estimators are within an absolute constant of the minimax risk even
among nonlinear estimators for various source/target distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12534</id>
        <link href="http://arxiv.org/abs/2106.12534"/>
        <updated>2021-06-24T01:51:43.435Z</updated>
        <summary type="html"><![CDATA[Reflecting on the last few years, the biggest breakthroughs in deep
reinforcement learning (RL) have been in the discrete action domain. Robotic
manipulation, however, is inherently a continuous control environment, but
these continuous control reinforcement learning algorithms often depend on
actor-critic methods that are sample-inefficient and inherently difficult to
train, due to the joint optimisation of the actor and critic. To that end, we
explore how we can bring the stability of discrete action RL algorithms to the
robot manipulation domain. We extend the recently released ARM algorithm, by
replacing the continuous next-best pose agent with a discrete next-best pose
agent. Discretisation of rotation is trivial given its bounded nature, while
translation is inherently unbounded, making discretisation difficult. We
formulate the translation prediction as the voxel prediction problem by
discretising the 3D space; however, voxelisation of a large workspace is memory
intensive and would not work with a high density of voxels, crucial to
obtaining the resolution needed for robotic manipulation. We therefore propose
to apply this voxel prediction in a coarse-to-fine manner by gradually
increasing the resolution. In each step, we extract the highest valued voxel as
the predicted location, which is then used as the centre of the
higher-resolution voxelisation in the next step. This coarse-to-fine prediction
is applied over several steps, giving a near-lossless prediction of the
translation. We show that our new coarse-to-fine algorithm is able to
accomplish RLBench tasks much more efficiently than the continuous control
equivalent, and even train some real-world tasks, tabular rasa, in less than 7
minutes, with only 3 demonstrations. Moreover, we show that by moving to a
voxel representation, we are able to easily incorporate observations from
multiple cameras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1"&gt;Stephen James&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wada_K/0/1/0/all/0/1"&gt;Kentaro Wada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1"&gt;Tristan Laidlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1"&gt;Andrew J. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking supervised learning: insights from biological learning and from calling it by its name. (arXiv:2012.02526v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02526</id>
        <link href="http://arxiv.org/abs/2012.02526"/>
        <updated>2021-06-24T01:51:43.429Z</updated>
        <summary type="html"><![CDATA[The renaissance of artificial neural networks was catalysed by the success of
classification models, tagged by the community with the broader term supervised
learning. The extraordinary results gave rise to a hype loaded with ambitious
promises and overstatements. Soon the community realised that the success owed
much to the availability of thousands of labelled examples and supervised
learning went, for many, from glory to shame: Some criticised deep learning as
a whole and others proclaimed that the way forward had to be alternatives to
supervised learning: predictive, unsupervised, semi-supervised and, more
recently, self-supervised learning. However, all these seem brand names, rather
than actual categories of a theoretically grounded taxonomy. Moreover, the call
to banish supervised learning was motivated by the questionable claim that
humans learn with little or no supervision and are capable of robust
out-of-distribution generalisation. Here, we review insights about learning and
supervision in nature, revisit the notion that learning and generalisation are
not possible without supervision or inductive biases and argue that we will
make better progress if we just call it by its name.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_A/0/1/0/all/0/1"&gt;Alex Hernandez-Garcia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment. (arXiv:2012.13936v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13936</id>
        <link href="http://arxiv.org/abs/2012.13936"/>
        <updated>2021-06-24T01:51:43.422Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a no-reference video quality assessment method,
aiming to achieve high-generalization capability in cross-content, -resolution
and -frame rate quality prediction. In particular, we evaluate the quality of a
video by learning effective feature representations in spatial-temporal domain.
In the spatial domain, to tackle the resolution and content variations, we
impose the Gaussian distribution constraints on the quality features. The
unified distribution can significantly reduce the domain gap between different
video samples, resulting in a more generalized quality feature representation.
Along the temporal dimension, inspired by the mechanism of visual perception,
we propose a pyramid temporal aggregation module by involving the short-term
and long-term memory to aggregate the frame-level quality. Experiments show
that our method outperforms the state-of-the-art methods on cross-dataset
settings, and achieves comparable performance on intra-dataset configurations,
demonstrating the high-generalization capability of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoliang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lingyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongfei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shiqi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-resolution Outlier Pooling for Sorghum Classification. (arXiv:2106.05748v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05748</id>
        <link href="http://arxiv.org/abs/2106.05748"/>
        <updated>2021-06-24T01:51:43.406Z</updated>
        <summary type="html"><![CDATA[Automated high throughput plant phenotyping involves leveraging sensors, such
as RGB, thermal and hyperspectral cameras (among others), to make large scale
and rapid measurements of the physical properties of plants for the purpose of
better understanding the difference between crops and facilitating rapid plant
breeding programs. One of the most basic phenotyping tasks is to determine the
cultivar, or species, in a particular sensor product. This simple phenotype can
be used to detect errors in planting and to learn the most differentiating
features between cultivars. It is also a challenging visual recognition task,
as a large number of highly related crops are grown simultaneously, leading to
a classification problem with low inter-class variance. In this paper, we
introduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum
captured by a state-of-the-art gantry system, a multi-resolution network
architecture that learns both global and fine-grained features on the crops,
and a new global pooling strategy called Dynamic Outlier Pooling which
outperforms standard global pooling strategies on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chao Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1"&gt;Justin Dulay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rolwes_G/0/1/0/all/0/1"&gt;Gregory Rolwes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauli_D/0/1/0/all/0/1"&gt;Duke Pauli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakoor_N/0/1/0/all/0/1"&gt;Nadia Shakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stylianou_A/0/1/0/all/0/1"&gt;Abby Stylianou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Human Perception to Improve Handwritten Document Transcription. (arXiv:1904.03734v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.03734</id>
        <link href="http://arxiv.org/abs/1904.03734"/>
        <updated>2021-06-24T01:51:43.401Z</updated>
        <summary type="html"><![CDATA[The subtleties of human perception, as measured by vision scientists through
the use of psychophysics, are important clues to the internal workings of
visual recognition. For instance, measured reaction time can indicate whether a
visual stimulus is easy for a subject to recognize, or whether it is hard. In
this paper, we consider how to incorporate psychophysical measurements of
visual perception into the loss function of a deep neural network being trained
for a recognition task, under the assumption that such information can enforce
consistency with human behavior. As a case study to assess the viability of
this approach, we look at the problem of handwritten document transcription.
While good progress has been made towards automatically transcribing modern
handwriting, significant challenges remain in transcribing historical
documents. Here we describe a general enhancement strategy, underpinned by the
new loss formulation, which can be applied to the training regime of any deep
learning-based document transcription system. Through experimentation, reliable
performance improvement is demonstrated for the standard IAM and RIMES datasets
for three different network architectures. Further, we go on to show
feasibility for our approach on a new dataset of digitized Latin manuscripts,
originally produced by scribes in the Cloister of St. Gall in the the 9th
century.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grieggs_S/0/1/0/all/0/1"&gt;Samuel Grieggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bingyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauch_G/0/1/0/all/0/1"&gt;Greta Rauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1"&gt;David Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Brian Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1"&gt;Walter J. Scheirer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Quanvolutional Neural Networks with enhanced image encoding. (arXiv:2106.07327v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07327</id>
        <link href="http://arxiv.org/abs/2106.07327"/>
        <updated>2021-06-24T01:51:43.394Z</updated>
        <summary type="html"><![CDATA[Image classification is an important task in various machine learning
applications. In recent years, a number of classification methods based on
quantum machine learning and different quantum image encoding techniques have
been proposed. In this paper, we study the effect of three different quantum
image encoding approaches on the performance of a convolution-inspired hybrid
quantum-classical image classification algorithm called quanvolutional neural
network (QNN). We furthermore examine the effect of variational - i.e.
trainable - quantum circuits on the classification results. Our experiments
indicate that some image encodings are better suited for variational circuits.
However, our experiments show as well that there is not one best image
encoding, but that the choice of the encoding depends on the specific
constraints of the application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattern_D/0/1/0/all/0/1"&gt;Denny Mattern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martyniuk_D/0/1/0/all/0/1"&gt;Darya Martyniuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willems_H/0/1/0/all/0/1"&gt;Henri Willems&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmann_F/0/1/0/all/0/1"&gt;Fabian Bergmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1"&gt;Adrian Paschke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Flows with Invertible Attentions. (arXiv:2106.03959v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03959</id>
        <link href="http://arxiv.org/abs/2106.03959"/>
        <updated>2021-06-24T01:51:43.388Z</updated>
        <summary type="html"><![CDATA[Flow-based generative models have shown excellent ability to explicitly learn
the probability density function of data via a sequence of invertible
transformations. Yet, modeling long-range dependencies over normalizing flows
remains understudied. To fill the gap, in this paper, we introduce two types of
invertible attention mechanisms for generative flow models. To be precise, we
propose map-based and scaled dot-product attention for unconditional and
conditional generative flow models. The key idea is to exploit split-based
attention mechanisms to learn the attention weights and input representations
on every two splits of flow feature maps. Our method provides invertible
attention modules with tractable Jacobian determinants, enabling seamless
integration of it at any positions of the flow-based models. The proposed
attention mechanism can model the global data dependencies, leading to more
comprehensive flow models. Evaluation on multiple generation tasks demonstrates
that the introduced attention flow idea results in efficient flow models and
compares favorably against the state-of-the-art unconditional and conditional
generative flow methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1"&gt;Rhea Sanjay Sukthanker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiwu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Suryansh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Taming Transformers for High-Resolution Image Synthesis. (arXiv:2012.09841v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09841</id>
        <link href="http://arxiv.org/abs/2012.09841"/>
        <updated>2021-06-24T01:51:43.383Z</updated>
        <summary type="html"><![CDATA[Designed to learn long-range interactions on sequential data, transformers
continue to show state-of-the-art results on a wide variety of tasks. In
contrast to CNNs, they contain no inductive bias that prioritizes local
interactions. This makes them expressive, but also computationally infeasible
for long sequences, such as high-resolution images. We demonstrate how
combining the effectiveness of the inductive bias of CNNs with the expressivity
of transformers enables them to model and thereby synthesize high-resolution
images. We show how to (i) use CNNs to learn a context-rich vocabulary of image
constituents, and in turn (ii) utilize transformers to efficiently model their
composition within high-resolution images. Our approach is readily applied to
conditional synthesis tasks, where both non-spatial information, such as object
classes, and spatial information, such as segmentations, can control the
generated image. In particular, we present the first results on
semantically-guided synthesis of megapixel images with transformers and obtain
the state of the art among autoregressive models on class-conditional ImageNet.
Code and pretrained models can be found at
https://github.com/CompVis/taming-transformers .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esser_P/0/1/0/all/0/1"&gt;Patrick Esser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1"&gt;Robin Rombach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-24T01:51:43.367Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15076</id>
        <link href="http://arxiv.org/abs/2105.15076"/>
        <updated>2021-06-24T01:51:43.361Z</updated>
        <summary type="html"><![CDATA[Person re-identification (re-ID) in the scenario with large spatial and
temporal spans has not been fully explored. This is partially because that,
existing benchmark datasets were mainly collected with limited spatial and
temporal ranges, e.g., using videos recorded in a few days by cameras in a
specific region of the campus. Such limited spatial and temporal ranges make it
hard to simulate the difficulties of person re-ID in real scenarios. In this
work, we contribute a novel Large-scale Spatio-Temporal LaST person re-ID
dataset, including 10,862 identities with more than 228k images. Compared with
existing datasets, LaST presents more challenging and high-diversity re-ID
settings, and significantly larger spatial and temporal ranges. For instance,
each person can appear in different cities or countries, and in various time
slots from daytime to night, and in different seasons from spring to winter. To
our best knowledge, LaST is a novel person re-ID dataset with the largest
spatio-temporal ranges. Based on LaST, we verified its challenge by conducting
a comprehensive performance evaluation of 14 re-ID algorithms. We further
propose an easy-to-implement baseline that works well on such challenging re-ID
setting. We also verified that models pre-trained on LaST can generalize well
on existing datasets with short-term and cloth-changing scenarios. We expect
LaST to inspire future works toward more realistic and challenging re-ID tasks.
More information about the dataset is available at
https://github.com/shuxjweb/last.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test-Time Adaptation for Out-of-distributed Image Inpainting. (arXiv:2102.01360v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01360</id>
        <link href="http://arxiv.org/abs/2102.01360"/>
        <updated>2021-06-24T01:51:43.356Z</updated>
        <summary type="html"><![CDATA[Deep learning-based image inpainting algorithms have shown great performance
via powerful learned prior from the numerous external natural images. However,
they show unpleasant results on the test image whose distribution is far from
the that of training images because their models are biased toward the training
images. In this paper, we propose a simple image inpainting algorithm with
test-time adaptation named AdaFill. Given a single out-of-distributed test
image, our goal is to complete hole region more naturally than the pre-trained
inpainting models. To achieve this goal, we treat remained valid regions of the
test image as another training cues because natural images have strong internal
similarities. From this test-time adaptation, our network can exploit
externally learned image priors from the pre-trained features as well as the
internal prior of the test image explicitly. Experimental results show that
AdaFill outperforms other models on the various out-of-distribution test
images. Furthermore, the model named ZeroFill, that are not pre-trained also
sometimes outperforms the pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1"&gt;Chajin Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taeoh Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sangyoun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent Space Representations. (arXiv:2101.07280v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07280</id>
        <link href="http://arxiv.org/abs/2101.07280"/>
        <updated>2021-06-24T01:51:43.349Z</updated>
        <summary type="html"><![CDATA[Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has
a high miss rate due to a number of factors, including the geometry of the
colon (haustral fold and sharp bends occlusions), endoscopist inexperience or
fatigue, endoscope field of view, etc. We present a framework to visualize the
missed regions per-frame during the colonoscopy, and provides a workable
clinical solution. Specifically, we make use of 3D reconstructed virtual
colonoscopy (VC) data and the insight that VC and OC share the same underlying
geometry but differ in color, texture and specular reflections, embedded in the
OC domain. A lossy unpaired image-to-image translation model is introduced with
enforced shared latent space for OC and VC. This shared latent space captures
the geometric information while deferring the color, texture, and specular
information creation to additional Gaussian noise input. This additional noise
input can be utilized to generate one-to-many mappings from VC to OC and OC to
OC. The code, data and trained models will be released via our Computational
Endoscopy Platform at https://github.com/nadeemlab/CEP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1"&gt;Shawn Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1"&gt;Arie Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stronger NAS with Weaker Predictors. (arXiv:2102.10490v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10490</id>
        <link href="http://arxiv.org/abs/2102.10490"/>
        <updated>2021-06-24T01:51:43.337Z</updated>
        <summary type="html"><![CDATA[Neural Architecture Search (NAS) often trains and evaluates a large number of
architectures. Recent predictor-based NAS approaches attempt to address such
heavy computation costs with two key steps: sampling some
architecture-performance pairs and fitting a proxy accuracy predictor. Given
limited samples, these predictors, however, are far from accurate to locate top
architectures due to the difficulty of fitting the huge search space. This
paper reflects on a simple yet crucial question: if our final goal is to find
the best architecture, do we really need to model the whole space well?. We
propose a paradigm shift from fitting the whole architecture space using one
strong predictor, to progressively fitting a search path towards the
high-performance sub-space through a set of weaker predictors. As a key
property of the proposed weak predictors, their probabilities of sampling
better architectures keep increasing. Hence we only sample a few well-performed
architectures guided by the previously learned predictor and estimate a new
better weak predictor. This embarrassingly easy framework produces
coarse-to-fine iteration to refine the ranking of sampling space gradually.
Extensive experiments demonstrate that our method costs fewer samples to find
top-performance architectures on NAS-Bench-101 and NAS-Bench-201, as well as
achieves the state-of-the-art ImageNet performance on the NASNet search space.
In particular, compared to state-of-the-art (SOTA) predictor-based NAS methods,
WeakNAS outperforms all of them with notable margins, e.g., requiring at least
7.5x less samples to find global optimal on NAS-Bench-101; and WeakNAS can also
absorb them for further performance boost. We further strike the new SOTA
result of 81.3% in the ImageNet MobileNet Search Space. The code is available
at https://github.com/VITA-Group/WeakNAS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junru Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Ye Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VariTex: Variational Neural Face Textures. (arXiv:2104.05988v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05988</id>
        <link href="http://arxiv.org/abs/2104.05988"/>
        <updated>2021-06-24T01:51:43.321Z</updated>
        <summary type="html"><![CDATA[Deep generative models have recently demonstrated the ability to synthesize
photorealistic images of human faces with novel identities. A key challenge to
the wide applicability of such techniques is to provide independent control
over semantically meaningful parameters: appearance, head pose, face shape, and
facial expressions. In this paper, we propose VariTex - to the best of our
knowledge the first method that learns a variational latent feature space of
neural face textures, which allows sampling of novel identities. We combine
this generative model with a parametric face model and gain explicit control
over head pose and facial expressions. To generate images of complete human
heads, we propose an additive decoder that generates plausible additional
details such as hair. A novel training scheme enforces a pose independent
latent space and in consequence, allows learning of a one-to-many mapping
between latent codes and pose-conditioned exterior regions. The resulting
method can generate geometrically consistent images of novel identities
allowing fine-grained control over head pose, face shape, and facial
expressions, facilitating a broad range of downstream tasks, like sampling
novel identities, re-posing, expression transfer, and more.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1"&gt;Marcel C. B&amp;#xfc;hler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Meka_A/0/1/0/all/0/1"&gt;Abhimitra Meka&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gengyan Li&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Beeler_T/0/1/0/all/0/1"&gt;Thabo Beeler&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1"&gt;Otmar Hilliges&lt;/a&gt; (1) ((1) ETH Zurich, (2) Google)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Horizontal-to-Vertical Video Conversion. (arXiv:2101.04051v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04051</id>
        <link href="http://arxiv.org/abs/2101.04051"/>
        <updated>2021-06-24T01:51:43.316Z</updated>
        <summary type="html"><![CDATA[Alongside the prevalence of mobile videos, the general public leans towards
consuming vertical videos on hand-held devices. To revitalize the exposure of
horizontal contents, we hereby set forth the exploration of automated
horizontal-to-vertical (abbreviated as H2V) video conversion with our proposed
H2V framework, accompanied by an accurately annotated H2V-142K dataset.
Concretely, H2V framework integrates video shot boundary detection, subject
selection and multi-object tracking to facilitate the subject-preserving
conversion, wherein the key is subject selection. To achieve so, we propose a
Rank-SS module that detects human objects, then selects the subject-to-preserve
via exploiting location, appearance, and salient cues. Afterward, the framework
automatically crops the video around the subject to produce vertical contents
from horizontal sources. To build and evaluate our H2V framework, H2V-142K
dataset is densely annotated with subject bounding boxes for 125 videos with
132K frames and 9,500 video covers, upon which we demonstrate superior subject
selection performance comparing to traditional salient approaches, and exhibit
promising horizontal-to-vertical conversion performance overall. By publicizing
this dataset as well as our approach, we wish to pave the way for more valuable
endeavors on the horizontal-to-vertical video conversion task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1"&gt;Tun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daoxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaolong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiawei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modality Attention and Sampling Enables Deep Learning with Heterogeneous Marker Combinations in Fluorescence Microscopy. (arXiv:2008.12380v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12380</id>
        <link href="http://arxiv.org/abs/2008.12380"/>
        <updated>2021-06-24T01:51:43.310Z</updated>
        <summary type="html"><![CDATA[Fluorescence microscopy allows for a detailed inspection of cells, cellular
networks, and anatomical landmarks by staining with a variety of
carefully-selected markers visualized as color channels. Quantitative
characterization of structures in acquired images often relies on automatic
image analysis methods. Despite the success of deep learning methods in other
vision applications, their potential for fluorescence image analysis remains
underexploited. One reason lies in the considerable workload required to train
accurate models, which are normally specific for a given combination of
markers, and therefore applicable to a very restricted number of experimental
settings. We herein propose Marker Sampling and Excite, a neural network
approach with a modality sampling strategy and a novel attention module that
together enable (i) flexible training with heterogeneous datasets with
combinations of markers and (ii) successful utility of learned models on
arbitrary subsets of markers prospectively. We show that our single neural
network solution performs comparably to an upper bound scenario where an
ensemble of many networks is na\"ively trained for each possible marker
combination separately. In addition, we demonstrate the feasibility of this
framework in high-throughput biological analysis by revising a recent
quantitative characterization of bone marrow vasculature in 3D confocal
microscopy datasets and further confirm the validity of our approach on an
additional, significantly different dataset of microvessels in fetal liver
tissues. Not only can our work substantially ameliorate the use of deep
learning in fluorescence microscopy analysis, but it can also be utilized in
other fields with incomplete data acquisitions and missing modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomariz_A/0/1/0/all/0/1"&gt;Alvaro Gomariz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portenier_T/0/1/0/all/0/1"&gt;Tiziano Portenier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helbling_P/0/1/0/all/0/1"&gt;Patrick M. Helbling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isringhausen_S/0/1/0/all/0/1"&gt;Stephan Isringhausen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suessbier_U/0/1/0/all/0/1"&gt;Ute Suessbier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nombela_Arrieta_C/0/1/0/all/0/1"&gt;C&amp;#xe9;sar Nombela-Arrieta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1"&gt;Orcun Goksel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Alignment for Approximated Reversibility in Neural Networks. (arXiv:2106.12562v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12562</id>
        <link href="http://arxiv.org/abs/2106.12562"/>
        <updated>2021-06-24T01:51:43.304Z</updated>
        <summary type="html"><![CDATA[We introduce feature alignment, a technique for obtaining approximate
reversibility in artificial neural networks. By means of feature extraction, we
can train a neural network to learn an estimated map for its reverse process
from outputs to inputs. Combined with variational autoencoders, we can generate
new samples from the same statistics as the training data. Improvements of the
results are obtained by using concepts from generative adversarial networks.
Finally, we show that the technique can be modified for training neural
networks locally, saving computational memory resources. Applying these
techniques, we report results for three vision generative tasks: MNIST,
CIFAR-10, and celebA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farias_T/0/1/0/all/0/1"&gt;Tiago de Souza Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maziero_J/0/1/0/all/0/1"&gt;Jonas Maziero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Classification of Intrusive Igneous Rock Thin Section Images using Edge Detection and Colour Analysis. (arXiv:1710.00189v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1710.00189</id>
        <link href="http://arxiv.org/abs/1710.00189"/>
        <updated>2021-06-24T01:51:43.297Z</updated>
        <summary type="html"><![CDATA[Classification of rocks is one of the fundamental tasks in a geological
study. The process requires a human expert to examine sampled thin section
images under a microscope. In this study, we propose a method that uses
microscope automation, digital image acquisition, edge detection and colour
analysis (histogram). We collected 60 digital images from 20 standard thin
sections using a digital camera mounted on a conventional microscope. Each
image is partitioned into a finite number of cells that form a grid structure.
Edge and colour profile of pixels inside each cell determine its
classification. The individual cells then determine the thin section image
classification via a majority voting scheme. Our method yielded successful
results as high as 90% to 100% precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_S/0/1/0/all/0/1"&gt;S. Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;H. Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;I. Hipiny&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Mean Teacher for Cross-domain Object Detection. (arXiv:2003.00707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00707</id>
        <link href="http://arxiv.org/abs/2003.00707"/>
        <updated>2021-06-24T01:51:43.262Z</updated>
        <summary type="html"><![CDATA[Cross-domain object detection is challenging, because object detection model
is often vulnerable to data variance, especially to the considerable domain
shift between two distinctive domains. In this paper, we propose a new Unbiased
Mean Teacher (UMT) model for cross-domain object detection. We reveal that
there often exists a considerable model bias for the simple mean teacher (MT)
model in cross-domain scenarios, and eliminate the model bias with several
simple yet highly effective strategies. In particular, for the teacher model,
we propose a cross-domain distillation method for MT to maximally exploit the
expertise of the teacher model. Moreover, for the student model, we alleviate
its bias by augmenting training samples with pixel-level adaptation. Finally,
for the teaching process, we employ an out-of-distribution estimation strategy
to select samples that most fit the current model to further enhance the
cross-domain distillation process. By tackling the model bias issue with these
strategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on
benchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,
respectively, which outperforms the existing state-of-the-art results in
notable margins. Our implementation is available at
https://github.com/kinredon/umt.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Jinhong Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuhua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Lixin Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A System for Automatic Rice Disease Detection from Rice Paddy Images Serviced via a Chatbot. (arXiv:2011.10823v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10823</id>
        <link href="http://arxiv.org/abs/2011.10823"/>
        <updated>2021-06-24T01:51:43.256Z</updated>
        <summary type="html"><![CDATA[A LINE Bot System to diagnose rice diseases from actual paddy field images
was developed and presented in this paper. It was easy-to-use and automatic
system designed to help rice farmers improve the rice yield and quality. The
targeted images were taken from the actual paddy environment without special
sample preparation. We used a deep learning neural networks technique to detect
rice diseases from the images. We developed an object detection model training
and refinement process to improve the performance of our previous research on
rice leave diseases detection. The process was based on analyzing the model's
predictive results and could be repeatedly used to improve the quality of the
database in the next training of the model. The deployment model for our LINE
Bot system was created from the selected best performance technique in our
previous paper, YOLOv3, trained by refined training data set. The performance
of the deployment model was measured on 5 target classes and found that the
Average True Positive Point improved from 91.1% in the previous paper to 95.6%
in this study. Therefore, we used this deployment model for Rice Disease LINE
Bot system. Our system worked automatically real-time to suggest primary
diagnosis results to the users in the LINE group, which included rice farmers
and rice disease specialists. They could communicate freely via chat. In the
real LINE Bot deployment, the model's performance was measured by our own
defined measurement Average True Positive Point and was found to be an average
of 78.86%. The system was fast and took only 2-3 s for detection process in our
system server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Temniranrat_P/0/1/0/all/0/1"&gt;Pitchayagan Temniranrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiratiratanapruk_K/0/1/0/all/0/1"&gt;Kantip Kiratiratanapruk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kitvimonrat_A/0/1/0/all/0/1"&gt;Apichon Kitvimonrat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sinthupinyo_W/0/1/0/all/0/1"&gt;Wasin Sinthupinyo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patarapuwadol_S/0/1/0/all/0/1"&gt;Sujin Patarapuwadol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Blur, Noise, and Compression Robust Generative Adversarial Networks. (arXiv:2003.07849v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07849</id>
        <link href="http://arxiv.org/abs/2003.07849"/>
        <updated>2021-06-24T01:51:43.250Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) have gained considerable attention
owing to their ability to reproduce images. However, they can recreate training
images faithfully despite image degradation in the form of blur, noise, and
compression, generating similarly degraded images. To solve this problem, the
recently proposed noise robust GAN (NR-GAN) provides a partial solution by
demonstrating the ability to learn a clean image generator directly from noisy
images using a two-generator model comprising image and noise generators.
However, its application is limited to noise, which is relatively easy to
decompose owing to its additive and reversible characteristics, and its
application to irreversible image degradation, in the form of blur,
compression, and combination of all, remains a challenge. To address these
problems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that
can learn a clean image generator directly from degraded images without
knowledge of degradation parameters (e.g., blur kernel types, noise amounts, or
quality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator
model composed of image, blur-kernel, noise, and quality-factor generators.
However, in contrast to NR-GAN, to address irreversible characteristics, we
introduce masking architectures adjusting degradation strength values in a
data-driven manner using bypasses before and after degradation. Furthermore, to
suppress uncertainty caused by the combination of blur, noise, and compression,
we introduce adaptive consistency losses imposing consistency between
irreversible degradation processes according to the degradation strengths. We
demonstrate the effectiveness of BNCR-GAN through large-scale comparative
studies on CIFAR-10 and a generality analysis on FFHQ. In addition, we
demonstrate the applicability of BNCR-GAN in image restoration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaneko_T/0/1/0/all/0/1"&gt;Takuhiro Kaneko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Images V5 Text Annotation and Yet Another Mask Text Spotter. (arXiv:2106.12326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12326</id>
        <link href="http://arxiv.org/abs/2106.12326"/>
        <updated>2021-06-24T01:51:43.244Z</updated>
        <summary type="html"><![CDATA[A large scale human-labeled dataset plays an important role in creating high
quality deep learning models. In this paper we present text annotation for Open
Images V5 dataset. To our knowledge it is the largest among publicly available
manually created text annotations. Having this annotation we trained a simple
Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS),
which achieves competitive performance or even outperforms current
state-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and
Total-Text datasets. Code for text spotting model available online at:
https://github.com/openvinotoolkit/training_extensions. The model can be
exported to OpenVINO-format and run on Intel CPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krylov_I/0/1/0/all/0/1"&gt;Ilya Krylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nosov_S/0/1/0/all/0/1"&gt;Sergei Nosov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sovrasov_V/0/1/0/all/0/1"&gt;Vladislav Sovrasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers. (arXiv:2106.12442v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12442</id>
        <link href="http://arxiv.org/abs/2106.12442"/>
        <updated>2021-06-24T01:51:43.229Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of pedestrian and bicyclist paths is integral to the
development of reliable autonomous vehicles in dense urban environments. The
interactions between vehicle and pedestrian or bicyclist have a significant
impact on the trajectories of traffic participants e.g. stopping or turning to
avoid collisions. Although recent datasets and trajectory prediction approaches
have fostered the development of autonomous vehicles yet the amount of
vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,
we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In
particular, our dataset caters more diverse and complex interactions in dense
urban scenarios compared to the existing datasets. To address the challenges in
predicting future trajectories with dense interactions, we develop a joint
inference model that learns an expressive multi-modal shared latent space
across agents in the urban scene. This enables our Joint-$\beta$-cVAE approach
to better model the distribution of future trajectories. We achieve state of
the art results on the nuScenes and Euro-PVI datasets demonstrating the
importance of capturing interactions between ego-vehicle and pedestrians
(bicyclists) for accurate predictions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1"&gt;Apratim Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1"&gt;Daniel Olmeda Reino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1"&gt;Mario Fritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1"&gt;Bernt Schiele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos. (arXiv:2106.12522v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12522</id>
        <link href="http://arxiv.org/abs/2106.12522"/>
        <updated>2021-06-24T01:51:43.220Z</updated>
        <summary type="html"><![CDATA[Haustral folds are colon wall protrusions implicated for high polyp miss rate
during optical colonoscopy procedures. If segmented accurately, haustral folds
can allow for better estimation of missed surface and can also serve as
valuable landmarks for registering pre-treatment virtual (CT) and optical
colonoscopies, to guide navigation towards the anomalies found in pre-treatment
scans. We present a novel generative adversarial network, FoldIt, for
feature-consistent image translation of optical colonoscopy videos to virtual
colonoscopy renderings with haustral fold overlays. A new transitive loss is
introduced in order to leverage ground truth information between haustral fold
annotations and virtual colonoscopy renderings. We demonstrate the
effectiveness of our model on real challenging optical colonoscopy videos as
well as on textured virtual colonoscopy videos with clinician-verified haustral
fold annotations. All code and scripts to reproduce the experiments of this
paper will be made available via our Computational Endoscopy Platform at
https://github.com/nadeemlab/CEP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1"&gt;Shawn Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1"&gt;Saad Nadeem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1"&gt;Arie Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19 Diagnosis. (arXiv:2106.12313v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12313</id>
        <link href="http://arxiv.org/abs/2106.12313"/>
        <updated>2021-06-24T01:51:43.214Z</updated>
        <summary type="html"><![CDATA[The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world
since its first report in December 2019 and thoracic computed tomography (CT)
has become one of the main tools for its diagnosis. In recent years, deep
learning-based approaches have shown impressive performance in myriad image
recognition tasks. However, they usually require a large number of annotated
data for training. Inspired by Ground Glass Opacity (GGO), a common finding in
COIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised
pretraining method based on pseudo lesions generation and restoration for
COVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical
model, to generate lesion-like patterns, which were then randomly pasted to the
lung regions of normal CT images to generate pseudo COVID-19 images. The pairs
of normal and pseudo COVID-19 images were then used to train an encoder-decoder
architecture based U-Net for image restoration, which does not require any
labelled data. The pretrained encoder was then fine-tuned using labelled data
for COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of
CT images were employed for evaluation. Comprehensive experimental results
demonstrated that the proposed self-supervised learning approach could extract
better feature representation for COVID-19 diagnosis and the accuracy of the
proposed method outperformed the supervised model pretrained on large scale
images by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhongliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuechen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Properties of Foveated Perceptual Systems. (arXiv:2006.07991v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07991</id>
        <link href="http://arxiv.org/abs/2006.07991"/>
        <updated>2021-06-24T01:51:43.207Z</updated>
        <summary type="html"><![CDATA[The goal of this work is to characterize the representational impact that
foveation operations have for machine vision systems, inspired by the foveated
human visual system, which has higher acuity at the center of gaze and
texture-like encoding in the periphery. To do so, we introduce models
consisting of a first-stage \textit{fixed} image transform followed by a
second-stage \textit{learnable} convolutional neural network, and we varied the
first stage component. The primary model has a foveated-textural input stage,
which we compare to a model with foveated-blurred input and a model with
spatially-uniform blurred input (both matched for perceptual compression), and
a final reference model with minimal input-based compression. We find that: 1)
the foveated-texture model shows similar scene classification accuracy as the
reference model despite its compressed input, with greater i.i.d.
generalization than the other models; 2) the foveated-texture model has greater
sensitivity to high-spatial frequency information and greater robustness to
occlusion, w.r.t the comparison models; 3) both the foveated systems, show a
stronger center image-bias relative to the spatially-uniform systems even with
a weight sharing constraint. Critically, these results are preserved over
different classical CNN architectures throughout their learning dynamics.
Altogether, this suggests that foveation with peripheral texture-based
computations yields an efficient, distinct, and robust representational format
of scene information, and provides symbiotic computational insight into the
representational consequences that texture-based peripheral encoding may have
for processing in the human visual system, while also potentially inspiring the
next generation of computer vision models via spatially-adaptive computation.
Code + Data available here: https://github.com/ArturoDeza/EmergentProperties]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1"&gt;Arturo Deza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konkle_T/0/1/0/all/0/1"&gt;Talia Konkle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabetic Retinopathy Detection using Ensemble Machine Learning. (arXiv:2106.12545v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12545</id>
        <link href="http://arxiv.org/abs/2106.12545"/>
        <updated>2021-06-24T01:51:43.201Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in
diabetic patients. DR is a microvascular disease that affects the eye retina,
which causes vessel blockage and therefore cuts the main source of nutrition
for the retina tissues. Treatment for this visual disorder is most effective
when it is detected in its earliest stages, as severe DR can result in
irreversible blindness. Nonetheless, DR identification requires the expertise
of Ophthalmologists which is often expensive and time-consuming. Therefore,
automatic detection systems were introduced aiming to facilitate the
identification process, making it available globally in a time and
cost-efficient manner. However, due to the limited reliable datasets and
medical records for this particular eye disease, the obtained predictions
accuracies were relatively unsatisfying for eye specialists to rely on them as
diagnostic systems. Thus, we explored an ensemble-based learning strategy,
merging a substantial selection of well-known classification algorithms in one
sophisticated diagnostic model. The proposed framework achieved the highest
accuracy rates among all other common classification algorithms in the area. 4
subdatasets were generated to contain the top 5 and top 10 features of the
Messidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies
of 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original
dataset respectively. The results imply the impressive performance of the
subdataset, which significantly conduces to a less complex classification
process]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Odeh_I/0/1/0/all/0/1"&gt;Israa Odeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alkasassbeh_M/0/1/0/all/0/1"&gt;Mouhammd Alkasassbeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alauthman_M/0/1/0/all/0/1"&gt;Mohammad Alauthman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Co-advise: Cross Inductive Bias Distillation. (arXiv:2106.12378v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12378</id>
        <link href="http://arxiv.org/abs/2106.12378"/>
        <updated>2021-06-24T01:51:43.196Z</updated>
        <summary type="html"><![CDATA[Transformers recently are adapted from the community of natural language
processing as a promising substitute of convolution-based neural networks for
visual learning tasks. However, its supremacy degenerates given an insufficient
amount of training data (e.g., ImageNet). To make it into practical utility, we
propose a novel distillation-based method to train vision transformers. Unlike
previous works, where merely heavy convolution-based teachers are provided, we
introduce lightweight teachers with different architectural inductive biases
(e.g., convolution and involution) to co-advise the student transformer. The
key is that teachers with different inductive biases attain different knowledge
despite that they are trained on the same dataset, and such different knowledge
compounds and boosts the student's performance during distillation. Equipped
with this cross inductive bias distillation method, our vision transformers
(termed as CivT) outperform all previous transformers of the same architecture
on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1"&gt;Tianyu Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zihui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12449</id>
        <link href="http://arxiv.org/abs/2106.12449"/>
        <updated>2021-06-24T01:51:43.177Z</updated>
        <summary type="html"><![CDATA[Accurate detection of obstacles in 3D is an essential task for autonomous
driving and intelligent transportation. In this work, we propose a general
multimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D
point clouds at a semantic level for boosting the 3D object detection task.
Especially, the FusionPainting framework consists of three main modules: a
multi-modal semantic segmentation module, an adaptive attention-based semantic
fusion module, and a 3D object detector. First, semantic information is
obtained for 2D images and 3D Lidar point clouds based on 2D and 3D
segmentation approaches. Then the segmentation results from different sensors
are adaptively fused based on the proposed attention-based semantic fusion
module. Finally, the point clouds painted with the fused semantic label are
sent to the 3D detector for obtaining the 3D objection results. The
effectiveness of the proposed framework has been verified on the large-scale
nuScenes detection benchmark by comparing it with three different baselines.
The experimental results show that the fusion strategy can significantly
improve the detection performance compared to the methods using only point
clouds, and the methods using point clouds only painted with 2D segmentation
information. Furthermore, the proposed approach outperforms other
state-of-the-art methods on the nuScenes testing benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shaoqing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dingfu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Junbo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1"&gt;Zhou Bin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangjun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12423</id>
        <link href="http://arxiv.org/abs/2106.12423"/>
        <updated>2021-06-24T01:51:43.171Z</updated>
        <summary type="html"><![CDATA[We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1"&gt;Tero Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1"&gt;Miika Aittala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laine_S/0/1/0/all/0/1"&gt;Samuli Laine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harkonen_E/0/1/0/all/0/1"&gt;Erik H&amp;#xe4;rk&amp;#xf6;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1"&gt;Janne Hellsten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1"&gt;Jaakko Lehtinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1"&gt;Timo Aila&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep unsupervised 3D human body reconstruction from a sparse set of landmarks. (arXiv:2106.12282v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12282</id>
        <link href="http://arxiv.org/abs/2106.12282"/>
        <updated>2021-06-24T01:51:43.163Z</updated>
        <summary type="html"><![CDATA[In this paper we propose the first deep unsupervised approach in human body
reconstruction to estimate body surface from a sparse set of landmarks, so
called DeepMurf. We apply a denoising autoencoder to estimate missing
landmarks. Then we apply an attention model to estimate body joints from
landmarks. Finally, a cascading network is applied to regress parameters of a
statistical generative model that reconstructs body. Our set of proposed loss
functions allows us to train the network in an unsupervised way. Results on
four public datasets show that our approach accurately reconstructs the human
body from real world mocap data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madadi_M/0/1/0/all/0/1"&gt;Meysam Madadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertiche_H/0/1/0/all/0/1"&gt;Hugo Bertiche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1"&gt;Sergio Escalera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. (arXiv:2012.07177v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07177</id>
        <link href="http://arxiv.org/abs/2012.07177"/>
        <updated>2021-06-24T01:51:43.157Z</updated>
        <summary type="html"><![CDATA[Building instance segmentation models that are data-efficient and can handle
rare object categories is an important challenge in computer vision. Leveraging
data augmentations is a promising direction towards addressing this challenge.
Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12])
for instance segmentation where we randomly paste objects onto an image. Prior
studies on Copy-Paste relied on modeling the surrounding visual context for
pasting the objects. However, we find that the simple mechanism of pasting
objects randomly is good enough and can provide solid gains on top of strong
baselines. Furthermore, we show Copy-Paste is additive with semi-supervised
methods that leverage extra data through pseudo labeling (e.g. self-training).
On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an
improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art.
We further demonstrate that Copy-Paste can lead to significant improvements on
the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge
winning entry by +3.6 mask AP on rare categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1"&gt;Golnaz Ghiasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1"&gt;Ekin D. Cubuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1"&gt;Barret Zoph&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Segmentation of Action Segments in Egocentric Videos using Gaze. (arXiv:1710.00187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1710.00187</id>
        <link href="http://arxiv.org/abs/1710.00187"/>
        <updated>2021-06-24T01:51:43.141Z</updated>
        <summary type="html"><![CDATA[Unsupervised segmentation of action segments in egocentric videos is a
desirable feature in tasks such as activity recognition and content-based video
retrieval. Reducing the search space into a finite set of action segments
facilitates a faster and less noisy matching. However, there exist a
substantial gap in machine understanding of natural temporal cuts during a
continuous human activity. This work reports on a novel gaze-based approach for
segmenting action segments in videos captured using an egocentric camera. Gaze
is used to locate the region-of-interest inside a frame. By tracking two simple
motion-based parameters inside successive regions-of-interest, we discover a
finite set of temporal cuts. We present several results using combinations (of
the two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains
egocentric videos depicting several daily-living activities. The quality of the
temporal cuts is further improved by implementing two entropy measures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hipiny_I/0/1/0/all/0/1"&gt;I. Hipiny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ujir_H/0/1/0/all/0/1"&gt;H. Ujir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minoi_J/0/1/0/all/0/1"&gt;J.L. Minoi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juan_S/0/1/0/all/0/1"&gt;S.F. Samson Juan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khairuddin_M/0/1/0/all/0/1"&gt;M.A. Khairuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunar_M/0/1/0/all/0/1"&gt;M.S. Sunar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-Based Interpretability Methods and Binarized Neural Networks. (arXiv:2106.12569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12569</id>
        <link href="http://arxiv.org/abs/2106.12569"/>
        <updated>2021-06-24T01:51:43.126Z</updated>
        <summary type="html"><![CDATA[Binarized Neural Networks (BNNs) have the potential to revolutionize the way
that deep learning is carried out in edge computing platforms. However, the
effectiveness of interpretability methods on these networks has not been
assessed.

In this paper, we compare the performance of several widely used saliency
map-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when
applied to Binarized or Full Precision Neural Networks (FPNNs). We found that
the basic Gradient method produces very similar-looking maps for both types of
network. However, SmoothGrad produces significantly noisier maps for BNNs.
GradCAM also produces saliency maps which differ between network types, with
some of the BNNs having seemingly nonsensical explanations. We comment on
possible reasons for these differences in explanations and present it as an
example of why interpretability techniques should be tested on a wider range of
network types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Widdicombe_A/0/1/0/all/0/1"&gt;Amy Widdicombe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1"&gt;Simon J. Julier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12387</id>
        <link href="http://arxiv.org/abs/2106.12387"/>
        <updated>2021-06-24T01:51:43.120Z</updated>
        <summary type="html"><![CDATA[The subject of "fairness" in artificial intelligence (AI) refers to assessing
AI algorithms for potential bias based on demographic characteristics such as
race and gender, and the development of algorithms to address this bias. Most
applications to date have been in computer vision, although some work in
healthcare has started to emerge. The use of deep learning (DL) in cardiac MR
segmentation has led to impressive results in recent years, and such techniques
are starting to be translated into clinical practice. However, no work has yet
investigated the fairness of such models. In this work, we perform such an
analysis for racial/gender groups, focusing on the problem of training data
imbalance, using a nnU-Net model trained and evaluated on cine short axis
cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from
6 different racial groups. We find statistically significant differences in
Dice performance between different racial groups. To reduce the racial bias, we
investigated three strategies: (1) stratified batch sampling, in which batch
sampling is stratified to ensure balance between racial groups; (2) fair
meta-learning for segmentation, in which a DL classifier is trained to classify
race and jointly optimized with the segmentation model; and (3) protected group
models, in which a different segmentation model is trained for each racial
group. We also compared the results to the scenario where we have a perfectly
balanced database. To assess fairness we used the standard deviation (SD) and
skewed error ratio (SER) of the average Dice values. Our results demonstrate
that the racial bias results from the use of imbalanced training data, and that
all proposed bias mitigation strategies improved fairness, with the best SD and
SER resulting from the use of protected group models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1"&gt;Esther Puyol-Anton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1"&gt;Bram Ruijsink&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1"&gt;Stefan K. Piechnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1"&gt;Stefan Neubauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1"&gt;Steffen E. Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1"&gt;Reza Razavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1"&gt;Andrew P. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A new Video Synopsis Based Approach Using Stereo Camera. (arXiv:2106.12362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12362</id>
        <link href="http://arxiv.org/abs/2106.12362"/>
        <updated>2021-06-24T01:51:43.114Z</updated>
        <summary type="html"><![CDATA[In today's world, the amount of data produced in every field has increased at
an unexpected level. In the face of increasing data, the importance of data
processing has increased remarkably. Our resource topic is on the processing of
video data, which has an important place in increasing data, and the production
of summary videos. Within the scope of this resource, a new method for anomaly
detection with object-based unsupervised learning has been developed while
creating a video summary. By using this method, the video data is processed as
pixels and the result is produced as a video segment. The process flow can be
briefly summarized as follows. Objects on the video are detected according to
their type, and then they are tracked. Then, the tracking history data of the
objects are processed, and the classifier is trained with the object type.
Thanks to this classifier, anomaly behavior of objects is detected. Video
segments are determined by processing video moments containing anomaly
behaviors. The video summary is created by extracting the detected video
segments from the original video and combining them. The model we developed has
been tested and verified separately for single camera and dual camera systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dilber_T/0/1/0/all/0/1"&gt;Talha Dilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1"&gt;Mehmet Serdar Guzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bostanci_E/0/1/0/all/0/1"&gt;Erkan Bostanci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating the Robustness of Classification Models by the Structure of the Learned Feature-Space. (arXiv:2106.12303v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12303</id>
        <link href="http://arxiv.org/abs/2106.12303"/>
        <updated>2021-06-24T01:51:43.109Z</updated>
        <summary type="html"><![CDATA[Over the last decade, the development of deep image classification networks
has mostly been driven by the search for the best performance in terms of
classification accuracy on standardized benchmarks like ImageNet. More
recently, this focus has been expanded by the notion of model robustness, i.e.
the generalization abilities of models towards previously unseen changes in the
data distribution. While new benchmarks, like ImageNet-C, have been introduced
to measure robustness properties, we argue that fixed testsets are only able to
capture a small portion of possible data variations and are thus limited and
prone to generate new overfitted solutions. To overcome these drawbacks, we
suggest to estimate the robustness of a model directly from the structure of
its learned feature-space. We introduce robustness indicators which are
obtained via unsupervised clustering of latent representations inside a trained
classifier and show very high correlations to the model performance on
corrupted test data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1"&gt;Kalun Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1"&gt;Franz-Josef Pfreundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Label Management Mechanism for Retinal Fundus Image Classification of Diabetic Retinopathy. (arXiv:2106.12284v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12284</id>
        <link href="http://arxiv.org/abs/2106.12284"/>
        <updated>2021-06-24T01:51:43.104Z</updated>
        <summary type="html"><![CDATA[Diabetic retinopathy (DR) remains the most prevalent cause of vision
impairment and irreversible blindness in the working-age adults. Due to the
renaissance of deep learning (DL), DL-based DR diagnosis has become a promising
tool for the early screening and severity grading of DR. However, training deep
neural networks (DNNs) requires an enormous amount of carefully labeled data.
Noisy label data may be introduced when labeling plenty of data, degrading the
performance of models. In this work, we propose a novel label management
mechanism (LMM) for the DNN to overcome overfitting on the noisy data. LMM
utilizes maximum posteriori probability (MAP) in the Bayesian statistic and
time-weighted technique to selectively correct the labels of unclean data,
which gradually purify the training data and improve classification
performance. Comprehensive experiments on both synthetic noise data (Messidor
\& our collected DR dataset) and real-world noise data (ANIMAL-10N)
demonstrated that LMM could boost performance of models and is superior to
three state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mengdi Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Ximeng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1"&gt;Mufeng Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhe Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiangxi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuanqing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1"&gt;Qiushi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yanye Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition. (arXiv:2106.12368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12368</id>
        <link href="http://arxiv.org/abs/2106.12368"/>
        <updated>2021-06-24T01:51:43.087Z</updated>
        <summary type="html"><![CDATA[In this paper, we present Vision Permutator, a conceptually simple and data
efficient MLP-like architecture for visual recognition. By realizing the
importance of the positional information carried by 2D feature representations,
unlike recent MLP-like models that encode the spatial information along the
flattened spatial dimensions, Vision Permutator separately encodes the feature
representations along the height and width dimensions with linear projections.
This allows Vision Permutator to capture long-range dependencies along one
spatial direction and meanwhile preserve precise positional information along
the other direction. The resulting position-sensitive outputs are then
aggregated in a mutually complementing manner to form expressive
representations of the objects of interest. We show that our Vision Permutators
are formidable competitors to convolutional neural networks (CNNs) and vision
transformers. Without the dependence on spatial convolutions or attention
mechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without
extra large-scale training data (e.g., ImageNet-22k) using only 25M learnable
parameters, which is much better than most CNNs and vision transformers under
the same model size constraint. When scaling up to 88M, it attains 83.2% top-1
accuracy. We hope this work could encourage research on rethinking the way of
encoding spatial information and facilitate the development of MLP-like models.
Code is available at https://github.com/Andrew-Qibin/VisionPermutator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1"&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentinel-1 and Sentinel-2 Spatio-Temporal Data Fusion for Clouds Removal. (arXiv:2106.12226v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12226</id>
        <link href="http://arxiv.org/abs/2106.12226"/>
        <updated>2021-06-24T01:51:43.082Z</updated>
        <summary type="html"><![CDATA[The abundance of clouds, located both spatially and temporally, often makes
remote sensing applications with optical images difficult or even impossible.
In this manuscript, a novel method for clouds-corrupted optical image
restoration has been presented and developed, based on a joint data fusion
paradigm, where three deep neural networks have been combined in order to fuse
spatio-temporal features extracted from Sentinel-1 and Sentinel-2 time-series
of data. It is worth highlighting that both the code and the dataset have been
implemented from scratch and made available to interested research for further
analysis and investigation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1"&gt;Alessandro Sebastianelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1"&gt;Artur Nowakowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puglisi_E/0/1/0/all/0/1"&gt;Erika Puglisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1"&gt;Maria Pia Del Rosso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mifdal_J/0/1/0/all/0/1"&gt;Jamila Mifdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirri_F/0/1/0/all/0/1"&gt;Fiora Pirri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1"&gt;Pierre Philippe Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1"&gt;Silvia Liberata Ullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12023</id>
        <link href="http://arxiv.org/abs/2106.12023"/>
        <updated>2021-06-24T01:51:43.076Z</updated>
        <summary type="html"><![CDATA[This report describes the technical details of our submission to the
EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action
Recognition. The EPIC-Kitchens dataset is more difficult than other video
domain adaptation datasets due to multi-tasks with more modalities. Firstly, to
participate in the challenge, we employ a transformer to capture the spatial
information from each modality. Secondly, we employ a temporal attention module
to model temporal-wise inter-dependency. Thirdly, we employ the adversarial
domain adaptation network to learn the general features between labeled source
and unlabeled target domain. Finally, we incorporate multiple modalities to
improve the performance by a three-stream network with late fusion. Our network
achieves the comparable performance with the state-of-the-art baseline T$A^3$N
and outperforms the baseline on top-1 accuracy for verb class and top-5
accuracies for all three tasks which are verb, noun and action. Under the team
name xy9, our submission achieved 5th place in terms of top-1 accuracy for verb
class and all top-5 accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1"&gt;Tao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-06-24T01:51:43.062Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine MRI Synthesis. (arXiv:2106.12499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12499</id>
        <link href="http://arxiv.org/abs/2106.12499"/>
        <updated>2021-06-24T01:51:43.057Z</updated>
        <summary type="html"><![CDATA[Self-training based unsupervised domain adaptation (UDA) has shown great
potential to address the problem of domain shift, when applying a trained deep
learning model in a source domain to unlabeled target domains. However, while
the self-training UDA has demonstrated its effectiveness on discriminative
tasks, such as classification and segmentation, via the reliable pseudo-label
selection based on the softmax discrete histogram, the self-training UDA for
generative tasks, such as image synthesis, is not fully investigated. In this
work, we propose a novel generative self-training (GST) UDA framework with
continuous value prediction and regression objective for cross-domain image
synthesis. Specifically, we propose to filter the pseudo-label with an
uncertainty mask, and quantify the predictive confidence of generated images
with practical variational Bayes learning. The fast test-time adaptation is
achieved by a round-based alternative optimization scheme. We validated our
framework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis
problem, where datasets in the source and target domains were acquired from
different scanners or centers. Extensive validations were carried out to verify
our framework against popular adversarial training UDA methods. Results show
that our GST, with tagged MRI of test subjects in new target domains, improved
the synthesis quality by a large margin, compared with the adversarial training
UDA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Jiachen Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timothy_R/0/1/0/all/0/1"&gt;Reese Timothy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in Arabic Language. (arXiv:2106.12488v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12488</id>
        <link href="http://arxiv.org/abs/2106.12488"/>
        <updated>2021-06-24T01:51:43.048Z</updated>
        <summary type="html"><![CDATA[The prominence of figurative language devices, such as sarcasm and irony,
poses serious challenges for Arabic Sentiment Analysis (SA). While previous
research works tackle SA and sarcasm detection separately, this paper
introduces an end-to-end deep Multi-Task Learning (MTL) model, allowing
knowledge interaction between the two tasks. Our MTL model's architecture
consists of a Bidirectional Encoder Representation from Transformers (BERT)
model, a multi-task attention interaction module, and two task classifiers. The
overall obtained results show that our proposed model outperforms its
single-task counterparts on both SA and sarcasm detection sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1"&gt;Abdelkader El Mahdaouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1"&gt;Abdellah El Mekki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1"&gt;Kabil Essefar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1"&gt;Nabil El Mamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1"&gt;Ismail Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1"&gt;Ahmed Khoumsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Tuning StyleGAN2 For Cartoon Face Generation. (arXiv:2106.12445v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12445</id>
        <link href="http://arxiv.org/abs/2106.12445"/>
        <updated>2021-06-24T01:51:43.043Z</updated>
        <summary type="html"><![CDATA[Recent studies have shown remarkable success in the unsupervised image to
image (I2I) translation. However, due to the imbalance in the data, learning
joint distribution for various domains is still very challenging. Although
existing models can generate realistic target images, it's difficult to
maintain the structure of the source image. In addition, training a generative
model on large data in multiple domains requires a lot of time and computer
resources. To address these limitations, we propose a novel image-to-image
translation method that generates images of the target domain by finetuning a
stylegan2 pretrained model. The stylegan2 model is suitable for unsupervised
I2I translation on unbalanced datasets; it is highly stable, produces realistic
images, and even learns properly from limited data when applied with simple
fine-tuning techniques. Thus, in this paper, we propose new methods to preserve
the structure of the source images and generate realistic images in the target
domain. The code and results are available at
https://github.com/happy-jihye/Cartoon-StyleGan2]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Back_J/0/1/0/all/0/1"&gt;Jihye Back&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Class Classification of Blood Cells - End to End Computer Vision based diagnosis case study. (arXiv:2106.12548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12548</id>
        <link href="http://arxiv.org/abs/2106.12548"/>
        <updated>2021-06-24T01:51:43.037Z</updated>
        <summary type="html"><![CDATA[The diagnosis of blood-based diseases often involves identifying and
characterizing patient blood samples. Automated methods to detect and classify
blood cell subtypes have important medical applications. Automated medical
image processing and analysis offers a powerful tool for medical diagnosis. In
this work we tackle the problem of white blood cell classification based on the
morphological characteristics of their outer contour, color. The work we would
explore a set of preprocessing and segmentation (Color-based segmentation,
Morphological processing, contouring) algorithms along with a set of features
extraction methods (Corner detection algorithms and Histogram of
Gradients(HOG)), dimensionality reduction algorithms (Principal Component
Analysis(PCA)) that are able to recognize and classify through various
Unsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,
Decision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,
Naive Bayes) algorithms different categories of white blood cells to
Eosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards
to explore various Deep Convolutional Neural network architecture (Sqeezent,
MobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation
and with preprocessing. We would like to explore many algorithms to identify
the robust algorithm with least time complexity and low resource requirement.
The outcome of this work can be a cue to selection of algorithms as per
requirement for automated blood cell classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bezugam_S/0/1/0/all/0/1"&gt;Sai Sukruth Bezugam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning. (arXiv:2106.12511v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12511</id>
        <link href="http://arxiv.org/abs/2106.12511"/>
        <updated>2021-06-24T01:51:43.031Z</updated>
        <summary type="html"><![CDATA[Left ventricular hypertrophy (LVH) results from chronic remodeling caused by
a broad range of systemic and cardiovascular disease including hypertension,
aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early
detection and characterization of LVH can significantly impact patient care but
is limited by under-recognition of hypertrophy, measurement error and
variability, and difficulty differentiating etiologies of LVH. To overcome this
challenge, we present EchoNet-LVH - a deep learning workflow that automatically
quantifies ventricular hypertrophy with precision equal to human experts and
predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model
accurately measures intraventricular wall thickness (mean absolute error [MAE]
1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI
2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and
classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic
cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets
from independent domestic and international healthcare systems, EchoNet-LVH
accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)
and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy
(AUC 0.89) on the domestic external validation site. Leveraging measurements
across multiple heart beats, our model can more accurately identify subtle
changes in LV geometry and its causal etiologies. Compared to human experts,
EchoNet-LVH is fully automated, allowing for reproducible, precise
measurements, and lays the foundation for precision diagnosis of cardiac
hypertrophy. As a resource to promote further innovation, we also make publicly
available a large dataset of 23,212 annotated echocardiogram videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Duffy_G/0/1/0/all/0/1"&gt;Grant Duffy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Paul P Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Neal Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_B/0/1/0/all/0/1"&gt;Bryan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwan_A/0/1/0/all/0/1"&gt;Alan C. Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shun_Shin_M/0/1/0/all/0/1"&gt;Matthew J. Shun-Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alexander_K/0/1/0/all/0/1"&gt;Kevin M. Alexander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebinger_J/0/1/0/all/0/1"&gt;Joseph Ebinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rader_F/0/1/0/all/0/1"&gt;Florian Rader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1"&gt;David H. Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schnittger_I/0/1/0/all/0/1"&gt;Ingela Schnittger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ashley_E/0/1/0/all/0/1"&gt;Euan A. Ashley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Y. Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1"&gt;Jignesh Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Witteles_R/0/1/0/all/0/1"&gt;Ronald Witteles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Susan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ouyang_D/0/1/0/all/0/1"&gt;David Ouyang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images. (arXiv:2106.12413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12413</id>
        <link href="http://arxiv.org/abs/2106.12413"/>
        <updated>2021-06-24T01:51:43.026Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation from very fine resolution (VFR) urban scene images
plays a significant role in several application scenarios including autonomous
driving, land cover classification, and urban planning, etc. However, the
tremendous details contained in the VFR image severely limit the potential of
the existing deep learning approaches. More seriously, the considerable
variations in scale and appearance of objects further deteriorate the
representational capacity of those se-mantic segmentation methods, leading to
the confusion of adjacent objects. Addressing such is-sues represents a
promising research field in the remote sensing community, which paves the way
for scene-level landscape pattern analysis and decision making. In this
manuscript, we pro-pose a bilateral awareness network (BANet) which contains a
dependency path and a texture path to fully capture the long-range
relationships and fine-grained details in VFR images. Specif-ically, the
dependency path is conducted based on the ResT, a novel Transformer backbone
with memory-efficient multi-head self-attention, while the texture path is
built on the stacked convo-lution operation. Besides, using the linear
attention mechanism, a feature aggregation module (FAM) is designed to
effectively fuse the dependency features and texture features. Extensive
experiments conducted on the three large-scale urban scene image segmentation
datasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid
dataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%
mIoU is achieved on the UAVid dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Libo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenxi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Teng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiaoliang Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral image denoising. (arXiv:2106.12489v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12489</id>
        <link href="http://arxiv.org/abs/2106.12489"/>
        <updated>2021-06-24T01:51:43.020Z</updated>
        <summary type="html"><![CDATA[Low-rankness is important in the hyperspectral image (HSI) denoising tasks.
The tensor nuclear norm (TNN), defined based on the tensor singular value
decomposition, is a state-of-the-art method to describe the low-rankness of
HSI. However, TNN ignores some of the physical meanings of HSI in tackling the
denoising tasks, leading to suboptimal denoising performance. In this paper, we
propose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and
the non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the
physical meaning of frequency components and reconsider their weights to
improve the low-rank representation ability of TNN. Meanwhile, we also consider
the correlation among two spatial dimensions and the spectral dimension of HSI
and combine the above improvements to TNN to propose MFWTNN. Secondly, we use
non-convex functions to approximate the rank function of the frequency tensor
and propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively
choose bigger weights for slices mainly containing noise information and
smaller weights for slices containing profile information. Finally, we develop
the efficient alternating direction method of multiplier (ADMM) based algorithm
to solve the proposed models, and the effectiveness of our models are
substantiated in simulated and real HSI datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaozhen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kong_W/0/1/0/all/0/1"&gt;Wenfeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ning_J/0/1/0/all/0/1"&gt;Jifeng Ning&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Visual Inertial SLAM for Multiple Smart Phones. (arXiv:2106.12186v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.12186</id>
        <link href="http://arxiv.org/abs/2106.12186"/>
        <updated>2021-06-24T01:51:43.001Z</updated>
        <summary type="html"><![CDATA[The efficiency and accuracy of mapping are crucial in a large scene and
long-term AR applications. Multi-agent cooperative SLAM is the precondition of
multi-user AR interaction. The cooperation of multiple smart phones has the
potential to improve efficiency and robustness of task completion and can
complete tasks that a single agent cannot do. However, it depends on robust
communication, efficient location detection, robust mapping, and efficient
information sharing among agents. We propose a multi-intelligence collaborative
monocular visual-inertial SLAM deployed on multiple ios mobile devices with a
centralized architecture. Each agent can independently explore the environment,
run a visual-inertial odometry module online, and then send all the measurement
information to a central server with higher computing resources. The server
manages all the information received, detects overlapping areas, merges and
optimizes the map, and shares information with the agents when needed. We have
verified the performance of the system in public datasets and real
environments. The accuracy of mapping and fusion of the proposed system is
comparable to VINS-Mono which requires higher computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kaiqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianhua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Dongyan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Off-the-Shelf Source Segmenter for Target Medical Image Segmentation. (arXiv:2106.12497v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12497</id>
        <link href="http://arxiv.org/abs/2106.12497"/>
        <updated>2021-06-24T01:51:42.987Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled and unseen target domain, which is
usually trained on data from both domains. Access to the source domain data at
the adaptation stage, however, is often limited, due to data storage or privacy
issues. To alleviate this, in this work, we target source free UDA for
segmentation, and propose to adapt an ``off-the-shelf" segmentation model
pre-trained in the source domain to the target domain, with an adaptive
batch-wise normalization statistics adaptation framework. Specifically, the
domain-specific low-order batch statistics, i.e., mean and variance, are
gradually adapted with an exponential momentum decay scheme, while the
consistency of domain shareable high-order batch statistics, i.e., scaling and
shifting parameters, is explicitly enforced by our optimization objective. The
transferability of each channel is adaptively measured first from which to
balance the contribution of each channel. Moreover, the proposed source free
UDA framework is orthogonal to unsupervised learning methods, e.g.,
self-entropy minimization, which can thus be simply added on top of our
framework. Extensive experiments on the BraTS 2018 database show that our
source free UDA framework outperformed existing source-relaxed UDA methods for
the cross-subtype UDA segmentation task and yielded comparable results for the
cross-modality UDA segmentation task, compared with a supervised UDA methods
with the source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CharacterChat: Supporting the Creation of Fictional Characters through Conversation and Progressive Manifestation with a Chatbot. (arXiv:2106.12314v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12314</id>
        <link href="http://arxiv.org/abs/2106.12314"/>
        <updated>2021-06-24T01:51:42.964Z</updated>
        <summary type="html"><![CDATA[We present CharacterChat, a concept and chatbot to support writers in
creating fictional characters. Concretely, writers progressively turn the bot
into their imagined character through conversation. We iteratively developed
CharacterChat in a user-centred approach, starting with a survey on character
creation with writers (N=30), followed by two qualitative user studies (N=7 and
N=8). Our prototype combines two modes: (1) Guided prompts help writers define
character attributes (e.g. User: "Your name is Jane."), including suggestions
for attributes (e.g. Bot: "What is my main motivation?") and values, realised
as a rule-based system with a concept network. (2) Open conversation with the
chatbot helps writers explore their character and get inspiration, realised
with a language model that takes into account the defined character attributes.
Our user studies reveal benefits particularly for early stages of character
creation, and challenges due to limited conversational capabilities. We
conclude with lessons learned and ideas for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_O/0/1/0/all/0/1"&gt;Oliver Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buschek_D/0/1/0/all/0/1"&gt;Daniel Buschek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02561</id>
        <link href="http://arxiv.org/abs/2106.02561"/>
        <updated>2021-06-24T01:51:42.958Z</updated>
        <summary type="html"><![CDATA[Broad-coverage meaning representations in NLP mostly focus on explicitly
expressed content. More importantly, the scarcity of datasets annotating
diverse implicit roles limits empirical studies into their linguistic nuances.
For example, in the web review "Great service!", the provider and consumer are
implicit arguments of different types. We examine an annotated corpus of
fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully
re-annotating it, resolving several inconsistencies. Subsequently, we present
the first transition-based neural parser that can handle implicit arguments
dynamically, and experiment with two different transition systems on the
improved dataset. We find that certain types of implicit arguments are more
difficult to parse than others and that the simpler system is more accurate in
recovering implicit arguments, despite having a lower overall parsing score,
attesting current reasoning limitations of NLP models. This work will
facilitate a better understanding of implicit and underspecified language, by
incorporating it holistically into meaning representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D human tongue reconstruction from single "in-the-wild" images. (arXiv:2106.12302v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12302</id>
        <link href="http://arxiv.org/abs/2106.12302"/>
        <updated>2021-06-24T01:51:42.953Z</updated>
        <summary type="html"><![CDATA[3D face reconstruction from a single image is a task that has garnered
increased interest in the Computer Vision community, especially due to its
broad use in a number of applications such as realistic 3D avatar creation,
pose invariant face recognition and face hallucination. Since the introduction
of the 3D Morphable Model in the late 90's, we witnessed an explosion of
research aiming at particularly tackling this task. Nevertheless, despite the
increasing level of detail in the 3D face reconstructions from single images
mainly attributed to deep learning advances, finer and highly deformable
components of the face such as the tongue are still absent from all 3D face
models in the literature, although being very important for the realness of the
3D avatar representations. In this work we present the first, to the best of
our knowledge, end-to-end trainable pipeline that accurately reconstructs the
3D face together with the tongue. Moreover, we make this pipeline robust in
"in-the-wild" images by introducing a novel GAN method tailored for 3D tongue
surface generation. Finally, we make publicly available to the community the
first diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals
varying in gender, age, and ethnicity backgrounds. As we demonstrate in an
extensive series of quantitative as well as qualitative experiments, our model
proves to be robust and realistically captures the 3D tongue structure, even in
adverse "in-the-wild" conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ploumpis_S/0/1/0/all/0/1"&gt;Stylianos Ploumpis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschoglou_S/0/1/0/all/0/1"&gt;Stylianos Moschoglou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triantafyllou_V/0/1/0/all/0/1"&gt;Vasileios Triantafyllou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1"&gt;Stefanos Zafeiriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-to-Image Translation of Synthetic Samples for Rare Classes. (arXiv:2106.12212v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12212</id>
        <link href="http://arxiv.org/abs/2106.12212"/>
        <updated>2021-06-24T01:51:42.937Z</updated>
        <summary type="html"><![CDATA[The natural world is long-tailed: rare classes are observed orders of
magnitudes less frequently than common ones, leading to highly-imbalanced data
where rare classes can have only handfuls of examples. Learning from few
examples is a known challenge for deep learning based classification
algorithms, and is the focus of the field of low-shot learning. One potential
approach to increase the training data for these rare classes is to augment the
limited real data with synthetic samples. This has been shown to help, but the
domain shift between real and synthetic hinders the approaches' efficacy when
tested on real data.

We explore the use of image-to-image translation methods to close the domain
gap between synthetic and real imagery for animal species classification in
data collected from camera traps: motion-activated static cameras used to
monitor wildlife. We use low-level feature alignment between source and target
domains to make synthetic data for a rare species generated using a graphics
engine more "realistic". Compared against a system augmented with unaligned
synthetic data, our experiments show a considerable decrease in classification
error rates on a rare species.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lanzini_E/0/1/0/all/0/1"&gt;Edoardo Lanzini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1"&gt;Sara Beery&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Circular-Structured Representation for Visual Emotion Distribution Learning. (arXiv:2106.12450v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12450</id>
        <link href="http://arxiv.org/abs/2106.12450"/>
        <updated>2021-06-24T01:51:42.925Z</updated>
        <summary type="html"><![CDATA[Visual Emotion Analysis (VEA) has attracted increasing attention recently
with the prevalence of sharing images on social networks. Since human emotions
are ambiguous and subjective, it is more reasonable to address VEA in a label
distribution learning (LDL) paradigm rather than a single-label classification
task. Different from other LDL tasks, there exist intrinsic relationships
between emotions and unique characteristics within them, as demonstrated in
psychological theories. Inspired by this, we propose a well-grounded
circular-structured representation to utilize the prior knowledge for visual
emotion distribution learning. To be specific, we first construct an Emotion
Circle to unify any emotional state within it. On the proposed Emotion Circle,
each emotion distribution is represented with an emotion vector, which is
defined with three attributes (i.e., emotion polarity, emotion type, emotion
intensity) as well as two properties (i.e., similarity, additivity). Besides,
we design a novel Progressive Circular (PC) loss to penalize the
dissimilarities between predicted emotion vector and labeled one in a
coarse-to-fine manner, which further boosts the learning process in an
emotion-specific way. Extensive experiments and comparisons are conducted on
public visual emotion distribution datasets, and the results demonstrate that
the proposed method outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jingyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lie_J/0/1/0/all/0/1"&gt;Ji Lie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Leida Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiumei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xinbo Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12407</id>
        <link href="http://arxiv.org/abs/2106.12407"/>
        <updated>2021-06-24T01:51:42.915Z</updated>
        <summary type="html"><![CDATA[Fetal motion is unpredictable and rapid on the scale of conventional MR scan
times. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and
dynamics of fetal function, is limited to fast imaging techniques with
compromises in image quality and resolution. Super-resolution for dynamic fetal
MRI is still a challenge, especially when multi-oriented stacks of image slices
for oversampling are not available and high temporal resolution for recording
the dynamics of the fetus or placenta is desired. Further, fetal motion makes
it difficult to acquire high-resolution images for supervised learning methods.
To address this problem, in this work, we propose STRESS (Spatio-Temporal
Resolution Enhancement with Simulated Scans), a self-supervised
super-resolution framework for dynamic fetal MRI with interleaved slice
acquisitions. Our proposed method simulates an interleaved slice acquisition
along the high-resolution axis on the originally acquired data to generate
pairs of low- and high-resolution images. Then, it trains a super-resolution
network by exploiting both spatial and temporal correlations in the MR time
series, which is used to enhance the resolution of the original data.
Evaluations on both simulated and in utero data show that our proposed method
outperforms other self-supervised super-resolution methods and improves image
quality, which is beneficial to other downstream tasks and evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1"&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1"&gt;P. Ellen Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1"&gt;Polina Golland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs. (arXiv:2106.12144v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12144</id>
        <link href="http://arxiv.org/abs/2106.12144"/>
        <updated>2021-06-24T01:51:42.903Z</updated>
        <summary type="html"><![CDATA[Conventional representation learning algorithms for knowledge graphs (KG) map
each entity to a unique embedding vector. Such a shallow lookup results in a
linear growth of memory consumption for storing the embedding matrix and incurs
high computational costs when working with real-world KGs. Drawing parallels
with subword tokenization commonly used in NLP, we explore the landscape of
more parameter-efficient node embedding strategies with possibly sublinear
memory requirements. To this end, we propose NodePiece, an anchor-based
approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of
subword/sub-entity units is constructed from anchor nodes in a graph with known
relation types. Given such a fixed-size vocabulary, it is possible to bootstrap
an encoding and embedding for any entity, including those unseen during
training. Experiments show that NodePiece performs competitively in node
classification, link prediction, and relation prediction tasks while retaining
less than 10% of explicit nodes in a graph as anchors and often having 10x
fewer parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_M/0/1/0/all/0/1"&gt;Mikhail Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiapeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denis_E/0/1/0/all/0/1"&gt;Etienne Denis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12204</id>
        <link href="http://arxiv.org/abs/2106.12204"/>
        <updated>2021-06-24T01:51:42.879Z</updated>
        <summary type="html"><![CDATA[Although instance segmentation has made considerable advancement over recent
years, it's still a challenge to design high accuracy algorithms with real-time
performance. In this paper, we propose a real-time instance segmentation
framework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask
head is added to predict some discriminative orientation maps, which are
explicitly defined as spatial offset vectors for both foreground and background
pixels. Thanks to the discrimination ability of orientation maps, masks can be
recovered without the need for extra foreground segmentation. All instances
that match with the same anchor size share a common orientation map. This
special sharing strategy reduces the amortized memory utilization for mask
predictions but without loss of mask granularity. Given the surviving box
predictions after NMS, instance masks can be concurrently constructed from the
corresponding orientation maps with low complexity. Owing to the concise design
for mask representation and its effective integration with the anchor-based
object detector, our method is qualified under real-time conditions while
maintaining competitive accuracy. Experiments on COCO benchmark show that
OrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a
single RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wentao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1"&gt;Chengyu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiman Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tingming Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixtures of Deep Neural Experts for Automated Speech Scoring. (arXiv:2106.12475v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12475</id>
        <link href="http://arxiv.org/abs/2106.12475"/>
        <updated>2021-06-24T01:51:42.862Z</updated>
        <summary type="html"><![CDATA[The paper copes with the task of automatic assessment of second language
proficiency from the language learners' spoken responses to test prompts. The
task has significant relevance to the field of computer assisted language
learning. The approach presented in the paper relies on two separate modules:
(1) an automatic speech recognition system that yields text transcripts of the
spoken interactions involved, and (2) a multiple classifier system based on
deep learners that ranks the transcripts into proficiency classes. Different
deep neural network architectures (both feed-forward and recurrent) are
specialized over diverse representations of the texts in terms of: a reference
grammar, the outcome of probabilistic language models, several word embeddings,
and two bag-of-word models. Combination of the individual classifiers is
realized either via a probabilistic pseudo-joint model, or via a neural mixture
of experts. Using the data of the third Spoken CALL Shared Task challenge, the
highest values to date were obtained in terms of three popular evaluation
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1"&gt;Sara Papi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trentin_E/0/1/0/all/0/1"&gt;Edmondo Trentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretter_R/0/1/0/all/0/1"&gt;Roberto Gretter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matassoni_M/0/1/0/all/0/1"&gt;Marco Matassoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falavigna_D/0/1/0/all/0/1"&gt;Daniele Falavigna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12373</id>
        <link href="http://arxiv.org/abs/2106.12373"/>
        <updated>2021-06-24T01:51:42.857Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models achieves high performance on machine reading
comprehension (MRC) tasks but the results are hard to explain. An appealing
approach to make models explainable is to provide rationales for its decision.
To facilitate supervised learning of human rationales, here we present PALRACE
(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for
800 passages selected from the RACE dataset. We further classified the question
to each passage into 6 types. Each passage was read by at least 26
participants, who labeled their rationales to answer the question. Besides, we
conducted a rationale evaluation session in which participants were asked to
answering the question solely based on labeled rationales, confirming that the
labeled rationales were of high quality and can sufficiently support question
answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Jiajie Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuran Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1"&gt;Peiqing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Cheng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xunyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Nai Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Behavior Mimics Distribution: Combining Individual and Group Behaviors for Federated Learning. (arXiv:2106.12300v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12300</id>
        <link href="http://arxiv.org/abs/2106.12300"/>
        <updated>2021-06-24T01:51:42.818Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has become an active and promising distributed
machine learning paradigm. As a result of statistical heterogeneity, recent
studies clearly show that the performance of popular FL methods (e.g., FedAvg)
deteriorates dramatically due to the client drift caused by local updates. This
paper proposes a novel Federated Learning algorithm (called IGFL), which
leverages both Individual and Group behaviors to mimic distribution, thereby
improving the ability to deal with heterogeneity. Unlike existing FL methods,
our IGFL can be applied to both client and server optimization. As a
by-product, we propose a new attention-based federated learning in the server
optimization of IGFL. To the best of our knowledge, this is the first time to
incorporate attention mechanisms into federated optimization. We conduct
extensive experiments and show that IGFL can significantly improve the
performance of existing federated learning methods. Especially when the
distributions of data among individuals are diverse, IGFL can improve the
classification accuracy by about 13% compared with prior baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image. (arXiv:2106.12265v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12265</id>
        <link href="http://arxiv.org/abs/2106.12265"/>
        <updated>2021-06-24T01:51:42.810Z</updated>
        <summary type="html"><![CDATA[Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.
type 2, is an essential prognostic factor. The two subtypes of pRCC have a
similar pattern, i.e., the papillary architecture, yet some subtle differences,
including cellular and cell-layer level patterns. However, the cellular and
cell-layer level patterns almost cannot be captured by existing CNN-based
models in large-size histopathological images, which brings obstacles to
directly applying these models to such a fine-grained classification task. This
paper proposes a novel instance-based Vision Transformer (i-ViT) to learn
robust representations of histopathological images for the pRCC subtyping task
by extracting finer features from instance patches (by cropping around
segmented nuclei and assigning predicted grades). The proposed i-ViT takes
top-K instances as input and aggregates them for capturing both the cellular
and cell-layer level patterns by a position-embedding layer, a grade-embedding
layer, and a multi-head multi-layer self-attention module. To evaluate the
performance of the proposed framework, experienced pathologists are invited to
selected 1162 regions of interest from 171 whole slide images of type 1 and
type 2 pRCC. Experimental results show that the proposed method achieves better
performance than existing CNN-based models with a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zeyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1"&gt;Bangyang Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1"&gt;Chang Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunbao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual-Information Based Few-Shot Classification. (arXiv:2106.12252v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12252</id>
        <link href="http://arxiv.org/abs/2106.12252"/>
        <updated>2021-06-24T01:51:42.804Z</updated>
        <summary type="html"><![CDATA[We introduce Transductive Infomation Maximization (TIM) for few-shot
learning. Our method maximizes the mutual information between the query
features and their label predictions for a given few-shot task, in conjunction
with a supervision loss based on the support set. We motivate our transductive
loss by deriving a formal relation between the classification accuracy and
mutual-information maximization. Furthermore, we propose a new
alternating-direction solver, which substantially speeds up transductive
inference over gradient-based optimization, while yielding competitive
accuracy. We also provide a convergence analysis of our solver based on
Zangwill's theory and bound-optimization arguments. TIM inference is modular:
it can be used on top of any base-training feature extractor. Following
standard transductive few-shot settings, our comprehensive experiments
demonstrate that TIM outperforms state-of-the-art methods significantly across
various datasets and networks, while used on top of a fixed feature extractor
trained with simple cross-entropy on the base classes, without resorting to
complex meta-learning schemes. It consistently brings between 2 % and 5 %
improvement in accuracy over the best performing method, not only on all the
well-established few-shot benchmarks but also on more challenging scenarios,
with random tasks, domain shift and larger numbers of classes, as in the
recently introduced META-DATASET. Our code is publicly available at
https://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorch
implementation of META-DATASET, along with additional benchmarking results, at
https://github.com/mboudiaf/pytorch-meta-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1"&gt;Malik Boudiaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masud_Z/0/1/0/all/0/1"&gt;Ziko Imtiaz Masud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xf4;me Rony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging. (arXiv:2106.12175v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12175</id>
        <link href="http://arxiv.org/abs/2106.12175"/>
        <updated>2021-06-24T01:51:42.786Z</updated>
        <summary type="html"><![CDATA[Image denoising is of great importance for medical imaging system, since it
can improve image quality for disease diagnosis and downstream image analyses.
In a variety of applications, dynamic imaging techniques are utilized to
capture the time-varying features of the subject, where multiple images are
acquired for the same subject at different time points. Although
signal-to-noise ratio of each time frame is usually limited by the short
acquisition time, the correlation among different time frames can be exploited
to improve denoising results with shared information across time frames. With
the success of neural networks in computer vision, supervised deep learning
methods show prominent performance in single-image denoising, which rely on
large datasets with clean-vs-noisy image pairs. Recently, several
self-supervised deep denoising models have been proposed, achieving promising
results without needing the pairwise ground truth of clean images. In the field
of multi-image denoising, however, very few works have been done on extracting
correlated information from multiple slices for denoising using self-supervised
deep learning methods. In this work, we propose Deformed2Self, an end-to-end
self-supervised deep learning framework for dynamic imaging denoising. It
combines single-image and multi-image denoising to improve image quality and
use a spatial transformer network to model motion between different slices.
Further, it only requires a single noisy image with a few auxiliary
observations at different time frames for training and inference. Evaluations
on phantom and in vivo data with different noise statistics show that our
method has comparable performance to other state-of-the-art unsupervised or
self-supervised denoising methods and outperforms under high noise levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Junshen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1"&gt;Elfar Adalsteinsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores. (arXiv:2106.12169v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.12169</id>
        <link href="http://arxiv.org/abs/2106.12169"/>
        <updated>2021-06-24T01:51:42.780Z</updated>
        <summary type="html"><![CDATA[Over the years, accelerating neural networks with quantization has been
widely studied. Unfortunately, prior efforts with diverse precisions (e.g.,
1-bit weights and 2-bit activations) are usually restricted by limited
precision support on GPUs (e.g., int1 and int4). To break such restrictions, we
introduce the first Arbitrary Precision Neural Network framework (APNN-TC) to
fully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,
APNN-TC first incorporates a novel emulation algorithm to support arbitrary
short bit-width computation with int1 compute primitives and XOR/AND Boolean
operations. Second, APNN-TC integrates arbitrary precision layer designs to
efficiently map our emulation algorithm to Tensor Cores with novel batching
strategies and specialized memory organization. Third, APNN-TC embodies a novel
arbitrary precision NN design to minimize memory access across layers and
further improve performance. Extensive evaluations show that APNN-TC can
achieve significant speedup over CUTLASS kernels and various NN models, such as
ResNet and VGG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Boyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1"&gt;Tong Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yufei Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Assistive Technologies for Activities of Daily Living of Elderly. (arXiv:2106.12183v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.12183</id>
        <link href="http://arxiv.org/abs/2106.12183"/>
        <updated>2021-06-24T01:51:42.774Z</updated>
        <summary type="html"><![CDATA[One of the distinct features of this century has been the population of older
adults which has been on a constant rise. Elderly people have several needs and
requirements due to physical disabilities, cognitive issues, weakened memory
and disorganized behavior, that they face with increasing age. The extent of
these limitations also differs according to the varying diversities in elderly,
which include age, gender, background, experience, skills, knowledge and so on.
These varying needs and challenges with increasing age, limits abilities of
older adults to perform Activities of Daily Living (ADLs) in an independent
manner. To add to it, the shortage of caregivers creates a looming need for
technology-based services for elderly people, to assist them in performing
their daily routine tasks to sustain their independent living and active aging.
To address these needs, this work consists of making three major contributions
in this field. First, it provides a rather comprehensive review of assisted
living technologies aimed at helping elderly people to perform ADLs. Second,
the work discusses the challenges identified through this review, that
currently exist in the context of implementation of assisted living services
for elderly care in Smart Homes and Smart Cities. Finally, the work also
outlines an approach for implementation, extension and integration of the
existing works in this field for development of a much-needed framework that
can provide personalized assistance and user-centered behavior interventions to
elderly as per their varying and ever-changing needs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism for Crowd Counting. (arXiv:2106.12163v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12163</id>
        <link href="http://arxiv.org/abs/2106.12163"/>
        <updated>2021-06-24T01:51:42.768Z</updated>
        <summary type="html"><![CDATA[Background noise and scale variation are common problems that have been long
recognized in crowd counting. Humans glance at a crowd image and instantly know
the approximate number of human and where they are through attention the crowd
regions and the congestion degree of crowd regions with a global receptive
filed. Hence, in this paper, we propose a novel feedback network with
Region-Aware block called RANet by modeling human's Top-Down visual perception
mechanism. Firstly, we introduce a feedback architecture to generate priority
maps that provide prior about candidate crowd regions in input images. The
prior enables the RANet pay more attention to crowd regions. Then we design
Region-Aware block that could adaptively encode the contextual information into
input images through global receptive field. More specifically, we scan the
whole input images and its priority maps in the form of column vector to obtain
a relevance matrix estimating their similarity. The relevance matrix obtained
would be utilized to build global relationships between pixels. Our method
outperforms state-of-the-art crowd counting methods on several public datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuehai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Badong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Shaoyi Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database. (arXiv:2106.12139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12139</id>
        <link href="http://arxiv.org/abs/2106.12139"/>
        <updated>2021-06-24T01:51:42.762Z</updated>
        <summary type="html"><![CDATA[In deep learning area, large-scale image datasets bring a breakthrough in the
success of object recognition and retrieval. Nowadays, as the embodiment of
innovation, the diversity of the industrial goods is significantly larger, in
which the incomplete multiview, multimodal and multilabel are different from
the traditional dataset. In this paper, we introduce an industrial goods
dataset, namely PatentNet, with numerous highly diverse, accurate and detailed
annotations of industrial goods images, and corresponding texts. In PatentNet,
the images and texts are sourced from design patent. Within over 6M images and
corresponding texts of industrial goods labeled manually checked by
professionals, PatentNet is the first ongoing industrial goods image database
whose varieties are wider than industrial goods datasets used previously for
benchmarking. PatentNet organizes millions of images into 32 classes and 219
subclasses based on the Locarno Classification Agreement. Through extensive
experiments on image classification, image retrieval and incomplete multiview
clustering, we demonstrate that our PatentNet is much more diverse, complex,
and challenging, enjoying higher potentials than existing industrial image
datasets. Furthermore, the characteristics of incomplete multiview, multimodal
and multilabel in PatentNet are able to offer unparalleled opportunities in the
artificial intelligence community and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_F/0/1/0/all/0/1"&gt;Fangyuan Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Da Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianjian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Ruijun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Senhong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jiangzhong Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yusen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qingyun Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-based Behavioral Recognition of Novelty Preference in Pigs. (arXiv:2106.12181v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12181</id>
        <link href="http://arxiv.org/abs/2106.12181"/>
        <updated>2021-06-24T01:51:42.746Z</updated>
        <summary type="html"><![CDATA[Behavioral scoring of research data is crucial for extracting domain-specific
metrics but is bottlenecked on the ability to analyze enormous volumes of
information using human labor. Deep learning is widely viewed as a key
advancement to relieve this bottleneck. We identify one such domain, where deep
learning can be leveraged to alleviate the process of manual scoring. Novelty
preference paradigms have been widely used to study recognition memory in pigs,
but analysis of these videos requires human intervention. We introduce a subset
of such videos in the form of the 'Pig Novelty Preference Behavior' (PNPB)
dataset that is fully annotated with pig actions and keypoints. In order to
demonstrate the application of state-of-the-art action recognition models on
this dataset, we compare LRCN, C3D, and TSM on the basis of various analytical
metrics and discuss common pitfalls of the models. Our methods achieve an
accuracy of 93% and a mean Average Precision of 96% in estimating piglet
behavior.

We open-source our code and annotated dataset at
https://github.com/AIFARMS/NOR-behavior-recognition]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shirke_A/0/1/0/all/0/1"&gt;Aniket Shirke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golden_R/0/1/0/all/0/1"&gt;Rebecca Golden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_M/0/1/0/all/0/1"&gt;Mrinal Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Green_Miller_A/0/1/0/all/0/1"&gt;Angela Green-Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_M/0/1/0/all/0/1"&gt;Matthew Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilger_R/0/1/0/all/0/1"&gt;Ryan N. Dilger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis. (arXiv:2106.12157v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.12157</id>
        <link href="http://arxiv.org/abs/2106.12157"/>
        <updated>2021-06-24T01:51:42.741Z</updated>
        <summary type="html"><![CDATA[The coronavirus continues to disrupt our everyday lives as it spreads at an
exponential rate. It needs to be detected quickly in order to quarantine
positive patients so as to avoid further spread. This work proposes a new
convolutional neural network (CNN) architecture called 'slow Encoding CNN. The
proposed model's best performance wrt Sensitivity, Positive Predictive Value
(PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19
- Screening X-ray images for COVID-19 Infections competition's test data
samples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive
class, while PN and SN stand for the Sensitivity and PPV of the COVID-19
negative class.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Akilan_T/0/1/0/all/0/1"&gt;Thangarajah Akilan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Natural Language Processing: A Survey. (arXiv:2003.08271v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.08271</id>
        <link href="http://arxiv.org/abs/2003.08271"/>
        <updated>2021-06-24T01:51:42.735Z</updated>
        <summary type="html"><![CDATA[Recently, the emergence of pre-trained models (PTMs) has brought natural
language processing (NLP) to a new era. In this survey, we provide a
comprehensive review of PTMs for NLP. We first briefly introduce language
representation learning and its research progress. Then we systematically
categorize existing PTMs based on a taxonomy with four perspectives. Next, we
describe how to adapt the knowledge of PTMs to the downstream tasks. Finally,
we outline some potential directions of PTMs for future research. This survey
is purposed to be a hands-on guide for understanding, using, and developing
PTMs for various NLP tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1"&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yige Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfan Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_N/0/1/0/all/0/1"&gt;Ning Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Negative Learning for Implicit Pseudo Label Rectification in Source-Free Domain Adaptive Semantic Segmentation. (arXiv:2106.12123v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12123</id>
        <link href="http://arxiv.org/abs/2106.12123"/>
        <updated>2021-06-24T01:51:42.728Z</updated>
        <summary type="html"><![CDATA[It is desirable to transfer the knowledge stored in a well-trained source
model onto non-annotated target domain in the absence of source data. However,
state-of-the-art methods for source free domain adaptation (SFDA) are subject
to strict limits: 1) access to internal specifications of source models is a
must; and 2) pseudo labels should be clean during self-training, making
critical tasks relying on semantic segmentation unreliable. Aiming at these
pitfalls, this study develops a domain adaptive solution to semantic
segmentation with pseudo label rectification (namely \textit{PR-SFDA}), which
operates in two phases: 1) \textit{Confidence-regularized unsupervised
learning}: Maximum squares loss applies to regularize the target model to
ensure the confidence in prediction; and 2) \textit{Noise-aware pseudo label
learning}: Negative learning enables tolerance to noisy pseudo labels in
training, meanwhile positive learning achieves fast convergence. Extensive
experiments have been performed on domain adaptive semantic segmentation
benchmark, \textit{GTA5 $\to$ Cityscapes}. Overall, \textit{PR-SFDA} achieves a
performance of 49.0 mIoU, which is very close to that of the state-of-the-art
counterparts. Note that the latter demand accesses to the source model's
internal specifications, whereas the \textit{PR-SFDA} solution needs none as a
sharp contrast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yusong Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaogang Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reachability Analysis of Convolutional Neural Networks. (arXiv:2106.12074v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12074</id>
        <link href="http://arxiv.org/abs/2106.12074"/>
        <updated>2021-06-24T01:51:42.722Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks have been widely employed as an effective
technique to handle complex and practical problems. However, one of the
fundamental problems is the lack of formal methods to analyze their behavior.
To address this challenge, we propose an approach to compute the exact
reachable sets of a network given an input domain, where the reachable set is
represented by the face lattice structure. Besides the computation of reachable
sets, our approach is also capable of backtracking to the input domain given an
output reachable set. Therefore, a full analysis of a network's behavior can be
realized. In addition, an approach for fast analysis is also introduced, which
conducts fast computation of reachable sets by considering selected sensitive
neurons in each layer. The exact pixel-level reachability analysis method is
evaluated on a CNN for the CIFAR10 dataset and compared to related works. The
fast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16
architecture for the ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaodong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_T/0/1/0/all/0/1"&gt;Tomoya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hoang-Dung Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1"&gt;Bardh Hoxha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1"&gt;Taylor T Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prokhorov_D/0/1/0/all/0/1"&gt;Danil Prokhorov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04054</id>
        <link href="http://arxiv.org/abs/2105.04054"/>
        <updated>2021-06-24T01:51:42.706Z</updated>
        <summary type="html"><![CDATA[Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1"&gt;Emily Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1"&gt;Premkumar Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volume Rendering of Neural Implicit Surfaces. (arXiv:2106.12052v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12052</id>
        <link href="http://arxiv.org/abs/2106.12052"/>
        <updated>2021-06-24T01:51:42.700Z</updated>
        <summary type="html"><![CDATA[Neural volume rendering became increasingly popular recently due to its
success in synthesizing novel views of a scene from a sparse set of input
images. So far, the geometry learned by neural volume rendering techniques was
modeled using a generic density function. Furthermore, the geometry itself was
extracted using an arbitrary level set of the density function leading to a
noisy, often low fidelity reconstruction. The goal of this paper is to improve
geometry representation and reconstruction in neural volume rendering. We
achieve that by modeling the volume density as a function of the geometry. This
is in contrast to previous work modeling the geometry as a function of the
volume density. In more detail, we define the volume density function as
Laplace's cumulative distribution function (CDF) applied to a signed distance
function (SDF) representation. This simple density representation has three
benefits: (i) it provides a useful inductive bias to the geometry learned in
the neural volume rendering process; (ii) it facilitates a bound on the opacity
approximation error, leading to an accurate sampling of the viewing ray.
Accurate sampling is important to provide a precise coupling of geometry and
radiance; and (iii) it allows efficient unsupervised disentanglement of shape
and appearance in volume rendering. Applying this new density representation to
challenging scene multiview datasets produced high quality geometry
reconstructions, outperforming relevant baselines. Furthermore, switching shape
and appearance between scenes is possible due to the disentanglement of the
two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yariv_L/0/1/0/all/0/1"&gt;Lior Yariv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1"&gt;Yoni Kasten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Fashion Image Captioning : Accounting for Data Diversity. (arXiv:2106.12154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12154</id>
        <link href="http://arxiv.org/abs/2106.12154"/>
        <updated>2021-06-24T01:51:42.695Z</updated>
        <summary type="html"><![CDATA[Image captioning has increasingly large domains of application, and fashion
is not an exception. Having automatic item descriptions is of great interest
for fashion web platforms hosting sometimes hundreds of thousands of images.
This paper is one of the first tackling image captioning for fashion images. To
contribute addressing dataset diversity issues, we introduced the InFashAIv1
dataset containing almost 16.000 African fashion item images with their titles,
prices and general descriptions. We also used the well known DeepFashion
dataset in addition to InFashAIv1. Captions are generated using the
\textit{Show and Tell} model made of CNN encoder and RNN Decoder. We showed
that jointly training the model on both datasets improves captions quality for
African style fashion images, suggesting a transfer learning from Western style
data. The InFashAIv1 dataset is released on
\href{https://github.com/hgilles06/infashai}{Github} to encourage works with
more diversity inclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hacheme_G/0/1/0/all/0/1"&gt;Gilles Hacheme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayouti_N/0/1/0/all/0/1"&gt;Noureini Sayouti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net. (arXiv:2106.12054v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12054</id>
        <link href="http://arxiv.org/abs/2106.12054"/>
        <updated>2021-06-24T01:51:42.690Z</updated>
        <summary type="html"><![CDATA[Transmission electron microscopy (TEM) is one of the primary tools to show
microstructural characterization of materials as well as film thickness.
However, manual determination of film thickness from TEM images is
time-consuming as well as subjective, especially when the films in question are
very thin and the need for measurement precision is very high. Such is the case
for head overcoat (HOC) thickness measurements in the magnetic hard disk drive
industry. It is therefore necessary to develop software to automatically
measure HOC thickness. In this paper, for the first time, we propose a HOC
layer segmentation method using NASNet-Large as an encoder and then followed by
a decoder architecture, which is one of the most commonly used architectures in
deep learning for image segmentation. To further improve segmentation results,
we are the first to propose a post-processing layer to remove irrelevant
portions in the segmentation result. To measure the thickness of the segmented
HOC layer, we propose a regressive convolutional neural network (RCNN) model as
well as orthogonal thickness calculation methods. Experimental results
demonstrate a higher dice score for our model which has lower mean squared
error and outperforms current state-of-the-art manual measurement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1"&gt;Brian D. Davison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talghader_V/0/1/0/all/0/1"&gt;Vivien W. Talghader&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1"&gt;Zhiyong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunkel_G/0/1/0/all/0/1"&gt;Gary J. Kunkel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Matrix Factorizations in Subspace Clustering. (arXiv:2106.12016v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12016</id>
        <link href="http://arxiv.org/abs/2106.12016"/>
        <updated>2021-06-24T01:51:42.684Z</updated>
        <summary type="html"><![CDATA[This article explores subspace clustering algorithms using CUR
decompositions, and examines the effect of various hyperparameters in these
algorithms on clustering performance on two real-world benchmark datasets, the
Hopkins155 motion segmentation dataset and the Yale face dataset. Extensive
experiments are done for a variety of sampling methods and oversampling
parameters for these datasets, and some guidelines for parameter choices are
given for practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arian_R/0/1/0/all/0/1"&gt;Reeshad Arian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Consistent Predictive Confidence through Fitted Ensembles. (arXiv:2106.12070v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12070</id>
        <link href="http://arxiv.org/abs/2106.12070"/>
        <updated>2021-06-24T01:51:42.668Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are behind many of the recent successes in machine
learning applications. However, these models can produce overconfident
decisions while encountering out-of-distribution (OOD) examples or making a
wrong prediction. This inconsistent predictive confidence limits the
integration of independently-trained learning models into a larger system. This
paper introduces separable concept learning framework to realistically measure
the performance of classifiers in presence of OOD examples. In this setup,
several instances of a classifier are trained on different parts of a partition
of the set of classes. Later, the performance of the combination of these
models is evaluated on a separate test set. Unlike current OOD detection
techniques, this framework does not require auxiliary OOD datasets and does not
separate classification from detection performance. Furthermore, we present a
new strong baseline for more consistent predictive confidence in deep models,
called fitted ensembles, where overconfident predictions are rectified by
transformed versions of the original classification task. Fitted ensembles can
naturally detect OOD examples without requiring auxiliary data by observing
contradicting predictions among its components. Experiments on MNIST, SVHN,
CIFAR-10/100, and ImageNet show fitted ensemble significantly outperform
conventional ensembles on OOD examples and are possible to scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Ankit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1"&gt;Kenneth O. Stanley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical Labeling of Over-segmented 3D Shapes. (arXiv:2106.12026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12026</id>
        <link href="http://arxiv.org/abs/2106.12026"/>
        <updated>2021-06-24T01:51:42.663Z</updated>
        <summary type="html"><![CDATA[Many learning-based 3D shape semantic segmentation methods assign labels to
shape atoms (e.g. points in a point cloud or faces in a mesh) with a
single-pass approach trained in an end-to-end fashion. Such methods achieve
impressive performance but require large amounts of labeled training data. This
paradigm entangles two separable subproblems: (1) decomposing a shape into
regions and (2) assigning semantic labels to these regions. We claim that
disentangling these subproblems reduces the labeled data burden: (1) region
decomposition requires no semantic labels and could be performed in an
unsupervised fashion, and (2) labeling shape regions instead of atoms results
in a smaller search space and should be learnable with less labeled training
data. In this paper, we investigate this second claim by presenting the
Neurally-Guided Shape Parser (NGSP), a method that learns how to assign
semantic labels to regions of an over-segmented 3D shape. We solve this problem
via MAP inference, modeling the posterior probability of a labeling assignment
conditioned on an input shape. We employ a Monte Carlo importance sampling
approach guided by a neural proposal network, a search-based approach made
feasible by assuming the input shape is decomposed into discrete regions. We
evaluate NGSP on the task of hierarchical semantic segmentation on manufactured
3D shapes from PartNet. We find that NGSP delivers significant performance
improvements over baselines that learn to label shape atoms and then aggregate
predictions for each shape region, especially in low-data regimes. Finally, we
demonstrate that NGSP is robust to region granularity, as it maintains strong
segmentation performance even as the regions undergo significant corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1"&gt;R. Kenny Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1"&gt;Rana Hanocka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1"&gt;Daniel Ritchie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12102</id>
        <link href="http://arxiv.org/abs/2106.12102"/>
        <updated>2021-06-24T01:51:42.657Z</updated>
        <summary type="html"><![CDATA[Most modern deep learning-based multi-view 3D reconstruction techniques use
RNNs or fusion modules to combine information from multiple images after
encoding them. These two separate steps have loose connections and do not
consider all available information while encoding each view. We propose
LegoFormer, a transformer-based model that unifies object reconstruction under
a single framework and parametrizes the reconstructed occupancy grid by its
decomposition factors. This reformulation allows the prediction of an object as
a set of independent structures then aggregated to obtain the final
reconstruction. Experiments conducted on ShapeNet display the competitive
performance of our network with respect to the state-of-the-art methods. We
also demonstrate how the use of self-attention leads to increased
interpretability of the model output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yagubbayli_F/0/1/0/all/0/1"&gt;Farid Yagubbayli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1"&gt;Alessio Tonioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences. (arXiv:2106.12153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12153</id>
        <link href="http://arxiv.org/abs/2106.12153"/>
        <updated>2021-06-24T01:51:42.649Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel straightforward method for medical volume
and sequence segmentation with limited annotations. To avert laborious
annotating, the recent success of self-supervised learning(SSL) motivates the
pre-training on unlabeled data. Despite its success, it is still challenging to
adapt typical SSL methods to volume/sequence segmentation, due to their lack of
mining on local semantic discrimination and rare exploitation on volume and
sequence structures. Based on the continuity between slices/frames and the
common spatial layout of organs across volumes/sequences, we introduced a novel
bootstrap self-supervised representation learning method by leveraging the
predictable possibility of neighboring slices. At the core of our method is a
simple and straightforward dense self-supervision on the predictions of local
representations and a strategy of predicting locals based on global context,
which enables stable and reliable supervision for both global and local
representation mining among volumes. Specifically, we first proposed an
asymmetric network with an attention-guided predictor to enforce
distance-specific prediction and supervision on slices within and across
volumes/sequences. Secondly, we introduced a novel prototype-based
foreground-background calibration module to enhance representation consistency.
The two parts are trained jointly on labeled and unlabeled data. When evaluated
on three benchmark datasets of medical volumes and sequences, our model
outperforms existing methods with a large margin of 4.5\% DSC on ACDC, 1.7\% on
Prostate, and 2.3\% on CAMUS. Intensive evaluations reveals the effectiveness
and superiority of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zejian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1"&gt;Wei Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianfu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wufeng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maria: A Visual Experience Powered Conversational Agent. (arXiv:2105.13073v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13073</id>
        <link href="http://arxiv.org/abs/2105.13073"/>
        <updated>2021-06-24T01:51:42.643Z</updated>
        <summary type="html"><![CDATA[Arguably, the visual perception of conversational agents to the physical
world is a key way for them to exhibit the human-like intelligence.
Image-grounded conversation is thus proposed to address this challenge.
Existing works focus on exploring the multimodal dialog models that ground the
conversation on a given image. In this paper, we take a step further to study
image-grounded conversation under a fully open-ended setting where no paired
dialog and image are assumed available. Specifically, we present Maria, a
neural conversation agent powered by the visual world experiences which are
retrieved from a large-scale image index. Maria consists of three flexible
components, i.e., text-to-image retriever, visual concept detector and
visual-knowledge-grounded response generator. The retriever aims to retrieve a
correlated image to the dialog from an image index, while the visual concept
detector extracts rich visual knowledge from the image. Then, the response
generator is grounded on the extracted visual knowledge and dialog context to
generate the target response. Extensive experiments demonstrate Maria
outperforms previous state-of-the-art methods on automatic metrics and human
evaluation, and can generate informative responses that have some visual
commonsense of the physical world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chongyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xiubo Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1"&gt;Fan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations. (arXiv:2106.12479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12479</id>
        <link href="http://arxiv.org/abs/2106.12479"/>
        <updated>2021-06-24T01:51:42.624Z</updated>
        <summary type="html"><![CDATA[Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pretrained vision models such as VGG16 and ResNet

Index Terms: Natural language, Vision, BERT, Transfer Learning, CNN, Domain
Adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Benarab_C/0/1/0/all/0/1"&gt;Charaf Eddine Benarab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Based Keyword Localisation in Speech using Visual Grounding. (arXiv:2106.08859v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08859</id>
        <link href="http://arxiv.org/abs/2106.08859"/>
        <updated>2021-06-24T01:51:42.618Z</updated>
        <summary type="html"><![CDATA[Visually grounded speech models learn from images paired with spoken
captions. By tagging images with soft text labels using a trained visual
classifier with a fixed vocabulary, previous work has shown that it is possible
to train a model that can detect whether a particular text keyword occurs in
speech utterances or not. Here we investigate whether visually grounded speech
models can also do keyword localisation: predicting where, within an utterance,
a given textual keyword occurs without any explicit text-based or alignment
supervision. We specifically consider whether incorporating attention into a
convolutional model is beneficial for localisation. Although absolute
localisation performance with visually supervised models is still modest
(compared to using unordered bag-of-word text labels for supervision), we show
that attention provides a large gain in performance over previous visually
grounded models. As in many other speech-image studies, we find that many of
the incorrect localisations are due to semantic confusions, e.g. locating the
word 'backstroke' for the query keyword 'swimming'.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olaleye_K/0/1/0/all/0/1"&gt;Kayode Olaleye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12014</id>
        <link href="http://arxiv.org/abs/2106.12014"/>
        <updated>2021-06-24T01:51:42.611Z</updated>
        <summary type="html"><![CDATA[Detailed analysis of seizure semiology, the symptoms and signs which occur
during a seizure, is critical for management of epilepsy patients. Inter-rater
reliability using qualitative visual analysis is often poor for semiological
features. Therefore, automatic and quantitative analysis of video-recorded
seizures is needed for objective assessment.

We present GESTURES, a novel architecture combining convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) to learn deep
representations of arbitrarily long videos of epileptic seizures.

We use a spatiotemporal CNN (STCNN) pre-trained on large human action
recognition (HAR) datasets to extract features from short snippets (approx. 0.5
s) sampled from seizure videos. We then train an RNN to learn seizure-level
representations from the sequence of features.

We curated a dataset of seizure videos from 68 patients and evaluated
GESTURES on its ability to classify seizures into focal onset seizures (FOSs)
(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),
obtaining an accuracy of 98.9% using bidirectional long short-term memory
(BLSTM) units.

We demonstrate that an STCNN trained on a HAR dataset can be used in
combination with an RNN to accurately represent arbitrarily long videos of
seizures. GESTURES can provide accurate seizure classification by modeling
sequences of semiologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Catherine Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1"&gt;Beate Diehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly. (arXiv:2106.12037v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12037</id>
        <link href="http://arxiv.org/abs/2106.12037"/>
        <updated>2021-06-24T01:51:42.604Z</updated>
        <summary type="html"><![CDATA[Deep learning has recently been applied to optical music recognition (OMR).
However, currently OMR processing from various sheet music images still lacks
precision to be widely applicable. Here, we present an MMdA (Measure-based
Multimodal deep learning (DL)-driven Assembly) method allowing for end-to-end
OMR processing from various images including inclined photo images. Using this
method, measures are extracted by a deep learning model, aligned, and resized
to be used for inference of given musical symbol components by using multiple
deep learning models in sequence or in parallel. Use of each standardized
measure enables efficient training of the models and accurate adjustment of
five staff lines in each measure. Multiple musical symbol component category
models with a small number of feature types can represent a diverse set of
notes and other musical symbols including chords. This MMdA method provides a
solution to end-to-end OMR processing with precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shishido_T/0/1/0/all/0/1"&gt;Tomoyuki Shishido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fati_F/0/1/0/all/0/1"&gt;Fehmiju Fati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tokushige_D/0/1/0/all/0/1"&gt;Daisuke Tokushige&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ono_Y/0/1/0/all/0/1"&gt;Yasuhiro Ono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-based Multi-Task Model for Country and Province Level Modern Standard Arabic and Dialectal Arabic Identification. (arXiv:2106.12495v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12495</id>
        <link href="http://arxiv.org/abs/2106.12495"/>
        <updated>2021-06-24T01:51:42.599Z</updated>
        <summary type="html"><![CDATA[Dialect and standard language identification are crucial tasks for many
Arabic natural language processing applications. In this paper, we present our
deep learning-based system, submitted to the second NADI shared task for
country-level and province-level identification of Modern Standard Arabic (MSA)
and Dialectal Arabic (DA). The system is based on an end-to-end deep Multi-Task
Learning (MTL) model to tackle both country-level and province-level MSA/DA
identification. The latter MTL model consists of a shared Bidirectional Encoder
Representation Transformers (BERT) encoder, two task-specific attention layers,
and two classifiers. Our key idea is to leverage both the task-discriminative
and the inter-task shared features for country and province MSA/DA
identification. The obtained results show that our MTL model outperforms
single-task models on most subtasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mekki_A/0/1/0/all/0/1"&gt;Abdellah El Mekki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahdaouy_A/0/1/0/all/0/1"&gt;Abdelkader El Mahdaouy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Essefar_K/0/1/0/all/0/1"&gt;Kabil Essefar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamoun_N/0/1/0/all/0/1"&gt;Nabil El Mamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrada_I/0/1/0/all/0/1"&gt;Ismail Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoumsi_A/0/1/0/all/0/1"&gt;Ahmed Khoumsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Legal Proceedings Status: Approaches Based on Sequential Text Data. (arXiv:2003.11561v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.11561</id>
        <link href="http://arxiv.org/abs/2003.11561"/>
        <updated>2021-06-24T01:51:42.584Z</updated>
        <summary type="html"><![CDATA[The objective of this paper is to develop predictive models to classify
Brazilian legal proceedings in three possible classes of status: (i) archived
proceedings, (ii) active proceedings, and (iii) suspended proceedings. This
problem's resolution is intended to assist public and private institutions in
managing large portfolios of legal proceedings, providing gains in scale and
efficiency. In this paper, legal proceedings are made up of sequences of short
texts called "motions." We combined several natural language processing (NLP)
and machine learning techniques to solve the problem. Although working with
Portuguese NLP, which can be challenging due to lack of resources, our
approaches performed remarkably well in the classification task, achieving
maximum accuracy of .93 and top average F1 Scores of .89 (macro) and .93
(weighted). Furthermore, we could extract and interpret the patterns learned by
one of our models besides quantifying how those patterns relate to the
classification task. The interpretability step is important among machine
learning legal applications and gives us an exciting insight into how black-box
models make decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Polo_F/0/1/0/all/0/1"&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciochetti_I/0/1/0/all/0/1"&gt;Itamar Ciochetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolo_E/0/1/0/all/0/1"&gt;Emerson Bertolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.12011</id>
        <link href="http://arxiv.org/abs/2106.12011"/>
        <updated>2021-06-24T01:51:42.577Z</updated>
        <summary type="html"><![CDATA[This paper jointly resolves two problems in vision transformer: i) the
computation of Multi-Head Self-Attention (MHSA) has high computational/space
complexity; ii) recent vision transformer networks are overly tuned for image
classification, ignoring the difference between image classification (simple
scenarios, more similar to NLP) and downstream scene understanding tasks
(complicated scenarios, rich structural and contextual information). To this
end, we note that pyramid pooling has been demonstrated to be effective in
various vision tasks owing to its powerful context abstraction, and its natural
property of spatial invariance is suitable to address the loss of structural
information (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA
for alleviating its high requirement on computational resources (problem i)).
In this way, this pooling-based MHSA can well address the above two problems
and is thus flexible and powerful for downstream scene understanding tasks.
Plugged with our pooling-based MHSA, we build a downstream-task-oriented
transformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive
experiments demonstrate that, when applied P2T as the backbone network, it
shows substantial superiority in various downstream scene understanding tasks
such as semantic segmentation, object detection, instance segmentation, and
visual saliency detection, compared to previous CNN- and transformer-based
networks. The code will be released at https://github.com/yuhuan-wu/P2T. Note
that this technical report will keep updating.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xin Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LPF: A Language-Prior Feedback Objective Function for De-biased Visual Question Answering. (arXiv:2105.14300v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14300</id>
        <link href="http://arxiv.org/abs/2105.14300"/>
        <updated>2021-06-24T01:51:42.572Z</updated>
        <summary type="html"><![CDATA[Most existing Visual Question Answering (VQA) systems tend to overly rely on
language bias and hence fail to reason from the visual clue. To address this
issue, we propose a novel Language-Prior Feedback (LPF) objective function, to
re-balance the proportion of each answer's loss value in the total VQA loss.
The LPF firstly calculates a modulating factor to determine the language bias
using a question-only branch. Then, the LPF assigns a self-adaptive weight to
each training sample in the training process. With this reweighting mechanism,
the LPF ensures that the total VQA loss can be reshaped to a more balanced
form. By this means, the samples that require certain visual information to
predict will be efficiently used during training. Our method is simple to
implement, model-agnostic, and end-to-end trainable. We conduct extensive
experiments and the results show that the LPF (1) brings a significant
improvement over various VQA models, (2) achieves competitive performance on
the bias-sensitive VQA-CP v2 benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zujie Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haifeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiaying Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. (arXiv:2106.00130v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00130</id>
        <link href="http://arxiv.org/abs/2106.00130"/>
        <updated>2021-06-24T01:51:42.566Z</updated>
        <summary type="html"><![CDATA[Faceted summarization provides briefings of a document from different
perspectives. Readers can quickly comprehend the main points of a long document
with the help of a structured outline. However, little research has been
conducted on this subject, partially due to the lack of large-scale faceted
summarization datasets. In this study, we present FacetSum, a faceted
summarization benchmark built on Emerald journal articles, covering a diverse
range of domains. Different from traditional document-summary pairs, FacetSum
provides multiple summaries, each targeted at specific sections of a long
document, including the purpose, method, findings, and value. Analyses and
empirical results on our dataset reveal the importance of bringing structure
into summaries. We believe FacetSum will spur further advances in summarization
research and foster the development of NLP systems that can leverage the
structured information in both long texts and summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaker_K/0/1/0/all/0/1"&gt;Khushboo Thaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yue Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xingdi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Daqing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages. (arXiv:2106.12398v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12398</id>
        <link href="http://arxiv.org/abs/2106.12398"/>
        <updated>2021-06-24T01:51:42.503Z</updated>
        <summary type="html"><![CDATA[Lexically constrained machine translation allows the user to manipulate the
output sentence by enforcing the presence or absence of certain words and
phrases. Although current approaches can enforce terms to appear in the
translation, they often struggle to make the constraint word form agree with
the rest of the generated output. Our manual analysis shows that 46% of the
errors in the output of a baseline constrained model for English to Czech
translation are related to agreement. We investigate mechanisms to allow neural
machine translation to infer the correct word inflection given lemmatized
constraints. In particular, we focus on methods based on training the model
with constraints provided as part of the input sequence. Our experiments on the
English-Czech language pair show that this approach improves the translation of
constrained terms in both automatic and manual evaluation by reducing errors in
agreement. Our approach thus eliminates inflection errors, without introducing
new errors or decreasing the overall quality of the translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1"&gt;Josef Jon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aires_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Aires&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1"&gt;Du&amp;#x161;an Vari&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09807</id>
        <link href="http://arxiv.org/abs/2012.09807"/>
        <updated>2021-06-24T01:51:42.485Z</updated>
        <summary type="html"><![CDATA[Word embeddings (e.g., word2vec) have been applied successfully to eCommerce
products through~\textit{prod2vec}. Inspired by the recent performance
improvements on several NLP tasks brought by contextualized embeddings, we
propose to transfer BERT-like architectures to eCommerce: our model --
~\textit{Prod2BERT} -- is trained to generate representations of products
through masked session modeling. Through extensive experiments over multiple
shops, different tasks, and a range of design choices, we systematically
compare the accuracy of~\textit{Prod2BERT} and~\textit{prod2vec} embeddings:
while~\textit{Prod2BERT} is found to be superior in several scenarios, we
highlight the importance of resources and hyperparameters in the best
performing models. Finally, we provide guidelines to practitioners for training
embeddings under a variety of computational and data constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling. (arXiv:2010.03802v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03802</id>
        <link href="http://arxiv.org/abs/2010.03802"/>
        <updated>2021-06-24T01:51:42.479Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to the problem of text style transfer. Unlike
previous approaches requiring style-labeled training data, our method makes use
of readily-available unlabeled text by relying on the implicit connection in
style between adjacent sentences, and uses labeled data only at inference time.
We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to
extract a style vector from text and use it to condition the decoder to perform
style transfer. As our label-free training results in a style vector space
encoding many facets of style, we recast transfers as "targeted restyling"
vector operations that adjust specific attributes of the input while preserving
others. We demonstrate that training on unlabeled Amazon reviews data results
in a model that is competitive on sentiment transfer, even compared to models
trained fully on labeled data. Furthermore, applying our novel method to a
diverse corpus of unlabeled web text results in a single model capable of
transferring along multiple dimensions of style (dialect, emotiveness,
formality, politeness, sentiment) despite no additional training and using only
a handful of exemplars at inference time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Riley_P/0/1/0/all/0/1"&gt;Parker Riley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mandy Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Girish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uthus_D/0/1/0/all/0/1"&gt;David Uthus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_Z/0/1/0/all/0/1"&gt;Zarana Parekh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:42.471Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.05297</id>
        <link href="http://arxiv.org/abs/2001.05297"/>
        <updated>2021-06-24T01:51:42.453Z</updated>
        <summary type="html"><![CDATA[This paper addresses a series of complex and unresolved issues in the
historical phonology of West Iranian languages. The West Iranian languages
(Persian, Kurdish, Balochi, and other languages) display a high degree of
non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to
language contact; we argue, however, that an oversimplified view of the
processes at work has prevailed in the literature on West Iranian dialectology,
with specialists assuming that deviations from an expected outcome in a given
non-Persian language are due to lexical borrowing from some chronological stage
of Persian. It is demonstrated that this qualitative approach yields at times
problematic conclusions stemming from the lack of explicit probabilistic
inferences regarding the distribution of the data: Persian may not be the sole
donor language; additionally, borrowing at the lexical level is not always the
mechanism that introduces irregularity. In many cases, the possibility that
West Iranian languages show different reflexes in different conditioning
environments remains under-explored. We employ a novel Bayesian approach
designed to overcome these problems and tease apart the different determinants
of irregularity in patterns of West Iranian sound change. Our methodology
allows us to provisionally resolve a number of outstanding questions in the
literature on West Iranian dialectology concerning the dialectal affiliation of
certain sound changes. We outline future directions for work of this sort.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cathcart_C/0/1/0/all/0/1"&gt;Chundra Aroor Cathcart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Positivity Bias in Negative Reviews. (arXiv:2106.12056v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12056</id>
        <link href="http://arxiv.org/abs/2106.12056"/>
        <updated>2021-06-24T01:51:42.447Z</updated>
        <summary type="html"><![CDATA[Prior work has revealed that positive words occur more frequently than
negative words in human expressions, which is typically attributed to
positivity bias, a tendency for people to report positive views of reality. But
what about the language used in negative reviews? Consistent with prior work,
we show that English negative reviews tend to contain more positive words than
negative words, using a variety of datasets. We reconcile this observation with
prior findings on the pragmatics of negation, and show that negations are
commonly associated with positive words in negative reviews. Furthermore, in
negative reviews, the majority of sentences with positive words express
negative opinions based on sentiment classifiers, indicating some form of
negation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aithal_M/0/1/0/all/0/1"&gt;Madhusudhan Aithal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chenhao Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. (arXiv:2106.12066v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12066</id>
        <link href="http://arxiv.org/abs/2106.12066"/>
        <updated>2021-06-24T01:51:42.424Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning is one of the key problems in natural language
processing, but the relative scarcity of labeled data holds back the progress
for languages other than English. Pretrained cross-lingual models are a source
of powerful language-agnostic representations, yet their inherent reasoning
capabilities are still actively studied. In this work, we design a simple
approach to commonsense reasoning which trains a linear classifier with weights
of multi-head attention as features. To evaluate this approach, we create a
multilingual Winograd Schema corpus by processing several datasets from prior
work within a standardized pipeline and measure cross-lingual generalization
ability in terms of out-of-sample performance. The method performs
competitively with recent supervised and unsupervised approaches for
commonsense reasoning, even when applied to other languages in a zero-shot
manner. Also, we demonstrate that most of the performance is given by the same
small subset of attention heads for all studied languages, which provides
evidence of universal reasoning capabilities in multilingual encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Spelling Inconsistencies in Code-Switching ASR using Contextualized CTC Loss. (arXiv:2005.07920v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07920</id>
        <link href="http://arxiv.org/abs/2005.07920"/>
        <updated>2021-06-24T01:51:42.416Z</updated>
        <summary type="html"><![CDATA[Code-Switching (CS) remains a challenge for Automatic Speech Recognition
(ASR), especially character-based models. With the combined choice of
characters from multiple languages, the outcome from character-based models
suffers from phoneme duplication, resulting in language-inconsistent spellings.
We propose Contextualized Connectionist Temporal Classification (CCTC) loss to
encourage spelling consistencies of a character-based non-autoregressive ASR
which allows for faster inference. The CCTC loss conditions the main prediction
on the predicted contexts to ensure language consistency in the spellings. In
contrast to existing CTC-based approaches, CCTC loss does not require
frame-level alignments, since the context ground truth is obtained from the
model's estimated path. Compared to the same model trained with regular CTC
loss, our method consistently improved the ASR performance on both CS and
monolingual corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Naowarat_B/0/1/0/all/0/1"&gt;Burin Naowarat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kongthaworn_T/0/1/0/all/0/1"&gt;Thananchai Kongthaworn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karunratanakul_K/0/1/0/all/0/1"&gt;Korrawe Karunratanakul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sheng Hui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chuangsuwanich_E/0/1/0/all/0/1"&gt;Ekapol Chuangsuwanich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations. (arXiv:2106.12384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12384</id>
        <link href="http://arxiv.org/abs/2106.12384"/>
        <updated>2021-06-24T01:51:42.409Z</updated>
        <summary type="html"><![CDATA[Event extraction is a fundamental task for natural language processing.
Finding the roles of event arguments like event participants is essential for
event extraction. However, doing so for real-life event descriptions is
challenging because an argument's role often varies in different contexts.
While the relationship and interactions between multiple arguments are useful
for settling the argument roles, such information is largely ignored by
existing approaches. This paper presents a better approach for event extraction
by explicitly utilizing the relationships of event arguments. We achieve this
through a carefully designed task-oriented dialogue system. To model the
argument relation, we employ reinforcement learning and incremental learning to
extract multiple arguments via a multi-turned, iterative process. Our approach
leverages knowledge of the already extracted arguments of the same sentence to
determine the role of arguments that would be difficult to decide individually.
It then uses the newly obtained information to improve the decisions of
previously extracted arguments. This two-way feedback process allows us to
exploit the argument relations to effectively settle argument roles, leading to
better sentence understanding and event extraction. Experimental results show
that our approach consistently outperforms seven state-of-the-art event
extraction methods for the classification of events and argument role and
argument identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1"&gt;Yuanxing Ning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. (arXiv:2106.12566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12566</id>
        <link href="http://arxiv.org/abs/2106.12566"/>
        <updated>2021-06-24T01:51:42.400Z</updated>
        <summary type="html"><![CDATA[The attention module, which is a crucial component in Transformer, cannot
scale efficiently to long sequences due to its quadratic complexity. Many works
focus on approximating the dot-then-exponentiate softmax function in the
original attention, leading to sub-quadratic or even linear-complexity
Transformer architectures. However, we show that these methods cannot be
applied to more powerful attention modules that go beyond the
dot-then-exponentiate style, e.g., Transformers with relative positional
encoding (RPE). Since in many state-of-the-art models, relative positional
encoding is used as default, designing efficient Transformers that can
incorporate RPE is appealing. In this paper, we propose a novel way to
accelerate attention calculation for Transformers with RPE on top of the
kernelized attention. Based upon the observation that relative positional
encoding forms a Toeplitz matrix, we mathematically show that kernelized
attention with RPE can be calculated efficiently using Fast Fourier Transform
(FFT). With FFT, our method achieves $\mathcal{O}(n\log n)$ time complexity.
Interestingly, we further demonstrate that properly using relative positional
encoding can mitigate the training instability problem of vanilla kernelized
attention. On a wide range of tasks, we empirically show that our models can be
trained from scratch without any optimization issues. The learned model
performs better than many efficient Transformer variants and is faster than
standard Transformer in the long-sequence regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shengjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shanda Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Tianle Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Di He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Dinglan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1"&gt;Guolin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple and Practical Approach to Improve Misspellings in OCR Text. (arXiv:2106.12030v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12030</id>
        <link href="http://arxiv.org/abs/2106.12030"/>
        <updated>2021-06-24T01:51:42.391Z</updated>
        <summary type="html"><![CDATA[The focus of our paper is the identification and correction of non-word
errors in OCR text. Such errors may be the result of incorrect insertion,
deletion, or substitution of a character, or the transposition of two adjacent
characters within a single word. Or, it can be the result of word boundary
problems that lead to run-on errors and incorrect-split errors. The traditional
N-gram correction methods can handle single-word errors effectively. However,
they show limitations when dealing with split and merge errors. In this paper,
we develop an unsupervised method that can handle both errors. The method we
develop leads to a sizable improvement in the correction rates. This tutorial
paper addresses very difficult word correction problems - namely incorrect
run-on and split errors - and illustrates what needs to be considered when
addressing such problems. We outline a possible approach and assess its success
on a limited study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junxia Lin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Ledolter_J/0/1/0/all/0/1"&gt;Johannes Ledolter&lt;/a&gt; (2) ((1) Georgetown University Medical Center, Georgetown University, (2) Tippie College of Business, University of Iowa)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ad Text Classification with Transformer-Based Natural Language Processing Methods. (arXiv:2106.10899v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10899</id>
        <link href="http://arxiv.org/abs/2106.10899"/>
        <updated>2021-06-24T01:51:42.366Z</updated>
        <summary type="html"><![CDATA[In this study, a natural language processing-based (NLP-based) method is
proposed for the sector-wise automatic classification of ad texts created on
online advertising platforms. Our data set consists of approximately 21,000
labeled advertising texts from 12 different sectors. In the study, the
Bidirectional Encoder Representations from Transformers (BERT) model, which is
a transformer-based language model that is recently used in fields such as text
classification in the natural language processing literature, was used. The
classification efficiencies obtained using a pre-trained BERT model for the
Turkish language are shown in detail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arslan_B/0/1/0/all/0/1"&gt;B&amp;#xfc;&amp;#x15f;ra Arslan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1"&gt;G&amp;#xf6;k&amp;#xe7;e Polat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences. (arXiv:2106.12027v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12027</id>
        <link href="http://arxiv.org/abs/2106.12027"/>
        <updated>2021-06-24T01:51:42.350Z</updated>
        <summary type="html"><![CDATA[Atomic clauses are fundamental text units for understanding complex
sentences. Identifying the atomic sentences within complex sentences is
important for applications such as summarization, argument mining, discourse
analysis, discourse parsing, and question answering. Previous work mainly
relies on rule-based methods dependent on parsing. We propose a new task to
decompose each complex sentence into simple sentences derived from the tensed
clauses in the source, and a novel problem formulation as a graph edit task.
Our neural model learns to Accept, Break, Copy or Drop elements of a graph that
combines word adjacency and grammatical dependencies. The full processing
pipeline includes modules for graph construction, graph editing, and sentence
generation from the output graph. We introduce DeSSE, a new dataset designed to
train and evaluate complex sentence decomposition, and MinWiki, a subset of
MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on
MinWiki. On DeSSE, which has a more even balance of complex sentence types, our
model achieves higher accuracy on the number of atomic sentences than an
encoder-decoder baseline. Results include a detailed error analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanjun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ting-hao/0/1/0/all/0/1"&gt;Ting-hao&lt;/a&gt; (Kenneth) &lt;a href="http://arxiv.org/find/cs/1/au:+Huang/0/1/0/all/0/1"&gt;Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passonneau_R/0/1/0/all/0/1"&gt;Rebecca J. Passonneau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recognising Biomedical Names: Challenges and Solutions. (arXiv:2106.12230v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12230</id>
        <link href="http://arxiv.org/abs/2106.12230"/>
        <updated>2021-06-24T01:51:42.344Z</updated>
        <summary type="html"><![CDATA[The growth rate in the amount of biomedical documents is staggering.
Unlocking information trapped in these documents can enable researchers and
practitioners to operate confidently in the information world. Biomedical NER,
the task of recognising biomedical names, is usually employed as the first step
of the NLP pipeline. Standard NER models, based on sequence tagging technique,
are good at recognising short entity mentions in the generic domain. However,
there are several open challenges of applying these models to recognise
biomedical names: 1) Biomedical names may contain complex inner structure
(discontinuity and overlapping) which cannot be recognised using standard
sequence tagging technique; 2) The training of NER models usually requires
large amount of labelled data, which are difficult to obtain in the biomedical
domain; and, 3) Commonly used language representation models are pre-trained on
generic data; a domain shift therefore exists between these models and target
biomedical data. To deal with these challenges, we explore several research
directions and make the following contributions: 1) we propose a
transition-based NER model which can recognise discontinuous mentions; 2) We
develop a cost-effective approach that nominates the suitable pre-training
data; and, 3) We design several data augmentation methods for NER. Our
contributions have obvious practical implications, especially when new
biomedical applications are needed. Our proposed data augmentation methods can
help the NER model achieve decent performance, requiring only a small amount of
labelled data. Our investigation regarding selecting pre-training data can
improve the model by incorporating language representation models, which are
pre-trained using in-domain data. Finally, our proposed transition-based NER
model can further improve the performance by recognising discontinuous
mentions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiang Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry. (arXiv:2003.07723v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07723</id>
        <link href="http://arxiv.org/abs/2003.07723"/>
        <updated>2021-06-24T01:51:42.337Z</updated>
        <summary type="html"><![CDATA[Most approaches to emotion analysis of social media, literature, news, and
other domains focus exclusively on basic emotion categories as defined by Ekman
or Plutchik. However, art (such as literature) enables engagement in a broader
range of more complex and subtle emotions. These have been shown to also
include mixed emotional responses. We consider emotions in poetry as they are
elicited in the reader, rather than what is expressed in the text or intended
by the author. Thus, we conceptualize a set of aesthetic emotions that are
predictive of aesthetic appreciation in the reader, and allow the annotation of
multiple labels per line to capture mixed emotions within their context. We
evaluate this novel setting in an annotation experiment both with carefully
trained experts and via crowdsourcing. Our annotation with experts leads to an
acceptable agreement of kappa = .70, resulting in a consistent dataset for
future large scale analysis. Finally, we conduct first emotion classification
experiments based on BERT, showing that identifying aesthetic emotions is
challenging in our data, with up to .52 F1-micro on the German subset. Data and
resources are available at https://github.com/tnhaider/poetry-emotion]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haider_T/0/1/0/all/0/1"&gt;Thomas Haider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Evgeny Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1"&gt;Roman Klinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menninghaus_W/0/1/0/all/0/1"&gt;Winfried Menninghaus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11988</id>
        <link href="http://arxiv.org/abs/2106.11988"/>
        <updated>2021-06-24T01:51:42.329Z</updated>
        <summary type="html"><![CDATA[A growing effort in NLP aims to build datasets of human explanations.
However, the term explanation encompasses a broad range of notions, each with
different properties and ramifications. Our goal is to provide an overview of
diverse types of explanations and human limitations, and discuss implications
for collecting and using explanations in NLP. Inspired by prior work in
psychology and cognitive sciences, we group existing human explanations in NLP
into three categories: proximal mechanism, evidence, and procedure. These three
types differ in nature and have implications for the resultant explanations.
For instance, procedure is not considered explanations in psychology and
connects with a rich body of work on learning from instructions. The diversity
of explanations is further evidenced by proxy questions that are needed for
annotators to interpret and answer open-ended why questions. Finally,
explanations may require different, often deeper, understandings than
predictions, which casts doubt on whether humans can provide useful
explanations in some tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chenhao Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender System. (arXiv:2106.10898v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10898</id>
        <link href="http://arxiv.org/abs/2106.10898"/>
        <updated>2021-06-24T01:51:42.312Z</updated>
        <summary type="html"><![CDATA[Multi-armed bandits (MAB) provide a principled online learning approach to
attain the balance between exploration and exploitation. Due to the superior
performance and low feedback learning without the learning to act in multiple
situations, Multi-armed Bandits drawing widespread attention in applications
ranging such as recommender systems. Likewise, within the recommender system,
collaborative filtering (CF) is arguably the earliest and most influential
method in the recommender system. Crucially, new users and an ever-changing
pool of recommended items are the challenges that recommender systems need to
address. For collaborative filtering, the classical method is training the
model offline, then perform the online testing, but this approach can no longer
handle the dynamic changes in user preferences which is the so-called cold
start. So how to effectively recommend items to users in the absence of
effective information? To address the aforementioned problems, a multi-armed
bandit based collaborative filtering recommender system has been proposed,
named BanditMF. BanditMF is designed to address two challenges in the
multi-armed bandits algorithm and collaborative filtering: (1) how to solve the
cold start problem for collaborative filtering under the condition of scarcity
of valid information, (2) how to solve the sub-optimal problem of bandit
algorithms in strong social relations domains caused by independently
estimating unknown parameters associated with each user and ignoring
correlations between users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shenghao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks using Switching Tokens. (arXiv:2106.12131v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.12131</id>
        <link href="http://arxiv.org/abs/2106.12131"/>
        <updated>2021-06-24T01:51:42.304Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel spoken-text-style conversion method that
can simultaneously execute multiple style conversion modules such as
punctuation restoration and disfluency deletion without preparing matched
datasets. In practice, transcriptions generated by automatic speech recognition
systems are not highly readable because they often include many disfluencies
and do not include punctuation marks. To improve their readability, multiple
spoken-text-style conversion modules that individually model a single
conversion task are cascaded because matched datasets that simultaneously
handle multiple conversion tasks are often unavailable. However, the cascading
is unstable against the order of tasks because of the chain of conversion
errors. Besides, the computation cost of the cascading must be higher than the
single conversion. To execute multiple conversion tasks simultaneously without
preparing matched datasets, our key idea is to distinguish individual
conversion tasks using the on-off switch. In our proposed zero-shot joint
modeling, we switch the individual tasks using multiple switching tokens,
enabling us to utilize a zero-shot learning approach to executing simultaneous
conversions. Our experiments on joint modeling of disfluency deletion and
punctuation restoration demonstrate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ihori_M/0/1/0/all/0/1"&gt;Mana Ihori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makishima_N/0/1/0/all/0/1"&gt;Naoki Makishima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_T/0/1/0/all/0/1"&gt;Tomohiro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takashima_A/0/1/0/all/0/1"&gt;Akihiko Takashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orihashi_S/0/1/0/all/0/1"&gt;Shota Orihashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masumura_R/0/1/0/all/0/1"&gt;Ryo Masumura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Energy-Based Models for End-to-End Speech Recognition. (arXiv:2103.14152v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14152</id>
        <link href="http://arxiv.org/abs/2103.14152"/>
        <updated>2021-06-24T01:51:42.294Z</updated>
        <summary type="html"><![CDATA[End-to-end models with auto-regressive decoders have shown impressive results
for automatic speech recognition (ASR). These models formulate the
sequence-level probability as a product of the conditional probabilities of all
individual tokens given their histories. However, the performance of locally
normalised models can be sub-optimal because of factors such as exposure bias.
Consequently, the model distribution differs from the underlying data
distribution. In this paper, the residual energy-based model (R-EBM) is
proposed to complement the auto-regressive ASR model to close the gap between
the two distributions. Meanwhile, R-EBMs can also be regarded as
utterance-level confidence estimators, which may benefit many downstream tasks.
Experiments on a 100hr LibriSpeech dataset show that R-EBMs can reduce the word
error rates (WERs) by 8.2%/6.7% while improving areas under precision-recall
curves of confidence scores by 12.6%/28.4% on test-clean/test-other sets.
Furthermore, on a state-of-the-art model using self-supervised learning
(wav2vec 2.0), R-EBMs still significantly improves both the WER and confidence
estimation performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiujia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liangliang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woodland_P/0/1/0/all/0/1"&gt;Philip C. Woodland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable Zero-shot modeling of clinical depression symptoms from text. (arXiv:2106.10928v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10928</id>
        <link href="http://arxiv.org/abs/2106.10928"/>
        <updated>2021-06-24T01:51:42.283Z</updated>
        <summary type="html"><![CDATA[We focus on exploring various approaches of Zero-Shot Learning (ZSL) and
their explainability for a challenging yet important supervised learning task
notorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)
from text. We start with a comprehensive synthesis of different components of
our ZSL modeling and analysis of our ground truth samples and Depression
symptom clues curation process with the help of a practicing clinician. We next
analyze the accuracy of various state-of-the-art ZSL models and their potential
enhancements for our task. Further, we sketch a framework for the use of ZSL
for hierarchical text-based explanation mechanism, which we call, Syntax
Tree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from
which we conclude that we can use ZSL models and achieve reasonable accuracy
and explainability, measured by a proposed Explainability Index (EI). This work
is, to our knowledge, the first work to exhaustively explore the efficacy of
ZSL models for DSD task, both in terms of accuracy and explainability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivapalan_S/0/1/0/all/0/1"&gt;Sudhakar Sivapalan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Transformer-based Sequential Recommenders through Preference Editing. (arXiv:2106.12120v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12120</id>
        <link href="http://arxiv.org/abs/2106.12120"/>
        <updated>2021-06-24T01:51:42.106Z</updated>
        <summary type="html"><![CDATA[One of the key challenges in Sequential Recommendation (SR) is how to extract
and represent user preferences. Traditional SR methods rely on the next item as
the supervision signal to guide preference extraction and representation. We
propose a novel learning strategy, named preference editing. The idea is to
force the SR model to discriminate the common and unique preferences in
different sequences of interactions between users and the recommender system.
By doing so, the SR model is able to learn how to identify common and unique
user preferences, and thereby do better user preference extraction and
representation. We propose a transformer based SR model, named MrTransformer
(Multi-preference Transformer), that concatenates some special tokens in front
of the sequence to represent multiple user preferences and makes sure they
capture different aspects through a preference coverage mechanism. Then, we
devise a preference editing-based self-supervised learning mechanism for
training MrTransformer which contains two main operations: preference
separation and preference recombination. The former separates the common and
unique user preferences for a given pair of sequences. The latter swaps the
common preferences to obtain recombined user preferences for each sequence.
Based on the preference separation and preference recombination operations, we
define two types of SSL loss that require that the recombined preferences are
similar to the original ones, and the common preferences are close to each
other.

We carry out extensive experiments on two benchmark datasets. MrTransformer
with preference editing significantly outperforms state-of-the-art SR methods
in terms of Recall, MRR and NDCG. We find that long sequences whose user
preferences are harder to extract and represent benefit most from preference
editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Muyang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Pengjie Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Huasheng Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph-based Method for Session-based Recommendations. (arXiv:2106.12085v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12085</id>
        <link href="http://arxiv.org/abs/2106.12085"/>
        <updated>2021-06-24T01:51:42.093Z</updated>
        <summary type="html"><![CDATA[We present a graph-based approach for the data management tasks and the
efficient operation of a system for session-based next-item recommendations.
The proposed method can collect data continuously and incrementally from an
ecommerce web site, thus seemingly prepare the necessary data infrastructure
for the recommendation algorithm to operate without any excessive training
phase. Our work aims at developing a recommender method that represents a
balance between data processing and management efficiency requirements and the
effectiveness of the recommendations produced. We use the Neo4j graph database
to implement a prototype of such a system. Furthermore, we use an industry
dataset corresponding to a typical e-commerce session-based scenario, and we
report on experiments using our graph-based approach and other state-of-the-art
machine learning and deep learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delianidi_M/0/1/0/all/0/1"&gt;Marina Delianidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salampasis_M/0/1/0/all/0/1"&gt;Michail Salampasis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diamantaras_K/0/1/0/all/0/1"&gt;Konstantinos Diamantaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siomos_T/0/1/0/all/0/1"&gt;Theodosios Siomos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katsalis_A/0/1/0/all/0/1"&gt;Alkiviadis Katsalis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karaveli_I/0/1/0/all/0/1"&gt;Iphigenia Karaveli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal Dimension for Similarity Search of Environmental Sounds. (arXiv:2102.02964v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02964</id>
        <link href="http://arxiv.org/abs/2102.02964"/>
        <updated>2021-06-24T01:51:42.072Z</updated>
        <summary type="html"><![CDATA[This paper proposes new acoustic feature signatures based on the multiscale
fractal dimension (MFD), which are robust against the diversity of
environmental sounds, for the content-based similarity search. The diversity of
sound sources and acoustic compositions is a typical feature of environmental
sounds. Several acoustic features have been proposed for environmental sounds.
Among them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),
which describes frequency-domain features. However, in addition to these
features in the frequency domain, environmental sounds have other important
features in the time domain with various time scales. In our previous paper, we
proposed enhanced multiscale fractal dimension signature (EMFD) for
environmental sounds. This paper extends EMFD by using the kernel density
estimation method, which results in better performance of the similarity search
tasks. Furthermore, it newly proposes another acoustic feature signature based
on MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).
The MFD-VL signature describes several features of the time-varying envelope
for long periods of time. The MFD-VL signature has stability and robustness
against background noise and small fluctuations in the parameters of sound
sources, which are produced in field recordings. We discuss the effectiveness
of these signatures in the similarity sound search by comparing with acoustic
features proposed in the DCASE 2018 challenges. Due to the unique
descriptiveness of our proposed signatures, we confirmed the signatures are
effective when they are used with other acoustic features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sunouchi_M/0/1/0/all/0/1"&gt;Motohiro Sunouchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1"&gt;Masaharu Yoshioka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Approach to Detect Redundant Activity Labels For More Representative Event Logs. (arXiv:2103.16061v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16061</id>
        <link href="http://arxiv.org/abs/2103.16061"/>
        <updated>2021-06-24T01:51:42.012Z</updated>
        <summary type="html"><![CDATA[The insights revealed from process mining heavily rely on the quality of
event logs. Activities extracted from healthcare information systems with the
free-text nature may lead to inconsistent labels. Such inconsistency would then
lead to redundancy of activity labels, which refer to labels that have
different syntax but share the same behaviours. The identifications of these
labels from data-driven process discovery are difficult and rely heavily on
resource-intensive human review. Existing work achieves low accuracy either
redundant activity labels are in low occurrence frequency or the existence of
numerical data values as attributes in event logs. However, these phenomena are
commonly observed in healthcare information systems. In this paper, we propose
an approach to detect redundant activity labels using control-flow relations
and numerical data values from event logs. Natural Language Processing is also
integrated into our method to assess semantic similarity between labels, which
provides users with additional insights. We have evaluated our approach through
synthetic logs generated from the real-life Sepsis log and a case study using
the MIMIC-III data set. The results demonstrate that our approach can
successfully detect redundant activity labels. This approach can add value to
the preprocessing step to generate more representative event logs for process
mining tasks in the healthcare domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tam_C/0/1/0/all/0/1"&gt;Charmaine Tam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poon_S/0/1/0/all/0/1"&gt;Simon Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Goes Shopping: Comparing Distributional Models for Product Representations. (arXiv:2012.09807v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09807</id>
        <link href="http://arxiv.org/abs/2012.09807"/>
        <updated>2021-06-24T01:51:41.975Z</updated>
        <summary type="html"><![CDATA[Word embeddings (e.g., word2vec) have been applied successfully to eCommerce
products through~\textit{prod2vec}. Inspired by the recent performance
improvements on several NLP tasks brought by contextualized embeddings, we
propose to transfer BERT-like architectures to eCommerce: our model --
~\textit{Prod2BERT} -- is trained to generate representations of products
through masked session modeling. Through extensive experiments over multiple
shops, different tasks, and a range of design choices, we systematically
compare the accuracy of~\textit{Prod2BERT} and~\textit{prod2vec} embeddings:
while~\textit{Prod2BERT} is found to be superior in several scenarios, we
highlight the importance of resources and hyperparameters in the best
performing models. Finally, we provide guidelines to practitioners for training
embeddings under a variety of computational and data constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1"&gt;Federico Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bingqing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tagliabue_J/0/1/0/all/0/1"&gt;Jacopo Tagliabue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphConfRec: A Graph Neural Network-Based Conference Recommender System. (arXiv:2106.12340v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12340</id>
        <link href="http://arxiv.org/abs/2106.12340"/>
        <updated>2021-06-24T01:51:41.960Z</updated>
        <summary type="html"><![CDATA[In today's academic publishing model, especially in Computer Science,
conferences commonly constitute the main platforms for releasing the latest
peer-reviewed advancements in their respective fields. However, choosing a
suitable academic venue for publishing one's research can represent a
challenging task considering the plethora of available conferences,
particularly for those at the start of their academic careers, or for those
seeking to publish outside of their usual domain. In this paper, we propose
GraphConfRec, a conference recommender system which combines SciGraph and graph
neural networks, to infer suggestions based not only on title and abstract, but
also on co-authorship and citation relationships. GraphConfRec achieves a
recall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention
network-based recommendation model. A user study with 25 subjects supports the
positive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iana_A/0/1/0/all/0/1"&gt;Andreea Iana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1"&gt;Heiko Paulheim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and Processing. (arXiv:2106.12320v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.12320</id>
        <link href="http://arxiv.org/abs/2106.12320"/>
        <updated>2021-06-24T01:51:41.930Z</updated>
        <summary type="html"><![CDATA[Automatic processing of bibliographic data becomes very important in digital
libraries, data science and machine learning due to its importance in keeping
pace with the significant increase of published papers every year from one side
and to the inherent challenges from the other side. This processing has several
aspects including but not limited to I) Automatic extraction of references from
PDF documents, II) Building an accurate citation graph, III) Author name
disambiguation, etc. Bibliographic data is heterogeneous by nature and occurs
in both structured (e.g. citation graph) and unstructured (e.g. publications)
formats. Therefore, it requires data science and machine learning techniques to
be processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on
Bibliographic Data Analysis and Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1"&gt;Zeyd Boukhers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mayr_P/0/1/0/all/0/1"&gt;Philipp Mayr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peroni_S/0/1/0/all/0/1"&gt;Silvio Peroni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnt Sparsity for Effective and Interpretable Document Ranking. (arXiv:2106.12460v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.12460</id>
        <link href="http://arxiv.org/abs/2106.12460"/>
        <updated>2021-06-24T01:51:41.910Z</updated>
        <summary type="html"><![CDATA[Machine learning models for the ad-hoc retrieval of documents and passages
have recently shown impressive improvements due to better language
understanding using large pre-trained language models. However, these
over-parameterized models are inherently non-interpretable and do not provide
any information on the parts of the documents that were used to arrive at a
certain prediction.

In this paper we introduce the select and rank paradigm for document ranking,
where interpretability is explicitly ensured when scoring longer documents.
Specifically, we first select sentences in a document based on the input query
and then predict the query-document score based only on the selected sentences,
acting as an explanation. We treat sentence selection as a latent variable
trained jointly with the ranker from the final output. We conduct extensive
experiments to demonstrate that our inherently interpretable select-and-rank
approach is competitive in comparison to other state-of-the-art methods and
sometimes even outperforms them. This is due to our novel end-to-end training
approach based on weighted reservoir sampling that manages to train the
selector despite the stochastic sentence selection. We also show that our
sentence selection approach can be used to provide explanations for models that
operate on only parts of the document, such as BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonhardt_J/0/1/0/all/0/1"&gt;Jurek Leonhardt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1"&gt;Koustav Rudra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Based Respiratory Pathology Classification Using Cough Sounds. (arXiv:2106.12174v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.12174</id>
        <link href="http://arxiv.org/abs/2106.12174"/>
        <updated>2021-06-24T01:51:41.860Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are transforming the world, as well as our healthcare
system. We propose a deep learning-based cough sound classification model that
can distinguish between children with healthy versus pathological coughs such
as asthma, upper respiratory tract infection (URTI), and lower respiratory
tract infection (LRTI). In order to train a deep neural network model, we
collected a new dataset of cough sounds, labelled with clinician's diagnosis.
The chosen model is a bidirectional long-short term memory network (BiLSTM)
based on Mel Frequency Cepstral Coefficients (MFCCs) features. The resulting
trained model when trained for classifying two classes of coughs -- healthy or
pathology (in general or belonging to a specific respiratory pathology),
reaches accuracy exceeding 84\% when classifying cough to the label provided by
the physicians' diagnosis. In order to classify subject's respiratory pathology
condition, results of multiple cough epochs per subject were combined. The
resulting prediction accuracy exceeds 91\% for all three respiratory
pathologies. However, when the model is trained to classify and discriminate
among the four classes of coughs, overall accuracy dropped: one class of
pathological coughs are often misclassified as other. However, if one consider
the healthy cough classified as healthy and pathological cough classified to
have some kind of pathologies, then the overall accuracy of four class model is
above 84\%. A longitudinal study of MFCC feature space when comparing
pathological and recovered coughs collected from the same subjects revealed the
fact that pathological cough irrespective of the underlying conditions occupy
the same feature space making it harder to differentiate only using MFCC
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+T_B/0/1/0/all/0/1"&gt;Balamurali B T&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hee_H/0/1/0/all/0/1"&gt;Hwan Ing Hee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1"&gt;Saumitra Kapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teoh_O/0/1/0/all/0/1"&gt;Oon Hoe Teoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1"&gt;Sung Shin Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Khai Pin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herremans_D/0/1/0/all/0/1"&gt;Dorien Herremans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jer Ming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rank-one matrix estimation with groupwise heteroskedasticity. (arXiv:2106.11950v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11950</id>
        <link href="http://arxiv.org/abs/2106.11950"/>
        <updated>2021-06-23T01:48:42.476Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating a rank-one matrix from Gaussian
observations where different blocks of the matrix are observed under different
noise levels. This problem is motivated by applications in clustering and
community detection where latent variables can be partitioned into a fixed
number of known groups (e.g., users and items) and the blocks of the matrix
correspond to different types of pairwise interactions (e.g., user-user,
user-item, or item-item interactions). In the setting where the number of
blocks is fixed while the number of variables tends to infinity, we prove
asymptotically exact formulas for the minimum mean-squared error in estimating
both the matrix and the latent variables. These formulas describe the weak
recovery thresholds for the problem and reveal invariance properties with
respect to certain scalings of the noise variance. We also derive an
approximate message passing algorithm and a gradient descent algorithm and show
empirically that these algorithms achieve the information-theoretic limits in
certain regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Behne_J/0/1/0/all/0/1"&gt;Joshua K. Behne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Reeves_G/0/1/0/all/0/1"&gt;Galen Reeves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Optimization Algorithms via Sum-of-Squares. (arXiv:1906.04648v4 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.04648</id>
        <link href="http://arxiv.org/abs/1906.04648"/>
        <updated>2021-06-23T01:48:42.470Z</updated>
        <summary type="html"><![CDATA[We introduce a new framework for unifying and systematizing the performance
analysis of first-order black-box optimization algorithms for unconstrained
convex minimization. The low-cost iteration complexity enjoyed by first-order
algorithms renders them particularly relevant for applications in machine
learning and large-scale data analysis. Relying on sum-of-squares (SOS)
optimization, we introduce a hierarchy of semidefinite programs that give
increasingly better convergence bounds for higher levels of the hierarchy.
Alluding to the power of the SOS hierarchy, we show that the (dual of the)
first level corresponds to the Performance Estimation Problem (PEP) introduced
by Drori and Teboulle [Math. Program., 145(1):451--482, 2014], a powerful
framework for determining convergence rates of first-order optimization
algorithms. Consequently, many results obtained within the PEP framework can be
reinterpreted as degree-1 SOS proofs, and thus, the SOS framework provides a
promising new approach for certifying improved rates of convergence by means of
higher-order SOS certificates. To determine analytical rate bounds, in this
work we use the first level of the SOS hierarchy and derive new result{s} for
noisy gradient descent with inexact line search methods (Armijo, Wolfe, and
Goldstein).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sandra S. Y. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1"&gt;Antonios Varvitsiotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligned Contrastive Predictive Coding. (arXiv:2104.11946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11946</id>
        <link href="http://arxiv.org/abs/2104.11946"/>
        <updated>2021-06-23T01:48:42.386Z</updated>
        <summary type="html"><![CDATA[We investigate the possibility of forcing a self-supervised model trained
using a contrastive predictive loss to extract slowly varying latent
representations. Rather than producing individual predictions for each of the
future representations, the model emits a sequence of predictions shorter than
that of the upcoming representations to which they will be aligned. In this
way, the prediction network solves a simpler task of predicting the next
symbols, but not their exact timing, while the encoding network is trained to
produce piece-wise constant latent codes. We evaluate the model on a speech
coding task and demonstrate that the proposed Aligned Contrastive Predictive
Coding (ACPC) leads to higher linear phone prediction accuracy and lower ABX
error rates, while being slightly faster to train due to the reduced number of
prediction heads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1"&gt;Jan Chorowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1"&gt;Grzegorz Ciesielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Dzikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1"&gt;Adrian &amp;#x141;a&amp;#x144;cucki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1"&gt;Ricard Marxer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1"&gt;Mateusz Opala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1"&gt;Piotr Pusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Rychlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Stypu&amp;#x142;kowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Optimisation of Complex Systems with a Quantum Annealer. (arXiv:2105.13945v3 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13945</id>
        <link href="http://arxiv.org/abs/2105.13945"/>
        <updated>2021-06-23T01:48:42.369Z</updated>
        <summary type="html"><![CDATA[We perform an in-depth comparison of quantum annealing with several classical
optimisation techniques, namely thermal annealing, Nelder-Mead, and gradient
descent. We begin with a direct study of the 2D Ising model on a quantum
annealer, and compare its properties directly with those of the thermal 2D
Ising model. These properties include an Ising-like phase transition that can
be induced by either a change in 'quantum-ness' of the theory, or by a scaling
the Ising couplings up or down. This behaviour is in accord with what is
expected from the physical understanding of the quantum system. We then go on
to demonstrate the efficacy of the quantum annealer at minimising several
increasingly hard two dimensional potentials. For all the potentials we find
the general behaviour that Nelder-Mead and gradient descent methods are very
susceptible to becoming trapped in false minima, while the thermal anneal
method is somewhat better at discovering the true minimum. However, and despite
current limitations on its size, the quantum annealer performs a minimisation
very markedly better than any of these classical techniques. A quantum anneal
can be designed so that the system almost never gets trapped in a false
minimum, and rapidly and successfully minimises the potentials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Abel_S/0/1/0/all/0/1"&gt;Steve Abel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Blance_A/0/1/0/all/0/1"&gt;Andrew Blance&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Copyright in Generative Deep Learning. (arXiv:2105.09266v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09266</id>
        <link href="http://arxiv.org/abs/2105.09266"/>
        <updated>2021-06-23T01:48:42.362Z</updated>
        <summary type="html"><![CDATA[Machine-generated artworks are now part of the contemporary art scene: they
are attracting significant investments and they are presented in exhibitions
together with those created by human artists. These artworks are mainly based
on generative deep learning techniques. Also given their success, several legal
problems arise when working with these techniques.

In this article we consider a set of key questions in the area of generative
deep learning for the arts. Is it possible to use copyrighted works as training
set for generative models? How do we legally store their copies in order to
perform the training process? And then, who (if someone) will own the copyright
on the generated data? We try to answer these questions considering the law in
force in both US and EU and the future alternatives, trying to define a set of
guidelines for artists and developers working on deep learning generated art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Franceschelli_G/0/1/0/all/0/1"&gt;Giorgio Franceschelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1"&gt;Mirco Musolesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhance Multimodal Model Performance with Data Augmentation: Facebook Hateful Meme Challenge Solution. (arXiv:2105.13132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13132</id>
        <link href="http://arxiv.org/abs/2105.13132"/>
        <updated>2021-06-23T01:48:42.356Z</updated>
        <summary type="html"><![CDATA[Hateful content detection is one of the areas where deep learning can and
should make a significant difference. The Hateful Memes Challenge from Facebook
helps fulfill such potential by challenging the contestants to detect hateful
speech in multi-modal memes using deep learning algorithms. In this paper, we
utilize multi-modal, pre-trained models VilBERT and Visual BERT. We improved
models' performance by adding training datasets generated from data
augmentation. Enlarging the training data set helped us get a more than 2%
boost in terms of AUROC with the Visual BERT model. Our approach achieved
0.7439 AUROC along with an accuracy of 0.7037 on the challenge's test set,
which revealed remarkable progress.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zinc Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hutchin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Spatial-Temporal Feature Learning for EEG Decoding. (arXiv:2106.11170v1 [eess.SP] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.11170</id>
        <link href="http://arxiv.org/abs/2106.11170"/>
        <updated>2021-06-23T01:48:42.350Z</updated>
        <summary type="html"><![CDATA[At present, people usually use some methods based on convolutional neural
networks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have
limitations in perceiving global dependencies, which is not adequate for common
EEG paradigms with a strong overall relationship. Regarding this issue, we
propose a novel EEG decoding method that mainly relies on the attention
mechanism. The EEG data is firstly preprocessed and spatially filtered. And
then, we apply attention transforming on the feature-channel dimension so that
the model can enhance more relevant spatial features. The most crucial step is
to slice the data in the time dimension for attention transforming, and finally
obtain a highly distinguishable representation. At this time, global averaging
pooling and a simple fully-connected layer are used to classify different
categories of EEG data. Experiments on two public datasets indicate that the
strategy of attention transforming effectively utilizes spatial and temporal
features. And we have reached the level of the state-of-the-art in
multi-classification of EEG, with fewer parameters. As far as we know, it is
the first time that a detailed and complete method based on the transformer
idea has been proposed in this field. It has good potential to promote the
practicality of brain-computer interface (BCI). The source code can be found
at: \textit{https://github.com/anranknight/EEG-Transformer}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yonghao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xueyu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1"&gt;Longhan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network insensitivity to parameter noise via adversarial regularization. (arXiv:2106.05009v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05009</id>
        <link href="http://arxiv.org/abs/2106.05009"/>
        <updated>2021-06-23T01:48:42.334Z</updated>
        <summary type="html"><![CDATA[Neuromorphic neural network processors, in the form of compute-in-memory
crossbar arrays of memristors, or in the form of subthreshold analog and
mixed-signal ASICs, promise enormous advantages in compute density and energy
efficiency for NN-based ML tasks. However, these technologies are prone to
computational non-idealities, due to process variation and intrinsic device
physics. This degrades the task performance of networks deployed to the
processor, by introducing parameter noise into the deployed model. While it is
possible to calibrate each device, or train networks individually for each
processor, these approaches are expensive and impractical for commercial
deployment. Alternative methods are therefore needed to train networks that are
inherently robust against parameter variation, as a consequence of network
architecture and parameters. We present a new adversarial network optimisation
algorithm that attacks network parameters during training, and promotes robust
performance during inference in the face of parameter variation. Our approach
introduces a regularization term penalising the susceptibility of a network to
weight perturbation. We compare against previous approaches for producing
parameter insensitivity such as dropout, weight smoothing and introducing
parameter noise during training. We show that our approach produces models that
are more robust to targeted parameter variation, and equally robust to random
parameter variation. Our approach finds minima in flatter locations in the
weight-loss landscape compared with other approaches, highlighting that the
networks found by our technique are less sensitive to parameter perturbation.
Our work provides an approach to deploy neural network architectures to
inference devices that suffer from computational non-idealities, with minimal
loss of performance. ...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1"&gt;Julian B&amp;#xfc;chel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faber_F/0/1/0/all/0/1"&gt;Fynn Faber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muir_D/0/1/0/all/0/1"&gt;Dylan R. Muir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Conditional Gaussian Mixture Model for Constrained Clustering. (arXiv:2106.06385v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06385</id>
        <link href="http://arxiv.org/abs/2106.06385"/>
        <updated>2021-06-23T01:48:42.318Z</updated>
        <summary type="html"><![CDATA[Constrained clustering has gained significant attention in the field of
machine learning as it can leverage prior information on a growing amount of
only partially labeled data. Following recent advances in deep generative
models, we propose a novel framework for constrained clustering that is
intuitive, interpretable, and can be trained efficiently in the framework of
stochastic gradient variational inference. By explicitly integrating domain
knowledge in the form of probabilistic relations, our proposed model (DC-GMM)
uncovers the underlying distribution of data conditioned on prior clustering
preferences, expressed as pairwise constraints. These constraints guide the
clustering process towards a desirable partition of the data by indicating
which samples should or should not belong to the same cluster. We provide
extensive experiments to demonstrate that DC-GMM shows superior clustering
performances and robustness compared to state-of-the-art deep constrained
clustering methods on a wide range of data sets. We further demonstrate the
usefulness of our approach on two challenging real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1"&gt;Laura Manduchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1"&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1"&gt;Holger Michel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1"&gt;Sven Wellmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1"&gt;Julia E. Vogt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07467</id>
        <link href="http://arxiv.org/abs/2105.07467"/>
        <updated>2021-06-23T01:48:42.310Z</updated>
        <summary type="html"><![CDATA[Background: Colonoscopy remains the gold-standard screening for colorectal
cancer. However, significant miss rates for polyps have been reported,
particularly when there are multiple small adenomas. This presents an
opportunity to leverage computer-aided systems to support clinicians and reduce
the number of polyps missed.

Method: In this work we introduce the Focus U-Net, a novel dual
attention-gated deep neural network, which combines efficient spatial and
channel-based attention into a single Focus Gate module to encourage selective
learning of polyp features. The Focus U-Net further incorporates short-range
skip connections and deep supervision. Furthermore, we introduce the Hybrid
Focal loss, a new compound loss function based on the Focal loss and Focal
Tversky loss, to handle class-imbalanced image segmentation. For our
experiments, we selected five public datasets containing images of polyps
obtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,
ETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we
use the Dice similarity coefficient (DSC) and Intersection over Union (IoU)
metrics.

Results: Our model achieves state-of-the-art results for both CVC-ClinicDB
and Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When
evaluated on a combination of five public polyp datasets, our model similarly
achieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of
0.809, a 14% and 15% improvement over the previous state-of-the-art results of
0.768 and 0.702, respectively.

Conclusions: This study shows the potential for deep learning to provide fast
and accurate polyp segmentation results for use during colonoscopy. The Focus
U-Net may be adapted for future use in newer non-invasive screening and more
broadly to other biomedical image segmentation tasks involving class imbalance
and requiring efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-06-23T01:48:42.302Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retraining. Our key innovation is to
redefine the gradient to a new synaptic parameter, allowing better exploration
of network structures by taking full advantage of the competition between
pruning and regrowth of connections. The experimental results show that the
proposed method achieves minimal loss of SNNs' performance on MNIST and
CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under
unprecedented 0.73% connectivity, which reveals remarkable structure refining
capability in SNNs. Our work suggests that there exists extremely high
redundancy in deep SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradient Bayesian Robust Optimization for Imitation Learning. (arXiv:2106.06499v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06499</id>
        <link href="http://arxiv.org/abs/2106.06499"/>
        <updated>2021-06-23T01:48:42.292Z</updated>
        <summary type="html"><![CDATA[The difficulty in specifying rewards for many real-world problems has led to
an increased focus on learning rewards from human feedback, such as
demonstrations. However, there are often many different reward functions that
explain the human feedback, leaving agents with uncertainty over what the true
reward function is. While most policy optimization approaches handle this
uncertainty by optimizing for expected performance, many applications demand
risk-averse behavior. We derive a novel policy gradient-style robust
optimization approach, PG-BROIL, that optimizes a soft-robust objective that
balances expected performance and risk. To the best of our knowledge, PG-BROIL
is the first policy optimization algorithm robust to a distribution of reward
hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL
can produce a family of behaviors ranging from risk-neutral to risk-averse and
outperforms state-of-the-art imitation learning algorithms when learning from
ambiguous demonstrations by hedging against uncertainty, rather than seeking to
uniquely identify the demonstrator's reward function.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javed_Z/0/1/0/all/0/1"&gt;Zaynah Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1"&gt;Daniel S. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Satvik Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jerry Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrik_M/0/1/0/all/0/1"&gt;Marek Petrik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragan_A/0/1/0/all/0/1"&gt;Anca D. Dragan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Polyak Stepsize with a Moving Target. (arXiv:2106.11851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11851</id>
        <link href="http://arxiv.org/abs/2106.11851"/>
        <updated>2021-06-23T01:48:42.250Z</updated>
        <summary type="html"><![CDATA[We propose a new stochastic gradient method that uses recorded past loss
values to reduce the variance. Our method can be interpreted as a new
stochastic variant of the Polyak Stepsize that converges globally without
assuming interpolation. Our method introduces auxiliary variables, one for each
data point, that track the loss value for each data point. We provide a global
convergence theory for our method by showing that it can be interpreted as a
special variant of online SGD. The new method only stores a single scalar per
data point, opening up new applications for variance reduction where memory is
the bottleneck.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gower_R/0/1/0/all/0/1"&gt;Robert M. Gower&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1"&gt;Aaron Defazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SISA: Securing Images by Selective Alteration. (arXiv:2106.11770v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11770</id>
        <link href="http://arxiv.org/abs/2106.11770"/>
        <updated>2021-06-23T01:48:42.233Z</updated>
        <summary type="html"><![CDATA[With an increase in mobile and camera devices' popularity, digital content in
the form of images has increased drastically. As personal life is being
continuously documented in pictures, the risk of losing it to eavesdroppers is
a matter of grave concern. Secondary storage is the most preferred medium for
the storage of personal and other images. Our work is concerned with the
security of such images. While encryption is the best way to ensure image
security, full encryption and decryption is a computationally-intensive
process. Moreover, as cameras are getting better every day, image quality, and
thus, the pixel density has increased considerably. The increased pixel density
makes encryption and decryption more expensive. We, therefore, delve into
selective encryption and selective blurring based on the region of interest.
Instead of encrypting or blurring the entire photograph, we only encode
selected regions of the image. We present a comparative analysis of the partial
and full encryption of the photos. This kind of encoding will help us lower the
encryption overhead without compromising security. The applications utilizing
this technique will become more usable due to the reduction in the decryption
time. Additionally, blurred images being more readable than encrypted ones,
allowed us to define the level of security. We leverage the machine learning
algorithms like Mask-RCNN (Region-based convolutional neural network) and YOLO
(You Only Look Once) to select the region of interest. These algorithms have
set new benchmarks for object recognition. We develop an end to end system to
demonstrate our idea of selective encryption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaherwar_P/0/1/0/all/0/1"&gt;Prutha Gaherwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shraddha Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Raviraj Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khengare_R/0/1/0/all/0/1"&gt;Rahul Khengare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting. (arXiv:2106.11712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11712</id>
        <link href="http://arxiv.org/abs/2106.11712"/>
        <updated>2021-06-23T01:48:42.226Z</updated>
        <summary type="html"><![CDATA[Modeling dynamical systems plays a crucial role in capturing and
understanding complex physical phenomena. When physical models are not
sufficiently accurate or hardly describable by analytical formulas, one can use
generic function approximators such as neural networks to capture the system
dynamics directly from sensor measurements. As for now, current methods to
learn the parameters of these neural networks are highly sensitive to the
inherent instability of most dynamical systems of interest, which in turn
prevents the study of very long sequences. In this work, we introduce a generic
and scalable method based on multiple shooting to learn latent representations
of indirectly observed dynamical systems. We achieve state-of-the-art
performances on systems observed directly from raw images. Further, we
demonstrate that our method is robust to noisy measurements and can handle
complex dynamical systems, such as chaotic ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jordana_A/0/1/0/all/0/1"&gt;Armand Jordana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carpentier_J/0/1/0/all/0/1"&gt;Justin Carpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1"&gt;Ludovic Righetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Constrained Optimization in Differentiable Neural Architecture Search. (arXiv:2106.11655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11655</id>
        <link href="http://arxiv.org/abs/2106.11655"/>
        <updated>2021-06-23T01:48:42.219Z</updated>
        <summary type="html"><![CDATA[Differentiable Architecture Search (DARTS) is a recently proposed neural
architecture search (NAS) method based on a differentiable relaxation. Due to
its success, numerous variants analyzing and improving parts of the DARTS
framework have recently been proposed. By considering the problem as a
constrained bilevel optimization, we propose and analyze three improvements to
architectural weight competition, update scheduling, and regularization towards
discretization. First, we introduce a new approach to the activation of
architecture weights, which prevents confounding competition within an edge and
allows for fair comparison across edges to aid in discretization. Next, we
propose a dynamic schedule based on per-minibatch network information to make
architecture updates more informed. Finally, we consider two regularizations,
based on proximity to discretization and the Alternating Directions Method of
Multipliers (ADMM) algorithm, to promote early discretization. Our results show
that this new activation scheme reduces final architecture size and the
regularizations improve reliability in search results while maintaining
comparable performance to state-of-the-art in NAS, especially when used with
our new dynamic informed schedule.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maile_K/0/1/0/all/0/1"&gt;Kaitlin Maile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lecarpentier_E/0/1/0/all/0/1"&gt;Erwan Lecarpentier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luga_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Luga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Dennis G. Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-23T01:48:41.861Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Including Sparse Production Knowledge into Variational Autoencoders to Increase Anomaly Detection Reliability. (arXiv:2103.12998v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12998</id>
        <link href="http://arxiv.org/abs/2103.12998"/>
        <updated>2021-06-23T01:48:41.855Z</updated>
        <summary type="html"><![CDATA[Digitalization leads to data transparency for production systems that we can
benefit from with data-driven analysis methods like neural networks. For
example, automated anomaly detection enables saving resources and optimizing
the production. We study using rarely occurring information about labeled
anomalies into Variational Autoencoder neural network structures to overcome
information deficits of supervised and unsupervised approaches. This method
outperforms all other models in terms of accuracy, precision, and recall. We
evaluate the following methods: Principal Component Analysis, Isolation Forest,
Classifying Neural Networks, and Variational Autoencoders on seven time series
datasets to find the best performing detection methods. We extend this idea to
include more infrequently occurring meta information about production
processes. This use of sparse labels, both of anomalies or production data,
allows to harness any additional information available for increasing anomaly
detection performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hammerbacher_T/0/1/0/all/0/1"&gt;Tom Hammerbacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lange_Hegermann_M/0/1/0/all/0/1"&gt;Markus Lange-Hegermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Platz_G/0/1/0/all/0/1"&gt;Gorden Platz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07092</id>
        <link href="http://arxiv.org/abs/2010.07092"/>
        <updated>2021-06-23T01:48:41.849Z</updated>
        <summary type="html"><![CDATA[Conventional image classifiers are trained by randomly sampling mini-batches
of images. To achieve state-of-the-art performance, practitioners use
sophisticated data augmentation schemes to expand the amount of training data
available for sampling. In contrast, meta-learning algorithms sample support
data, query data, and tasks on each training step. In this complex sampling
scenario, data augmentation can be used not only to expand the number of images
available per class, but also to generate entirely new classes/tasks. We
systematically dissect the meta-learning pipeline and investigate the distinct
ways in which data augmentation can be integrated at both the image and class
levels. Our proposed meta-specific data augmentation significantly improves the
performance of meta-learners on few-shot classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1"&gt;Renkun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1"&gt;Amr Sharaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-23T01:48:41.843Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Suicide and Depression Identification with Unsupervised Label Correction. (arXiv:2102.09427v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09427</id>
        <link href="http://arxiv.org/abs/2102.09427"/>
        <updated>2021-06-23T01:48:41.837Z</updated>
        <summary type="html"><![CDATA[Early detection of suicidal ideation in depressed individuals can allow for
adequate medical attention and support, which in many cases is life-saving.
Recent NLP research focuses on classifying, from a given piece of text, if an
individual is suicidal or clinically healthy. However, there have been no major
attempts to differentiate between depression and suicidal ideation, which is an
important clinical challenge. Due to the scarce availability of EHR data,
suicide notes, or other similar verified sources, web query data has emerged as
a promising alternative. Online sources, such as Reddit, allow for anonymity
that prompts honest disclosure of symptoms, making it a plausible source even
in a clinical setting. However, these online datasets also result in lower
performance, which can be attributed to the inherent noise in web-scraped
labels, which necessitates a noise-removal process. Thus, we propose SDCNL, a
suicide versus depression classification method through a deep learning
approach. We utilize online content from Reddit to train our algorithm, and to
verify and correct noisy labels, we propose a novel unsupervised label
correction method which, unlike previous work, does not require prior noise
distribution information. Our extensive experimentation with multiple deep word
embedding models and classifiers display the strong performance of the method
in anew, challenging classification application. We make our code and dataset
available at https://github.com/ayaanzhaque/SDCNL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Viraaj Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giallanza_T/0/1/0/all/0/1"&gt;Tyler Giallanza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08760</id>
        <link href="http://arxiv.org/abs/2104.08760"/>
        <updated>2021-06-23T01:48:41.830Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (especially contrastive learning) has attracted
great interest due to its tremendous potentials in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we discover two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
categories. Over-clustering implies that the model cannot efficiently learn the
feature representation from excessive negative sample pairs, which enforces the
model to over-cluster samples of the same actual categories into different
clusters. To simultaneously overcome these two problems, we propose a novel
self-supervised learning framework using a median triplet loss. Precisely, we
employ a triplet loss tending to maximize the relative distance between the
positive pair and negative pairs to address the under-clustering problem; and
we construct the negative pair by selecting the negative sample of a median
similarity score from all negative samples to avoid the over-clustering
problem, guaranteed by the Bernoulli Distribution model. We extensively
evaluate our proposed framework in several large-scale benchmarks (e.g.,
ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance
(e.g., the learning efficiency) of our model over the latest state-of-the-art
methods by a clear margin. Codes available at:
https://github.com/wanggrun/triplet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Keze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangcong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02601</id>
        <link href="http://arxiv.org/abs/2106.02601"/>
        <updated>2021-06-23T01:48:41.824Z</updated>
        <summary type="html"><![CDATA[Optimization problems are ubiquitous in our societies and are present in
almost every segment of the economy. Most of these optimization problems are
NP-hard and computationally demanding, often requiring approximate solutions
for large-scale instances. Machine learning frameworks that learn to
approximate solutions to such hard optimization problems are a potentially
promising avenue to address these difficulties, particularly when many closely
related problem instances must be solved repeatedly. Supervised learning
frameworks can train a model using the outputs of pre-solved instances.
However, when the outputs are themselves approximations, when the optimization
problem has symmetric solutions, and/or when the solver uses randomization,
solutions to closely related instances may exhibit large differences and the
learning task can become inherently more difficult. This paper demonstrates
this critical challenge, connects the volatility of the training data to the
ability of a model to approximate it, and proposes a method for producing
(exact or approximate) solutions to optimization problems that are more
amenable to supervised learning tasks. The effectiveness of the method is
tested on hard non-linear nonconvex and discrete combinatorial problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1"&gt;James Kotary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-06-23T01:48:41.816Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v4 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10201</id>
        <link href="http://arxiv.org/abs/2103.10201"/>
        <updated>2021-06-23T01:48:41.809Z</updated>
        <summary type="html"><![CDATA[Context: Software engineering researchers have undertaken many experiments
investigating the potential of software defect prediction algorithms.
Unfortunately, some widely used performance metrics are known to be
problematic, most notably F1, but nevertheless F1 is widely used.

Objective: To investigate the potential impact of using F1 on the validity of
this large body of research.

Method: We undertook a systematic review to locate relevant experiments and
then extract all pairwise comparisons of defect prediction performance using F1
and the un-biased Matthews correlation coefficient (MCC).

Results: We found a total of 38 primary studies. These contain 12,471 pairs
of results. Of these, 21.95% changed direction when the MCC metric is used
instead of the biased F1 metric. Unfortunately, we also found evidence
suggesting that F1 remains widely used in software defect prediction research.

Conclusions: We reiterate the concerns of statisticians that the F1 is a
problematic metric outside of an information retrieval context, since we are
concerned about both classes (defect-prone and not defect-prone units). This
inappropriate usage has led to a substantial number (more than one fifth) of
erroneous (in terms of direction) results. Therefore we urge researchers to (i)
use an unbiased metric and (ii) publish detailed results including confusion
matrices such that alternative analyses become possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jingxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1"&gt;Martin Shepperd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guaranteed Fixed-Confidence Best Arm Identification in Multi-Armed Bandits: Simple Sequential Elimination Algorithms. (arXiv:2106.06848v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06848</id>
        <link href="http://arxiv.org/abs/2106.06848"/>
        <updated>2021-06-23T01:48:41.767Z</updated>
        <summary type="html"><![CDATA[We consider the problem of finding, through adaptive sampling, which of $n$
options (arms) has the largest mean. Our objective is to determine a rule which
identifies the best arm with a fixed minimum confidence using as few
observations as possible, i.e. this is a fixed-confidence (FC) best arm
identification (BAI) in multi-armed bandits. We study such problems under the
Bayesian setting with both Bernoulli and Gaussian arms. We propose to use the
classical "vector at a time" (VT) rule, which samples each remaining arm once
in each round. We show how VT can be implemented and analyzed in our Bayesian
setting and be improved by early elimination. Our analysis show that these
algorithms guarantee an optimal strategy under the prior. We also propose and
analyze a variant of the classical "play the winner" (PW) algorithm. Numerical
results show that these rules compare favorably with state-of-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;MohammadJavad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_S/0/1/0/all/0/1"&gt;Sheldon M Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially-Trained Nonnegative Matrix Factorization. (arXiv:2104.04757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04757</id>
        <link href="http://arxiv.org/abs/2104.04757"/>
        <updated>2021-06-23T01:48:41.757Z</updated>
        <summary type="html"><![CDATA[We consider an adversarially-trained version of the nonnegative matrix
factorization, a popular latent dimensionality reduction technique. In our
formulation, an attacker adds an arbitrary matrix of bounded norm to the given
data matrix. We design efficient algorithms inspired by adversarial training to
optimize for dictionary and coefficient matrices with enhanced generalization
abilities. Extensive simulations on synthetic and benchmark datasets
demonstrate the superior predictive performance on matrix completion tasks of
our proposed method compared to state-of-the-art competitors, including other
variants of adversarial nonnegative matrix factorization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1"&gt;Ting Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric F&amp;#xe9;votte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Routine Clustering of Mobile Sensor Data Facilitates Psychotic Relapse Prediction in Schizophrenia Patients. (arXiv:2106.11487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11487</id>
        <link href="http://arxiv.org/abs/2106.11487"/>
        <updated>2021-06-23T01:48:41.748Z</updated>
        <summary type="html"><![CDATA[We aim to develop clustering models to obtain behavioral representations from
continuous multimodal mobile sensing data towards relapse prediction tasks. The
identified clusters could represent different routine behavioral trends related
to daily living of patients as well as atypical behavioral trends associated
with impending relapse.

We used the mobile sensing data obtained in the CrossCheck project for our
analysis. Continuous data from six different mobile sensing-based modalities
(e.g. ambient light, sound/conversation, acceleration etc.) obtained from a
total of 63 schizophrenia patients, each monitored for up to a year, were used
for the clustering models and relapse prediction evaluation. Two clustering
models, Gaussian Mixture Model (GMM) and Partition Around Medoids (PAM), were
used to obtain behavioral representations from the mobile sensing data. The
features obtained from the clustering models were used to train and evaluate a
personalized relapse prediction model using Balanced Random Forest. The
personalization was done by identifying optimal features for a given patient
based on a personalization subset consisting of other patients who are of
similar age.

The clusters identified using the GMM and PAM models were found to represent
different behavioral patterns (such as clusters representing sedentary days,
active but with low communications days, etc.). Significant changes near the
relapse periods were seen in the obtained behavioral representation features
from the clustering models. The clustering model based features, together with
other features characterizing the mobile sensing data, resulted in an F2 score
of 0.24 for the relapse prediction task in a leave-one-patient-out evaluation
setting. This obtained F2 score is significantly higher than a random
classification baseline with an average F2 score of 0.042.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joanne Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamichhane_B/0/1/0/all/0/1"&gt;Bishal Lamichhane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Zeev_D/0/1/0/all/0/1"&gt;Dror Ben-Zeev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campbell_A/0/1/0/all/0/1"&gt;Andrew Campbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sano_A/0/1/0/all/0/1"&gt;Akane Sano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14602</id>
        <link href="http://arxiv.org/abs/2103.14602"/>
        <updated>2021-06-23T01:48:41.733Z</updated>
        <summary type="html"><![CDATA[Voice anti-spoofing aims at classifying a given utterance either as a
bonafide human sample, or a spoofing attack (e.g. synthetic or replayed
sample). Many anti-spoofing methods have been proposed but most of them fail to
generalize across domains (corpora) -- and we do not know \emph{why}. We
outline a novel interpretative framework for gauging the impact of data quality
upon anti-spoofing performance. Our within- and between-domain experiments pool
data from seven public corpora and three anti-spoofing methods based on
Gaussian mixture and convolutive neural network models. We assess the impacts
of long-term spectral information, speaker population (through x-vector speaker
embeddings), signal-to-noise ratio, and selected voice quality features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1"&gt;Bhusan Chettri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1"&gt;Rosa Gonz&amp;#xe1;lez Hautam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniform-PAC Bounds for Reinforcement Learning with Linear Function Approximation. (arXiv:2106.11612v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11612</id>
        <link href="http://arxiv.org/abs/2106.11612"/>
        <updated>2021-06-23T01:48:41.718Z</updated>
        <summary type="html"><![CDATA[We study reinforcement learning (RL) with linear function approximation.
Existing algorithms for this problem only have high-probability regret and/or
Probably Approximately Correct (PAC) sample complexity guarantees, which cannot
guarantee the convergence to the optimal policy. In this paper, in order to
overcome the limitation of existing algorithms, we propose a new algorithm
called FLUTE, which enjoys uniform-PAC convergence to the optimal policy with
high probability. The uniform-PAC guarantee is the strongest possible guarantee
for reinforcement learning in the literature, which can directly imply both PAC
and high probability regret bounds, making our algorithm superior to all
existing algorithms with linear function approximation. At the core of our
algorithm is a novel minimax value function estimator and a multi-level
partition scheme to select the training samples from historical observations.
Both of these techniques are new and of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiafan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:41.688Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations. (arXiv:2106.11519v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11519</id>
        <link href="http://arxiv.org/abs/2106.11519"/>
        <updated>2021-06-23T01:48:41.679Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances on provably efficient Reinforcement
Learning (RL) in problems with rich observation spaces. However, all these
works share a strong realizability assumption about the optimal value function
of the true MDP. Such realizability assumptions are often too strong to hold in
practice. In this work, we consider the more realistic setting of agnostic RL
with rich observation spaces and a fixed class of policies $\Pi$ that may not
contain any near-optimal policy. We provide an algorithm for this setting whose
error is bounded in terms of the rank $d$ of the underlying MDP. Specifically,
our algorithm enjoys a sample complexity bound of $\widetilde{O}\left((H^{4d}
K^{3d} \log |\Pi|)/\epsilon^2\right)$ where $H$ is the length of episodes, $K$
is the number of actions and $\epsilon>0$ is the desired sub-optimality. We
also provide a nearly matching lower bound for this agnostic setting that shows
that the exponential dependence on rank is unavoidable, without further
assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1"&gt;Christoph Dann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1"&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1"&gt;Ayush Sekhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1"&gt;Karthik Sridharan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Invertible Architectures on Inverse Problems. (arXiv:2101.10763v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10763</id>
        <link href="http://arxiv.org/abs/2101.10763"/>
        <updated>2021-06-23T01:48:41.672Z</updated>
        <summary type="html"><![CDATA[Recent work demonstrated that flow-based invertible neural networks are
promising tools for solving ambiguous inverse problems. Following up on this,
we investigate how ten invertible architectures and related models fare on two
intuitive, low-dimensional benchmark problems, obtaining the best results with
coupling layers and simple autoencoders. We hope that our initial efforts
inspire other researchers to evaluate their invertible architectures in the
same setting and put forth additional benchmarks, so our evaluation may
eventually grow into an official community challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kruse_J/0/1/0/all/0/1"&gt;Jakob Kruse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ardizzone_L/0/1/0/all/0/1"&gt;Lynton Ardizzone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1"&gt;Carsten Rother&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1"&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Identification across Social Networking Sites using User Profiles and Posting Patterns. (arXiv:2106.11815v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11815</id>
        <link href="http://arxiv.org/abs/2106.11815"/>
        <updated>2021-06-23T01:48:41.665Z</updated>
        <summary type="html"><![CDATA[With the prevalence of online social networking sites (OSNs) and mobile
devices, people are increasingly reliant on a variety of OSNs for keeping in
touch with family and friends, and using it as a source of information. For
example, a user might utilise multiple OSNs for different purposes, such as
using Flickr to share holiday pictures with family and friends, and Twitter to
post short messages about their thoughts. Identifying the same user across
multiple OSNs is an important task as this allows us to understand the usage
patterns of users among different OSNs, make recommendations when a user
registers for a new OSN, and various other useful applications. To address this
problem, we proposed an algorithm based on the multilayer perceptron using
various types of features, namely: (i) user profile, such as name, location,
description; (ii) temporal distribution of user generated content; and (iii)
embedding based on user name, real name and description. Using a Twitter and
Flickr dataset of users and their posting activities, we perform an empirical
study on how these features affect the performance of user identification
across the two OSNs and discuss our main findings based on the different
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solanki_P/0/1/0/all/0/1"&gt;Prashant Solanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harwood_A/0/1/0/all/0/1"&gt;Aaron Harwood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11821</id>
        <link href="http://arxiv.org/abs/2106.11821"/>
        <updated>2021-06-23T01:48:41.659Z</updated>
        <summary type="html"><![CDATA[Data augmentation has been successfully used in many areas of deep-learning
to significantly improve model performance. Typically data augmentation
simulates realistic variations in data in order to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis,
where deep learning methods are already achieving state of the art performance,
it is not immediately clear how to apply data augmentation. In this paper we
study different methods of data augmentation starting with basic methods using
fixed transformations and moving to methods that adapt to the data. We propose
a novel data augmentation method based on using an opcode embedding layer
within the network and its corresponding opcode embedding matrix to perform
adaptive data augmentation during training. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods applied to opcode sequence based malware classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1"&gt;Niall McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1"&gt;Jesus Martinez del Rincon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition. (arXiv:2012.04053v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04053</id>
        <link href="http://arxiv.org/abs/2012.04053"/>
        <updated>2021-06-23T01:48:41.640Z</updated>
        <summary type="html"><![CDATA[We study the stochastic shortest path problem with adversarial costs and
known transition, and show that the minimax regret is
$\widetilde{O}(\sqrt{DT^\star K})$ and $\widetilde{O}(\sqrt{DT^\star SA K})$
for the full-information setting and the bandit feedback setting respectively,
where $D$ is the diameter, $T^\star$ is the expected hitting time of the
optimal policy, $S$ is the number of states, $A$ is the number of actions, and
$K$ is the number of episodes. Our results significantly improve upon the
existing work of (Rosenberg and Mansour, 2020) which only considers the
full-information setting and achieves suboptimal regret. Our work is also the
first to consider bandit feedback with adversarial costs.

Our algorithms are built on top of the Online Mirror Descent framework with a
variety of new techniques that might be of independent interest, including an
improved multi-scale expert algorithm, a reduction from general stochastic
shortest path to a special loop-free case, a skewed occupancy measure space,
and a novel correction term added to the cost estimators. Interestingly, the
last two elements reduce the variance of the learner via positive bias and the
variance of the optimal policy via negative bias respectively, and having them
simultaneously is critical for obtaining the optimal high-probability bound in
the bandit feedback setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning under Pool Set Distribution Shift and Noisy Data. (arXiv:2106.11719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11719</id>
        <link href="http://arxiv.org/abs/2106.11719"/>
        <updated>2021-06-23T01:48:41.632Z</updated>
        <summary type="html"><![CDATA[Active Learning is essential for more label-efficient deep learning. Bayesian
Active Learning has focused on BALD, which reduces model parameter uncertainty.
However, we show that BALD gets stuck on out-of-distribution or junk data that
is not relevant for the task. We examine a novel *Expected Predictive
Information Gain (EPIG)* to deal with distribution shifts of the pool set. EPIG
reduces the uncertainty of *predictions* on an unlabelled *evaluation set*
sampled from the test data distribution whose distribution might be different
to the pool set distribution. Based on this, our new EPIG-BALD acquisition
function for Bayesian Neural Networks selects samples to improve the
performance on the test data distribution instead of selecting samples that
reduce model uncertainty everywhere, including for out-of-distribution regions
with low density in the test data distribution. Our method outperforms
state-of-the-art Bayesian active learning methods on high-dimensional datasets
and avoids out-of-distribution junk data in cases where current
state-of-the-art methods fail.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1"&gt;Andreas Kirsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11531</id>
        <link href="http://arxiv.org/abs/2106.11531"/>
        <updated>2021-06-23T01:48:41.626Z</updated>
        <summary type="html"><![CDATA[Routing methods in capsule networks often learn a hierarchical relationship
for capsules in successive layers, but the intra-relation between capsules in
the same layer is less studied, while this intra-relation is a key factor for
the semantic understanding in text data. Therefore, in this paper, we introduce
a new capsule network with graph routing to learn both relationships, where
capsules in each layer are treated as the nodes of a graph. We investigate
strategies to yield adjacency and degree matrix with three different distances
from a layer of capsules, and propose the graph routing mechanism between those
capsules. We validate our approach on five text classification datasets, and
our findings suggest that the approach combining bottom-up routing and top-down
attention performs the best. Such an approach demonstrates generalization
capability across datasets. Compared to the state-of-the-art routing methods,
the improvements in accuracy in the five datasets we used were 0.82, 0.39,
0.07, 1.01, and 0.02, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Learning in Decentralized Matching Markets. (arXiv:2012.07348v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07348</id>
        <link href="http://arxiv.org/abs/2012.07348"/>
        <updated>2021-06-23T01:48:41.619Z</updated>
        <summary type="html"><![CDATA[We study two-sided matching markets in which one side of the market (the
players) does not have a priori knowledge about its preferences for the other
side (the arms) and is required to learn its preferences from experience. Also,
we assume the players have no direct means of communication. This model extends
the standard stochastic multi-armed bandit framework to a decentralized
multiple player setting with competition. We introduce a new algorithm for this
setting that, over a time horizon $T$, attains $\mathcal{O}(\log(T))$ stable
regret when preferences of the arms over players are shared, and
$\mathcal{O}(\log(T)^2)$ regret when there are no assumptions on the
preferences on either side. Moreover, in the setting where a single player may
deviate, we show that the algorithm is incentive compatible whenever the arms'
preferences are shared, but not necessarily so when preferences are fully
general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lydia T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_F/0/1/0/all/0/1"&gt;Feng Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mania_H/0/1/0/all/0/1"&gt;Horia Mania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications. (arXiv:2102.01046v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01046</id>
        <link href="http://arxiv.org/abs/2102.01046"/>
        <updated>2021-06-23T01:48:41.612Z</updated>
        <summary type="html"><![CDATA[We resolve the long-standing "impossible tuning" issue for the classic expert
problem and show that, it is in fact possible to achieve regret
$O\left(\sqrt{(\ln d)\sum_t \ell_{t,i}^2}\right)$ simultaneously for all expert
$i$ in a $T$-round $d$-expert problem where $\ell_{t,i}$ is the loss for expert
$i$ in round $t$. Our algorithm is based on the Mirror Descent framework with a
correction term and a weighted entropy regularizer. While natural, the
algorithm has not been studied before and requires a careful analysis. We also
generalize the bound to $O\left(\sqrt{(\ln d)\sum_t
(\ell_{t,i}-m_{t,i})^2}\right)$ for any prediction vector $m_t$ that the
learner receives, and recover or improve many existing results by choosing
different $m_t$. Furthermore, we use the same framework to create a master
algorithm that combines a set of base algorithms and learns the best one with
little overhead. The new guarantee of our master allows us to derive many new
results for both the expert problem and more generally Online Linear
Optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Haipeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Chen-Yu Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10399</id>
        <link href="http://arxiv.org/abs/2008.10399"/>
        <updated>2021-06-23T01:48:41.592Z</updated>
        <summary type="html"><![CDATA[In this work, we present a novel neural network to generate high resolution
images. We replace the decoder of VAE with a discriminator while using the
encoder as it is. The encoder is fed data from a normal distribution while the
generator is fed from a gaussian distribution. The combination from both is
given to a discriminator which tells whether the generated image is correct or
not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA
dataset. Our network beats the previous state of the art using MMD, SSIM, log
likelihood, reconstruction error, ELBO and KL divergence as the evaluation
metrics while generating much sharper images. This work is potentially very
exciting as we are able to combine the advantages of generative models and
inference models in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-BEHRT: Hierarchical Transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. (arXiv:2106.11360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11360</id>
        <link href="http://arxiv.org/abs/2106.11360"/>
        <updated>2021-06-23T01:48:41.584Z</updated>
        <summary type="html"><![CDATA[Electronic health records represent a holistic overview of patients'
trajectories. Their increasing availability has fueled new hopes to leverage
them and develop accurate risk prediction models for a wide range of diseases.
Given the complex interrelationships of medical records and patient outcomes,
deep learning models have shown clear merits in achieving this goal. However, a
key limitation of these models remains their capacity in processing long
sequences. Capturing the whole history of medical encounters is expected to
lead to more accurate predictions, but the inclusion of records collected for
decades and from multiple resources can inevitably exceed the receptive field
of the existing deep learning architectures. This can result in missing
crucial, long-term dependencies. To address this gap, we present Hi-BEHRT, a
hierarchical Transformer-based model that can significantly expand the
receptive field of Transformers and extract associations from much longer
sequences. Using a multimodal large-scale linked longitudinal electronic health
records, the Hi-BEHRT exceeds the state-of-the-art BEHRT 1% to 5% for area
under the receiver operating characteristic (AUROC) curve and 3% to 6% for area
under the precision recall (AUPRC) curve on average, and 3% to 6% (AUROC) and
3% to 11% (AUPRC) for patients with long medical history for 5-year heart
failure, diabetes, chronic kidney disease, and stroke risk prediction.
Additionally, because pretraining for hierarchical Transformer is not
well-established, we provide an effective end-to-end contrastive pre-training
strategy for Hi-BEHRT using EHR, improving its transferability on predicting
clinical events with relatively small training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yikuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamouei_M/0/1/0/all/0/1"&gt;Mohammad Mamouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salimi_Khorshidi_G/0/1/0/all/0/1"&gt;Gholamreza Salimi-Khorshidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Shishir Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassaine_A/0/1/0/all/0/1"&gt;Abdelaali Hassaine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canoy_D/0/1/0/all/0/1"&gt;Dexter Canoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahimi_K/0/1/0/all/0/1"&gt;Kazem Rahimi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Ordinary Differential Equations. (arXiv:1911.07532v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07532</id>
        <link href="http://arxiv.org/abs/1911.07532"/>
        <updated>2021-06-23T01:48:41.576Z</updated>
        <summary type="html"><![CDATA[We introduce the framework of continuous--depth graph neural networks (GNNs).
Graph neural ordinary differential equations (GDEs) are formalized as the
counterpart to GNNs where the input-output relationship is determined by a
continuum of GNN layers, blending discrete topological structures and
differential equations. The proposed framework is shown to be compatible with
various static and autoregressive GNN models. Results prove general
effectiveness of GDEs: in static settings they offer computational advantages
by incorporating numerical methods in their forward pass; in dynamic settings,
on the other hand, they are shown to improve performance by exploiting the
geometry of the underlying dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11453</id>
        <link href="http://arxiv.org/abs/2101.11453"/>
        <updated>2021-06-23T01:48:41.568Z</updated>
        <summary type="html"><![CDATA[Recently demonstrated physical-world adversarial attacks have exposed
vulnerabilities in perception systems that pose severe risks for
safety-critical applications such as autonomous driving. These attacks place
adversarial artifacts in the physical world that indirectly cause the addition
of a universal patch to inputs of a model that can fool it in a variety of
contexts. Adversarial training is the most effective defense against
image-dependent adversarial attacks. However, tailoring adversarial training to
universal patches is computationally expensive since the optimal universal
patch depends on the model weights which change during training. We propose
meta adversarial training (MAT), a novel combination of adversarial training
with meta-learning, which overcomes this challenge by meta-learning universal
patches along with model training. MAT requires little extra computation while
continuously adapting a large set of patches to the current model. MAT
considerably increases robustness against universal patch attacks on image
classification and traffic-light detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1"&gt;Nicole Finnie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1"&gt;Robin Hutmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11359</id>
        <link href="http://arxiv.org/abs/2106.11359"/>
        <updated>2021-06-23T01:48:41.559Z</updated>
        <summary type="html"><![CDATA[The advent of social media platforms has been a catalyst for the development
of digital photography that engendered a boom in vision applications. With this
motivation, we introduce a large-scale dataset termed 'Photozilla', which
includes over 990k images belonging to 10 different photographic styles. The
dataset is then used to train 3 classification models to automatically classify
the images into the relevant style which resulted in an accuracy of ~96%. With
the rapid evolution of digital photography, we have seen new types of
photography styles emerging at an exponential rate. On that account, we present
a novel Siamese-based network that uses the trained classification models as
the base architecture to adapt and classify unseen styles with only 25 training
samples. We report an accuracy of over 68% for identifying 10 other distinct
types of photography styles. This dataset can be found at
https://trisha025.github.io/Photozilla/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1"&gt;Trisha Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T. M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Over-Air Subspace Tracking from Incomplete and Corrupted Data. (arXiv:2002.12873v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12873</id>
        <link href="http://arxiv.org/abs/2002.12873"/>
        <updated>2021-06-23T01:48:41.549Z</updated>
        <summary type="html"><![CDATA[Subspace tracking (ST) with missing data (ST-miss) or outliers (Robust ST) or
both (Robust ST-miss) has been extensively studied in the last many years. This
work provides a new simple algorithm and guarantee for both ST with missing
data (ST-miss) and RST-miss. Unlike past work on this topic, the algorithm is
much simpler (uses fewer parameters) and the guarantee does not make the
artificial assumption of piecewise constant subspace change, although it still
handles that setting. Secondly, we extend our approach and its analysis to
provably solving these problems when the raw data is federated and when the
over-air data communication modality is used for information exchange between
the $K$ peer nodes and the center.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narayanamurthy_P/0/1/0/all/0/1"&gt;Praneeth Narayanamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_N/0/1/0/all/0/1"&gt;Namrata Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1"&gt;Aditya Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data. (arXiv:1910.00482v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00482</id>
        <link href="http://arxiv.org/abs/1910.00482"/>
        <updated>2021-06-23T01:48:41.527Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of estimating smooth Generalized Linear
Models (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.
Different from its classical setting, our model allows the server to access
some additional public but unlabeled data. By using Stein's lemma and its
variants, we first show that there is an $(\epsilon, \delta)$-NLDP algorithm
for GLM (under some mild assumptions), if each data record is i.i.d sampled
from some sub-Gaussian distribution with bounded $\ell_1$-norm. Then with high
probability, the sample complexity of the public and private data, for the
algorithm to achieve an $\alpha$ estimation error (in $\ell_\infty$-norm), is
$O(p^2\alpha^{-2})$ and ${O}(p^2\alpha^{-2}\epsilon^{-2})$, respectively, if
$\alpha$ is not too small ({\em i.e.,} $\alpha\geq
\Omega(\frac{1}{\sqrt{p}})$), where $p$ is the dimensionality of the data. This
is a significant improvement over the previously known quasi-polynomial (in
$\alpha$) or exponential (in $p$) complexity of GLM with no public data. Also,
our algorithm can answer multiple (at most $\exp(O(p))$) GLM queries with the
same sample complexities as in the one GLM query case with at least constant
probability. We then extend our idea to the non-linear regression problem and
show a similar phenomenon for it. Finally, we demonstrate the effectiveness of
our algorithms through experiments on both synthetic and real world datasets.
To our best knowledge, this is the first paper showing the existence of
efficient and effective algorithms for GLM and non-linear regression in the
NLDP model with public unlabeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lijie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1"&gt;Marco Gaboardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinhui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preconditioned Riemannian Optimization on the Generalized Stiefel Manifold. (arXiv:1902.01635v3 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.01635</id>
        <link href="http://arxiv.org/abs/1902.01635"/>
        <updated>2021-06-23T01:48:41.520Z</updated>
        <summary type="html"><![CDATA[Optimization problems on the generalized Stiefel manifold (and products of
it) are prevalent across science and engineering. For example, in computational
science they arise in the symmetric (generalized) eigenvalue problem, in
nonlinear eigenvalue problems, and in electronic structures computations, to
name a few problems. In statistics and machine learning, they arise, for
example, in various dimensionality reduction techniques such as canonical
correlation analysis. In deep learning, regularization and improved stability
can be obtained by constraining some layers to have parameter matrices that
belong to the Stiefel manifold. Solving problems on the generalized Stiefel
manifold can be approached via the tools of Riemannian optimization. However,
using the standard geometric components for the generalized Stiefel manifold
has two possible shortcoming: computing some of the geometric components can be
too expensive and converge can be rather slow in certain cases. Both
shortcomings can be addressed using a technique called Riemannian
preconditioning, which amounts to using geometric components derived using a
precoditioner that defines a Riemannian metric on the constraint manifold. In
this paper we develop the geometric components required to perform Riemannian
optimization on the generalized Stiefel manifold equipped with a non-standard
metric, and illustrate theoretically and numerically the use of those
components and the effect of Riemannian preconditioning for solving
optimization problems on the generalized Stiefel manifold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Shustin_B/0/1/0/all/0/1"&gt;Boris Shustin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Avron_H/0/1/0/all/0/1"&gt;Haim Avron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11959</id>
        <link href="http://arxiv.org/abs/2106.11959"/>
        <updated>2021-06-23T01:48:41.510Z</updated>
        <summary type="html"><![CDATA[The necessity of deep learning for tabular data is still an unanswered
question addressed by a large number of research efforts. The recent literature
on tabular DL proposes several deep architectures reported to be superior to
traditional "shallow" models like Gradient Boosted Decision Trees. However,
since existing works often use different benchmarks and tuning protocols, it is
unclear if the proposed models universally outperform GBDT. Moreover, the
models are often not compared to each other, therefore, it is challenging to
identify the best deep model for practitioners.

In this work, we start from a thorough review of the main families of DL
models recently developed for tabular data. We carefully tune and evaluate them
on a wide range of datasets and reveal two significant findings. First, we show
that the choice between GBDT and DL models highly depends on data and there is
still no universally superior solution. Second, we demonstrate that a simple
ResNet-like architecture is a surprisingly effective baseline, which
outperforms most of the sophisticated models from the DL literature. Finally,
we design a simple adaptation of the Transformer architecture for tabular data
that becomes a new strong DL baseline and reduces the gap between GBDT and DL
models on datasets where GBDT dominates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gorishniy_Y/0/1/0/all/0/1"&gt;Yury Gorishniy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1"&gt;Ivan Rubachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1"&gt;Valentin Khrulkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Invisible Visible: Data-Driven Seismic Inversion with Physics-Informed Data Augmentation. (arXiv:2106.11892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11892</id>
        <link href="http://arxiv.org/abs/2106.11892"/>
        <updated>2021-06-23T01:48:41.503Z</updated>
        <summary type="html"><![CDATA[Deep learning and data-driven approaches have shown great potential in
scientific domains. The promise of data-driven techniques relies on the
availability of a large volume of high-quality training datasets. Due to the
high cost of obtaining data through expensive physical experiments,
instruments, and simulations, data augmentation techniques for scientific
applications have emerged as a new direction for obtaining scientific data
recently. However, existing data augmentation techniques originating from
computer vision, yield physically unacceptable data samples that are not
helpful for the domain problems that we are interested in. In this paper, we
develop new physics-informed data augmentation techniques based on
convolutional neural networks. Specifically, our generative models leverage
different physics knowledge (such as governing equations, observable
perception, and physics phenomena) to improve the quality of the synthetic
data. To validate the effectiveness of our data augmentation techniques, we
apply them to solve a subsurface seismic full-waveform inversion using
simulated CO$_2$ leakage data. Our interest is to invert for subsurface
velocity models associated with very small CO$_2$ leakage. We validate the
performance of our methods using comprehensive numerical tests. Via comparison
and analysis, we show that data-driven seismic imaging can be significantly
enhanced by using our physics-informed data augmentation techniques.
Particularly, the imaging quality has been improved by 15% in test scenarios of
general-sized leakage and 17% in small-sized leakage when using an augmented
training set obtained with our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxin Yang&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xitong Zhang&lt;/a&gt; (1 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1"&gt;Qiang Guan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youzuo Lin&lt;/a&gt; (1) ((1) Earth and Environmental Sciences Division, Los Alamos National Laboratory, (2) Department of Computer Science, Kent State University, (3) Department of Computational Mathematics, Science and Engineering, Michigan State University)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deciding What to Learn: A Rate-Distortion Approach. (arXiv:2101.06197v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06197</id>
        <link href="http://arxiv.org/abs/2101.06197"/>
        <updated>2021-06-23T01:48:41.494Z</updated>
        <summary type="html"><![CDATA[Agents that learn to select optimal actions represent a prominent focus of
the sequential decision-making literature. In the face of a complex environment
or constraints on time and resources, however, aiming to synthesize such an
optimal policy can become infeasible. These scenarios give rise to an important
trade-off between the information an agent must acquire to learn and the
sub-optimality of the resulting policy. While an agent designer has a
preference for how this trade-off is resolved, existing approaches further
require that the designer translate these preferences into a fixed learning
target for the agent. In this work, leveraging rate-distortion theory, we
automate this process such that the designer need only express their
preferences via a single hyperparameter and the agent is endowed with the
ability to compute its own learning targets that best achieve the desired
trade-off. We establish a general bound on expected discounted regret for an
agent that decides what to learn in this manner along with computational
experiments that illustrate the expressiveness of designer preferences and even
show improvements over Thompson sampling in identifying an optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arumugam_D/0/1/0/all/0/1"&gt;Dilip Arumugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1"&gt;Benjamin Van Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11058</id>
        <link href="http://arxiv.org/abs/2101.11058"/>
        <updated>2021-06-23T01:48:41.468Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to transfer information from one task to enable
generalization on novel tasks given a few examples. This information is present
both in the domain and the class labels. In this work we investigate the
complementary roles of these two sources of information by combining
instance-discriminative contrastive learning and supervised learning in a
single framework called Supervised Momentum Contrastive learning (SUPMOCO). Our
approach avoids a problem observed in supervised learning where information in
images not relevant to the task is discarded, which hampers their
generalization to novel tasks. We show that (self-supervised) contrastive
learning and supervised learning are mutually beneficial, leading to a new
state-of-the-art on the META-DATASET - a recently introduced benchmark for
few-shot learning. Our method is based on a simple modification of MOCO and
scales better than prior work on combining supervised and self-supervised
learning. This allows us to easily combine data from multiple domains leading
to further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1"&gt;Orchid Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Covariance Matrix Estimation in Stochastic Gradient Descent. (arXiv:2002.03979v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03979</id>
        <link href="http://arxiv.org/abs/2002.03979"/>
        <updated>2021-06-23T01:48:41.459Z</updated>
        <summary type="html"><![CDATA[The stochastic gradient descent (SGD) algorithm is widely used for parameter
estimation, especially for huge data sets and online learning. While this
recursive algorithm is popular for computation and memory efficiency,
quantifying variability and randomness of the solutions has been rarely
studied. This paper aims at conducting statistical inference of SGD-based
estimates in an online setting. In particular, we propose a fully online
estimator for the covariance matrix of averaged SGD iterates (ASGD) only using
the iterates from SGD. We formally establish our online estimator's consistency
and show that the convergence rate is comparable to offline counterparts. Based
on the classic asymptotic normality results of ASGD, we construct
asymptotically valid confidence intervals for model parameters. Upon receiving
new observations, we can quickly update the covariance matrix estimate and the
confidence intervals. This approach fits in an online setting and takes full
advantage of SGD: efficiency in computation and memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Biao Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cogment: Open Source Framework For Distributed Multi-actor Training, Deployment & Operations. (arXiv:2106.11345v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11345</id>
        <link href="http://arxiv.org/abs/2106.11345"/>
        <updated>2021-06-23T01:48:41.451Z</updated>
        <summary type="html"><![CDATA[Involving humans directly for the benefit of AI agents' training is getting
traction thanks to several advances in reinforcement learning and
human-in-the-loop learning. Humans can provide rewards to the agent,
demonstrate tasks, design a curriculum, or act in the environment, but these
benefits also come with architectural, functional design and engineering
complexities. We present Cogment, a unifying open-source framework that
introduces an actor formalism to support a variety of humans-agents
collaboration typologies and training approaches. It is also scalable out of
the box thanks to a distributed micro service architecture, and offers
solutions to the aforementioned complexities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Redefined_A/0/1/0/all/0/1"&gt;AI Redefined&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1"&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurandwad_S/0/1/0/all/0/1"&gt;Sagar Kurandwad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mars_C/0/1/0/all/0/1"&gt;Clod&amp;#xe9;ric Mars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szriftgiser_G/0/1/0/all/0/1"&gt;Gregory Szriftgiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chabot_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Chabot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12561</id>
        <link href="http://arxiv.org/abs/1903.12561"/>
        <updated>2021-06-23T01:48:41.444Z</updated>
        <summary type="html"><![CDATA[It is well known that deep neural networks (DNNs) are vulnerable to
adversarial attacks, which are implemented by adding crafted perturbations onto
benign examples. Min-max robust optimization based adversarial training can
provide a notion of security against adversarial attacks. However, adversarial
robustness requires a significantly larger capacity of the network than that
for the natural training with only benign examples. This paper proposes a
framework of concurrent adversarial training and weight pruning that enables
model compression while still preserving the adversarial robustness and
essentially tackles the dilemma of adversarial training. Furthermore, this work
studies two hypotheses about weight pruning in the conventional setting and
finds that weight pruning is essential for reducing the network model size in
the adversarial setting, training a small model from scratch even with
inherited initialization from the large model cannot achieve both adversarial
robustness and high standard accuracy. Code is available at
https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shaokai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1"&gt;Jan-Henrik Lambrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kaisheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reusing Combinatorial Structure: Faster Iterative Projections over Submodular Base Polytopes. (arXiv:2106.11943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11943</id>
        <link href="http://arxiv.org/abs/2106.11943"/>
        <updated>2021-06-23T01:48:41.436Z</updated>
        <summary type="html"><![CDATA[Optimization algorithms such as projected Newton's method, FISTA, mirror
descent and its variants enjoy near-optimal regret bounds and convergence
rates, but suffer from a computational bottleneck of computing "projections''
in potentially each iteration (e.g., $O(T^{1/2})$ regret of online mirror
descent). On the other hand, conditional gradient variants solve a linear
optimization in each iteration, but result in suboptimal rates (e.g.,
$O(T^{3/4})$ regret of online Frank-Wolfe). Motivated by this trade-off in
runtime v/s convergence rates, we consider iterative projections of close-by
points over widely-prevalent submodular base polytopes $B(f)$. We develop a
toolkit to speed up the computation of projections using both discrete and
continuous perspectives. We subsequently adapt the away-step Frank-Wolfe
algorithm to use this information and enable early termination. For the special
case of cardinality based submodular polytopes, we improve the runtime of
computing certain Bregman projections by a factor of $\Omega(n/\log(n))$. Our
theoretical results show orders of magnitude reduction in runtime in
preliminary computational experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moondra_J/0/1/0/all/0/1"&gt;Jai Moondra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortagy_H/0/1/0/all/0/1"&gt;Hassan Mortagy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Swati Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Semi-Supervised Node Classification on Few-Labeled Graph Data. (arXiv:1910.02684v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.02684</id>
        <link href="http://arxiv.org/abs/1910.02684"/>
        <updated>2021-06-23T01:48:41.416Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are designed for semi-supervised node
classification on graphs where only a small subset of nodes have class labels.
However, under extreme cases when very few labels are available (e.g., 1
labeled node per class), GNNs suffer from severe result quality degradation.
Several existing studies make an initial effort to ease this situation, but are
still far from satisfactory.

In this paper, on few-labeled graph data, we propose an effective framework
ABN that is readily applicable to both shallow and deep GNN architectures and
significantly boosts classification accuracy. In particular, on a benchmark
dataset Cora with only 1 labeled node per class, while the classic graph
convolutional network (GCN) only has 44.6% accuracy, an immediate instantiation
of ABN over GCN achieves 62.5% accuracy; when applied to a deep architecture
DAGNN, ABN improves accuracy from 59.8% to 66.4%, which is state of the art.

ABN obtains superior performance through three main algorithmic designs.
First, it selects high-quality unlabeled nodes via an adaptive pseudo labeling
technique, so as to adaptively enhance the training process of GNNs. Second,
ABN balances the labels of the selected nodes on real-world skewed graph data
by pseudo label balancing. Finally, a negative sampling regularizer is designed
for ABN to further utilize the unlabeled nodes. The effectiveness of the three
techniques in ABN is well-validated by both theoretical and empirical analysis.
Extensive experiments, comparing 12 existing approaches on 4 benchmark
datasets, demonstrate that ABN achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Ziang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jieming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengzhong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization. (arXiv:2106.11890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11890</id>
        <link href="http://arxiv.org/abs/2106.11890"/>
        <updated>2021-06-23T01:48:41.408Z</updated>
        <summary type="html"><![CDATA[When tuning the architecture and hyperparameters of large machine learning
models for on-device deployment, it is desirable to understand the optimal
trade-offs between on-device latency and model accuracy. In this work, we
leverage recent methodological advances in Bayesian optimization over
high-dimensional search spaces and multi-objective Bayesian optimization to
efficiently explore these trade-offs for a production-scale on-device natural
language understanding model at Facebook.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_D/0/1/0/all/0/1"&gt;David Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1"&gt;Pierce I-Jen Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daulton_S/0/1/0/all/0/1"&gt;Sam Daulton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1"&gt;Ahmed Aly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1"&gt;Arun Babu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Akshat Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1"&gt;Peng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shicong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1"&gt;Ganesh Venkatesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balandat_M/0/1/0/all/0/1"&gt;Maximilian Balandat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks. (arXiv:2106.11908v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11908</id>
        <link href="http://arxiv.org/abs/2106.11908"/>
        <updated>2021-06-23T01:48:41.402Z</updated>
        <summary type="html"><![CDATA[In this work, we extend standard neural networks by building upon an
assumption that neuronal activations correspond to the angle of a complex
number lying on the unit circle, or 'phasor.' Each layer in such a network
produces new activations by taking a weighted superposition of the previous
layer's phases and calculating the new phase value. This generalized
architecture allows models to reach high accuracy and carries the singular
advantage that mathematically equivalent versions of the network can be
executed with or without regard to a temporal variable. Importantly, the value
of a phase angle in the temporal domain can be sparsely represented by a
periodically repeating series of delta functions or 'spikes'. We demonstrate
the atemporal training of a phasor network on standard deep learning tasks and
show that these networks can then be executed in either the traditional
atemporal domain or spiking temporal domain with no conversion step needed.
This provides a novel basis for constructing deep networkswhich operate via
temporal, spike-based calculations suitable for neuromorphic computing
hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Olin_Ammentorp_W/0/1/0/all/0/1"&gt;Wilkie Olin-Ammentorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bazhenov_M/0/1/0/all/0/1"&gt;Maxim Bazhenov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NetFense: Adversarial Defenses against Privacy Attacks on Neural Networks for Graph Data. (arXiv:2106.11865v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11865</id>
        <link href="http://arxiv.org/abs/2106.11865"/>
        <updated>2021-06-23T01:48:41.396Z</updated>
        <summary type="html"><![CDATA[Recent advances in protecting node privacy on graph data and attacking graph
neural networks (GNNs) gain much attention. The eye does not bring these two
essential tasks together yet. Imagine an adversary can utilize the powerful
GNNs to infer users' private labels in a social network. How can we
adversarially defend against such privacy attacks while maintaining the utility
of perturbed graphs? In this work, we propose a novel research task,
adversarial defenses against GNN-based privacy attacks, and present a graph
perturbation-based approach, NetFense, to achieve the goal. NetFense can
simultaneously keep graph data unnoticeability (i.e., having limited changes on
the graph structure), maintain the prediction confidence of targeted label
classification (i.e., preserving data utility), and reduce the prediction
confidence of private label classification (i.e., protecting the privacy of
nodes). Experiments conducted on single- and multiple-target perturbations
using three real graph data exhibit that the perturbed graphs by NetFense can
effectively maintain data utility (i.e., model unnoticeability) on targeted
label classification and significantly decrease the prediction confidence of
private label classification (i.e., privacy protection). Extensive studies also
bring several insights, such as the flexibility of NetFense, preserving local
neighborhoods in data unnoticeability, and better privacy protection for
high-degree nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_I/0/1/0/all/0/1"&gt;I-Chung Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dangers of Bayesian Model Averaging under Covariate Shift. (arXiv:2106.11905v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11905</id>
        <link href="http://arxiv.org/abs/2106.11905"/>
        <updated>2021-06-23T01:48:41.388Z</updated>
        <summary type="html"><![CDATA[Approximate Bayesian inference for neural networks is considered a robust
alternative to standard training, often providing good performance on
out-of-distribution data. However, Bayesian neural networks (BNNs) with
high-fidelity approximate inference via full-batch Hamiltonian Monte Carlo
achieve poor generalization under covariate shift, even underperforming
classical estimation. We explain this surprising result, showing how a Bayesian
model average can in fact be problematic under covariate shift, particularly in
cases where linear dependencies in the input features cause a lack of posterior
contraction. We additionally show why the same issue does not affect many
approximate inference procedures, or classical maximum a-posteriori (MAP)
training. Finally, we propose novel priors that improve the robustness of BNNs
to many sources of covariate shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izmailov_P/0/1/0/all/0/1"&gt;Pavel Izmailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nicholson_P/0/1/0/all/0/1"&gt;Patrick Nicholson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lotfi_S/0/1/0/all/0/1"&gt;Sanae Lotfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1"&gt;Andrew Gordon Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems. (arXiv:2106.11929v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11929</id>
        <link href="http://arxiv.org/abs/2106.11929"/>
        <updated>2021-06-23T01:48:41.368Z</updated>
        <summary type="html"><![CDATA[Temperature monitoring during the life time of heat-source components in
engineering systems becomes essential to ensure the normal work and even the
long working life of the heat sources. However, prior methods, which mainly use
the interpolate estimation, require large amounts of temperature tensors for an
accurate estimation. To solve this problem, this work develops a novel
physics-informed deep surrogate models for temperature field reconstruction.
First, we defines the temperature field reconstruction task of heat-source
systems. Then, this work develops the deep surrogate model mapping for the
proposed task. Finally, considering the physical properties of heat transfer,
this work proposes four different losses and joint learns the deep surrogate
model with these losses. Experimental studies have conducted over typical
two-dimensional heat-source systems to demonstrate the effectiveness and
efficiency of the proposed physics-informed deep surrogate models for
temperature field reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1"&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Weien Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1"&gt;Wen Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph coarsening: From scientific computing to machine learning. (arXiv:2106.11863v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11863</id>
        <link href="http://arxiv.org/abs/2106.11863"/>
        <updated>2021-06-23T01:48:41.361Z</updated>
        <summary type="html"><![CDATA[The general method of graph coarsening or graph reduction has been a
remarkably useful and ubiquitous tool in scientific computing and it is now
just starting to have a similar impact in machine learning. The goal of this
paper is to take a broad look into coarsening techniques that have been
successfully deployed in scientific computing and see how similar principles
are finding their way in more recent applications related to machine learning.
In scientific computing, coarsening plays a central role in algebraic multigrid
methods as well as the related class of multilevel incomplete LU
factorizations. In machine learning, graph coarsening goes under various names,
e.g., graph downsampling or graph reduction. Its goal in most cases is to
replace some original graph by one which has fewer nodes, but whose structure
and characteristics are similar to those of the original graph. As will be
seen, a common strategy in these methods is to rely on spectral properties to
define the coarse graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_Y/0/1/0/all/0/1"&gt;Yousef Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zechen Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive Statistical Learning with Random Feature Moments. (arXiv:1706.07180v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1706.07180</id>
        <link href="http://arxiv.org/abs/1706.07180"/>
        <updated>2021-06-23T01:48:41.355Z</updated>
        <summary type="html"><![CDATA[We describe a general framework -- compressive statistical learning -- for
resource-efficient large-scale learning: the training collection is compressed
in one pass into a low-dimensional sketch (a vector of random empirical
generalized moments) that captures the information relevant to the considered
learning task. A near-minimizer of the risk is computed from the sketch through
the solution of a nonlinear least squares problem. We investigate sufficient
sketch sizes to control the generalization error of this procedure. The
framework is illustrated on compressive PCA, compressive clustering, and
compressive Gaussian mixture Modeling with fixed known variance. The latter two
are further developed in a companion paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gribonval_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Gribonval&lt;/a&gt; (PANAMA, DANTE), &lt;a href="http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1"&gt;Gilles Blanchard&lt;/a&gt; (DATASHAPE, LMO), &lt;a href="http://arxiv.org/find/stat/1/au:+Keriven_N/0/1/0/all/0/1"&gt;Nicolas Keriven&lt;/a&gt; (PANAMA, GIPSA-GAIA), &lt;a href="http://arxiv.org/find/stat/1/au:+Traonmilin_Y/0/1/0/all/0/1"&gt;Yann Traonmilin&lt;/a&gt; (PANAMA, IMB)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From SIR to SEAIRD: a novel data-driven modeling approach based on the Grey-box System Theory to predict the dynamics of COVID-19. (arXiv:2106.11918v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2106.11918</id>
        <link href="http://arxiv.org/abs/2106.11918"/>
        <updated>2021-06-23T01:48:41.347Z</updated>
        <summary type="html"><![CDATA[Common compartmental modeling for COVID-19 is based on a priori knowledge and
numerous assumptions. Additionally, they do not systematically incorporate
asymptomatic cases. Our study aimed at providing a framework for data-driven
approaches, by leveraging the strengths of the grey-box system theory or
grey-box identification, known for its robustness in problem solving under
partial, incomplete, or uncertain data. Empirical data on confirmed cases and
deaths, extracted from an open source repository were used to develop the
SEAIRD compartment model. Adjustments were made to fit current knowledge on the
COVID-19 behavior. The model was implemented and solved using an Ordinary
Differential Equation solver and an optimization tool. A cross-validation
technique was applied, and the coefficient of determination $R^2$ was computed
in order to evaluate the goodness-of-fit of the model. %to the data. Key
epidemiological parameters were finally estimated and we provided the rationale
for the construction of SEAIRD model. When applied to Brazil's cases, SEAIRD
produced an excellent agreement to the data, with an %coefficient of
determination $R^2$ $\geq 90\%$. The probability of COVID-19 transmission was
generally high ($\geq 95\%$). On the basis of a 20-day modeling data, the
incidence rate of COVID-19 was as low as 3 infected cases per 100,000 exposed
persons in Brazil and France. Within the same time frame, the fatality rate of
COVID-19 was the highest in France (16.4\%) followed by Brazil (6.9\%), and the
lowest in Russia ($\leq 1\%$). SEAIRD represents an asset for modeling
infectious diseases in their dynamical stable phase, especially for new viruses
when pathophysiology knowledge is very limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pekpe_K/0/1/0/all/0/1"&gt;Komi Midzodzi P&amp;#xe9;kp&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zitouni_D/0/1/0/all/0/1"&gt;Djamel Zitouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dhifli_W/0/1/0/all/0/1"&gt;Wajdi Dhifli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guinhouya_B/0/1/0/all/0/1"&gt;Benjamin C. Guinhouya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asynchronous Stochastic Optimization Robust to Arbitrary Delays. (arXiv:2106.11879v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11879</id>
        <link href="http://arxiv.org/abs/2106.11879"/>
        <updated>2021-06-23T01:48:41.338Z</updated>
        <summary type="html"><![CDATA[We consider stochastic optimization with delayed gradients where, at each
time step $t$, the algorithm makes an update using a stale stochastic gradient
from step $t - d_t$ for some arbitrary delay $d_t$. This setting abstracts
asynchronous distributed optimization where a central server receives gradient
updates computed by worker machines. These machines can experience computation
and communication loads that might vary significantly over time. In the general
non-convex smooth optimization setting, we give a simple and efficient
algorithm that requires $O( \sigma^2/\epsilon^4 + \tau/\epsilon^2 )$ steps for
finding an $\epsilon$-stationary point $x$, where $\tau$ is the \emph{average}
delay $\smash{\frac{1}{T}\sum_{t=1}^T d_t}$ and $\sigma^2$ is the variance of
the stochastic gradients. This improves over previous work, which showed that
stochastic gradient decent achieves the same rate but with respect to the
\emph{maximal} delay $\max_{t} d_t$, that can be significantly larger than the
average delay especially in heterogeneous distributed systems. Our experiments
demonstrate the efficacy and robustness of our algorithm in cases where the
delay distribution is skewed or heavy-tailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cohen_A/0/1/0/all/0/1"&gt;Alon Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Daniely_A/0/1/0/all/0/1"&gt;Amit Daniely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Drori_Y/0/1/0/all/0/1"&gt;Yoel Drori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schain_M/0/1/0/all/0/1"&gt;Mariano Schain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11942</id>
        <link href="http://arxiv.org/abs/2106.11942"/>
        <updated>2021-06-23T01:48:41.317Z</updated>
        <summary type="html"><![CDATA[Organ-at-risk contouring is still a bottleneck in radiotherapy, with many
deep learning methods falling short of promised results when evaluated on
clinical data. We investigate the accuracy and time-savings resulting from the
use of an interactive-machine-learning method for an organ-at-risk contouring
task. We compare the method to the Eclipse contouring software and find strong
agreement with manual delineations, with a dice score of 0.95. The annotations
created using corrective-annotation also take less time to create as more
images are annotated, resulting in substantial time savings compared to manual
methods, with hearts that take 2 minutes and 2 seconds to delineate on average,
after 923 images have been delineated, compared to 7 minutes and 1 seconds when
delineating manually. Our experiment demonstrates that
interactive-machine-learning with corrective-annotation provides a fast and
accessible way for non computer-scientists to train deep-learning models to
segment their own structures of interest as part of routine clinical workflows.

Source code is available at
\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Abraham George Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1"&gt;Cynthia Terrones-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1"&gt;Anne Kiil Berthelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1"&gt;Nora Jarrett Forbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1"&gt;Lena Specht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1"&gt;Ivan Richter Vogelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Representation Learning in Low-rank Markov Decision Processes. (arXiv:2106.11935v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11935</id>
        <link href="http://arxiv.org/abs/2106.11935"/>
        <updated>2021-06-23T01:48:41.309Z</updated>
        <summary type="html"><![CDATA[The success of deep reinforcement learning (DRL) is due to the power of
learning a representation that is suitable for the underlying exploration and
exploitation task. However, existing provable reinforcement learning algorithms
with linear function approximation often assume the feature representation is
known and fixed. In order to understand how representation learning can improve
the efficiency of RL, we study representation learning for a class of low-rank
Markov Decision Processes (MDPs) where the transition kernel can be represented
in a bilinear form. We propose a provably efficient algorithm called ReLEX that
can simultaneously learn the representation and perform exploration. We show
that ReLEX always performs no worse than a state-of-the-art algorithm without
representation learning, and will be strictly better in terms of sample
efficiency if the function class of representations enjoys a certain mild
"coverage'' property over the whole state-action space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weitong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiafan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders. (arXiv:2106.11914v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11914</id>
        <link href="http://arxiv.org/abs/2106.11914"/>
        <updated>2021-06-23T01:48:41.303Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel neuroevolutionary method to identify the
architecture and hyperparameters of convolutional autoencoders. Remarkably, we
used a hypervolume indicator in the context of neural architecture search for
autoencoders, for the first time to our current knowledge. Results show that
images were compressed by a factor of more than 10, while still retaining
enough information to achieve image classification for the majority of the
tasks. Thus, this new approach can be used to speed up the AutoML pipeline for
image compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dimanov_D/0/1/0/all/0/1"&gt;Daniel Dimanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaguer_Ballester_E/0/1/0/all/0/1"&gt;Emili Balaguer-Ballester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singleton_C/0/1/0/all/0/1"&gt;Colin Singleton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostami_S/0/1/0/all/0/1"&gt;Shahin Rostami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes on the H-measure of classifier performance. (arXiv:2106.11888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11888</id>
        <link href="http://arxiv.org/abs/2106.11888"/>
        <updated>2021-06-23T01:48:41.293Z</updated>
        <summary type="html"><![CDATA[The H-measure is a classifier performance measure which takes into account
the context of application without requiring a rigid value of relative
misclassification costs to be set. Since its introduction in 2009 it has become
widely adopted. This paper answers various queries which users have raised
since its introduction, including questions about its interpretation, the
choice of a weighting function, whether it is strictly proper, and its
coherence, and relates the measure to other work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hand_D/0/1/0/all/0/1"&gt;D. J. Hand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anagnostopoulos_C/0/1/0/all/0/1"&gt;C. Anagnostopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local policy search with Bayesian optimization. (arXiv:2106.11899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11899</id>
        <link href="http://arxiv.org/abs/2106.11899"/>
        <updated>2021-06-23T01:48:41.286Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) aims to find an optimal policy by interaction
with an environment. Consequently, learning complex behavior requires a vast
number of samples, which can be prohibitive in practice. Nevertheless, instead
of systematically reasoning and actively choosing informative samples, policy
gradients for local search are often obtained from random perturbations. These
random samples yield high variance estimates and hence are sub-optimal in terms
of sample complexity. Actively selecting informative samples is at the core of
Bayesian optimization, which constructs a probabilistic surrogate of the
objective from past samples to reason about informative subsequent ones. In
this paper, we propose to join both worlds. We develop an algorithm utilizing a
probabilistic model of the objective function and its gradient. Based on the
model, the algorithm decides where to query a noisy zeroth-order oracle to
improve the gradient estimates. The resulting algorithm is a novel type of
policy search method, which we compare to existing black-box algorithms. The
comparison reveals improved sample complexity and reduced variance in extensive
empirical evaluations on synthetic objectives. Further, we highlight the
benefits of active sampling on popular RL benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Sarah M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohr_A/0/1/0/all/0/1"&gt;Alexander von Rohr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1"&gt;Sebastian Trimpe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Policy Reinforcement Learning with Delayed Rewards. (arXiv:2106.11854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11854</id>
        <link href="http://arxiv.org/abs/2106.11854"/>
        <updated>2021-06-23T01:48:41.278Z</updated>
        <summary type="html"><![CDATA[We study deep reinforcement learning (RL) algorithms with delayed rewards. In
many real-world tasks, instant rewards are often not readily accessible or even
defined immediately after the agent performs actions. In this work, we first
formally define the environment with delayed rewards and discuss the challenges
raised due to the non-Markovian nature of such environments. Then, we introduce
a general off-policy RL framework with a new Q-function formulation that can
handle the delayed rewards with theoretical convergence guarantees. For
practical tasks with high dimensional state spaces, we further introduce the
HC-decomposition rule of the Q-function in our framework which naturally leads
to an approximation scheme that helps boost the training efficiency and
stability. We finally conduct extensive experiments to demonstrate the superior
performance of our algorithms over the existing work and their variants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Beining Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhizhou Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zuofan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jian Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Organ Failure Prediction with Classifier-Guided Generative Adversarial Imputation Networks. (arXiv:2106.11878v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11878</id>
        <link href="http://arxiv.org/abs/2106.11878"/>
        <updated>2021-06-23T01:48:41.247Z</updated>
        <summary type="html"><![CDATA[Multiple organ failure (MOF) is a severe syndrome with a high mortality rate
among Intensive Care Unit (ICU) patients. Early and precise detection is
critical for clinicians to make timely decisions. An essential challenge in
applying machine learning models to electronic health records (EHRs) is the
pervasiveness of missing values. Most existing imputation methods are involved
in the data preprocessing phase, failing to capture the relationship between
data and outcome for downstream predictions. In this paper, we propose
classifier-guided generative adversarial imputation networks Classifier-GAIN)
for MOF prediction to bridge this gap, by incorporating both observed data and
label information. Specifically, the classifier takes imputed values from the
generator(imputer) to predict task outcomes and provides additional supervision
signals to the generator by joint training. The classifier-guide generator
imputes missing values with label-awareness during training, improving the
classifier's performance during inference. We conduct extensive experiments
showing that our approach consistently outperforms classical and state-of-art
neural baselines across a range of missing data scenarios and evaluation
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinlu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Callcut_R/0/1/0/all/0/1"&gt;Rachael Callcut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petzold_L/0/1/0/all/0/1"&gt;Linda Petzold&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Recourse in Partially and Fully Confounded Settings Through Bounding Counterfactual Effects. (arXiv:2106.11849v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11849</id>
        <link href="http://arxiv.org/abs/2106.11849"/>
        <updated>2021-06-23T01:48:41.236Z</updated>
        <summary type="html"><![CDATA[Algorithmic recourse aims to provide actionable recommendations to
individuals to obtain a more favourable outcome from an automated
decision-making system. As it involves reasoning about interventions performed
in the physical world, recourse is fundamentally a causal problem. Existing
methods compute the effect of recourse actions using a causal model learnt from
data under the assumption of no hidden confounding and modelling assumptions
such as additive noise. Building on the seminal work of Balke and Pearl (1994),
we propose an alternative approach for discrete random variables which relaxes
these assumptions and allows for unobserved confounding and arbitrary
structural equations. The proposed approach only requires specification of the
causal graph and confounding structure and bounds the expected counterfactual
effect of recourse actions. If the lower bound is above a certain threshold,
i.e., on the other side of the decision boundary, recourse is guaranteed in
expectation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1"&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_N/0/1/0/all/0/1"&gt;Nikita Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zeitler_J/0/1/0/all/0/1"&gt;Jakob Zeitler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mastouri_A/0/1/0/all/0/1"&gt;Afsaneh Mastouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11921</id>
        <link href="http://arxiv.org/abs/2106.11921"/>
        <updated>2021-06-23T01:48:41.227Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have reached very high accuracy on object detection but
their success hinges on large amounts of labeled data. To reduce the dependency
on labels, various active-learning strategies have been proposed, typically
based on the confidence of the detector. However, these methods are biased
towards best-performing classes and can lead to acquired datasets that are not
good representatives of the data in the testing set. In this work, we propose a
unified framework for active learning, that considers both the uncertainty and
the robustness of the detector, ensuring that the network performs accurately
in all classes. Furthermore, our method is able to pseudo-label the very
confident predictions, suppressing a potential distribution drift while further
boosting the performance of the model. Experiments show that our method
comprehensively outperforms a wide range of active-learning methods on PASCAL
VOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%
reduction in labeling cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Failing with Grace: Learning Neural Network Controllers that are Boundedly Unsafe. (arXiv:2106.11881v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.11881</id>
        <link href="http://arxiv.org/abs/2106.11881"/>
        <updated>2021-06-23T01:48:41.219Z</updated>
        <summary type="html"><![CDATA[In this work, we consider the problem of learning a feed-forward neural
network (NN) controller to safely steer an arbitrarily shaped planar robot in a
compact and obstacle-occluded workspace. Unlike existing methods that depend
strongly on the density of data points close to the boundary of the safe state
space to train NN controllers with closed-loop safety guarantees, we propose an
approach that lifts such assumptions on the data that are hard to satisfy in
practice and instead allows for graceful safety violations, i.e., of a bounded
magnitude that can be spatially controlled. To do so, we employ reachability
analysis methods to encapsulate safety constraints in the training process.
Specifically, to obtain a computationally efficient over-approximation of the
forward reachable set of the closed-loop system, we partition the robot's state
space into cells and adaptively subdivide the cells that contain states which
may escape the safe set under the trained control law. To do so, we first
design appropriate under- and over-approximations of the robot's footprint to
adaptively subdivide the configuration space into cells. Then, using the
overlap between each cell's forward reachable set and the set of infeasible
robot configurations as a measure for safety violations, we introduce penalty
terms into the loss function that penalize this overlap in the training
process. As a result, our method can learn a safe vector field for the
closed-loop system and, at the same time, provide numerical worst-case bounds
on safety violation over the whole configuration space, defined by the overlap
between the over-approximation of the forward reachable set of the closed-loop
system and the set of unsafe states. Moreover, it can control the tradeoff
between computational complexity and tightness of these bounds. Finally, we
provide a simulation study that verifies the efficacy of the proposed scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vlantis_P/0/1/0/all/0/1"&gt;Panagiotis Vlantis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zavlanos_M/0/1/0/all/0/1"&gt;Michael M. Zavlanos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for risk assessment in gender-based crime. (arXiv:2106.11847v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.11847</id>
        <link href="http://arxiv.org/abs/2106.11847"/>
        <updated>2021-06-23T01:48:41.212Z</updated>
        <summary type="html"><![CDATA[Gender-based crime is one of the most concerning scourges of contemporary
society. Governments worldwide have invested lots of economic and human
resources to radically eliminate this threat. Despite these efforts, providing
accurate predictions of the risk that a victim of gender violence has of being
attacked again is still a very hard open problem. The development of new
methods for issuing accurate, fair and quick predictions would allow police
forces to select the most appropriate measures to prevent recidivism. In this
work, we propose to apply Machine Learning (ML) techniques to create models
that accurately predict the recidivism risk of a gender-violence offender. The
relevance of the contribution of this work is threefold: (i) the proposed ML
method outperforms the preexisting risk assessment algorithm based on classical
statistical techniques, (ii) the study has been conducted through an official
specific-purpose database with more than 40,000 reports of gender violence, and
(iii) two new quality measures are proposed for assessing the effective police
protection that a model supplies and the overload in the invested resources
that it generates. Additionally, we propose a hybrid model that combines the
statistical prediction methods with the ML method, permitting authorities to
implement a smooth transition from the preexisting model to the ML-based model.
This hybrid nature enables a decision-making process to optimally balance
between the efficiency of the police system and aggressiveness of the
protection measures taken.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bru_A/0/1/0/all/0/1"&gt;Antonio Br&amp;#xfa;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuno_J/0/1/0/all/0/1"&gt;Juan Carlos Nu&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Alvarez_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Luis Gonz&amp;#xe1;lez-&amp;#xc1;lvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-06-23T01:48:41.189Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising image regions,
in each acquisition step. The problem is framed in an exploration-exploitation
framework by combining an embedding based on Uniform Manifold Approximation to
model representativeness with entropy as uncertainty measure to model
informativeness. We applied our proposed method to the challenging autonomous
driving data sets CamVid and Cityscapes and performed a quantitative comparison
with state-of-the-art methods. We find that our active learning method achieves
better performance on CamVid compared to other methods, while on Cityscapes,
the performance lift was negligible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Any equation is a forest: Symbolic genetic algorithm for discovering open-form partial differential equations (SGA-PDE). (arXiv:2106.11927v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11927</id>
        <link href="http://arxiv.org/abs/2106.11927"/>
        <updated>2021-06-23T01:48:41.183Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) are concise and understandable
representations of domain knowledge, which are essential for deepening our
understanding of physical processes and predicting future responses. However,
the PDEs of many real-world problems are uncertain, which calls for PDE
discovery. We propose the symbolic genetic algorithm (SGA-PDE) to discover
open-form PDEs directly from data without prior knowledge about the equation
structure. SGA-PDE focuses on the representation and optimization of PDE.
Firstly, SGA-PDE uses symbolic mathematics to realize the flexible
representation of any given PDE, transforms a PDE into a forest, and converts
each function term into a binary tree. Secondly, SGA-PDE adopts a specially
designed genetic algorithm to efficiently optimize the binary trees by
iteratively updating the tree topology and node attributes. The SGA-PDE is
gradient-free, which is a desirable characteristic in PDE discovery since it is
difficult to obtain the gradient between the PDE loss and the PDE structure. In
the experiment, SGA-PDE not only successfully discovered nonlinear Burgers'
equation, Korteweg-de Vries (KdV) equation, and Chafee-Infante equation, but
also handled PDEs with fractional structure and compound functions that cannot
be solved by conventional PDE discovery methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuntian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yingtao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Customer Embeddings for Financial Service Applications. (arXiv:2106.11880v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11880</id>
        <link href="http://arxiv.org/abs/2106.11880"/>
        <updated>2021-06-23T01:48:41.175Z</updated>
        <summary type="html"><![CDATA[As financial services (FS) companies have experienced drastic technology
driven changes, the availability of new data streams provides the opportunity
for more comprehensive customer understanding. We propose Dynamic Customer
Embeddings (DCE), a framework that leverages customers' digital activity and a
wide range of financial context to learn dense representations of customers in
the FS industry. Our method examines customer actions and pageviews within a
mobile or web digital session, the sequencing of the sessions themselves, and
snapshots of common financial features across our organization at the time of
login. We test our customer embeddings using real world data in three
prediction problems: 1) the intent of a customer in their next digital session,
2) the probability of a customer calling the call centers after a session, and
3) the probability of a digital session to be fraudulent. DCE showed
performance lift in all three downstream problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chitsazan_N/0/1/0/all/0/1"&gt;Nima Chitsazan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1"&gt;Samuel Sharpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katariya_D/0/1/0/all/0/1"&gt;Dwipam Katariya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qianyu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajasethupathy_K/0/1/0/all/0/1"&gt;Karthik Rajasethupathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy Amplification via Iteration for Shuffled and Online PNSGD. (arXiv:2106.11767v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11767</id>
        <link href="http://arxiv.org/abs/2106.11767"/>
        <updated>2021-06-23T01:48:41.169Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the framework of privacy amplification via
iteration, which is originally proposed by Feldman et al. and subsequently
simplified by Asoodeh et al. in their analysis via the contraction coefficient.
This line of work focuses on the study of the privacy guarantees obtained by
the projected noisy stochastic gradient descent (PNSGD) algorithm with hidden
intermediate updates. A limitation in the existing literature is that only the
early stopped PNSGD has been studied, while no result has been proved on the
more widely-used PNSGD applied on a shuffled dataset. Moreover, no scheme has
been yet proposed regarding how to decrease the injected noise when new data
are received in an online fashion. In this work, we first prove a privacy
guarantee for shuffled PNSGD, which is investigated asymptotically when the
noise is fixed for each sample size $n$ but reduced at a predetermined rate
when $n$ increases, in order to achieve the convergence of privacy loss. We
then analyze the online setting and provide a faster decaying scheme for the
magnitude of the injected noise that also guarantees the convergence of privacy
loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sordello_M/0/1/0/all/0/1"&gt;Matteo Sordello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1"&gt;Zhiqi Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jinshuo Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower and Upper Bounds on the VC-Dimension of Tensor Network Models. (arXiv:2106.11827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11827</id>
        <link href="http://arxiv.org/abs/2106.11827"/>
        <updated>2021-06-23T01:48:41.161Z</updated>
        <summary type="html"><![CDATA[Tensor network methods have been a key ingredient of advances in condensed
matter physics and have recently sparked interest in the machine learning
community for their ability to compactly represent very high-dimensional
objects. Tensor network methods can for example be used to efficiently learn
linear models in exponentially large feature spaces [Stoudenmire and Schwab,
2016]. In this work, we derive upper and lower bounds on the VC dimension and
pseudo-dimension of a large class of tensor network models for classification,
regression and completion. Our upper bounds hold for linear models
parameterized by arbitrary tensor network structures, and we derive lower
bounds for common tensor decomposition models~(CP, Tensor Train, Tensor Ring
and Tucker) showing the tightness of our general upper bound. These results are
used to derive a generalization bound which can be applied to classification
with low rank matrices as well as linear classifiers based on any of the
commonly used tensor decomposition models. As a corollary of our results, we
obtain a bound on the VC dimension of the matrix product state classifier
introduced in [Stoudenmire and Schwab, 2016] as a function of the so-called
bond dimension~(i.e. tensor train rank), which answers an open problem listed
by Cirac, Garre-Rubio and P\'erez-Garc\'ia in [Cirac et al., 2019].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khavari_B/0/1/0/all/0/1"&gt;Behnoush Khavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Long-Term Cooperation in Cross-Silo Federated Learning: A Repeated Game Perspective. (arXiv:2106.11814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11814</id>
        <link href="http://arxiv.org/abs/2106.11814"/>
        <updated>2021-06-23T01:48:41.141Z</updated>
        <summary type="html"><![CDATA[Cross-silo federated learning (FL) is a distributed learning approach where
clients train a global model cooperatively while keeping their local data
private. Different from cross-device FL, clients in cross-silo FL are usually
organizations or companies which may execute multiple cross-silo FL processes
repeatedly due to their time-varying local data sets, and aim to optimize their
long-term benefits by selfishly choosing their participation levels. While
there has been some work on incentivizing clients to join FL, the analysis of
the long-term selfish participation behaviors of clients in cross-silo FL
remains largely unexplored. In this paper, we analyze the selfish participation
behaviors of heterogeneous clients in cross-silo FL. Specifically, we model the
long-term selfish participation behaviors of clients as an infinitely repeated
game, with the stage game being a selfish participation game in one cross-silo
FL process (SPFL). For the stage game SPFL, we derive the unique Nash
equilibrium (NE), and propose a distributed algorithm for each client to
calculate its equilibrium participation strategy. For the long-term
interactions among clients, we derive a cooperative strategy for clients which
minimizes the number of free riders while increasing the amount of local data
for model training. We show that enforced by a punishment strategy, such a
cooperative strategy is a SPNE of the infinitely repeated game, under which
some clients who are free riders at the NE of the stage game choose to be
(partial) contributors. We further propose an algorithm to calculate the
optimal SPNE which minimizes the number of free riders while maximizing the
amount of local data for model training. Simulation results show that our
proposed cooperative strategy at the optimal SPNE can effectively reduce the
number of free riders and increase the amount of local data for model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ning Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evo* 2021 -- Late-Breaking Abstracts Volume. (arXiv:2106.11804v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11804</id>
        <link href="http://arxiv.org/abs/2106.11804"/>
        <updated>2021-06-23T01:48:41.134Z</updated>
        <summary type="html"><![CDATA[Volumen with the Late-Breaking Abstracts submitted to the Evo* 2021
Conference, held online from 7 to 9 of April 2021. These papers present ongoing
research and preliminary results investigating on the application of different
approaches of Bioinspired Methods (mainly Evolutionary Computation) to
different problems, most of them real world ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mora_A/0/1/0/all/0/1"&gt;A.M. Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Esparcia_Alcazar_A/0/1/0/all/0/1"&gt;A.I. Esparcia-Alc&amp;#xe1;zar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emphatic Algorithms for Deep Reinforcement Learning. (arXiv:2106.11779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11779</id>
        <link href="http://arxiv.org/abs/2106.11779"/>
        <updated>2021-06-23T01:48:41.128Z</updated>
        <summary type="html"><![CDATA[Off-policy learning allows us to learn about possible policies of behavior
from experience generated by a different behavior policy. Temporal difference
(TD) learning algorithms can become unstable when combined with function
approximation and off-policy sampling - this is known as the ''deadly triad''.
Emphatic temporal difference (ETD($\lambda$)) algorithm ensures convergence in
the linear case by appropriately weighting the TD($\lambda$) updates. In this
paper, we extend the use of emphatic methods to deep reinforcement learning
agents. We show that naively adapting ETD($\lambda$) to popular deep
reinforcement learning algorithms, which use forward view multi-step returns,
results in poor performance. We then derive new emphatic algorithms for use in
the context of such algorithms, and we demonstrate that they provide noticeable
benefits in small problems designed to highlight the instability of TD methods.
Finally, we observed improved performance when applying these algorithms at
scale on classic Atari games from the Arcade Learning Environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1"&gt;Ray Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhongwen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1"&gt;Adam White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_M/0/1/0/all/0/1"&gt;Matteo Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1"&gt;Hado van Hasselt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Credal Self-Supervised Learning. (arXiv:2106.11853v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11853</id>
        <link href="http://arxiv.org/abs/2106.11853"/>
        <updated>2021-06-23T01:48:41.121Z</updated>
        <summary type="html"><![CDATA[Self-training is an effective approach to semi-supervised learning. The key
idea is to let the learner itself iteratively generate "pseudo-supervision" for
unlabeled instances based on its current hypothesis. In combination with
consistency regularization, pseudo-labeling has shown promising performance in
various domains, for example in computer vision. To account for the
hypothetical nature of the pseudo-labels, these are commonly provided in the
form of probability distributions. Still, one may argue that even a probability
distribution represents an excessive level of informedness, as it suggests that
the learner precisely knows the ground-truth conditional probabilities. In our
approach, we therefore allow the learner to label instances in the form of
credal sets, that is, sets of (candidate) probability distributions. Thanks to
this increased expressiveness, the learner is able to represent uncertainty and
a lack of knowledge in a more flexible and more faithful manner. To learn from
weakly labeled data of that kind, we leverage methods that have recently been
proposed in the realm of so-called superset learning. In an exhaustive
empirical evaluation, we compare our methodology to state-of-the-art
self-supervision approaches, showing competitive to superior performance
especially in low-label scenarios incorporating a high degree of uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lienen_J/0/1/0/all/0/1"&gt;Julian Lienen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speeding Up OPFython with Numba. (arXiv:2106.11828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11828</id>
        <link href="http://arxiv.org/abs/2106.11828"/>
        <updated>2021-06-23T01:48:41.114Z</updated>
        <summary type="html"><![CDATA[A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven
to be a state-of-the-art algorithm comparable to Logistic Regressors, Support
Vector Machines in a wide variety of tasks. Recently, its Python-based version,
denoted as OPFython, has been proposed to provide a more friendly framework and
a faster prototyping environment. Nevertheless, Python-based algorithms are
slower than their counterpart C-based algorithms, impacting their performance
when confronted with large amounts of data. Therefore, this paper proposed a
simple yet highly efficient speed up using the Numba package, which accelerates
Numpy-based calculations and attempts to increase the algorithm's overall
performance. Experimental results showed that the proposed approach achieved
better results than the na\"ive Python-based OPF and speeded up its distance
measurement calculation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo H. de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-06-23T01:48:41.095Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features.
This is especially important when the number of classes per task is small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:41.087Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Clustering-based Framework for Classifying Data Streams. (arXiv:2106.11823v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11823</id>
        <link href="http://arxiv.org/abs/2106.11823"/>
        <updated>2021-06-23T01:48:41.080Z</updated>
        <summary type="html"><![CDATA[The non-stationary nature of data streams strongly challenges traditional
machine learning techniques. Although some solutions have been proposed to
extend traditional machine learning techniques for handling data streams, these
approaches either require an initial label set or rely on specialized design
parameters. The overlap among classes and the labeling of data streams
constitute other major challenges for classifying data streams. In this paper,
we proposed a clustering-based data stream classification framework to handle
non-stationary data streams without utilizing an initial label set. A
density-based stream clustering procedure is used to capture novel concepts
with a dynamic threshold and an effective active label querying strategy is
introduced to continuously learn the new concepts from the data streams. The
sub-cluster structure of each cluster is explored to handle the overlap among
classes. Experimental results and quantitative comparison studies reveal that
the proposed method provides statistically better or comparable performance
than the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xuyang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Homaifar_A/0/1/0/all/0/1"&gt;Abdollah Homaifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1"&gt;Mrinmoy Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girma_A/0/1/0/all/0/1"&gt;Abenezer Girma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunstel_E/0/1/0/all/0/1"&gt;Edward Tunstel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Automated Evaluation of Explanations in Graph Neural Networks. (arXiv:2106.11864v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11864</id>
        <link href="http://arxiv.org/abs/2106.11864"/>
        <updated>2021-06-23T01:48:41.073Z</updated>
        <summary type="html"><![CDATA[Explaining Graph Neural Networks predictions to end users of AI applications
in easily understandable terms remains an unsolved problem. In particular, we
do not have well developed methods for automatically evaluating explanations,
in ways that are closer to how users consume those explanations. Based on
recent application trends and our own experiences in real world problems, we
propose automatic evaluation approaches for GNN Explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+BK_V/0/1/0/all/0/1"&gt;Vanya BK&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_B/0/1/0/all/0/1"&gt;Balaji Ganesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1"&gt;Aniket Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1"&gt;Devbrat Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Arvind Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsistent Model Discovery. (arXiv:2106.11936v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11936</id>
        <link href="http://arxiv.org/abs/2106.11936"/>
        <updated>2021-06-23T01:48:41.066Z</updated>
        <summary type="html"><![CDATA[Discovering the partial differential equations underlying a spatio-temporal
datasets from very limited observations is of paramount interest in many
scientific fields. However, it remains an open question to know when model
discovery algorithms based on sparse regression can actually recover the
underlying physical processes. We trace back the poor of performance of Lasso
based model discovery algorithms to its potential variable selection
inconsistency: meaning that even if the true model is present in the library,
it might not be selected. By first revisiting the irrepresentability condition
(IRC) of the Lasso, we gain some insights of when this might occur. We then
show that the adaptive Lasso will have more chances of verifying the IRC than
the Lasso and propose to integrate it within a deep learning model discovery
framework with stability selection and error control. Experimental results show
we can recover several nonlinear and chaotic canonical PDEs with a single set
of hyperparameters from a very limited number of samples at high noise levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tod_G/0/1/0/all/0/1"&gt;Georges Tod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Both_G/0/1/0/all/0/1"&gt;Gert-Jan Both&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kusters_R/0/1/0/all/0/1"&gt;Remy Kusters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLEA: Provably Fair Multisource Learning from Unreliable Training Data. (arXiv:2106.11732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11732</id>
        <link href="http://arxiv.org/abs/2106.11732"/>
        <updated>2021-06-23T01:48:41.060Z</updated>
        <summary type="html"><![CDATA[Fairness-aware learning aims at constructing classifiers that not only make
accurate predictions, but do not discriminate against specific groups. It is a
fast-growing area of machine learning with far-reaching societal impact.
However, existing fair learning methods are vulnerable to accidental or
malicious artifacts in the training data, which can cause them to unknowingly
produce unfair classifiers. In this work we address the problem of fair
learning from unreliable training data in the robust multisource setting, where
the available training data comes from multiple sources, a fraction of which
might be not representative of the true data distribution. We introduce FLEA, a
filtering-based algorithm that allows the learning system to identify and
suppress those data sources that would have a negative impact on fairness or
accuracy if they were used for training. We show the effectiveness of our
approach by a diverse range of experiments on multiple datasets. Additionally
we prove formally that, given enough data, FLEA protects the learner against
unreliable data as long as the fraction of affected data sources is less than
half.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1"&gt;Eugenia Iofinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinov_N/0/1/0/all/0/1"&gt;Nikola Konstantinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1"&gt;Christoph H. Lampert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifted Model Checking for Relational MDPs. (arXiv:2106.11735v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11735</id>
        <link href="http://arxiv.org/abs/2106.11735"/>
        <updated>2021-06-23T01:48:41.041Z</updated>
        <summary type="html"><![CDATA[Model checking has been developed for verifying the behaviour of systems with
stochastic and non-deterministic behavior. It is used to provide guarantees
about such systems. While most model checking methods focus on propositional
models, various probabilistic planning and reinforcement frameworks deal with
relational domains, for instance, STRIPS planning and relational Markov
Decision Processes. Using propositional model checking in relational settings
requires one to ground the model, which leads to the well known state explosion
problem and intractability. We present pCTL-REBEL, a lifted model checking
approach for verifying pCTL properties on relational MDPs. It extends REBEL,
the relational Bellman update operator, which is a lifted value iteration
approach for model-based relational reinforcement learning, toward relational
model-checking. PCTL-REBEL is lifted, which means that rather than grounding,
the model exploits symmetries and reasons at an abstract relational level.
Theoretically, we show that the pCTL model checking approach is decidable for
relational MDPs even for possibly infinite domains provided that the states
have a bounded size. Practically, we contribute algorithms and an
implementation of lifted relational model checking, and we show that the lifted
approach improves the scalability of the model checking approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wen-Chi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskin_J/0/1/0/all/0/1"&gt;Jean-Fran&amp;#xe7;ois Raskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1"&gt;Luc De Raedt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Anomalous User Behavior in Remote Patient Monitoring. (arXiv:2106.11844v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11844</id>
        <link href="http://arxiv.org/abs/2106.11844"/>
        <updated>2021-06-23T01:48:41.032Z</updated>
        <summary type="html"><![CDATA[The growth in Remote Patient Monitoring (RPM) services using wearable and
non-wearable Internet of Medical Things (IoMT) promises to improve the quality
of diagnosis and facilitate timely treatment for a gamut of medical conditions.
At the same time, the proliferation of IoMT devices increases the potential for
malicious activities that can lead to catastrophic results including theft of
personal information, data breach, and compromised medical devices, putting
human lives at risk. IoMT devices generate tremendous amount of data that
reflect user behavior patterns including both personal and day-to-day social
activities along with daily routine health monitoring. In this context, there
are possibilities of anomalies generated due to various reasons including
unexpected user behavior, faulty sensor, or abnormal values from
malicious/compromised devices. To address this problem, there is an imminent
need to develop a framework for securing the smart health care infrastructure
to identify and mitigate anomalies. In this paper, we present an anomaly
detection model for RPM utilizing IoMT and smart home devices. We propose
Hidden Markov Model (HMM) based anomaly detection that analyzes normal user
behavior in the context of RPM comprising both smart home and smart health
devices, and identifies anomalous user behavior. We design a testbed with
multiple IoMT devices and home sensors to collect data and use the HMM model to
train using network and user behavioral data. Proposed HMM based anomaly
detection model achieved over 98% accuracy in identifying the anomalies in the
context of RPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepti Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Maanak Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1"&gt;Smriti Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tosun_A/0/1/0/all/0/1"&gt;Ali Saman Tosun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11695</id>
        <link href="http://arxiv.org/abs/2106.11695"/>
        <updated>2021-06-23T01:48:41.023Z</updated>
        <summary type="html"><![CDATA[In many computer vision classification tasks, class priors at test time often
differ from priors on the training set. In the case of such prior shift,
classifiers must be adapted correspondingly to maintain close to optimal
performance. This paper analyzes methods for adaptation of probabilistic
classifiers to new priors and for estimating new priors on an unlabeled test
set. We propose a novel method to address a known issue of prior estimation
methods based on confusion matrices, where inconsistent estimates of decision
probabilities and confusion matrices lead to negative values in the estimated
priors. Experiments on fine-grained image classification datasets provide
insight into the best practice of prior shift estimation and classifier
adaptation and show that the proposed method achieves state-of-the-art results
in prior adaptation. Applying the best practice to two tasks with naturally
imbalanced priors, learning from web-crawled images and plant species
classification, increased the recognition accuracy by 1.1% and 3.4%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1"&gt;Tomas Sipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1"&gt;Milan Sulc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11740</id>
        <link href="http://arxiv.org/abs/2106.11740"/>
        <updated>2021-06-23T01:48:41.016Z</updated>
        <summary type="html"><![CDATA[Modern pre-trained language models are mostly built upon backbones stacking
self-attention and feed-forward layers in an interleaved order. In this paper,
beyond this stereotyped layer pattern, we aim to improve pre-trained models by
exploiting layer variety from two aspects: the layer type set and the layer
order. Specifically, besides the original self-attention and feed-forward
layers, we introduce convolution into the layer type set, which is
experimentally found beneficial to pre-trained models. Furthermore, beyond the
original interleaved order, we explore more layer orders to discover more
powerful architectures. However, the introduced layer variety leads to a large
architecture space of more than billions of candidates, while training a single
candidate model from scratch already requires huge computation cost, making it
not affordable to search such a space by directly training large amounts of
candidate models. To solve this problem, we first pre-train a supernet from
which the weights of all candidate models can be inherited, and then adopt an
evolutionary algorithm guided by pre-training accuracy to find the optimal
architecture. Extensive experiments show that LV-BERT model obtained by our
method outperforms BERT and its variants on various downstream tasks. For
example, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than
the strong baseline ELECTRA-small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weihao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11769</id>
        <link href="http://arxiv.org/abs/2106.11769"/>
        <updated>2021-06-23T01:48:40.998Z</updated>
        <summary type="html"><![CDATA[Speech production is a dynamic procedure, which involved multi human organs
including the tongue, jaw and lips. Modeling the dynamics of the vocal tract
deformation is a fundamental problem to understand the speech, which is the
most common way for human daily communication. Researchers employ several
sensory streams to describe the process simultaneously, which are
incontrovertibly statistically related to other streams. In this paper, we
address the following question: given an observable image sequences of lips,
can we picture the corresponding tongue motion. We formulated this problem as
the self-supervised learning problem, and employ the two-stream convolutional
network and long-short memory network for the learning task, with the attention
mechanism. We evaluate the performance of the proposed method by leveraging the
unlabeled lip videos to predict an upcoming ultrasound tongue image sequence.
The results show that our model is able to generate images that close to the
real ultrasound tongue images, and results in the matching between two imaging
modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jihan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Deep Learning for the Remote Characterisation of Ambulation in Multiple Sclerosis using Smartphones. (arXiv:2103.09171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09171</id>
        <link href="http://arxiv.org/abs/2103.09171"/>
        <updated>2021-06-23T01:48:40.991Z</updated>
        <summary type="html"><![CDATA[The emergence of digital technologies such as smartphones in healthcare
applications have demonstrated the possibility of developing rich, continuous,
and objective measures of multiple sclerosis (MS) disability that can be
administered remotely and out-of-clinic. In this work, deep convolutional
neural networks (DCNN) applied to smartphone inertial sensor data were shown to
better distinguish healthy from MS participant ambulation, compared to standard
Support Vector Machine (SVM) feature-based methodologies. To overcome the
typical limitations associated with remotely generated health data, such as low
subject numbers, sparsity, and heterogeneous data, a transfer learning (TL)
model from similar large open-source datasets was proposed. Our TL framework
utilised the ambulatory information learned on Human Activity Recognition (HAR)
tasks collected from similar smartphone-based sensor data. A lack of
transparency of "black-box" deep networks remains one of the largest stumbling
blocks to the wider acceptance of deep learning for clinical applications.
Ensuing work therefore aimed to visualise DCNN decisions attributed by
relevance heatmaps using Layer-Wise Relevance Propagation (LRP). Through the
LRP framework, the patterns captured from smartphone-based inertial sensor data
that were reflective of those who are healthy versus persons with MS (PwMS)
could begin to be established and understood. Interpretations suggested that
cadence-based measures, gait speed, and ambulation-related signal perturbations
were distinct characteristics that distinguished MS disability from healthy
participants. Robust and interpretable outcomes, generated from high-frequency
out-of-clinic assessments, could greatly augment the current in-clinic
assessment picture for PwMS, to inform better disease management techniques,
and enable the development of better therapeutic interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Creagh_A/0/1/0/all/0/1"&gt;Andrew P. Creagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipsmeier_F/0/1/0/all/0/1"&gt;Florian Lipsmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindemann_M/0/1/0/all/0/1"&gt;Michael Lindemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vos_M/0/1/0/all/0/1"&gt;Maarten De Vos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance-Aware Off-Policy Evaluation with Linear Function Approximation. (arXiv:2106.11960v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11960</id>
        <link href="http://arxiv.org/abs/2106.11960"/>
        <updated>2021-06-23T01:48:40.958Z</updated>
        <summary type="html"><![CDATA[We study the off-policy evaluation (OPE) problem in reinforcement learning
with linear function approximation, which aims to estimate the value function
of a target policy based on the offline data collected by a behavior policy. We
propose to incorporate the variance information of the value function to
improve the sample efficiency of OPE. More specifically, for time-inhomogeneous
episodic linear Markov decision processes (MDPs), we propose an algorithm,
VA-OPE, which uses the estimated variance of the value function to reweight the
Bellman residual in Fitted Q-Iteration. We show that our algorithm achieves a
tighter error bound than the best-known result. We also provide a fine-grained
characterization of the distribution shift between the behavior policy and the
target policy. Extensive numerical experiments corroborate our theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yifei Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Dongruo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11731</id>
        <link href="http://arxiv.org/abs/2106.11731"/>
        <updated>2021-06-23T01:48:40.951Z</updated>
        <summary type="html"><![CDATA[UK Biobank (UKB) is conducting a large-scale study of more than half a
million volunteers, collecting health-related information on genetics,
lifestyle, blood biochemistry, and more. Medical imaging furthermore targets
100,000 subjects, with 70,000 follow-up sessions, enabling measurements of
organs, muscle, and body composition. With up to 170,000 mounting MR images,
various methodologies are accordingly engaged in large-scale image analysis.
This work presents an experimental inference engine that can automatically
predict a comprehensive profile of subject metadata from UKB neck-to-knee body
MRI. In cross-validation, it accurately inferred baseline characteristics such
as age, height, weight, and sex, but also emulated measurements of body
composition by DXA, organ volumes, and abstract properties like grip strength,
pulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can
automatically analyze thousands of subjects within hours and provide individual
confidence intervals. The underlying methodology is based on convolutional
neural networks for image-based mean-variance regression on two-dimensional
representations of the MRI data. This work aims to make the proposed system
available for free to researchers, who can use it to obtain fast and
fully-automated estimates of 72 different measurements immediately upon release
of new UK Biobank image data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1"&gt;Taro Langner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Mart&amp;#xed;nez Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1"&gt;Robin Strand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kan Ahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1"&gt;Joel Kullberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Sequential Optimisation with Delayed Feedback. (arXiv:2106.11294v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11294</id>
        <link href="http://arxiv.org/abs/2106.11294"/>
        <updated>2021-06-23T01:48:40.944Z</updated>
        <summary type="html"><![CDATA[Stochastic delays in feedback lead to unstable sequential learning using
multi-armed bandits. Recently, empirical Bayesian shrinkage has been shown to
improve reward estimation in bandit learning. Here, we propose a novel
adaptation to shrinkage that estimates smoothed reward estimates from windowed
cumulative inputs, to deal with incomplete knowledge from delayed feedback and
non-stationary rewards. Using numerical simulations, we show that this
adaptation retains the benefits of shrinkage, and improves the stability of
reward estimation by more than 50%. Our proposal reduces variability in
treatment allocations to the best arm by up to 3.8x, and improves statistical
accuracy - with up to 8% improvement in true positive rates and 37% reduction
in false positive rates. Together, these advantages enable control of the
trade-off between speed and stability of adaptation, and facilitate
human-in-the-loop sequential optimisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chennu_S/0/1/0/all/0/1"&gt;Srivas Chennu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;Jamie Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liyanagama_P/0/1/0/all/0/1"&gt;Puli Liyanagama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohr_P/0/1/0/all/0/1"&gt;Phil Mohr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks. (arXiv:2103.06671v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06671</id>
        <link href="http://arxiv.org/abs/2103.06671"/>
        <updated>2021-06-23T01:48:40.933Z</updated>
        <summary type="html"><![CDATA[We study the statistical theory of offline reinforcement learning (RL) with
deep ReLU network function approximation. We analyze a variant of fitted-Q
iteration (FQI) algorithm under a new dynamic condition that we call Besov
dynamic closure, which encompasses the conditions from prior analyses for deep
neural network function approximation. Under Besov dynamic closure, we prove
that the FQI-type algorithm enjoys the sample complexity of
$\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 -
2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the
dimensionality of the state-action space, $\alpha$ is the (possibly fractional)
smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified
precision. This is an improvement over the sample complexity of
$\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 -
d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an
algorithmic iteration number which is arbitrarily large in practice.
Importantly, our sample complexity is obtained under the new general dynamic
condition and a data-dependent structure where the latter is either ignored in
prior algorithms or improperly handled by prior analyses. This is the first
comprehensive analysis for offline RL with deep ReLU network function
approximation under a general setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_Tang_T/0/1/0/all/0/1"&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Sunil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tran_The_H/0/1/0/all/0/1"&gt;Hung Tran-The&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkatesh_S/0/1/0/all/0/1"&gt;Svetha Venkatesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:40.910Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07810</id>
        <link href="http://arxiv.org/abs/2102.07810"/>
        <updated>2021-06-23T01:48:40.903Z</updated>
        <summary type="html"><![CDATA[Networks have been widely used to represent the relations between objects
such as academic networks and social networks, and learning embedding for
networks has thus garnered plenty of research attention. Self-supervised
network representation learning aims at extracting node embedding without
external supervision. Recently, maximizing the mutual information between the
local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI
for short) has shown promising results on many downstream tasks such as node
classification. However, there are two major limitations of DGI. Firstly, DGI
merely considers the extrinsic supervision signal (i.e., the mutual information
between node embedding and global summary) while ignores the intrinsic signal
(i.e., the mutual dependence between node embedding and node attributes).
Secondly, nodes in a real-world network are usually connected by multiple edges
with different relations, while DGI does not fully explore the various
relations among nodes. To address the above-mentioned problems, we propose a
novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning
node embedding on multiplex networks in a self-supervised way. To be more
specific, we first design a joint supervision signal containing both extrinsic
and intrinsic mutual information by high-order mutual information, and we
propose a High-order Deep Infomax (HDI) to optimize the proposed supervision
signal. Then we propose an attention based fusion module to combine node
embedding from different layers of the multiplex network. Finally, we evaluate
the proposed HDMI on various downstream tasks such as unsupervised clustering
and supervised classification. The experimental results show that HDMI achieves
state-of-the-art performance on these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Baoyu Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1"&gt;Chanyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1"&gt;Hanghang Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Regression Revisited: Acceleration and Improved Estimation Rates. (arXiv:2106.11938v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.11938</id>
        <link href="http://arxiv.org/abs/2106.11938"/>
        <updated>2021-06-23T01:48:40.883Z</updated>
        <summary type="html"><![CDATA[We study fast algorithms for statistical regression problems under the strong
contamination model, where the goal is to approximately optimize a generalized
linear model (GLM) given adversarially corrupted samples. Prior works in this
line of research were based on the robust gradient descent framework of Prasad
et. al., a first-order method using biased gradient queries, or the Sever
framework of Diakonikolas et. al., an iterative outlier-removal method calling
a stationary point finder.

We present nearly-linear time algorithms for robust regression problems with
improved runtime or estimation guarantees compared to the state-of-the-art. For
the general case of smooth GLMs (e.g. logistic regression), we show that the
robust gradient descent framework of Prasad et. al. can be accelerated, and
show our algorithm extends to optimizing the Moreau envelopes of Lipschitz GLMs
(e.g. support vector machines), answering several open questions in the
literature.

For the well-studied case of robust linear regression, we present an
alternative approach obtaining improved estimation rates over prior
nearly-linear time algorithms. Interestingly, our method starts with an
identifiability proof introduced in the context of the sum-of-squares algorithm
of Bakshi and Prasad, which achieved optimal error rates while requiring large
polynomial runtime and sample complexity. We reinterpret their proof within the
Sever framework and obtain a dramatically faster and more sample-efficient
algorithm under fewer distributional assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jambulapati_A/0/1/0/all/0/1"&gt;Arun Jambulapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1"&gt;Tselil Schramm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1"&gt;Kevin Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes. (arXiv:2005.08140v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08140</id>
        <link href="http://arxiv.org/abs/2005.08140"/>
        <updated>2021-06-23T01:48:40.877Z</updated>
        <summary type="html"><![CDATA[We consider the optimal approximate posterior over the top-layer weights in a
Bayesian neural network for regression, and show that it exhibits strong
dependencies on the lower-layer weights. We adapt this result to develop a
correlated approximate posterior over the weights at all layers in a Bayesian
neural network. We extend this approach to deep Gaussian processes, unifying
inference in the two model classes. Our approximate posterior uses learned
"global" inducing points, which are defined only at the input layer and
propagated through the network to obtain inducing inputs at subsequent layers.
By contrast, standard, "local", inducing point methods from the deep Gaussian
process literature optimise a separate set of inducing inputs at every layer,
and thus do not model correlations across layers. Our method gives
state-of-the-art performance for a variational Bayesian method, without data
augmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC
without tempering but with data augmentation (88% in Wenzel et al. 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ober_S/0/1/0/all/0/1"&gt;Sebastian W. Ober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1"&gt;Laurence Aitchison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-06-23T01:48:40.870Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting methodology for the Intellectual
Property protection of generative models. Prior solutions for discriminative
models usually adopt adversarial examples as the fingerprints, which give
anomalous inference behaviors and prediction results. Hence, these methods are
not stealthy and can be easily recognized by the adversary. Our approach
leverages the invisible backdoor technique to overcome the above limitation.
Specifically, we design verification samples, whose model outputs look normal
but can trigger a backdoor classifier to make abnormal predictions. We propose
a new backdoor embedding approach with Unique-Triplet Loss and fine-grained
categorization to enhance the effectiveness of our fingerprints. Extensive
evaluations show that this solution can outperform other strategies with higher
robustness, uniqueness and stealthiness for various GAN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1"&gt;Li Guanlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1"&gt;Guo Shangwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1"&gt;Wang Run&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1"&gt;Xu Guowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1"&gt;Zhang Tianwei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sphynx: ReLU-Efficient Network Design for Private Inference. (arXiv:2106.11755v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11755</id>
        <link href="http://arxiv.org/abs/2106.11755"/>
        <updated>2021-06-23T01:48:40.863Z</updated>
        <summary type="html"><![CDATA[The emergence of deep learning has been accompanied by privacy concerns
surrounding users' data and service providers' models. We focus on private
inference (PI), where the goal is to perform inference on a user's data sample
using a service provider's model. Existing PI methods for deep networks enable
cryptographically secure inference with little drop in functionality; however,
they incur severe latency costs, primarily caused by non-linear network
operations (such as ReLUs). This paper presents Sphynx, a ReLU-efficient
network design method based on micro-search strategies for convolutional cell
design. Sphynx achieves Pareto dominance over all existing private inference
methods on CIFAR-100. We also design large-scale networks that support
cryptographically private inference on Tiny-ImageNet and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1"&gt;Minsu Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1"&gt;Chinmay Hegde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11753</id>
        <link href="http://arxiv.org/abs/2106.11753"/>
        <updated>2021-06-23T01:48:40.856Z</updated>
        <summary type="html"><![CDATA[Machine learning methods are widely used in the natural sciences to model and
predict physical systems from observation data. Yet, they are often used as
poorly understood "black boxes," disregarding existing mathematical structure
and invariants of the problem. Recently, the proposal of Hamiltonian Neural
Networks (HNNs) took a first step towards a unified "gray box" approach, using
physical insight to improve performance for Hamiltonian systems. In this paper,
we explore a significantly improved training method for HNNs, exploiting the
symplectic structure of Hamiltonian systems with a different loss function.
This frees the loss from an artificial lower bound. We mathematically guarantee
the existence of an exact Hamiltonian function which the HNN can learn. This
allows us to prove and numerically analyze the errors made by HNNs which, in
turn, renders them fully explainable. Finally, we present a novel post-training
correction to obtain the true Hamiltonian only from discretized observation
data, up to an arbitrary order.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_M/0/1/0/all/0/1"&gt;Marco David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehats_F/0/1/0/all/0/1"&gt;Florian M&amp;#xe9;hats&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs. (arXiv:2103.09430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09430</id>
        <link href="http://arxiv.org/abs/2103.09430"/>
        <updated>2021-06-23T01:48:40.849Z</updated>
        <summary type="html"><![CDATA[Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a huge impact on both
industrial and scientific applications. However, community efforts to advance
large-scale graph ML have been severely limited by the lack of a suitable
public benchmark. For KDD Cup 2021, we present OGB Large-Scale Challenge
(OGB-LSC), a collection of three real-world datasets for advancing the
state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that
are orders of magnitude larger than existing ones and covers three core graph
learning tasks -- link prediction, graph regression, and node classification.
Furthermore, OGB-LSC provides dedicated baseline experiments, scaling up
expressive graph ML models to the massive datasets. We show that the expressive
models significantly outperform simple scalable baselines, indicating an
opportunity for dedicated efforts to further improve graph ML at scale. Our
datasets and baseline code are released and maintained as part of our OGB
initiative (Hu et al., 2020). We hope OGB-LSC at KDD Cup 2021 can empower the
community to discover innovative solutions for large-scale graph ML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weihua Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1"&gt;Matthias Fey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;Hongyu Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_M/0/1/0/all/0/1"&gt;Maho Nakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1"&gt;Jure Leskovec&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surrogate-based variational data assimilation for tidal modelling. (arXiv:2106.11926v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.11926</id>
        <link href="http://arxiv.org/abs/2106.11926"/>
        <updated>2021-06-23T01:48:40.842Z</updated>
        <summary type="html"><![CDATA[Data assimilation (DA) is widely used to combine physical knowledge and
observations. It is nowadays commonly used in geosciences to perform parametric
calibration. In a context of climate change, old calibrations can not
necessarily be used for new scenarios. This raises the question of DA
computational cost, as costly physics-based numerical models need to be
reanalyzed. Reduction and metamodelling represent therefore interesting
perspectives, for example proposed in recent contributions as hybridization
between ensemble and variational methods, to combine their advantages
(efficiency, non-linear framework). They are however often based on Monte Carlo
(MC) type sampling, which often requires considerable increase of the ensemble
size for better efficiency, therefore representing a computational burden in
ensemble-based methods as well. To address these issues, two methods to replace
the complex model by a surrogate are proposed and confronted : (i) PODEn3DVAR
directly inspired from PODEn4DVAR, relies on an ensemble-based joint
parameter-state Proper Orthogonal Decomposition (POD), which provides a linear
metamodel ; (ii) POD-PCE-3DVAR, where the model states are POD reduced then
learned using Polynomial Chaos Expansion (PCE), resulting in a non-linear
metamodel. Both metamodels allow to write an approximate cost function whose
minimum can be analytically computed, or deduced by a gradient descent at
negligible cost. Furthermore, adapted metamodelling error covariance matrix is
given for POD-PCE-3DVAR, allowing to substantially improve the metamodel-based
DA analysis. Proposed methods are confronted on a twin experiment, and compared
to classical 3DVAR on a measurement-based problem. Results are promising, in
particular superior with POD-PCE-3DVAR, showing good convergence to classical
3DVAR and robustness to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mouradi_R/0/1/0/all/0/1"&gt;Rem-Sophia Mouradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Goeury_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Goeury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thual_O/0/1/0/all/0/1"&gt;Olivier Thual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaoui_F/0/1/0/all/0/1"&gt;Fabrice Zaoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tassi_P/0/1/0/all/0/1"&gt;Pablo Tassi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-23T01:48:40.835Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no syntax
errors). Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speed Benchmarking of Genetic Programming Frameworks. (arXiv:2106.11919v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11919</id>
        <link href="http://arxiv.org/abs/2106.11919"/>
        <updated>2021-06-23T01:48:40.815Z</updated>
        <summary type="html"><![CDATA[Genetic Programming (GP) is known to suffer from the burden of being
computationally expensive by design. While, over the years, many techniques
have been developed to mitigate this issue, data vectorization, in particular,
is arguably still the most attractive strategy due to the parallel nature of
GP. In this work, we employ a series of benchmarks meant to compare both the
performance and evolution capabilities of different vectorized and iterative
implementation approaches across several existing frameworks. Namely, TensorGP,
a novel open-source engine written in Python, is shown to greatly benefit from
the TensorFlow library to accelerate the domain evaluation phase in GP. The
presented performance benchmarks demonstrate that the TensorGP engine manages
to pull ahead, with relative speedups above two orders of magnitude for
problems with a higher number of fitness cases. Additionally, as a consequence
of being able to compute larger domains, we argue that TensorGP performance
gains aid the discovery of more accurate candidate solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baeta_F/0/1/0/all/0/1"&gt;Francisco Baeta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correia_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Correia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_T/0/1/0/all/0/1"&gt;Tiago Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1"&gt;Penousal Machado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07346</id>
        <link href="http://arxiv.org/abs/2106.07346"/>
        <updated>2021-06-23T01:48:40.805Z</updated>
        <summary type="html"><![CDATA[Topic modeling is an unsupervised method for revealing the hidden semantic
structure of a corpus. It has been increasingly widely adopted as a tool in the
social sciences, including political science, digital humanities and
sociological research in general. One desirable property of topic models is to
allow users to find topics describing a specific aspect of the corpus. A
possible solution is to incorporate domain-specific knowledge into topic
modeling, but this requires a specification from domain experts. We propose a
novel query-driven topic model that allows users to specify a simple query in
words or phrases and return query-related topics, thus avoiding tedious work
from domain experts. Our proposed approach is particularly attractive when the
user-specified query has a low occurrence in a text corpus, making it difficult
for traditional topic models built on word cooccurrence patterns to identify
relevant topics. Experimental results demonstrate the effectiveness of our
model in comparison with both classical topic models and neural topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1"&gt;Rob Procter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomness In Neural Network Training: Characterizing The Impact of Tooling. (arXiv:2106.11872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11872</id>
        <link href="http://arxiv.org/abs/2106.11872"/>
        <updated>2021-06-23T01:48:40.795Z</updated>
        <summary type="html"><![CDATA[The quest for determinism in machine learning has disproportionately focused
on characterizing the impact of noise introduced by algorithmic design choices.
In this work, we address a less well understood and studied question: how does
our choice of tooling introduce randomness to deep neural network training. We
conduct large scale experiments across different types of hardware,
accelerators, state of art networks, and open-source datasets, to characterize
how tooling choices contribute to the level of non-determinism in a system, the
impact of said non-determinism, and the cost of eliminating different sources
of noise.

Our findings are surprising, and suggest that the impact of non-determinism
in nuanced. While top-line metrics such as top-1 accuracy are not noticeably
impacted, model performance on certain parts of the data distribution is far
more sensitive to the introduction of randomness. Our results suggest that
deterministic tooling is critical for AI safety. However, we also find that the
cost of ensuring determinism varies dramatically between neural network
architectures and hardware types, e.g., with overhead up to $746\%$, $241\%$,
and $196\%$ on a spectrum of widely used GPU accelerator architectures,
relative to non-deterministic training. The source code used in this paper is
available at https://github.com/usyd-fsalab/NeuralNetworkRandomness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1"&gt;Donglin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingyao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1"&gt;Sara Hooker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empirically explaining SGD from a line search perspective. (arXiv:2103.17132v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17132</id>
        <link href="http://arxiv.org/abs/2103.17132"/>
        <updated>2021-06-23T01:48:40.786Z</updated>
        <summary type="html"><![CDATA[Optimization in Deep Learning is mainly guided by vague intuitions and strong
assumptions, with a limited understanding how and why these work in practice.
To shed more light on this, our work provides some deeper understandings of how
SGD behaves by empirically analyzing the trajectory taken by SGD from a line
search perspective. Specifically, a costly quantitative analysis of the
full-batch loss along SGD trajectories from common used models trained on a
subset of CIFAR-10 is performed. Our core results include that the full-batch
loss along lines in update step direction is highly parabolically. Further on,
we show that there exists a learning rate with which SGD always performs almost
exact line searches on the full-batch loss. Finally, we provide a different
perspective why increasing the batch size has almost the same effect as
decreasing the learning rate by the same factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mutschler_M/0/1/0/all/0/1"&gt;Maximus Mutschler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Update of a Progressively Expanded Database for Automated Lung Sound Analysis. (arXiv:2102.04062v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04062</id>
        <link href="http://arxiv.org/abs/2102.04062"/>
        <updated>2021-06-23T01:48:40.774Z</updated>
        <summary type="html"><![CDATA[A continuous real-time respiratory sound automated analysis system is needed
in clinical practice. Previously, we established an open access lung sound
database, HF_Lung_V1, and automated lung sound analysis algorithms capable of
detecting inhalation, exhalation, continuous adventitious sounds (CASs) and
discontinuous adventitious sounds (DASs). In this study, HF-Lung-V1 has been
further expanded to HF-Lung-V2 with 1.45 times of increase in audio files. The
convolutional neural network (CNN)-bidirectional gated recurrent unit (BiGRU)
model was separately trained with training datasets of HF_Lung_V1 (V1_Train)
and HF_Lung_V2 (V2_Train), and then were used for the performance comparisons
of segment detection and event detection on both test datasets of HF_Lung_V1
(V1_Test) and HF_Lung_V2 (V2_Test). The performance of segment detection was
measured by accuracy, predictive positive value (PPV), sensitivity,
specificity, F1 score, receiver operating characteristic (ROC) curve and area
under the curve (AUC), whereas that of event detection was evaluated with PPV,
sensitivity, and F1 score. Results indicate that the model performance trained
by V2_Train showed improvement on both V1_Test and V2_Test in inhalation, CASs
and DASs, particularly in CASs, as well as on V1_Test in exhalation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_F/0/1/0/all/0/1"&gt;Fu-Shun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shang-Ran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chien-Wen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yuan-Ren Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Chieh Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_J/0/1/0/all/0/1"&gt;Jack Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chung-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1"&gt;Feipei Lai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepReDuce: ReLU Reduction for Fast Private Inference. (arXiv:2103.01396v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01396</id>
        <link href="http://arxiv.org/abs/2103.01396"/>
        <updated>2021-06-23T01:48:40.746Z</updated>
        <summary type="html"><![CDATA[The recent rise of privacy concerns has led researchers to devise methods for
private neural inference -- where inferences are made directly on encrypted
data, never seeing inputs. The primary challenge facing private inference is
that computing on encrypted data levies an impractically-high latency penalty,
stemming mostly from non-linear operators like ReLU. Enabling practical and
private inference requires new optimization methods that minimize network ReLU
counts while preserving accuracy. This paper proposes DeepReDuce: a set of
optimizations for the judicious removal of ReLUs to reduce private inference
latency. The key insight is that not all ReLUs contribute equally to accuracy.
We leverage this insight to drop, or remove, ReLUs from classic networks to
significantly reduce inference latency and maintain high accuracy. Given a
target network, DeepReDuce outputs a Pareto frontier of networks that tradeoff
the number of ReLUs and accuracy. Compared to the state-of-the-art for private
inference DeepReDuce improves accuracy and reduces ReLU count by up to 3.5%
(iso-ReLU count) and 3.5$\times$ (iso-accuracy), respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Nandan Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodsi_Z/0/1/0/all/0/1"&gt;Zahra Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Siddharth Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reagen_B/0/1/0/all/0/1"&gt;Brandon Reagen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04279</id>
        <link href="http://arxiv.org/abs/2102.04279"/>
        <updated>2021-06-23T01:48:40.728Z</updated>
        <summary type="html"><![CDATA[The classical Langevin Monte Carlo method looks for i.i.d. samples from a
target distribution by descending along the gradient of the target
distribution. It is popular partially due to its fast convergence rate.
However, the numerical cost is sometimes high because the gradient can be hard
to obtain. One approach to eliminate the gradient computation is to employ the
concept of "ensemble", where a large number of particles are evolved together
so that the neighboring particles provide gradient information to each other.
In this article, we discuss two algorithms that integrate the ensemble feature
into LMC, and the associated properties. There are two sides of our discovery:

1. By directly surrogating the gradient using the ensemble approximation, we
develop Ensemble Langevin Monte Carlo. We show that this method is unstable due
to a potentially small denominator that induces high variance. We provide a
counterexample to explicitly show this instability.

2. We then change the strategy and enact the ensemble approximation to the
gradient only in a constrained manner, to eliminate the unstable points. The
algorithm is termed Constrained Ensemble Langevin Monte Carlo. We show that,
with a proper tuning, the surrogation takes place often enough to bring the
reasonable numerical saving, while the induced error is still low enough for us
to maintain the fast convergence rate, up to a controllable discretization and
ensemble error.

Such combination of ensemble method and LMC shed light on inventing
gradient-free algorithms that produce i.i.d. samples almost exponentially fast.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhiyan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Learning Under Triage. (arXiv:2103.08902v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08902</id>
        <link href="http://arxiv.org/abs/2103.08902"/>
        <updated>2021-06-23T01:48:40.708Z</updated>
        <summary type="html"><![CDATA[Multiple lines of evidence suggest that predictive models may benefit from
algorithmic triage. Under algorithmic triage, a predictive model does not
predict all instances but instead defers some of them to human experts.
However, the interplay between the prediction accuracy of the model and the
human experts under algorithmic triage is not well understood. In this work, we
start by formally characterizing under which circumstances a predictive model
may benefit from algorithmic triage. In doing so, we also demonstrate that
models trained for full automation may be suboptimal under triage. Then, given
any model and desired level of triage, we show that the optimal triage policy
is a deterministic threshold rule in which triage decisions are derived
deterministically by thresholding the difference between the model and human
errors on a per-instance level. Building upon these results, we introduce a
practical gradient-based algorithm that is guaranteed to find a sequence of
triage policies and predictive models of increasing performance. Experiments on
a wide variety of supervised learning tasks using synthetic and real data from
two important applications -- content moderation and scientific discovery --
illustrate our theoretical results and show that the models and triage policies
provided by our gradient-based algorithm outperform those provided by several
competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Okati_N/0/1/0/all/0/1"&gt;Nastaran Okati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1"&gt;Abir De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1"&gt;Manuel Gomez-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Long Range Memory Effects in Deep Neural Networks. (arXiv:2105.02062v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02062</id>
        <link href="http://arxiv.org/abs/2105.02062"/>
        <updated>2021-06-23T01:48:40.701Z</updated>
        <summary type="html"><![CDATA[\textit{Stochastic gradient descent} (SGD) is of fundamental importance in
deep learning. Despite its simplicity, elucidating its efficacy remains
challenging. Conventionally, the success of SGD is attributed to the
\textit{stochastic gradient noise} (SGN) incurred in the training process.
Based on this general consensus, SGD is frequently treated and analyzed as the
Euler-Maruyama discretization of a \textit{stochastic differential equation}
(SDE) driven by either Brownian or L\'evy stable motion. In this study, we
argue that SGN is neither Gaussian nor stable. Instead, inspired by the
long-time correlation emerging in SGN series, we propose that SGD can be viewed
as a discretization of an SDE driven by \textit{fractional Brownian motion}
(FBM). Accordingly, the different convergence behavior of SGD dynamics is well
grounded. Moreover, the first passage time of an SDE driven by FBM is
approximately derived. This indicates a lower escaping rate for a larger Hurst
parameter, and thus SGD stays longer in flat minima. This happens to coincide
with the well-known phenomenon that SGD favors flat minima that generalize
well. Four groups of experiments are conducted to validate our conjecture, and
it is demonstrated that long-range memory effects persist across various model
architectures, datasets, and training strategies. Our study opens up a new
perspective and may contribute to a better understanding of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chengli Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiangshe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junmin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15864</id>
        <link href="http://arxiv.org/abs/2012.15864"/>
        <updated>2021-06-23T01:48:40.682Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has been gaining attention as it allows for
performing image analysis tasks such as classification with limited labeled
data. Some popular algorithms using Generative Adversarial Networks (GANs) for
semi-supervised classification share a single architecture for classification
and discrimination. However, this may require a model to converge to a separate
data distribution for each task, which may reduce overall performance. While
progress in semi-supervised learning has been made, less addressed are
small-scale, fully-supervised tasks where even unlabeled data is unavailable
and unattainable. We therefore, propose a novel GAN model namely External
Classifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to
improve classification in fully-supervised regimes. Our method leverages a GAN
to generate artificial data used to supplement supervised classification. More
specifically, we attach an external classifier, hence the name EC-GAN, to the
GAN's generator, as opposed to sharing an architecture with the discriminator.
Our experiments demonstrate that EC-GAN's performance is comparable to the
shared architecture method, far superior to the standard data augmentation and
regularization-based approach, and effective on a small, realistic dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilled Replay: Overcoming Forgetting through Synthetic Samples. (arXiv:2103.15851v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15851</id>
        <link href="http://arxiv.org/abs/2103.15851"/>
        <updated>2021-06-23T01:48:40.675Z</updated>
        <summary type="html"><![CDATA[Replay strategies are Continual Learning techniques which mitigate
catastrophic forgetting by keeping a buffer of patterns from previous
experiences, which are interleaved with new data during training. The amount of
patterns stored in the buffer is a critical parameter which largely influences
the final performance and the memory footprint of the approach. This work
introduces Distilled Replay, a novel replay strategy for Continual Learning
which is able to mitigate forgetting by keeping a very small buffer (1 pattern
per class) of highly informative samples. Distilled Replay builds the buffer
through a distillation process which compresses a large dataset into a tiny set
of informative examples. We show the effectiveness of our Distilled Replay
against popular replay-based strategies on four Continual Learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosasco_A/0/1/0/all/0/1"&gt;Andrea Rosasco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03375</id>
        <link href="http://arxiv.org/abs/2103.03375"/>
        <updated>2021-06-23T01:48:40.666Z</updated>
        <summary type="html"><![CDATA[Understanding the nutritional content of food from visual data is a
challenging computer vision problem, with the potential to have a positive and
widespread impact on public health. Studies in this area are limited to
existing datasets in the field that lack sufficient diversity or labels
required for training models with nutritional understanding capability. We
introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes
with corresponding video streams, depth images, component weights, and high
accuracy nutritional content annotation. We demonstrate the potential of this
dataset by training a computer vision algorithm capable of predicting the
caloric and macronutrient values of a complex, real world dish at an accuracy
that outperforms professional nutritionists. Further we present a baseline for
incorporating depth sensor data to improve nutrition predictions. We will
publicly release Nutrition5k in the hope that it will accelerate innovation in
the space of nutritional understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1"&gt;Quin Thames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1"&gt;Arjun Karpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1"&gt;Wade Norris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fangting Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1"&gt;Liviu Panait&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1"&gt;Tobias Weyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1"&gt;Jack Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent-CF: A Simple Baseline for Reverse Counterfactual Explanations. (arXiv:2012.09301v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09301</id>
        <link href="http://arxiv.org/abs/2012.09301"/>
        <updated>2021-06-23T01:48:40.659Z</updated>
        <summary type="html"><![CDATA[In the environment of fair lending laws and the General Data Protection
Regulation (GDPR), the ability to explain a model's prediction is of paramount
importance. High quality explanations are the first step in assessing fairness.
Counterfactuals are valuable tools for explainability. They provide actionable,
comprehensible explanations for the individual who is subject to decisions made
from the prediction. It is important to find a baseline for producing them. We
propose a simple method for generating counterfactuals by using gradient
descent to search in the latent space of an autoencoder and benchmark our
method against approaches that search for counterfactuals in feature space.
Additionally, we implement metrics to concretely evaluate the quality of the
counterfactuals. We show that latent space counterfactual generation strikes a
balance between the speed of basic feature gradient descent methods and the
sparseness and authenticity of counterfactuals generated by more complex
feature space oriented techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_R/0/1/0/all/0/1"&gt;Rachana Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpe_S/0/1/0/all/0/1"&gt;Samuel Sharpe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1"&gt;Brian Barr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wittenbach_J/0/1/0/all/0/1"&gt;Jason Wittenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1"&gt;C. Bayan Bruss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Best-Arm Identification Methods for Tail-Risk Measures. (arXiv:2008.07606v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07606</id>
        <link href="http://arxiv.org/abs/2008.07606"/>
        <updated>2021-06-23T01:48:40.648Z</updated>
        <summary type="html"><![CDATA[Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular
tail-risk measures in finance and insurance industries as well as in highly
reliable, safety-critical uncertain environments where often the underlying
probability distributions are heavy-tailed. We use the multi-armed bandit
best-arm identification framework and consider the problem of identifying the
arm from amongst finitely many that has the smallest CVaR, VaR, or weighted sum
of CVaR and mean. The latter captures the risk-return trade-off common in
finance. Our main contribution is an optimal $\delta$-correct algorithm that
acts on general arms, including heavy-tailed distributions, and matches the
lower bound on the expected number of samples needed, asymptotically (as
$\delta$ approaches $0$). The algorithm requires solving a non-convex
optimization problem in the space of probability measures, that requires
delicate analysis. En-route, we develop new non-asymptotic empirical
likelihood-based concentration inequalities for tail-risk measures which are
tighter than those for popular truncation-based empirical estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Shubhada Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koolen_W/0/1/0/all/0/1"&gt;Wouter M. Koolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juneja_S/0/1/0/all/0/1"&gt;Sandeep Juneja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01250</id>
        <link href="http://arxiv.org/abs/2006.01250"/>
        <updated>2021-06-23T01:48:40.634Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Algorithms for Hierarchical Agglomerative Clustering. (arXiv:2005.03197v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03197</id>
        <link href="http://arxiv.org/abs/2005.03197"/>
        <updated>2021-06-23T01:48:40.615Z</updated>
        <summary type="html"><![CDATA[Hierarchical Agglomerative Clustering (HAC) algorithms are extensively
utilized in modern data science, and seek to partition the dataset into
clusters while generating a hierarchical relationship between the data samples.
HAC algorithms are employed in many applications, such as biology, natural
language processing, and recommender systems. Thus, it is imperative to ensure
that these algorithms are fair -- even if the dataset contains biases against
certain protected groups, the cluster outputs generated should not discriminate
against samples from any of these groups. However, recent work in clustering
fairness has mostly focused on center-based clustering algorithms, such as
k-median and k-means clustering. In this paper, we propose fair algorithms for
performing HAC that enforce fairness constraints 1) irrespective of the
distance linkage criteria used, 2) generalize to any natural measures of
clustering fairness for HAC, 3) work for multiple protected groups, and 4) have
competitive running times to vanilla HAC. Through extensive experiments on
multiple real-world UCI datasets, we show that our proposed algorithm finds
fairer clusterings compared to vanilla HAC as well as other state-of-the-art
fair clustering approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1"&gt;Anshuman Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_V/0/1/0/all/0/1"&gt;Vidushi Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1"&gt;Prasant Mohapatra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets. (arXiv:2006.07616v10 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07616</id>
        <link href="http://arxiv.org/abs/2006.07616"/>
        <updated>2021-06-23T01:48:40.608Z</updated>
        <summary type="html"><![CDATA[This paper presents a batch-wise density-based clustering approach for local
outlier detection in massive-scale datasets. Unlike the well-known traditional
algorithms, which assume that all the data is memory-resident, our proposed
method is scalable and processes the input data chunk-by-chunk within the
confines of a limited memory buffer. A temporary clustering model is built at
the first phase; then, it is gradually updated by analyzing consecutive memory
loads of points. Subsequently, at the end of scalable clustering, the
approximate structure of the original clusters is obtained. Finally, by another
scan of the entire dataset and using a suitable criterion, an outlying score is
assigned to each object called SDCOR (Scalable Density-based Clustering
Outlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate
that the proposed method has a low linear time complexity and is more effective
and efficient compared to best-known conventional density-based methods, which
need to load all data into the memory; and also, to some fast distance-based
methods, which can perform on data resident in the disk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nozad_S/0/1/0/all/0/1"&gt;Sayyed Ahmad Naghavi Nozad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haeri_M/0/1/0/all/0/1"&gt;Maryam Amir Haeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Folino_G/0/1/0/all/0/1"&gt;Gianluigi Folino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11644</id>
        <link href="http://arxiv.org/abs/2106.11644"/>
        <updated>2021-06-23T01:48:40.601Z</updated>
        <summary type="html"><![CDATA[We propose a novel and effective input transformation based adversarial
defense method against gray- and black-box attack, which is computationally
efficient and does not require any adversarial training or retraining of a
classification model. We first show that a very simple iterative Gaussian
smoothing can effectively wash out adversarial noise and achieve substantially
high robust accuracy. Based on the observation, we propose Self-Supervised
Iterative Contextual Smoothing (SSICS), which aims to reconstruct the original
discriminative features from the Gaussian-smoothed image in context-adaptive
manner, while still smoothing out the adversarial noise. From the experiments
on ImageNet, we show that our SSICS achieves both high standard accuracy and
very competitive robust accuracy for the gray- and black-box attacks; e.g.,
transfer-based PGD-attack and score-based attack. A note-worthy point to stress
is that our defense is free of computationally expensive adversarial training,
yet, can approach its robust accuracy via input transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sungmin Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1"&gt;Naeun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Bayesian Neural Networks. (arXiv:2008.07587v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07587</id>
        <link href="http://arxiv.org/abs/2008.07587"/>
        <updated>2021-06-23T01:48:40.594Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks perform variational inference over the weights
however calculation of the posterior distribution remains a challenge. Our work
builds on variational inference techniques for bayesian neural networks using
the original Evidence Lower Bound. In this paper, we present a stochastic
bayesian neural network in which we maximize Evidence Lower Bound using a new
objective function which we name as Stochastic Evidence Lower Bound. We
evaluate our network on 5 publicly available UCI datasets using test RMSE and
log likelihood as the evaluation metrics. We demonstrate that our work not only
beats the previous state of the art algorithms but is also scalable to larger
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Neural Network via Stochastic Gradient Descent. (arXiv:2006.08453v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08453</id>
        <link href="http://arxiv.org/abs/2006.08453"/>
        <updated>2021-06-23T01:48:40.575Z</updated>
        <summary type="html"><![CDATA[The goal of bayesian approach used in variational inference is to minimize
the KL divergence between variational distribution and unknown posterior
distribution. This is done by maximizing the Evidence Lower Bound (ELBO). A
neural network is used to parametrize these distributions using Stochastic
Gradient Descent. This work extends the work done by others by deriving the
variational inference models. We show how SGD can be applied on bayesian neural
networks by gradient estimation techniques. For validation, we have tested our
model on 5 UCI datasets and the metrics chosen for evaluation are Root Mean
Square Error (RMSE) error and negative log likelihood. Our work considerably
beats the previous state of the art approaches for regression using bayesian
neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Stereo Image Compression with Decoder Side Information using Wyner Common Information. (arXiv:2106.11723v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11723</id>
        <link href="http://arxiv.org/abs/2106.11723"/>
        <updated>2021-06-23T01:48:40.568Z</updated>
        <summary type="html"><![CDATA[We present a novel deep neural network (DNN) architecture for compressing an
image when a correlated image is available as side information only at the
decoder. This problem is known as distributed source coding (DSC) in
information theory. In particular, we consider a pair of stereo images, which
generally have high correlation with each other due to overlapping fields of
view, and assume that one image of the pair is to be compressed and
transmitted, while the other image is available only at the decoder. In the
proposed architecture, the encoder maps the input image to a latent space,
quantizes the latent representation, and compresses it using entropy coding.
The decoder is trained to extract the Wyner's common information between the
input image and the correlated image from the latter. The received latent
representation and the locally generated common information are passed through
a decoder network to obtain an enhanced reconstruction of the input image. The
common information provides a succinct representation of the relevant
information at the receiver. We train and demonstrate the effectiveness of the
proposed approach on the KITTI dataset of stereo image pairs. Our results show
that the proposed architecture is capable of exploiting the decoder-only side
information, and outperforms previous work on stereo image compression with
decoder side information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mital_N/0/1/0/all/0/1"&gt;Nitish Mital&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ozyilkan_E/0/1/0/all/0/1"&gt;Ezgi Ozyilkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garjani_A/0/1/0/all/0/1"&gt;Ali Garjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Learning Rate and Momentum for Training Deep Neural Networks. (arXiv:2106.11548v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11548</id>
        <link href="http://arxiv.org/abs/2106.11548"/>
        <updated>2021-06-23T01:48:40.560Z</updated>
        <summary type="html"><![CDATA[Recent progress on deep learning relies heavily on the quality and efficiency
of training algorithms. In this paper, we develop a fast training method
motivated by the nonlinear Conjugate Gradient (CG) framework. We propose the
Conjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a
quadratic line-search determines the step size according to current loss
landscape. On the other hand, the momentum factor is dynamically updated in
computing the conjugate gradient parameter (like Polak-Ribiere). Theoretical
results to ensure the convergence of our method in strong convex settings is
developed. And experiments in image classification datasets show that our
method yields faster convergence than other local solvers and has better
generalization capability (test set accuracy). One major advantage of the paper
method is that tedious hand tuning of hyperparameters like the learning rate
and momentum is avoided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhiyong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yixuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Huihua Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1"&gt;Hsiao-Dong Chiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving stochastic optimal control problem via stochastic maximum principle with deep learning method. (arXiv:2007.02227v5 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02227</id>
        <link href="http://arxiv.org/abs/2007.02227"/>
        <updated>2021-06-23T01:48:40.553Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to solve the high dimensional stochastic optimal
control problem from the view of the stochastic maximum principle via deep
learning. By introducing the extended Hamiltonian system which is essentially
an FBSDE with a maximum condition, we reformulate the original control problem
as a new one. Three algorithms are proposed to solve the new control problem.
Numerical results for different examples demonstrate the effectiveness of our
proposed algorithms, especially in high dimensional cases. And an important
application of this method is to calculate the sub-linear expectations, which
correspond to a kind of fully nonlinear PDEs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shaolin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shige Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Ying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xichuan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study of COVID-19 Infodemic. (arXiv:2106.11702v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.11702</id>
        <link href="http://arxiv.org/abs/2106.11702"/>
        <updated>2021-06-23T01:48:40.547Z</updated>
        <summary type="html"><![CDATA[The spreading COVID-19 misinformation over social media already draws the
attention of many researchers. According to Google Scholar, about 26000
COVID-19 related misinformation studies have been published to date. Most of
these studies focusing on 1) detect and/or 2) analysing the characteristics of
COVID-19 related misinformation. However, the study of the social behaviours
related to misinformation is often neglected. In this paper, we introduce a
fine-grained annotated misinformation tweets dataset including social
behaviours annotation (e.g. comment or question to the misinformation). The
dataset not only allows social behaviours analysis but also suitable for both
evidence-based or non-evidence-based misinformation classification task. In
addition, we introduce leave claim out validation in our experiments and
demonstrate the misinformation classification performance could be
significantly different when applying to real-world unseen misinformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Ye Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1"&gt;Carolina Scarton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aker_A/0/1/0/all/0/1"&gt;Ahmet Aker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1"&gt;Kalina Bontcheva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness of Synthetic Code Generation. (arXiv:2106.11629v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11629</id>
        <link href="http://arxiv.org/abs/2106.11629"/>
        <updated>2021-06-23T01:48:40.539Z</updated>
        <summary type="html"><![CDATA[Automatic code synthesis from natural language descriptions is a challenging
task. We witness massive progress in developing code generation systems for
domain-specific languages (DSLs) employing sequence-to-sequence deep learning
techniques in the recent past. In this paper, we specifically experiment with
\textsc{AlgoLisp} DSL-based generative models and showcase the existence of
significant dataset bias through different classes of adversarial examples. We
also experiment with two variants of Transformer-based models that outperform
all existing \textsc{AlgoLisp} DSL-based code generation baselines. Consistent
with the current state-of-the-art systems, our proposed models, too, achieve
poor performance under adversarial settings. Therefore, we propose several
dataset augmentation techniques to reduce bias and showcase their efficacy
using robust experimentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anand_M/0/1/0/all/0/1"&gt;Mrinal Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kayal_P/0/1/0/all/0/1"&gt;Pratik Kayal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11642</id>
        <link href="http://arxiv.org/abs/2106.11642"/>
        <updated>2021-06-23T01:48:40.521Z</updated>
        <summary type="html"><![CDATA[Deep ensembles have recently gained popularity in the deep learning community
for their conceptual simplicity and efficiency. However, maintaining functional
diversity between ensemble members that are independently trained with gradient
descent is challenging. This can lead to pathologies when adding more ensemble
members, such as a saturation of the ensemble performance, which converges to
the performance of a single model. Moreover, this does not only affect the
quality of its predictions, but even more so the uncertainty estimates of the
ensemble, and thus its performance on out-of-distribution data. We hypothesize
that this limitation can be overcome by discouraging different ensemble members
from collapsing to the same function. To this end, we introduce a kernelized
repulsive term in the update rule of the deep ensembles. We show that this
simple modification not only enforces and maintains diversity among the members
but, even more importantly, transforms the maximum a posteriori inference into
proper Bayesian inference. Namely, we show that the training dynamics of our
proposed repulsive ensembles follow a Wasserstein gradient flow of the KL
divergence with the true posterior. We study repulsive terms in weight and
function space and empirically compare their performance to standard ensembles
and Bayesian baselines on synthetic and real-world prediction tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Vertical Federated Learning Framework for Graph Convolutional Network. (arXiv:2106.11593v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11593</id>
        <link href="http://arxiv.org/abs/2106.11593"/>
        <updated>2021-06-23T01:48:40.504Z</updated>
        <summary type="html"><![CDATA[Recently, Graph Neural Network (GNN) has achieved remarkable success in
various real-world problems on graph data. However in most industries, data
exists in the form of isolated islands and the data privacy and security is
also an important issue. In this paper, we propose FedVGCN, a federated GCN
learning paradigm for privacy-preserving node classification task under data
vertically partitioned setting, which can be generalized to existing GCN
models. Specifically, we split the computation graph data into two parts. For
each iteration of the training process, the two parties transfer intermediate
results to each other under homomorphic encryption. We conduct experiments on
benchmark data and the results demonstrate the effectiveness of FedVGCN in the
case of GraphSage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1"&gt;Xiang Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaolong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1"&gt;Lingjuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Changhua Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous-Depth Neural Models for Dynamic Graph Prediction. (arXiv:2106.11581v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11581</id>
        <link href="http://arxiv.org/abs/2106.11581"/>
        <updated>2021-06-23T01:48:40.497Z</updated>
        <summary type="html"><![CDATA[We introduce the framework of continuous-depth graph neural networks (GNNs).
Neural graph differential equations (Neural GDEs) are formalized as the
counterpart to GNNs where the input-output relationship is determined by a
continuum of GNN layers, blending discrete topological structures and
differential equations. The proposed framework is shown to be compatible with
static GNN models and is extended to dynamic and stochastic settings through
hybrid dynamical system theory. Here, Neural GDEs improve performance by
exploiting the underlying dynamics geometry, further introducing the ability to
accommodate irregularly sampled data. Results prove the effectiveness of the
proposed models across applications, such as traffic forecasting or prediction
in genetic regulatory networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poli_M/0/1/0/all/0/1"&gt;Michael Poli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massaroli_S/0/1/0/all/0/1"&gt;Stefano Massaroli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabideau_C/0/1/0/all/0/1"&gt;Clayton M. Rabideau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1"&gt;Atsushi Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asama_H/0/1/0/all/0/1"&gt;Hajime Asama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLRA: A Reference Architecture for Federated Learning Systems. (arXiv:2106.11570v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11570</id>
        <link href="http://arxiv.org/abs/2106.11570"/>
        <updated>2021-06-23T01:48:40.489Z</updated>
        <summary type="html"><![CDATA[Federated learning is an emerging machine learning paradigm that enables
multiple devices to train models locally and formulate a global model, without
sharing the clients' local data. A federated learning system can be viewed as a
large-scale distributed system, involving different components and stakeholders
with diverse requirements and constraints. Hence, developing a federated
learning system requires both software system design thinking and machine
learning knowledge. Although much effort has been put into federated learning
from the machine learning perspectives, our previous systematic literature
review on the area shows that there is a distinct lack of considerations for
software architecture design for federated learning. In this paper, we propose
FLRA, a reference architecture for federated learning systems, which provides a
template design for federated learning-based solutions. The proposed FLRA
reference architecture is based on an extensive review of existing patterns of
federated learning systems found in the literature and existing industrial
implementation. The FLRA reference architecture consists of a pool of
architectural patterns that could address the frequently recurring design
problems in federated learning architectures. The FLRA reference architecture
can serve as a design guideline to assist architects and developers with
practical solutions for their problems, which can be further customised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Sin Kit Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1"&gt;Hye-Young Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liming Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Framework for Conservative Exploration. (arXiv:2106.11692v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11692</id>
        <link href="http://arxiv.org/abs/2106.11692"/>
        <updated>2021-06-23T01:48:40.480Z</updated>
        <summary type="html"><![CDATA[We study bandits and reinforcement learning (RL) subject to a conservative
constraint where the agent is asked to perform at least as well as a given
baseline policy. This setting is particular relevant in real-world domains
including digital marketing, healthcare, production, finance, etc. For
multi-armed bandits, linear bandits and tabular RL, specialized algorithms and
theoretical analyses were proposed in previous work. In this paper, we present
a unified framework for conservative bandits and RL, in which our core
technique is to calculate the necessary and sufficient budget obtained from
running the baseline policy. For lower bounds, our framework gives a black-box
reduction that turns a certain lower bound in the nonconservative setting into
a new lower bound in the conservative setting. We strengthen the existing lower
bound for conservative multi-armed bandits and obtain new lower bounds for
conservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our
framework turns a certain nonconservative upper-confidence-bound (UCB)
algorithm into a conservative algorithm with a simple analysis. For multi-armed
bandits, linear bandits and tabular RL, our new upper bounds tighten or match
existing ones with significantly simpler analyses. We also obtain a new upper
bound for conservative low-rank MDP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunchang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1"&gt;Han Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcelon_E/0/1/0/all/0/1"&gt;Evrard Garcelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pirotta_M/0/1/0/all/0/1"&gt;Matteo Pirotta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazaric_A/0/1/0/all/0/1"&gt;Alessandro Lazaric&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Latent Space Model for Graph Representation Learning. (arXiv:2106.11721v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11721</id>
        <link href="http://arxiv.org/abs/2106.11721"/>
        <updated>2021-06-23T01:48:40.460Z</updated>
        <summary type="html"><![CDATA[Graph representation learning is a fundamental problem for modeling
relational data and benefits a number of downstream applications. Traditional
Bayesian-based graph models and recent deep learning based GNN either suffer
from impracticability or lack interpretability, thus combined models for
undirected graphs have been proposed to overcome the weaknesses. As a large
portion of real-world graphs are directed graphs (of which undirected graphs
are special cases), in this paper, we propose a Deep Latent Space Model (DLSM)
for directed graphs to incorporate the traditional latent variable based
generative model into deep learning frameworks. Our proposed model consists of
a graph convolutional network (GCN) encoder and a stochastic decoder, which are
layer-wise connected by a hierarchical variational auto-encoder architecture.
By specifically modeling the degree heterogeneity using node random factors,
our model possesses better interpretability in both community structure and
degree heterogeneity. For fast inference, the stochastic gradient variational
Bayes (SGVB) is adopted using a non-iterative recognition model, which is much
more scalable than traditional MCMC-based methods. The experiments on
real-world datasets show that the proposed model achieves the state-of-the-art
performances on both link prediction and community detection tasks while
learning interpretable node embeddings. The source code is available at
https://github.com/upperr/DLSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hanxuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1"&gt;Qingchao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1"&gt;Wenji Mao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Equivalence Between Private Classification and Online Prediction. (arXiv:2003.00563v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00563</id>
        <link href="http://arxiv.org/abs/2003.00563"/>
        <updated>2021-06-23T01:48:40.452Z</updated>
        <summary type="html"><![CDATA[We prove that every concept class with finite Littlestone dimension can be
learned by an (approximate) differentially-private algorithm. This answers an
open question of Alon et al. (STOC 2019) who proved the converse statement
(this question was also asked by Neel et al.~(FOCS 2019)). Together these two
results yield an equivalence between online learnability and private PAC
learnability.

We introduce a new notion of algorithmic stability called "global stability"
which is essential to our proof and may be of independent interest. We also
discuss an application of our results to boosting the privacy and accuracy
parameters of differentially-private learners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bun_M/0/1/0/all/0/1"&gt;Mark Bun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1"&gt;Roi Livni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1"&gt;Shay Moran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MMD-MIX: Value Function Factorisation with Maximum Mean Discrepancy for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2106.11652v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.11652</id>
        <link href="http://arxiv.org/abs/2106.11652"/>
        <updated>2021-06-23T01:48:40.445Z</updated>
        <summary type="html"><![CDATA[In the real world, many tasks require multiple agents to cooperate with each
other under the condition of local observations. To solve such problems, many
multi-agent reinforcement learning methods based on Centralized Training with
Decentralized Execution have been proposed. One representative class of work is
value decomposition, which decomposes the global joint Q-value $Q_\text{jt}$
into individual Q-values $Q_a$ to guide individuals' behaviors, e.g. VDN
(Value-Decomposition Networks) and QMIX. However, these baselines often ignore
the randomness in the situation. We propose MMD-MIX, a method that combines
distributional reinforcement learning and value decomposition to alleviate the
above weaknesses. Besides, to improve data sampling efficiency, we were
inspired by REM (Random Ensemble Mixture) which is a robust RL algorithm to
explicitly introduce randomness into the MMD-MIX. The experiments demonstrate
that MMD-MIX outperforms prior baselines in the StarCraft Multi-Agent Challenge
(SMAC) environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dapeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1"&gt;Guoliang Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding Valid Adjustments under Non-ignorability with Minimal DAG Knowledge. (arXiv:2106.11560v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11560</id>
        <link href="http://arxiv.org/abs/2106.11560"/>
        <updated>2021-06-23T01:48:40.438Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation from observational data is a fundamental problem
in causal inference. There are two very different schools of thought that have
tackled this problem. On the one hand, the Pearlian framework commonly assumes
structural knowledge (provided by an expert) in the form of Directed Acyclic
Graphs (DAGs) and provides graphical criteria such as the back-door criterion
to identify the valid adjustment sets. On the other hand, the potential
outcomes (PO) framework commonly assumes that all the observed features satisfy
ignorability (i.e., no hidden confounding), which in general is untestable. In
this work, we take steps to bridge these two frameworks. We show that even if
we know only one parent of the treatment variable (provided by an expert), then
quite remarkably it suffices to test a broad class of (but not all) back-door
criteria. Importantly, we also cover the non-trivial case where the entire set
of observed features is not ignorable (generalizing the PO framework) without
requiring all the parents of the treatment variable to be observed. Our key
technical idea involves a more general result -- Given a synthetic sub-sampling
(or environment) variable that is a function of the parent variable, we show
that an invariance test involving this sub-sampling variable is equivalent to
testing a broad class of back-door criteria. We demonstrate our approach on
synthetic data as well as real causal effect estimation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Abhin Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1"&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-based Label Binning in Multi-label Classification. (arXiv:2106.11690v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11690</id>
        <link href="http://arxiv.org/abs/2106.11690"/>
        <updated>2021-06-23T01:48:40.431Z</updated>
        <summary type="html"><![CDATA[In multi-label classification, where a single example may be associated with
several class labels at the same time, the ability to model dependencies
between labels is considered crucial to effectively optimize non-decomposable
evaluation measures, such as the Subset 0/1 loss. The gradient boosting
framework provides a well-studied foundation for learning models that are
specifically tailored to such a loss function and recent research attests the
ability to achieve high predictive accuracy in the multi-label setting. The
utilization of second-order derivatives, as used by many recent boosting
approaches, helps to guide the minimization of non-decomposable losses, due to
the information about pairs of labels it incorporates into the optimization
process. On the downside, this comes with high computational costs, even if the
number of labels is small. In this work, we address the computational
bottleneck of such approach -- the need to solve a system of linear equations
-- by integrating a novel approximation technique into the boosting procedure.
Based on the derivatives computed during training, we dynamically group the
labels into a predefined number of bins to impose an upper bound on the
dimensionality of the linear system. Our experiments, using an existing
rule-based algorithm, suggest that this may boost the speed of training,
without any significant loss in predictive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rapp_M/0/1/0/all/0/1"&gt;Michael Rapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mencia_E/0/1/0/all/0/1"&gt;Eneldo Loza Menc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization. (arXiv:2106.11514v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11514</id>
        <link href="http://arxiv.org/abs/2106.11514"/>
        <updated>2021-06-23T01:48:40.412Z</updated>
        <summary type="html"><![CDATA[Adaptive gradient methods, such as \textsc{Adam}, have achieved tremendous
success in machine learning. Scaling gradients by square roots of the running
averages of squared past gradients, such methods are able to attain rapid
training of modern deep neural networks. Nevertheless, they are observed to
generalize worse than stochastic gradient descent (\textsc{SGD}) and tend to be
trapped in local minima at an early stage during training. Intriguingly, we
discover that substituting the gradient in the preconditioner term with the
momentumized version in \textsc{Adam} can well solve the issues. The intuition
is that gradient with momentum contains more accurate directional information
and therefore its second moment estimation is a better choice for scaling than
raw gradient's. Thereby we propose \textsc{AdaMomentum} as a new optimizer
reaching the goal of training faster while generalizing better. We further
develop a theory to back up the improvement in optimization and generalization
and provide convergence guarantee under both convex and nonconvex settings.
Extensive experiments on various models and tasks demonstrate that
\textsc{AdaMomentum} exhibits comparable performance to \textsc{SGD} on vision
tasks, and achieves state-of-the-art results consistently on other tasks
including language processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yue Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Can Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Particle Cloud Generation with Message Passing Generative Adversarial Networks. (arXiv:2106.11535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11535</id>
        <link href="http://arxiv.org/abs/2106.11535"/>
        <updated>2021-06-23T01:48:40.405Z</updated>
        <summary type="html"><![CDATA[In high energy physics (HEP), jets are collections of correlated particles
produced ubiquitously in particle collisions such as those at the CERN Large
Hadron Collider (LHC). Machine-learning-based generative models, such as
generative adversarial networks (GANs), have the potential to significantly
accelerate LHC jet simulations. However, despite jets having a natural
representation as a set of particles in momentum-space, a.k.a. a particle
cloud, to our knowledge there exist no generative models applied to such a
dataset. We introduce a new particle cloud dataset (JetNet), and, due to
similarities between particle and point clouds, apply to it existing point
cloud GANs. Results are evaluated using (1) the 1-Wasserstein distance between
high- and low-level feature distributions, (2) a newly developed Fr\'{e}chet
ParticleNet Distance, and (3) the coverage and (4) minimum matching distance
metrics. Existing GANs are found to be inadequate for physics applications,
hence we develop a new message passing GAN (MPGAN), which outperforms existing
point cloud GANs on virtually every metric and shows promise for use in HEP. We
propose JetNet as a novel point-cloud-style dataset for the machine learning
community to experiment with, and set MPGAN as a benchmark to improve upon for
future generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kansal_R/0/1/0/all/0/1"&gt;Raghav Kansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orzari_B/0/1/0/all/0/1"&gt;Breno Orzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomei_T/0/1/0/all/0/1"&gt;Thiago Tomei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Touranakou_M/0/1/0/all/0/1"&gt;Mary Touranakou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlimant_J/0/1/0/all/0/1"&gt;Jean-Roch Vlimant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunopulos_D/0/1/0/all/0/1"&gt;Dimitrios Gunopulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning for Model Order Selection in MIMO OFDM Systems. (arXiv:2106.11633v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.11633</id>
        <link href="http://arxiv.org/abs/2106.11633"/>
        <updated>2021-06-23T01:48:40.398Z</updated>
        <summary type="html"><![CDATA[A variety of wireless channel estimation methods, e.g., MUSIC and ESPRIT,
rely on prior knowledge of the model order. Therefore, it is important to
correctly estimate the number of multipath components (MPCs) which compose such
channels. However, environments with many scatterers may generate MPCs which
are closely spaced. This clustering of MPCs in addition to noise makes the
model order selection task difficult in practice to currently known algorithms.
In this paper, we exploit the multidimensional characteristics of MIMO
orthogonal frequency division multiplexing (OFDM) systems and propose a machine
learning (ML) method capable of determining the number of MPCs with a higher
accuracy than state of the art methods in almost coherent scenarios. Moreover,
our results show that our proposed ML method has an enhanced reliability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Boas_B/0/1/0/all/0/1"&gt;Brenda Vilas Boas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zirwas_W/0/1/0/all/0/1"&gt;Wolfgang Zirwas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Haardt_M/0/1/0/all/0/1"&gt;Martin Haardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw. (arXiv:2106.11603v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11603</id>
        <link href="http://arxiv.org/abs/2106.11603"/>
        <updated>2021-06-23T01:48:40.391Z</updated>
        <summary type="html"><![CDATA[We present a number of low-resource approaches to the tasks of the Zero
Resource Speech Challenge 2021. We build on the unsupervised representations of
speech proposed by the organizers as a baseline, derived from CPC and clustered
with the k-means algorithm. We demonstrate that simple methods of refining
those representations can narrow the gap, or even improve upon the solutions
which use a high computational budget. The results lead to the conclusion that
the CPC-derived representations are still too noisy for training language
models, but stable enough for simpler forms of pattern matching and retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chorowski_J/0/1/0/all/0/1"&gt;Jan Chorowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciesielski_G/0/1/0/all/0/1"&gt;Grzegorz Ciesielski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzikowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Dzikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1"&gt;Adrian &amp;#x141;a&amp;#x144;cucki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marxer_R/0/1/0/all/0/1"&gt;Ricard Marxer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opala_M/0/1/0/all/0/1"&gt;Mateusz Opala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pusz_P/0/1/0/all/0/1"&gt;Piotr Pusz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rychlikowski_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Rychlikowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Stypu&amp;#x142;kowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardness of Samples Is All You Need: Protecting Deep Learning Models Using Hardness of Samples. (arXiv:2106.11424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11424</id>
        <link href="http://arxiv.org/abs/2106.11424"/>
        <updated>2021-06-23T01:48:40.383Z</updated>
        <summary type="html"><![CDATA[Several recent studies have shown that Deep Neural Network (DNN)-based
classifiers are vulnerable against model extraction attacks. In model
extraction attacks, an adversary exploits the target classifier to create a
surrogate classifier imitating the target classifier with respect to some
criteria. In this paper, we investigate the hardness degree of samples and
demonstrate that the hardness degree histogram of model extraction attacks
samples is distinguishable from the hardness degree histogram of normal
samples. Normal samples come from the target classifier's training data
distribution. As the training process of DNN-based classifiers is done in
several epochs, we can consider this process as a sequence of subclassifiers so
that each subclassifier is created at the end of an epoch. We use the sequence
of subclassifiers to calculate the hardness degree of samples. We investigate
the relation between hardness degree of samples and the trust in the classifier
outputs. We propose Hardness-Oriented Detection Approach (HODA) to detect the
sample sequences of model extraction attacks. The results demonstrate that HODA
can detect the sample sequences of model extraction attacks with a high success
rate by only watching 100 attack samples. We also investigate the hardness
degree of adversarial examples and indicate that the hardness degree histogram
of adversarial examples is distinct from the hardness degree histogram of
normal samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghzadeh_A/0/1/0/all/0/1"&gt;Amir Mahdi Sadeghzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghan_F/0/1/0/all/0/1"&gt;Faezeh Dehghan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobhanian_A/0/1/0/all/0/1"&gt;Amir Mohammad Sobhanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalili_R/0/1/0/all/0/1"&gt;Rasool Jalili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning for PHY layer communications. (arXiv:2106.11595v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.11595</id>
        <link href="http://arxiv.org/abs/2106.11595"/>
        <updated>2021-06-23T01:48:40.362Z</updated>
        <summary type="html"><![CDATA[In this chapter, we will give comprehensive examples of applying RL in
optimizing the physical layer of wireless communications by defining different
class of problems and the possible solutions to handle them. In Section 9.2, we
present all the basic theory needed to address a RL problem, i.e. Markov
decision process (MDP), Partially observable Markov decision process (POMDP),
but also two very important and widely used algorithms for RL, i.e. the
Q-learning and SARSA algorithms. We also introduce the deep reinforcement
learning (DRL) paradigm and the section ends with an introduction to the
multi-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples
to illustrate how the basic concepts of RL are employed in communication
systems. We present applications extracted from literature with simplified
system models using similar notation as in Section 9.2 of this Chapter. In
Section 9.3, we also focus on modeling RL problems, i.e. how action and state
spaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a
prospective thought on RL trends and it ends with a review of a broader state
of the art in Section 9.5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1"&gt;Philippe Mary&lt;/a&gt; (IETR), &lt;a href="http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1"&gt;Visa Koivunen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1"&gt;Christophe Moy&lt;/a&gt; (IETR)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Accurate Non-accelerometer-based PPG Motion Artifact Removal Technique using CycleGAN. (arXiv:2106.11512v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11512</id>
        <link href="http://arxiv.org/abs/2106.11512"/>
        <updated>2021-06-23T01:48:40.355Z</updated>
        <summary type="html"><![CDATA[A photoplethysmography (PPG) is an uncomplicated and inexpensive optical
technique widely used in the healthcare domain to extract valuable
health-related information, e.g., heart rate variability, blood pressure, and
respiration rate. PPG signals can easily be collected continuously and remotely
using portable wearable devices. However, these measuring devices are
vulnerable to motion artifacts caused by daily life activities. The most common
ways to eliminate motion artifacts use extra accelerometer sensors, which
suffer from two limitations: i) high power consumption and ii) the need to
integrate an accelerometer sensor in a wearable device (which is not required
in certain wearables). This paper proposes a low-power non-accelerometer-based
PPG motion artifacts removal method outperforming the accuracy of the existing
methods. We use Cycle Generative Adversarial Network to reconstruct clean PPG
signals from noisy PPG signals. Our novel machine-learning-based technique
achieves 9.5 times improvement in motion artifact removal compared to the
state-of-the-art without using extra sensors such as an accelerometer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1"&gt;Amir Hosein Afandizadeh Zargari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1"&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khodabandeh_H/0/1/0/all/0/1"&gt;Hadi Khodabandeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurdahi_F/0/1/0/all/0/1"&gt;Fadi Kurdahi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11562</id>
        <link href="http://arxiv.org/abs/2106.11562"/>
        <updated>2021-06-23T01:48:40.347Z</updated>
        <summary type="html"><![CDATA[We consider a class-incremental semantic segmentation (CISS) problem. While
some recently proposed algorithms utilized variants of knowledge distillation
(KD) technique to tackle the problem, they only partially addressed the key
additional challenges in CISS that causes the catastrophic forgetting; i.e.,
the semantic drift of the background class and multi-label prediction issue. To
better address these challenges, we propose a new method, dubbed as SSUL-M
(Semantic Segmentation with Unknown Label with Memory), by carefully combining
several techniques tailored for semantic segmentation. More specifically, we
make three main contributions; (1) modeling unknown class within the background
class to help learning future classes (help plasticity), (2) freezing backbone
network and past classifiers with binary cross-entropy loss and pseudo-labeling
to overcome catastrophic forgetting (help stability), and (3) utilizing tiny
exemplar memory for the first time in CISS to improve both plasticity and
stability. As a result, we show our method achieves significantly better
performance than the recent state-of-the-art baselines on the standard
benchmark datasets. Furthermore, we justify our contributions with thorough and
extensive ablation analyses and discuss different natures of the CISS problem
compared to the standard class-incremental learning for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungmin Cha. Beomyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-06-23T01:48:40.340Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn Like The Pro: Norms from Theory to Size Neural Computation. (arXiv:2106.11409v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11409</id>
        <link href="http://arxiv.org/abs/2106.11409"/>
        <updated>2021-06-23T01:48:40.332Z</updated>
        <summary type="html"><![CDATA[The optimal design of neural networks is a critical problem in many
applications. Here, we investigate how dynamical systems with polynomial
nonlinearities can inform the design of neural systems that seek to emulate
them. We propose a Learnability metric and its associated features to quantify
the near-equilibrium behavior of learning dynamics. Equating the Learnability
of neural systems with equivalent parameter estimation metric of the reference
system establishes bounds on network structure. In this way, norms from theory
provide a good first guess for neural structure, which may then further adapt
with data. The proposed approach neither requires training nor training data.
It reveals exact sizing for a class of neural networks with multiplicative
nodes that mimic continuous- or discrete-time polynomial dynamics. It also
provides relatively tight lower size bounds for classical feed-forward networks
that is consistent with simulated assessments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trautner_M/0/1/0/all/0/1"&gt;Margaret Trautner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1"&gt;Sai Ravela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Logical Neural Network Structure With More Direct Mapping From Logical Relations. (arXiv:2106.11463v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.11463</id>
        <link href="http://arxiv.org/abs/2106.11463"/>
        <updated>2021-06-23T01:48:40.312Z</updated>
        <summary type="html"><![CDATA[Logical relations widely exist in human activities. Human use them for making
judgement and decision according to various conditions, which are embodied in
the form of \emph{if-then} rules. As an important kind of cognitive
intelligence, it is prerequisite of representing and storing logical relations
rightly into computer systems so as to make automatic judgement and decision,
especially for high-risk domains like medical diagnosis. However, current
numeric ANN (Artificial Neural Network) models are good at perceptual
intelligence such as image recognition while they are not good at cognitive
intelligence such as logical representation, blocking the further application
of ANN. To solve it, researchers have tried to design logical ANN models to
represent and store logical relations. Although there are some advances in this
research area, recent works still have disadvantages because the structures of
these logical ANN models still don't map more directly with logical relations
which will cause the corresponding logical relations cannot be read out from
their network structures. Therefore, in order to represent logical relations
more clearly by the neural network structure and to read out logical relations
from it, this paper proposes a novel logical ANN model by designing the new
logical neurons and links in demand of logical representation. Compared with
the recent works on logical ANN models, this logical ANN model has more clear
corresponding with logical relations using the more direct mapping method
herein, thus logical relations can be read out following the connection
patterns of the network structure. Additionally, less neurons are used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Late Fusion Technique for Multi-modal Sentiment Analysis. (arXiv:2106.11473v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11473</id>
        <link href="http://arxiv.org/abs/2106.11473"/>
        <updated>2021-06-23T01:48:40.305Z</updated>
        <summary type="html"><![CDATA[Multi-modal sentiment analysis plays an important role for providing better
interactive experiences to users. Each modality in multi-modal data can provide
different viewpoints or reveal unique aspects of a user's emotional state. In
this work, we use text, audio and visual modalities from MOSI dataset and we
propose a novel fusion technique using a multi-head attention LSTM network.
Finally, we perform a classification task and evaluate its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1"&gt;Debapriya Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lygerakis_F/0/1/0/all/0/1"&gt;Fotios Lygerakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1"&gt;Fillia Makedon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConvDySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention and Convolutional Neural Networks. (arXiv:2106.11430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11430</id>
        <link href="http://arxiv.org/abs/2106.11430"/>
        <updated>2021-06-23T01:48:40.299Z</updated>
        <summary type="html"><![CDATA[Learning node representations on temporal graphs is a fundamental step to
learn real-word dynamic graphs efficiently. Real-world graphs have the nature
of continuously evolving over time, such as changing edges weights, removing
and adding nodes and appearing and disappearing of edges, while previous graph
representation learning methods focused generally on static graphs. We present
ConvDySAT as an enhancement of DySAT, one of the state-of-the-art dynamic
methods, by augmenting convolution neural networks with the self-attention
mechanism, the employed method in DySAT to express the structural and temporal
evolution. We conducted single-step link prediction on a communication network
and rating network, Experimental results show significant performance gains for
ConvDySAT over various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hafez_A/0/1/0/all/0/1"&gt;Ahmad Hafez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praphul_A/0/1/0/all/0/1"&gt;Atulya Praphul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaradt_Y/0/1/0/all/0/1"&gt;Yousef Jaradt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Godwin_E/0/1/0/all/0/1"&gt;Ezani Godwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:40.291Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance-Optimal Compressed Sensing via Posterior Sampling. (arXiv:2106.11438v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11438</id>
        <link href="http://arxiv.org/abs/2106.11438"/>
        <updated>2021-06-23T01:48:40.285Z</updated>
        <summary type="html"><![CDATA[We characterize the measurement complexity of compressed sensing of signals
drawn from a known prior distribution, even when the support of the prior is
the entire space (rather than, say, sparse vectors). We show for Gaussian
measurements and \emph{any} prior distribution on the signal, that the
posterior sampling estimator achieves near-optimal recovery guarantees.
Moreover, this result is robust to model mismatch, as long as the distribution
estimate (e.g., from an invertible generative model) is close to the true
distribution in Wasserstein distance. We implement the posterior sampling
estimator for deep generative priors using Langevin dynamics, and empirically
find that it produces accurate estimates with more diversity than MAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1"&gt;Sushrut Karmalkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Gradient Matching for Learning Uncertain Neural Dynamics Models. (arXiv:2106.11609v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11609</id>
        <link href="http://arxiv.org/abs/2106.11609"/>
        <updated>2021-06-23T01:48:40.263Z</updated>
        <summary type="html"><![CDATA[Differential equations in general and neural ODEs in particular are an
essential technique in continuous-time system identification. While many
deterministic learning algorithms have been designed based on numerical
integration via the adjoint method, many downstream tasks such as active
learning, exploration in reinforcement learning, robust control, or filtering
require accurate estimates of predictive uncertainties. In this work, we
propose a novel approach towards estimating epistemically uncertain neural
ODEs, avoiding the numerical integration bottleneck. Instead of modeling
uncertainty in the ODE parameters, we directly model uncertainties in the state
space. Our algorithm - distributional gradient matching (DGM) - jointly trains
a smoother and a dynamics model and matches their gradients via minimizing a
Wasserstein loss. Our experiments show that, compared to traditional
approximate inference methods based on numerical integration, our approach is
faster to train, faster at predicting previously unseen trajectories, and in
the context of neural ODEs, significantly more accurate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Treven_L/0/1/0/all/0/1"&gt;Lenart Treven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenk_P/0/1/0/all/0/1"&gt;Philippe Wenk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorfler_F/0/1/0/all/0/1"&gt;Florian D&amp;#xf6;rfler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11541</id>
        <link href="http://arxiv.org/abs/2106.11541"/>
        <updated>2021-06-23T01:48:40.253Z</updated>
        <summary type="html"><![CDATA[Kernel segmentation aims at partitioning a data sequence into several
non-overlapping segments that may have nonlinear and complex structures. In
general, it is formulated as a discrete optimization problem with combinatorial
constraints. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements.
Given that sequences in practice are too long, this algorithm is not a
practical approach. Although many heuristic algorithms have been proposed to
approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate
the aforementioned issues. First, we introduce a novel sigmoid-based
regularization to smoothly approximate the combinatorial constraints. Combining
it with objective of the balanced kernel clustering, we formulate a
differentiable model termed Kernel clustering with sigmoid-based regularization
(KCSR), where the gradient-based algorithm can be exploited to obtain the
optimal segmentation. Second, we develop a stochastic variant of the proposed
model. By using the stochastic gradient descent algorithm, which has much lower
time and space complexities, for optimization, the second model can perform
segmentation on overlong data sequences. Finally, for simultaneously segmenting
multiple data sequences, we slightly modify the sigmoid-based regularization to
further introduce an extended variant of the proposed model. Through extensive
experiments on various types of data sequences performances of our models are
evaluated and compared with those of the existing methods. The experimental
results validate advantages of the proposed models. Our Matlab source code is
available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1"&gt;Tung Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1"&gt;Atsuhiro Takasu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-constrained deep neural network method for estimating parameters in a redox flow battery. (arXiv:2106.11451v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.11451</id>
        <link href="http://arxiv.org/abs/2106.11451"/>
        <updated>2021-06-23T01:48:40.247Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a physics-constrained deep neural network (PCDNN)
method for parameter estimation in the zero-dimensional (0D) model of the
vanadium redox flow battery (VRFB). In this approach, we use deep neural
networks (DNNs) to approximate the model parameters as functions of the
operating conditions. This method allows the integration of the VRFB
computational models as the physical constraints in the parameter learning
process, leading to enhanced accuracy of parameter estimation and cell voltage
prediction. Using an experimental dataset, we demonstrate that the PCDNN method
can estimate model parameters for a range of operating conditions and improve
the 0D model prediction of voltage compared to the 0D model prediction with
constant operation-condition-independent parameters estimated with traditional
inverse methods. We also demonstrate that the PCDNN approach has an improved
generalization ability for estimating parameter values for operating conditions
not used in the DNN training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+He_Q/0/1/0/all/0/1"&gt;QiZhi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stinis_P/0/1/0/all/0/1"&gt;Panos Stinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre Tartakovsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11542</id>
        <link href="http://arxiv.org/abs/2106.11542"/>
        <updated>2021-06-23T01:48:40.240Z</updated>
        <summary type="html"><![CDATA[With leveraging the weight-sharing and continuous relaxation to enable
gradient-descent to alternately optimize the supernet weights and the
architecture parameters through a bi-level optimization paradigm,
\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream
method in Neural Architecture Search (NAS) due to its simplicity and
efficiency. However, more recent works found that the performance of the
searched architecture barely increases with the optimization proceeding in
DARTS. In addition, several concurrent works show that the NAS could find more
competitive architectures without labels. The above observations reveal that
the supervision signal in DARTS may be a poor indicator for architecture
optimization, inspiring a foundational question: instead of using the
supervision signal to perform bi-level optimization, \textit{can we find
high-quality architectures \textbf{without any training nor labels}}? We
provide an affirmative answer by customizing the NAS as a network pruning at
initialization problem. By leveraging recent techniques on the network pruning
at initialization, we designed a FreeFlow proxy to score the importance of
candidate operations in NAS without any training nor labels, and proposed a
novel framework called \textit{training and label free neural architecture
search} (\textbf{FreeNAS}) accordingly. We show that, without any training nor
labels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS
baselines. More importantly, our framework is extremely efficient, which
completes the architecture search within only \textbf{3.6s} and \textbf{79s} on
a single GPU for the NAS-Bench-201 and DARTS search space, respectively. We
hope our work inspires more attempts in solving NAS from the perspective of
pruning at initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11528</id>
        <link href="http://arxiv.org/abs/2106.11528"/>
        <updated>2021-06-23T01:48:40.232Z</updated>
        <summary type="html"><![CDATA[The author of this work proposes an overview of the recent semi-supervised
learning approaches and related works. Despite the remarkable success of neural
networks in various applications, there exist few formidable constraints
including the need for a large amount of labeled data. Therefore,
semi-supervised learning, which is a learning scheme in which the scarce labels
and a larger amount of unlabeled data are utilized to train models (e.g., deep
neural networks) is getting more important. Based on the key assumptions of
semi-supervised learning, which are the manifold assumption, cluster
assumption, and continuity assumption, the work reviews the recent
semi-supervised learning approaches. In particular, the methods in regard to
using deep neural networks in a semi-supervised learning setting are primarily
discussed. In addition, the existing works are first classified based on the
underlying idea and explained, and then the holistic approaches that unify the
aforementioned ideas are detailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyeongho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11447</id>
        <link href="http://arxiv.org/abs/2106.11447"/>
        <updated>2021-06-23T01:48:40.214Z</updated>
        <summary type="html"><![CDATA[Coronary X-ray angiography is a crucial clinical procedure for the diagnosis
and treatment of coronary artery disease, which accounts for roughly 16% of
global deaths every year. However, the images acquired in these procedures have
low resolution and poor contrast, making lesion detection and assessment
challenging. Accurate coronary artery segmentation not only helps mitigate
these problems, but also allows the extraction of relevant anatomical features
for further analysis by quantitative methods. Although automated segmentation
of coronary arteries has been proposed before, previous approaches have used
non-optimal segmentation criteria, leading to less useful results. Most methods
either segment only the major vessel, discarding important information from the
remaining ones, or segment the whole coronary tree based mostly on contrast
information, producing a noisy output that includes vessels that are not
relevant for diagnosis. We adopt a better-suited clinical criterion and segment
vessels according to their clinical relevance. Additionally, we simultaneously
perform catheter segmentation, which may be useful for diagnosis due to the
scale factor provided by the catheter's known diameter, and is a task that has
not yet been performed with good results. To derive the optimal approach, we
conducted an extensive comparative study of encoder-decoder architectures
trained on a combination of focal loss and a variant of generalized dice loss.
Based on the EfficientNet and the UNet++ architectures, we propose a line of
efficient and high-performance segmentation models using a new decoder
architecture, the EfficientUNet++, whose best-performing version achieved
average dice scores of 0.8904 and 0.7526 for the artery and catheter classes,
respectively, and an average generalized dice score of 0.9234.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Louren&amp;#xe7;o Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1"&gt;Miguel Nobre Menezes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1"&gt;Tiago Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1"&gt;Beatriz Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1"&gt;Fausto J. Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Model-based Hierarchical Reinforcement Learning using Inductive Logic Programming. (arXiv:2106.11417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11417</id>
        <link href="http://arxiv.org/abs/2106.11417"/>
        <updated>2021-06-23T01:48:40.204Z</updated>
        <summary type="html"><![CDATA[Recently deep reinforcement learning has achieved tremendous success in wide
ranges of applications. However, it notoriously lacks data-efficiency and
interpretability. Data-efficiency is important as interacting with the
environment is expensive. Further, interpretability can increase the
transparency of the black-box-style deep RL models and hence gain trust from
the users. In this work, we propose a new hierarchical framework via symbolic
RL, leveraging a symbolic transition model to improve the data-efficiency and
introduce the interpretability for learned policy. This framework consists of a
high-level agent, a subtask solver and a symbolic transition model. Without
assuming any prior knowledge on the state transition, we adopt inductive logic
programming (ILP) to learn the rules of symbolic state transitions, introducing
interpretability and making the learned behavior understandable to users. In
empirical experiments, we confirmed that the proposed framework offers
approximately between 30\% to 40\% more data efficiency over previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Duo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fekri_F/0/1/0/all/0/1"&gt;Faramarz Fekri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11339</id>
        <link href="http://arxiv.org/abs/2106.11339"/>
        <updated>2021-06-23T01:48:40.114Z</updated>
        <summary type="html"><![CDATA[Top-down attention allows neural networks, both artificial and biological, to
focus on the information most relevant for a given task. This is known to
enhance performance in visual perception. But it remains unclear how attention
brings about its perceptual boost, especially when it comes to naturalistic
settings like recognising an object in an everyday scene. What aspects of a
visual task does attention help to deal with? We aim to answer this with a
computational experiment based on a general framework called task-oriented
ablation design. First we define a broad range of visual tasks and identify six
factors that underlie task variability. Then on each task we compare the
performance of two neural networks, one with top-down attention and one
without. These comparisons reveal the task-dependence of attention's perceptual
boost, giving a clearer idea of the role attention plays. Whereas many existing
cognitive accounts link attention to stimulus-level variables, such as visual
clutter and object scale, we find greater explanatory power in system-level
variables that capture the interaction between the model, the distribution of
training data and the task format. This finding suggests a shift in how
attention is studied could be fruitful. We make publicly available our code and
results, along with statistics relevant to ImageNet-based experiments beyond
this one. Our contribution serves to support the development of more human-like
vision models and the design of more informative machine-learning experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1"&gt;Freddie Bickford Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Shaping: A Modeling Approach to Nurture Content Creation. (arXiv:2106.11312v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.11312</id>
        <link href="http://arxiv.org/abs/2106.11312"/>
        <updated>2021-06-23T01:48:40.105Z</updated>
        <summary type="html"><![CDATA[Social media platforms bring together content creators and content consumers
through recommender systems like newsfeed. The focus of such recommender
systems has thus far been primarily on modeling the content consumer
preferences and optimizing for their experience. However, it is equally
critical to nurture content creation by prioritizing the creators' interests,
as quality content forms the seed for sustainable engagement and conversations,
bringing in new consumers while retaining existing ones. In this work, we
propose a modeling approach to predict how feedback from content consumers
incentivizes creators. We then leverage this model to optimize the newsfeed
experience for content creators by reshaping the feedback distribution, leading
to a more active content ecosystem. Practically, we discuss how we balance the
user experience for both consumers and creators, and how we carry out online
A/B tests with strong network effects. We present a deployed use case on the
LinkedIn newsfeed, where we used this approach to improve content creation
significantly without compromising the consumers' experience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Y/0/1/0/all/0/1"&gt;Ye Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_C/0/1/0/all/0/1"&gt;Chun Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yiping Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1"&gt;Shaunak Chatterjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11401</id>
        <link href="http://arxiv.org/abs/2106.11401"/>
        <updated>2021-06-23T01:48:40.099Z</updated>
        <summary type="html"><![CDATA[Moving objects have special importance for Autonomous Driving tasks.
Detecting moving objects can be posed as Moving Object Segmentation, by
segmenting the object pixels, or Moving Object Detection, by generating a
bounding box for the moving targets. In this paper, we present a Multi-Task
Learning architecture, based on Transformers, to jointly perform both tasks
through one network. Due to the importance of the motion features to the task,
the whole setup is based on a Spatio-Temporal aggregation. We evaluate the
performance of the individual tasks architecture versus the MTL setup, both
with early shared encoders, and late shared encoder-decoder transformers. For
the latter, we present a novel joint tasks query decoder transformer, that
enables us to have tasks dedicated heads out of the shared model. To evaluate
our approach, we use the KITTI MOD [29] data set. Results show1.5% mAP
improvement for Moving Object Detection, and 2%IoU improvement for Moving
Object Segmentation, over the individual tasks networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmed El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Learning-based Precoder Codebooks for FD-MIMO Systems. (arXiv:2106.11374v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.11374</id>
        <link href="http://arxiv.org/abs/2106.11374"/>
        <updated>2021-06-23T01:48:40.082Z</updated>
        <summary type="html"><![CDATA[This paper develops an efficient procedure for designing low-complexity
codebooks for precoding in a full-dimension (FD) multiple-input multiple-output
(MIMO) system with a uniform planar array (UPA) antenna at the transmitter (Tx)
using tensor learning. In particular, instead of using statistical channel
models, we utilize a model-free data-driven approach with foundations in
machine learning to generate codebooks that adapt to the surrounding
propagation conditions. We use a tensor representation of the FD-MIMO channel
and exploit its properties to design quantized version of the channel
precoders. We find the best representation of the optimal precoder as a
function of Kronecker Product (KP) of two low-dimensional precoders,
respectively corresponding to the horizontal and vertical dimensions of the
UPA, obtained from the tensor decomposition of the channel. We then quantize
this precoder to design product codebooks such that an average loss in mutual
information due to quantization of channel state information (CSI) is
minimized. The key technical contribution lies in exploiting the constraints on
the precoders to reduce the product codebook design problem to an unsupervised
clustering problem on a Cartesian Product Grassmann manifold (CPM), where the
cluster centroids form a finite-sized precoder codebook. This codebook can be
found efficiently by running a $K$-means clustering on the CPM. With a suitable
induced distance metric on the CPM, we show that the construction of product
codebooks is equivalent to finding the optimal set of centroids on the factor
manifolds corresponding to the horizontal and vertical dimensions. Simulation
results are presented to demonstrate the capability of the proposed design
criterion in learning the codebooks and the attractive performance of the
designed codebooks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhogi_K/0/1/0/all/0/1"&gt;Keerthana Bhogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saha_C/0/1/0/all/0/1"&gt;Chiranjib Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dhillon_H/0/1/0/all/0/1"&gt;Harpreet S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11422</id>
        <link href="http://arxiv.org/abs/2106.11422"/>
        <updated>2021-06-23T01:48:40.074Z</updated>
        <summary type="html"><![CDATA[Moving Object Detection (MOD) is a crucial task for the Autonomous Driving
pipeline. MOD is usually handled via 2-stream convolutional architectures that
incorporates both appearance and motion cues, without considering the
inter-relations between the spatial or motion features. In this paper, we
tackle this problem through multi-head attention mechanisms, both across the
spatial and motion streams. We propose MODETR; a Moving Object DEtection
TRansformer network, comprised of multi-stream transformer encoders for both
spatial and motion modalities, and an object transformer decoder that produces
the moving objects bounding boxes using set predictions. The whole architecture
is trained end-to-end using bi-partite loss. Several methods of incorporating
motion cues with the Transformer model are explored, including two-stream RGB
and Optical Flow (OF) methods, and multi-stream architectures that take
advantage of sequence information. To incorporate the temporal information, we
propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial
Positional Encoding(SPE) in DETR. We explore two architectural choices for
that, balancing between speed and time. To evaluate the our network, we perform
the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of
the Transformer network for MOD over the state-of-the art methods. Moreover,
the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11384</id>
        <link href="http://arxiv.org/abs/2106.11384"/>
        <updated>2021-06-23T01:48:40.067Z</updated>
        <summary type="html"><![CDATA[In the text processing context, most ML models are built on word embeddings.
These embeddings are themselves trained on some datasets, potentially
containing sensitive data. In some cases this training is done independently,
in other cases, it occurs as part of training a larger, task-specific model. In
either case, it is of interest to consider membership inference attacks based
on the embedding layer as a way of understanding sensitive information leakage.
But, somewhat surprisingly, membership inference attacks on word embeddings and
their effect in other natural language processing (NLP) tasks that use these
embeddings, have remained relatively unexplored.

In this work, we show that word embeddings are vulnerable to black-box
membership inference attacks under realistic assumptions. Furthermore, we show
that this leakage persists through two other major NLP applications:
classification and text-generation, even when the embedding layer is not
exposed to the attacker. We show that our MI attack achieves high attack
accuracy against a classifier model and an LSTM-based language model. Indeed,
our attack is a cheaper membership inference attack on text-generative models,
which does not require the knowledge of the target model or any expensive
training of text-generative models as shadow models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1"&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1"&gt;Melissa Chase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1"&gt;Esha Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1"&gt;Marcello Hasegawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Inference via Universal LSH Kernel. (arXiv:2106.11426v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11426</id>
        <link href="http://arxiv.org/abs/2106.11426"/>
        <updated>2021-06-23T01:48:40.061Z</updated>
        <summary type="html"><![CDATA[Large machine learning models achieve unprecedented performance on various
tasks and have evolved as the go-to technique. However, deploying these compute
and memory hungry models on resource constraint environments poses new
challenges. In this work, we propose mathematically provable Representer
Sketch, a concise set of count arrays that can approximate the inference
procedure with simple hashing computations and aggregations. Representer Sketch
builds upon the popular Representer Theorem from kernel literature, hence the
name, providing a generic fundamental alternative to the problem of efficient
inference that goes beyond the popular approach such as quantization, iterative
pruning and knowledge distillation. A neural network function is transformed to
its weighted kernel density representation, which can be very efficiently
estimated with our sketching algorithm. Empirically, we show that Representer
Sketch achieves up to 114x reduction in storage requirement and 59x reduction
in computation complexity without any drop in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zichang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coleman_B/0/1/0/all/0/1"&gt;Benjamin Coleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local convexity of the TAP free energy and AMP convergence for Z2-synchronization. (arXiv:2106.11428v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2106.11428</id>
        <link href="http://arxiv.org/abs/2106.11428"/>
        <updated>2021-06-23T01:48:40.054Z</updated>
        <summary type="html"><![CDATA[We study mean-field variational Bayesian inference using the TAP approach,
for Z2-synchronization as a prototypical example of a high-dimensional Bayesian
model. We show that for any signal strength $\lambda > 1$ (the weak-recovery
threshold), there exists a unique local minimizer of the TAP free energy
functional near the mean of the Bayes posterior law. Furthermore, the TAP free
energy in a local neighborhood of this minimizer is strongly convex.
Consequently, a natural-gradient/mirror-descent algorithm achieves linear
convergence to this minimizer from a local initialization, which may be
obtained by a finite number of iterates of Approximate Message Passing (AMP).
This provides a rigorous foundation for variational inference in high
dimensions via minimization of the TAP free energy.

We also analyze the finite-sample convergence of AMP, showing that AMP is
asymptotically stable at the TAP minimizer for any $\lambda > 1$, and is
linearly convergent to this minimizer from a spectral initialization for
sufficiently large $\lambda$. Such a guarantee is stronger than results
obtainable by state evolution analyses, which only describe a fixed number of
AMP iterations in the infinite-sample limit.

Our proofs combine the Kac-Rice formula and Sudakov-Fernique Gaussian
comparison inequality to analyze the complexity of critical points that satisfy
strong convexity and stability conditions within their local neighborhoods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Celentano_M/0/1/0/all/0/1"&gt;Michael Celentano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_Z/0/1/0/all/0/1"&gt;Zhou Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mei_S/0/1/0/all/0/1"&gt;Song Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Quality as Predictor of Voice Anti-Spoofing Generalization. (arXiv:2103.14602v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14602</id>
        <link href="http://arxiv.org/abs/2103.14602"/>
        <updated>2021-06-23T01:48:40.033Z</updated>
        <summary type="html"><![CDATA[Voice anti-spoofing aims at classifying a given utterance either as a
bonafide human sample, or a spoofing attack (e.g. synthetic or replayed
sample). Many anti-spoofing methods have been proposed but most of them fail to
generalize across domains (corpora) -- and we do not know \emph{why}. We
outline a novel interpretative framework for gauging the impact of data quality
upon anti-spoofing performance. Our within- and between-domain experiments pool
data from seven public corpora and three anti-spoofing methods based on
Gaussian mixture and convolutive neural network models. We assess the impacts
of long-term spectral information, speaker population (through x-vector speaker
embeddings), signal-to-noise ratio, and selected voice quality features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chettri_B/0/1/0/all/0/1"&gt;Bhusan Chettri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hautamaki_R/0/1/0/all/0/1"&gt;Rosa Gonz&amp;#xe1;lez Hautam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahidullah_M/0/1/0/all/0/1"&gt;Md Sahidullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kinnunen_T/0/1/0/all/0/1"&gt;Tomi Kinnunen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:40.025Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Consistent Video Depth Estimation. (arXiv:2012.05901v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05901</id>
        <link href="http://arxiv.org/abs/2012.05901"/>
        <updated>2021-06-23T01:48:40.017Z</updated>
        <summary type="html"><![CDATA[We present an algorithm for estimating consistent dense depth maps and camera
poses from a monocular video. We integrate a learning-based depth prior, in the
form of a convolutional neural network trained for single-image depth
estimation, with geometric optimization, to estimate a smooth camera trajectory
as well as detailed and stable depth reconstruction. Our algorithm combines two
complementary techniques: (1) flexible deformation-splines for low-frequency
large-scale alignment and (2) geometry-aware depth filtering for high-frequency
alignment of fine depth details. In contrast to prior approaches, our method
does not require camera poses as input and achieves robust reconstruction for
challenging hand-held cell phone captures containing a significant amount of
noise, shake, motion blur, and rolling shutter deformations. Our method
quantitatively outperforms state-of-the-arts on the Sintel benchmark for both
depth and pose estimations and attains favorable qualitative results across
diverse wild datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1"&gt;Johannes Kopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_X/0/1/0/all/0/1"&gt;Xuejian Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Smoothing for Provably Robust Reinforcement Learning. (arXiv:2106.11420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11420</id>
        <link href="http://arxiv.org/abs/2106.11420"/>
        <updated>2021-06-23T01:48:40.010Z</updated>
        <summary type="html"><![CDATA[The study of provable adversarial robustness for deep neural network (DNN)
models has mainly focused on static supervised learning tasks such as image
classification. However, DNNs have been used extensively in real-world adaptive
tasks such as reinforcement learning (RL), making RL systems vulnerable to
adversarial attacks. The key challenge in adversarial RL is that the attacker
can adapt itself to the defense strategy used by the agent in previous
time-steps to strengthen its attack in future steps. In this work, we study the
provable robustness of RL against norm-bounded adversarial perturbations of the
inputs. We focus on smoothing-based provable defenses and propose policy
smoothing where the agent adds a Gaussian noise to its observation at each
time-step before applying the policy network to make itself less sensitive to
adversarial perturbations of its inputs. Our main theoretical contribution is
to prove an adaptive version of the Neyman-Pearson Lemma where the adversarial
perturbation at a particular time can be a stochastic function of current and
previous observations and states as well as previously observed actions. Using
this lemma, we adapt the robustness certificates produced by randomized
smoothing in the static setting of image classification to the dynamic setting
of RL. We generate certificates that guarantee that the total reward obtained
by the smoothed policy will not fall below a certain threshold under a
norm-bounded adversarial perturbation of the input. We show that our
certificates are tight by constructing a worst-case setting that achieves the
bounds derived in our analysis. In our experiments, we show that this method
can yield meaningful certificates in complex environments demonstrating its
effectiveness against adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Aounon Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1"&gt;Alexander Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1"&gt;Soheil Feizi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11388</id>
        <link href="http://arxiv.org/abs/2106.11388"/>
        <updated>2021-06-23T01:48:40.003Z</updated>
        <summary type="html"><![CDATA[State-of-the-art summarization systems are trained and evaluated on massive
datasets scraped from the web. Despite their prevalence, we know very little
about the underlying characteristics (data noise, summarization complexity,
etc.) of these datasets, and how these affect system performance and the
reliability of automatic metrics like ROUGE. In this study, we manually analyze
600 samples from three popular summarization datasets. Our study is driven by a
six-class typology which captures different noise types (missing facts,
entities) and degrees of summarization difficulty (extractive, abstractive). We
follow with a thorough analysis of 27 state-of-the-art summarization models and
5 popular metrics, and report our key insights: (1) Datasets have distinct data
quality and complexity distributions, which can be traced back to their
collection process. (2) The performance of models and reliability of metrics is
dependent on sample complexity. (3) Faithful summaries often receive low scores
because of the poor diversity of references. We release the code, annotated
data and model outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1"&gt;Priyam Tejaswin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1"&gt;Dhruv Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wallpaper Texture Generation and Style Transfer Based on Multi-label Semantics. (arXiv:2106.11482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11482</id>
        <link href="http://arxiv.org/abs/2106.11482"/>
        <updated>2021-06-23T01:48:39.981Z</updated>
        <summary type="html"><![CDATA[Textures contain a wealth of image information and are widely used in various
fields such as computer graphics and computer vision. With the development of
machine learning, the texture synthesis and generation have been greatly
improved. As a very common element in everyday life, wallpapers contain a
wealth of texture information, making it difficult to annotate with a simple
single label. Moreover, wallpaper designers spend significant time to create
different styles of wallpaper. For this purpose, this paper proposes to
describe wallpaper texture images by using multi-label semantics. Based on
these labels and generative adversarial networks, we present a framework for
perception driven wallpaper texture generation and style transfer. In this
framework, a perceptual model is trained to recognize whether the wallpapers
produced by the generator network are sufficiently realistic and have the
attribute designated by given perceptual description; these multi-label
semantic attributes are treated as condition variables to generate wallpaper
images. The generated wallpaper images can be converted to those with
well-known artist styles using CycleGAN. Finally, using the aesthetic
evaluation method, the generated wallpaper images are quantitatively measured.
The experimental results demonstrate that the proposed method can generate
wallpaper textures conforming to human aesthetics and have artistic
characteristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Ying Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xiaohan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tiange Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigall_E/0/1/0/all/0/1"&gt;Eric Rigall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1"&gt;Lin Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2106.11841v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11841</id>
        <link href="http://arxiv.org/abs/2106.11841"/>
        <updated>2021-06-23T01:48:39.974Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal
retrieval task, where abstract sketches are used as queries to retrieve natural
images under zero-shot scenario. Most existing methods regard ZS-SBIR as a
traditional classification problem and employ a cross-entropy or triplet-based
loss to achieve retrieval, which neglect the problems of the domain gap between
sketches and natural images and the large intra-class diversity in sketches.
Toward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.
Specifically, a cross-modal contrastive method is proposed to learn generalized
representations to smooth the domain gap by mining relations with additional
augmented samples. Furthermore, a category-specific memory bank with sketch
features is explored to reduce intra-class diversity in the sketch domain.
Extensive experiments demonstrate that our approach notably outperforms the
state-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source
code is publicly available at https://github.com/haowang1992/DSN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhipeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiexi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1"&gt;Aming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1"&gt;Cheng Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11344</id>
        <link href="http://arxiv.org/abs/2106.11344"/>
        <updated>2021-06-23T01:48:39.968Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is used in many machine learning applications
where, during training, a model has access to unlabeled data in the target
domain, and a related labeled dataset. In this paper, we introduce a novel and
general domain-adversarial framework. Specifically, we derive a novel
generalization bound for domain adaptation that exploits a new measure of
discrepancy between distributions based on a variational characterization of
f-divergences. It recovers the theoretical results from Ben-David et al.
(2010a) as a special case and supports divergences used in practice. Based on
this bound, we derive a new algorithmic framework that introduces a key
correction in the original adversarial training method of Ganin et al. (2016).
We show that many regularizers and ad-hoc objectives introduced over the last
years in this framework are then not required to achieve performance comparable
to (if not better than) state-of-the-art domain-adversarial methods.
Experimental analysis conducted on real-world natural language and computer
vision datasets show that our framework outperforms existing baselines, and
obtains the best results for f-divergences that were not considered previously
in domain-adversarial learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guojun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11821</id>
        <link href="http://arxiv.org/abs/2106.11821"/>
        <updated>2021-06-23T01:48:39.962Z</updated>
        <summary type="html"><![CDATA[Data augmentation has been successfully used in many areas of deep-learning
to significantly improve model performance. Typically data augmentation
simulates realistic variations in data in order to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis,
where deep learning methods are already achieving state of the art performance,
it is not immediately clear how to apply data augmentation. In this paper we
study different methods of data augmentation starting with basic methods using
fixed transformations and moving to methods that adapt to the data. We propose
a novel data augmentation method based on using an opcode embedding layer
within the network and its corresponding opcode embedding matrix to perform
adaptive data augmentation during training. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods applied to opcode sequence based malware classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McLaughlin_N/0/1/0/all/0/1"&gt;Niall McLaughlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rincon_J/0/1/0/all/0/1"&gt;Jesus Martinez del Rincon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.11516</id>
        <link href="http://arxiv.org/abs/2106.11516"/>
        <updated>2021-06-23T01:48:39.955Z</updated>
        <summary type="html"><![CDATA[LiDAR-based SLAM system is admittedly more accurate and stable than others,
while its loop closure detection is still an open issue. With the development
of 3D semantic segmentation for point cloud, semantic information can be
obtained conveniently and steadily, essential for high-level intelligence and
conductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM
with loop closure based on LOAM, named SA-LOAM, which leverages semantics in
odometry as well as loop closure detection. Specifically, we propose a
semantic-assisted ICP, including semantically matching, downsampling and plane
constraint, and integrates a semantic graph-based place recognition method in
our loop closure detection module. Benefitting from semantics, we can improve
the localization accuracy, detect loop closures effectively, and construct a
global consistent semantic map even in large-scale scenes. Extensive
experiments on KITTI and Ford Campus dataset show that our system significantly
improves baseline performance, has generalization ability to unseen data and
achieves competitive results compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xiangrui Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wanlong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1"&gt;Feng Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proposal Relation Network for Temporal Action Detection. (arXiv:2106.11812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11812</id>
        <link href="http://arxiv.org/abs/2106.11812"/>
        <updated>2021-06-23T01:48:39.948Z</updated>
        <summary type="html"><![CDATA[This technical report presents our solution for temporal action detection
task in AcitivityNet Challenge 2021. The purpose of this task is to locate and
identify actions of interest in long untrimmed videos. The crucial challenge of
the task comes from that the temporal duration of action varies dramatically,
and the target actions are typically embedded in a background of irrelevant
activities. Our solution builds on BMN, and mainly contains three steps: 1)
action classification and feature encoding by Slowfast, CSN and ViViT; 2)
proposal generation. We improve BMN by embedding the proposed Proposal Relation
Network (PRN), by which we can generate proposals of high quality; 3) action
detection. We calculate the detection results by assigning the proposals with
corresponding classification results. Finally, we ensemble the results under
different settings and achieve 44.7% on the test set, which improves the
champion result in ActivityNet 2020 by 1.9% in terms of average mAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Changxin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry. (arXiv:2106.11857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11857</id>
        <link href="http://arxiv.org/abs/2106.11857"/>
        <updated>2021-06-23T01:48:39.928Z</updated>
        <summary type="html"><![CDATA[We present HybVIO, a novel hybrid approach for combining filtering-based
visual-inertial odometry (VIO) with optimization-based SLAM. The core of our
method is highly robust, independent VIO with improved IMU bias modeling,
outlier rejection, stationarity detection, and feature track selection, which
is adjustable to run on embedded hardware. Long-term consistency is achieved
with a loosely-coupled SLAM module. In academic benchmarks, our solution yields
excellent performance in all categories, especially in the real-time use case,
where we outperform the current state-of-the-art. We also demonstrate the
feasibility of VIO for vehicular tracking on consumer-grade hardware using a
custom dataset, and show good performance in comparison to current commercial
VISLAM alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seiskari_O/0/1/0/all/0/1"&gt;Otto Seiskari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rantalankila_P/0/1/0/all/0/1"&gt;Pekka Rantalankila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1"&gt;Juho Kannala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ylilammi_J/0/1/0/all/0/1"&gt;Jerry Ylilammi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1"&gt;Esa Rahtu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks. (arXiv:2008.07404v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.07404</id>
        <link href="http://arxiv.org/abs/2008.07404"/>
        <updated>2021-06-23T01:48:39.922Z</updated>
        <summary type="html"><![CDATA[Skeleton-based Human Activity Recognition has achieved great interest in
recent years as skeleton data has demonstrated being robust to illumination
changes, body scales, dynamic camera views, and complex background. In
particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated
to be effective in learning both spatial and temporal dependencies on
non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding
of the latent information underlying the 3D skeleton is still an open problem,
especially when it comes to extracting effective information from joint motion
patterns and their correlations. In this work, we propose a novel
Spatial-Temporal Transformer network (ST-TR) which models dependencies between
joints using the Transformer self-attention operator. In our ST-TR model, a
Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network, whose performance is evaluated on three large-scale datasets,
NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving
backbone results. Compared with methods that use the same input data, the
proposed ST-TR achieves state-of-the-art performance on all datasets when using
joints' coordinates as input, and results on-par with state-of-the-art when
adding bones information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1"&gt;Chiara Plizzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1"&gt;Marco Cannici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"&gt;Matteo Matteucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RootPainter3D: Interactive-machine-learning enables rapid and accurate contouring for radiotherapy. (arXiv:2106.11942v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11942</id>
        <link href="http://arxiv.org/abs/2106.11942"/>
        <updated>2021-06-23T01:48:39.914Z</updated>
        <summary type="html"><![CDATA[Organ-at-risk contouring is still a bottleneck in radiotherapy, with many
deep learning methods falling short of promised results when evaluated on
clinical data. We investigate the accuracy and time-savings resulting from the
use of an interactive-machine-learning method for an organ-at-risk contouring
task. We compare the method to the Eclipse contouring software and find strong
agreement with manual delineations, with a dice score of 0.95. The annotations
created using corrective-annotation also take less time to create as more
images are annotated, resulting in substantial time savings compared to manual
methods, with hearts that take 2 minutes and 2 seconds to delineate on average,
after 923 images have been delineated, compared to 7 minutes and 1 seconds when
delineating manually. Our experiment demonstrates that
interactive-machine-learning with corrective-annotation provides a fast and
accessible way for non computer-scientists to train deep-learning models to
segment their own structures of interest as part of routine clinical workflows.

Source code is available at
\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Abraham George Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1"&gt;Jens Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terrones_Campos_C/0/1/0/all/0/1"&gt;Cynthia Terrones-Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berthelsen_A/0/1/0/all/0/1"&gt;Anne Kiil Berthelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forbes_N/0/1/0/all/0/1"&gt;Nora Jarrett Forbes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1"&gt;Sune Darkner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Specht_L/0/1/0/all/0/1"&gt;Lena Specht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelius_I/0/1/0/all/0/1"&gt;Ivan Richter Vogelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food. (arXiv:2103.03375v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03375</id>
        <link href="http://arxiv.org/abs/2103.03375"/>
        <updated>2021-06-23T01:48:39.888Z</updated>
        <summary type="html"><![CDATA[Understanding the nutritional content of food from visual data is a
challenging computer vision problem, with the potential to have a positive and
widespread impact on public health. Studies in this area are limited to
existing datasets in the field that lack sufficient diversity or labels
required for training models with nutritional understanding capability. We
introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes
with corresponding video streams, depth images, component weights, and high
accuracy nutritional content annotation. We demonstrate the potential of this
dataset by training a computer vision algorithm capable of predicting the
caloric and macronutrient values of a complex, real world dish at an accuracy
that outperforms professional nutritionists. Further we present a baseline for
incorporating depth sensor data to improve nutrition predictions. We will
publicly release Nutrition5k in the hope that it will accelerate innovation in
the space of nutritional understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thames_Q/0/1/0/all/0/1"&gt;Quin Thames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpur_A/0/1/0/all/0/1"&gt;Arjun Karpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norris_W/0/1/0/all/0/1"&gt;Wade Norris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Fangting Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panait_L/0/1/0/all/0/1"&gt;Liviu Panait&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weyand_T/0/1/0/all/0/1"&gt;Tobias Weyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sim_J/0/1/0/all/0/1"&gt;Jack Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Give Me Your Trained Model: Domain Adaptive Semantic Segmentation without Source Data. (arXiv:2106.11653v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11653</id>
        <link href="http://arxiv.org/abs/2106.11653"/>
        <updated>2021-06-23T01:48:39.881Z</updated>
        <summary type="html"><![CDATA[Benefited from considerable pixel-level annotations collected from a specific
situation (source), the trained semantic segmentation model performs quite
well, but fails in a new situation (target) due to the large domain shift. To
mitigate the domain gap, previous cross-domain semantic segmentation methods
always assume the co-existence of source data and target data during
distribution alignment. However, the access to source data in the real scenario
may raise privacy concerns and violate intellectual property. To tackle this
problem, we focus on an interesting and challenging cross-domain semantic
segmentation task where only the trained source model is provided to the target
domain, and further propose a unified framework called Domain Adaptive Semantic
Segmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$
consists of three schemes, i.e., feature alignment, self-training, and
information propagation. First, we mainly develop a focal entropic loss on the
network outputs to implicitly align the target features with unseen source
features via the provided source model. Second, besides positive pseudo labels
in vanilla self-training, we first introduce negative pseudo labels to the
field and develop a bi-directional self-training strategy to enhance the
representation learning in the target domain. Finally, the information
propagation scheme further reduces the intra-domain discrepancy within the
target domain via pseudo semi-supervised learning. Extensive results on
synthesis-to-real and cross-city driving datasets validate DAS$^3$ yields
state-of-the-art performance, even on par with methods that need access to
source data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuxi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhaoxiang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack. (arXiv:2106.11644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11644</id>
        <link href="http://arxiv.org/abs/2106.11644"/>
        <updated>2021-06-23T01:48:39.873Z</updated>
        <summary type="html"><![CDATA[We propose a novel and effective input transformation based adversarial
defense method against gray- and black-box attack, which is computationally
efficient and does not require any adversarial training or retraining of a
classification model. We first show that a very simple iterative Gaussian
smoothing can effectively wash out adversarial noise and achieve substantially
high robust accuracy. Based on the observation, we propose Self-Supervised
Iterative Contextual Smoothing (SSICS), which aims to reconstruct the original
discriminative features from the Gaussian-smoothed image in context-adaptive
manner, while still smoothing out the adversarial noise. From the experiments
on ImageNet, we show that our SSICS achieves both high standard accuracy and
very competitive robust accuracy for the gray- and black-box attacks; e.g.,
transfer-based PGD-attack and score-based attack. A note-worthy point to stress
is that our defense is free of computationally expensive adversarial training,
yet, can approach its robust accuracy via input transformation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1"&gt;Sungmin Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_N/0/1/0/all/0/1"&gt;Naeun Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding based Deep Learning. (arXiv:2106.11480v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11480</id>
        <link href="http://arxiv.org/abs/2106.11480"/>
        <updated>2021-06-23T01:48:39.865Z</updated>
        <summary type="html"><![CDATA[Recent advances in bioimaging have provided scientists a superior high
spatial-temporal resolution to observe dynamics of living cells as 3D
volumetric videos. Unfortunately, the 3D biomedical video analysis is lagging,
impeded by resource insensitive human curation using off-the-shelf 3D analytic
tools. Herein, biologists often need to discard a considerable amount of rich
3D spatial information by compromising on 2D analysis via maximum intensity
projection. Recently, pixel embedding-based cell instance segmentation and
tracking provided a neat and generalizable computing paradigm for understanding
cellular dynamics. In this work, we propose a novel spatial-temporal
voxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell
instance segmenting and tracking on 3D volumetric video sequences. Our
contribution is in four-fold: (1) The proposed voxel embedding generalizes the
pixel embedding with 3D context information; (2) Present a simple multi-stream
learning approach that allows effective spatial-temporal embedding; (3)
Accomplished an end-to-end framework for one-stage 3D cell instance
segmentation and tracking without heavy parameter tuning; (4) The proposed 3D
quantification is memory efficient via a single GPU with 12 GB memory. We
evaluate our VoxelEmbed method on four 3D datasets (with different cell types)
from the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved
consistent superior overall performance (OP) on two densely annotated datasets.
The performance is also competitive on two sparsely annotated cohorts with
20.6% and 2% of data-set having segmentation annotations. The results
demonstrate that the VoxelEmbed method is a generalizable and memory-efficient
solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Aadarsh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1"&gt;Ruining Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tianyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahadevan_Jansen_A/0/1/0/all/0/1"&gt;Anita Mahadevan-Jansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyska_M/0/1/0/all/0/1"&gt;Matthew J.Tyska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millis_B/0/1/0/all/0/1"&gt;Bryan A. Millis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness vs Model Compression, or Both?. (arXiv:1903.12561v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.12561</id>
        <link href="http://arxiv.org/abs/1903.12561"/>
        <updated>2021-06-23T01:48:39.858Z</updated>
        <summary type="html"><![CDATA[It is well known that deep neural networks (DNNs) are vulnerable to
adversarial attacks, which are implemented by adding crafted perturbations onto
benign examples. Min-max robust optimization based adversarial training can
provide a notion of security against adversarial attacks. However, adversarial
robustness requires a significantly larger capacity of the network than that
for the natural training with only benign examples. This paper proposes a
framework of concurrent adversarial training and weight pruning that enables
model compression while still preserving the adversarial robustness and
essentially tackles the dilemma of adversarial training. Furthermore, this work
studies two hypotheses about weight pruning in the conventional setting and
finds that weight pruning is essential for reducing the network model size in
the adversarial setting, training a small model from scratch even with
inherited initialization from the large model cannot achieve both adversarial
robustness and high standard accuracy. Code is available at
https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shaokai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lambrechts_J/0/1/0/all/0/1"&gt;Jan-Henrik Lambrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kaisheng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Human-aware Robot Navigation. (arXiv:2106.11650v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.11650</id>
        <link href="http://arxiv.org/abs/2106.11650"/>
        <updated>2021-06-23T01:48:39.850Z</updated>
        <summary type="html"><![CDATA[Intelligent systems are increasingly part of our everyday lives and have been
integrated seamlessly to the point where it is difficult to imagine a world
without them. Physical manifestations of those systems on the other hand, in
the form of embodied agents or robots, have so far been used only for specific
applications and are often limited to functional roles (e.g. in the industry,
entertainment and military fields). Given the current growth and innovation in
the research communities concerned with the topics of robot navigation,
human-robot-interaction and human activity recognition, it seems like this
might soon change. Robots are increasingly easy to obtain and use and the
acceptance of them in general is growing. However, the design of a socially
compliant robot that can function as a companion needs to take various areas of
research into account. This paper is concerned with the navigation aspect of a
socially-compliant robot and provides a survey of existing solutions for the
relevant areas of research as well as an outlook on possible future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moller_R/0/1/0/all/0/1"&gt;Ronja M&amp;#xf6;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1"&gt;Antonino Furnari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1"&gt;Sebastiano Battiato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harma_A/0/1/0/all/0/1"&gt;Aki H&amp;#xe4;rm&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1"&gt;Giovanni Maria Farinella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Object Tracking with Mixture Density Networks for Trajectory Estimation. (arXiv:2106.10950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10950</id>
        <link href="http://arxiv.org/abs/2106.10950"/>
        <updated>2021-06-23T01:48:39.843Z</updated>
        <summary type="html"><![CDATA[Multiple object tracking faces several challenges that may be alleviated with
trajectory information. Knowing the posterior locations of an object helps
disambiguating and solving situations such as occlusions, re-identification,
and identity switching. In this work, we show that trajectory estimation can
become a key factor for tracking, and present TrajE, a trajectory estimator
based on recurrent mixture density networks, as a generic module that can be
added to existing object trackers. To provide several trajectory hypotheses,
our method uses beam search. Also, relying on the same estimated trajectory, we
propose to reconstruct a track after an occlusion occurs. We integrate TrajE
into two state of the art tracking algorithms, CenterTrack [63] and Tracktor
[3]. Their respective performances in the MOTChallenge 2017 test set are
boosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a
new state of the art for the CenterTrack+TrajE configuration]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girbau_A/0/1/0/all/0/1"&gt;Andreu Girbau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Gir&amp;#xf3;-i-Nieto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rius_I/0/1/0/all/0/1"&gt;Ignasi Rius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_F/0/1/0/all/0/1"&gt;Ferran Marqu&amp;#xe9;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Alternative Auxiliary Task for Enhancing Image Classification. (arXiv:2106.11478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11478</id>
        <link href="http://arxiv.org/abs/2106.11478"/>
        <updated>2021-06-23T01:48:39.835Z</updated>
        <summary type="html"><![CDATA[Image reconstruction is likely the most predominant auxiliary task for image
classification. In this paper, we investigate ``estimating the Fourier
Transform of the input image" as a potential alternative auxiliary task, in the
hope that it may further boost the performances on the primary task or
introduce novel constraints not well covered by image reconstruction. We
experimented with five popular classification architectures on the CIFAR-10
dataset, and the empirical results indicated that our proposed auxiliary task
generally improves the classification accuracy. More notably, the results
showed that in certain cases our proposed auxiliary task may enhance the
classifiers' resistance to adversarial attacks generated using the fast
gradient sign method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chen Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropy-based Logic Explanations of Neural Networks. (arXiv:2106.06804v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06804</id>
        <link href="http://arxiv.org/abs/2106.06804"/>
        <updated>2021-06-23T01:48:39.808Z</updated>
        <summary type="html"><![CDATA[Explainable artificial intelligence has rapidly emerged since lawmakers have
started requiring interpretable models for safety-critical domains.
Concept-based neural networks have arisen as explainable-by-design methods as
they leverage human-understandable symbols (i.e. concepts) to predict class
memberships. However, most of these approaches focus on the identification of
the most relevant concepts but do not provide concise, formal explanations of
how such concepts are leveraged by the classifier to make predictions. In this
paper, we propose a novel end-to-end differentiable approach enabling the
extraction of logic explanations from neural networks using the formalism of
First-Order Logic. The method relies on an entropy-based criterion which
automatically identifies the most relevant concepts. We consider four different
case studies to demonstrate that: (i) this entropy-based criterion enables the
distillation of concise logic explanations in safety-critical domains from
clinical data to computer vision; (ii) the proposed approach outperforms
state-of-the-art white-box models in terms of classification accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1"&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1"&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf3;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1"&gt;Marco Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1"&gt;Stefano Melacci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Dataset Collaborative Learning for Semantic Segmentation. (arXiv:2103.11351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11351</id>
        <link href="http://arxiv.org/abs/2103.11351"/>
        <updated>2021-06-23T01:48:39.799Z</updated>
        <summary type="html"><![CDATA[Recent work attempts to improve semantic segmentation performance by
exploring well-designed architectures on a target dataset. However, it remains
challenging to build a unified system that simultaneously learns from various
datasets due to the inherent distribution shift across different datasets. In
this paper, we present a simple, flexible, and general method for semantic
segmentation, termed Cross-Dataset Collaborative Learning (CDCL). Given
multiple labeled datasets, we aim to improve the generalization and
discrimination of feature representations on each dataset. Specifically, we
first introduce a family of Dataset-Aware Blocks (DAB) as the fundamental
computing units of the network, which help capture homogeneous representations
and heterogeneous statistics across different datasets. Second, we propose a
Dataset Alternation Training (DAT) mechanism to efficiently facilitate the
optimization procedure. We conduct extensive evaluations on four diverse
datasets, i.e., Cityscapes, BDD100K, CamVid, and COCO Stuff, with
single-dataset and cross-dataset settings. Experimental results demonstrate our
method consistently achieves notable improvements over prior single-dataset and
cross-dataset training methods without introducing extra FLOPs. Particularly,
with the same architecture of PSPNet (ResNet-18), our method outperforms the
single-dataset baseline by 5.65\%, 6.57\%, and 5.79\% of mIoU on the validation
sets of Cityscapes, BDD100K, CamVid, respectively. Code and models will be
released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Li Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yousong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Lu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Yi Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11944</id>
        <link href="http://arxiv.org/abs/2106.11944"/>
        <updated>2021-06-23T01:48:39.769Z</updated>
        <summary type="html"><![CDATA[In this paper, we aim to create generalizable and controllable neural signed
distance fields (SDFs) that represent clothed humans from monocular depth
observations. Recent advances in deep learning, especially neural implicit
representations, have enabled human shape reconstruction and controllable
avatar generation from different sensor inputs. However, to generate realistic
cloth deformations from novel input poses, watertight meshes or dense full-body
scans are usually needed as inputs. Furthermore, due to the difficulty of
effectively modeling pose-dependent cloth deformations for diverse body shapes
and cloth types, existing approaches resort to per-subject/cloth-type
optimization from scratch, which is computationally expensive. In contrast, we
propose an approach that can quickly generate realistic clothed human avatars,
represented as controllable neural SDFs, given only monocular depth images. We
achieve this by using meta-learning to learn an initialization of a
hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is
conditioned on human poses and represents a clothed neural avatar that deforms
non-rigidly according to the input poses. Meanwhile, it is meta-learned to
effectively incorporate priors of diverse body shapes and cloth types and thus
can be much faster to fine-tune, compared to models trained from scratch. We
qualitatively and quantitatively show that our approach outperforms
state-of-the-art approaches that require complete meshes as inputs while our
approach requires only depth frames as inputs and runs orders of magnitudes
faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very
robust, being the first to generate avatars with realistic dynamic cloth
deformations given as few as 8 monocular depth frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shaofei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1"&gt;Marko Mihajlovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1"&gt;Qianli Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyu Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Obstacle Detection for BVLOS Drones. (arXiv:2106.11098v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11098</id>
        <link href="http://arxiv.org/abs/2106.11098"/>
        <updated>2021-06-23T01:48:39.756Z</updated>
        <summary type="html"><![CDATA[With the introduction of new regulations in the European Union, the future of
Beyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the
creation of the theBEAST project, which aims to create an autonomous security
drone, with focus on those regulations and on safety. This technical paper
describes the first steps of a module within this project, which revolves
around detecting obstacles so they can be avoided in a fail-safe landing. A
deep learning powered object detection method is the subject of our research,
and various experiments are held to maximize its performance, such as comparing
various data augmentation techniques or YOLOv3 and YOLOv5. According to the
results of the experiments, we conclude that although object detection is a
promising approach to resolve this problem, more volume of data is required for
potential usage in a real-life application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1"&gt;Jan Moros Esteban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loosdrecht_J/0/1/0/all/0/1"&gt;Jaap van de Loosdrecht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1"&gt;Maya Aghaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Object-Level Representation Learning from Scene Images. (arXiv:2106.11952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11952</id>
        <link href="http://arxiv.org/abs/2106.11952"/>
        <updated>2021-06-23T01:48:39.734Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has largely narrowed the gap to
supervised pre-training on ImageNet. However, its success highly relies on the
object-centric priors of ImageNet, i.e., different augmented views of the same
image correspond to the same object. Such a heavily curated constraint becomes
immediately infeasible when pre-trained on more complex scene images with many
objects. To overcome this limitation, we introduce Object-level Representation
Learning (ORL), a new self-supervised learning framework towards scene images.
Our key insight is to leverage image-level self-supervised pre-training as the
prior to discover object-level semantic correspondence, thus realizing
object-level representation learning from scene images. Extensive experiments
on COCO show that ORL significantly improves the performance of self-supervised
learning on scene images, even surpassing supervised ImageNet pre-training on
several downstream tasks. Furthermore, ORL improves the downstream performance
when more unlabeled scene images are available, demonstrating its great
potential of harnessing unlabeled data in the wild. We hope our approach can
motivate future research on more general-purpose unsupervised representation
learning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiahao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1"&gt;Xiaohang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew Soon Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Quantization Methods for Efficient Neural Network Inference. (arXiv:2103.13630v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13630</id>
        <link href="http://arxiv.org/abs/2103.13630"/>
        <updated>2021-06-23T01:48:39.712Z</updated>
        <summary type="html"><![CDATA[As soon as abstract mathematical computations were adapted to computation on
digital computers, the problem of efficient representation, manipulation, and
communication of the numerical values in those computations arose. Strongly
related to the problem of numerical representation is the problem of
quantization: in what manner should a set of continuous real-valued numbers be
distributed over a fixed discrete set of numbers to minimize the number of bits
required and also to maximize the accuracy of the attendant computations? This
perennial problem of quantization is particularly relevant whenever memory
and/or computational resources are severely restricted, and it has come to the
forefront in recent years due to the remarkable performance of Neural Network
models in computer vision, natural language processing, and related areas.
Moving from floating-point representations to low-precision fixed integer
values represented in four bits or less holds the potential to reduce the
memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x
to 8x are often realized in practice in these applications. Thus, it is not
surprising that quantization has emerged recently as an important and very
active sub-area of research in the efficient implementation of computations
associated with Neural Networks. In this article, we survey approaches to the
problem of quantizing the numerical values in deep Neural Network computations,
covering the advantages/disadvantages of current methods. With this survey and
its organization, we hope to have presented a useful snapshot of the current
research in quantization for Neural Networks and to have given an intelligent
organization to ease the evaluation of future research in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generation and frame characteristics of predefined evenly-distributed class centroids for pattern classification. (arXiv:2105.00401v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00401</id>
        <link href="http://arxiv.org/abs/2105.00401"/>
        <updated>2021-06-23T01:48:39.704Z</updated>
        <summary type="html"><![CDATA[Predefined evenly-distributed class centroids (PEDCC) can be widely used in
models and algorithms of pattern classification, such as CNN classifiers,
classification autoencoders, clustering, and semi-supervised learning, etc. Its
basic idea is to predefine the class centers, which are evenly-distributed on
the unit hypersphere in feature space, to maximize the inter-class distance.
The previous method of generating PEDCC uses an iterative algorithm based on a
charge model, that is, the initial values of various centers (charge positions)
are randomly set from the normal distribution, and the charge positions are
updated iteratively with the help of the repulsive force between charges of the
same polarity. The class centers generated by the algorithm will produce some
errors with the theoretically evenly-distributed points, and the generation
time will be longer. This paper takes advantage of regular polyhedron in
high-dimensional space and the evenly distribution of points on the n
dimensional hypersphere to generate PEDCC mathematically. Then, we discussed
the basic and extensive characteristics of the frames formed by PEDCC. Finally,
experiments show that new algorithm is not only faster than the iterative
method, but also more accurate in position. The mathematical analysis and
experimental results of this paper can provide a theoretical tool for using
PEDCC to solve the key problems in the field of pattern recognition, such as
interpretable supervised/unsupervised learning, incremental learning,
uncertainty analysis and so on.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haiping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yingying Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qiuyu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guohui Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the importance of cross-task features for class-incremental learning. (arXiv:2106.11930v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11930</id>
        <link href="http://arxiv.org/abs/2106.11930"/>
        <updated>2021-06-23T01:48:39.685Z</updated>
        <summary type="html"><![CDATA[In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features.
This is especially important when the number of classes per task is small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1"&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1"&gt;Marc Masana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost Van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation. (arXiv:2106.11958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11958</id>
        <link href="http://arxiv.org/abs/2106.11958"/>
        <updated>2021-06-23T01:48:39.676Z</updated>
        <summary type="html"><![CDATA[Multiple object tracking and segmentation requires detecting, tracking, and
segmenting objects belonging to a set of given classes. Most approaches only
exploit the temporal dimension to address the association problem, while
relying on single frame predictions for the segmentation mask itself. We
propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich
spatio-temporal information for online multiple object tracking and
segmentation. PCAN first distills a space-time memory into a set of prototypes
and then employs cross-attention to retrieve rich information from the past
frames. To segment each object, PCAN adopts a prototypical appearance module to
learn a set of contrastive foreground and background prototypes, which are then
propagated over time. Extensive experiments demonstrate that PCAN outperforms
current video instance tracking and segmentation competition winners on both
Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and
two-stage segmentation frameworks. Code will be available at
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1"&gt;Lei Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1"&gt;Martin Danelljan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chi-Keung Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fisher Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB Video. (arXiv:2106.11725v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11725</id>
        <link href="http://arxiv.org/abs/2106.11725"/>
        <updated>2021-06-23T01:48:39.669Z</updated>
        <summary type="html"><![CDATA[Tracking and reconstructing the 3D pose and geometry of two hands in
interaction is a challenging problem that has a high relevance for several
human-computer interaction applications, including AR/VR, robotics, or sign
language recognition. Existing works are either limited to simpler tracking
settings (e.g., considering only a single hand or two spatially separated
hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,
in this work we present the first real-time method for motion capture of
skeletal pose and 3D surface geometry of hands from a single RGB camera that
explicitly considers close interactions. In order to address the inherent depth
ambiguities in RGB data, we propose a novel multi-task CNN that regresses
multiple complementary pieces of information, including segmentation, dense
matchings to a 3D hand model, and 2D keypoint positions, together with newly
proposed intra-hand relative depth and inter-hand distance maps. These
predictions are subsequently used in a generative model fitting framework in
order to estimate pose and shape parameters of a 3D hand model for both hands.
We experimentally verify the individual components of our RGB two-hand tracking
and 3D reconstruction pipeline through an extensive ablation study. Moreover,
we demonstrate that our approach offers previously unseen two-hand tracking
performance from RGB, and quantitatively and qualitatively outperforms existing
RGB-based methods that were not explicitly designed for two-hand interactions.
Moreover, our method even performs on-par with depth-based real-time methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiayi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Franziska Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1"&gt;Florian Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorli_S/0/1/0/all/0/1"&gt;Suzanne Sorli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnychenko_O/0/1/0/all/0/1"&gt;Oleksandr Sotnychenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_N/0/1/0/all/0/1"&gt;Neng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1"&gt;Miguel A. Otaduy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1"&gt;Dan Casas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Focus U-Net: A novel dual attention-gated CNN for polyp segmentation during colonoscopy. (arXiv:2105.07467v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07467</id>
        <link href="http://arxiv.org/abs/2105.07467"/>
        <updated>2021-06-23T01:48:39.662Z</updated>
        <summary type="html"><![CDATA[Background: Colonoscopy remains the gold-standard screening for colorectal
cancer. However, significant miss rates for polyps have been reported,
particularly when there are multiple small adenomas. This presents an
opportunity to leverage computer-aided systems to support clinicians and reduce
the number of polyps missed.

Method: In this work we introduce the Focus U-Net, a novel dual
attention-gated deep neural network, which combines efficient spatial and
channel-based attention into a single Focus Gate module to encourage selective
learning of polyp features. The Focus U-Net further incorporates short-range
skip connections and deep supervision. Furthermore, we introduce the Hybrid
Focal loss, a new compound loss function based on the Focal loss and Focal
Tversky loss, to handle class-imbalanced image segmentation. For our
experiments, we selected five public datasets containing images of polyps
obtained during optical colonoscopy: CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB,
ETIS-Larib PolypDB and EndoScene test set. To evaluate model performance, we
use the Dice similarity coefficient (DSC) and Intersection over Union (IoU)
metrics.

Results: Our model achieves state-of-the-art results for both CVC-ClinicDB
and Kvasir-SEG, with a mean DSC of 0.941 and 0.910, respectively. When
evaluated on a combination of five public polyp datasets, our model similarly
achieves state-of-the-art results with a mean DSC of 0.878 and mean IoU of
0.809, a 14% and 15% improvement over the previous state-of-the-art results of
0.768 and 0.702, respectively.

Conclusions: This study shows the potential for deep learning to provide fast
and accurate polyp segmentation results for use during colonoscopy. The Focus
U-Net may be adapted for future use in newer non-invasive screening and more
broadly to other biomedical image segmentation tasks involving class imbalance
and requiring efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yeung_M/0/1/0/all/0/1"&gt;Michael Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sala_E/0/1/0/all/0/1"&gt;Evis Sala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rundo_L/0/1/0/all/0/1"&gt;Leonardo Rundo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unified Shape and SVBRDF Recovery using Differentiable Monte Carlo Rendering. (arXiv:2103.15208v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15208</id>
        <link href="http://arxiv.org/abs/2103.15208"/>
        <updated>2021-06-23T01:48:39.655Z</updated>
        <summary type="html"><![CDATA[Reconstructing the shape and appearance of real-world objects using measured
2D images has been a long-standing problem in computer vision. In this paper,
we introduce a new analysis-by-synthesis technique capable of producing
high-quality reconstructions through robust coarse-to-fine optimization and
physics-based differentiable rendering.

Unlike most previous methods that handle geometry and reflectance largely
separately, our method unifies the optimization of both by leveraging image
gradients with respect to both object reflectance and geometry. To obtain
physically accurate gradient estimates, we develop a new GPU-based Monte Carlo
differentiable renderer leveraging recent advances in differentiable rendering
theory to offer unbiased gradients while enjoying better performance than
existing tools like PyTorch3D and redner. To further improve robustness, we
utilize several shape and material priors as well as a coarse-to-fine
optimization strategy to reconstruct geometry. We demonstrate that our
technique can produce reconstructions with higher quality than previous methods
such as COLMAP and Kinect Fusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1"&gt;Fujun Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Shuang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1"&gt;Kavita Bala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lasry-Lions Envelopes and Nonconvex Optimization: A Homotopy Approach. (arXiv:2103.08533v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08533</id>
        <link href="http://arxiv.org/abs/2103.08533"/>
        <updated>2021-06-23T01:48:39.633Z</updated>
        <summary type="html"><![CDATA[In large-scale optimization, the presence of nonsmooth and nonconvex terms in
a given problem typically makes it hard to solve. A popular approach to address
nonsmooth terms in convex optimization is to approximate them with their
respective Moreau envelopes. In this work, we study the use of Lasry-Lions
double envelopes to approximate nonsmooth terms that are also not convex. These
envelopes are an extension of the Moreau ones but exhibit an additional
smoothness property that makes them amenable to fast optimization algorithms.
Lasry-Lions envelopes can also be seen as an "intermediate" between a given
function and its convex envelope, and we make use of this property to develop a
method that builds a sequence of approximate subproblems that are easier to
solve than the original problem. We discuss convergence properties of this
method when used to address composite minimization problems; additionally,
based on a number of experiments, we discuss settings where it may be more
useful than classical alternatives in two domains: signal decoding and spectral
unmixing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Simoes_M/0/1/0/all/0/1"&gt;Miguel Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Themelis_A/0/1/0/all/0/1"&gt;Andreas Themelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Patrinos_P/0/1/0/all/0/1"&gt;Panagiotis Patrinos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Solving Inefficiency of Self-supervised Representation Learning. (arXiv:2104.08760v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08760</id>
        <link href="http://arxiv.org/abs/2104.08760"/>
        <updated>2021-06-23T01:48:39.626Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (especially contrastive learning) has attracted
great interest due to its tremendous potentials in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we discover two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
categories. Over-clustering implies that the model cannot efficiently learn the
feature representation from excessive negative sample pairs, which enforces the
model to over-cluster samples of the same actual categories into different
clusters. To simultaneously overcome these two problems, we propose a novel
self-supervised learning framework using a median triplet loss. Precisely, we
employ a triplet loss tending to maximize the relative distance between the
positive pair and negative pairs to address the under-clustering problem; and
we construct the negative pair by selecting the negative sample of a median
similarity score from all negative samples to avoid the over-clustering
problem, guaranteed by the Bernoulli Distribution model. We extensively
evaluate our proposed framework in several large-scale benchmarks (e.g.,
ImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance
(e.g., the learning efficiency) of our model over the latest state-of-the-art
methods by a clear margin. Codes available at:
https://github.com/wanggrun/triplet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Keze Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangcong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Networks as Flows of Velocity Fields for Diffeomorphic Time Series Alignment. (arXiv:2106.11911v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11911</id>
        <link href="http://arxiv.org/abs/2106.11911"/>
        <updated>2021-06-23T01:48:39.619Z</updated>
        <summary type="html"><![CDATA[Non-linear (large) time warping is a challenging source of nuisance in
time-series analysis. In this paper, we propose a novel diffeomorphic temporal
transformer network for both pairwise and joint time-series alignment. Our
ResNet-TW (Deep Residual Network for Time Warping) tackles the alignment
problem by compositing a flow of incremental diffeomorphic mappings. Governed
by the flow equation, our Residual Network (ResNet) builds smooth, fluid and
regular flows of velocity fields and consequently generates smooth and
invertible transformations (i.e. diffeomorphic warping functions). Inspired by
the elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,
the final transformation is built by the flow of time-dependent vector fields
which are none other than the building blocks of our Residual Network. The
latter is naturally viewed as an Eulerian discretization schema of the flow
equation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single
inexpensive forward pass. As we show in experiments on both univariate (84
datasets from UCR archive) and multivariate time-series (MSR Action-3D,
Florence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance
in joint alignment and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1"&gt;Boulbaba Ben Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xichan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11963</id>
        <link href="http://arxiv.org/abs/2106.11963"/>
        <updated>2021-06-23T01:48:39.612Z</updated>
        <summary type="html"><![CDATA[Recently, query based deep networks catch lots of attention owing to their
end-to-end pipeline and competitive results on several fundamental computer
vision tasks, such as object detection, semantic segmentation, and instance
segmentation. However, how to establish a query based video instance
segmentation (VIS) framework with elegant architecture and strong performance
remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e.,
tracking instances as queries), a unified query based VIS framework fully
leveraging the intrinsic one-to-one correspondence between instances and
queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on
YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS
Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single
scale testing \& modest amount of training data}. We also provide
QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references
for the VIS community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shusheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14240</id>
        <link href="http://arxiv.org/abs/2105.14240"/>
        <updated>2021-06-23T01:48:39.593Z</updated>
        <summary type="html"><![CDATA[Adversarial training is one of the most effective approaches to improve model
robustness against adversarial examples. However, previous works mainly focus
on the overall robustness of the model, and the in-depth analysis on the role
of each class involved in adversarial training is still missing. In this paper,
we propose to analyze the class-wise robustness in adversarial training. First,
we provide a detailed diagnosis of adversarial training on six benchmark
datasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.
Surprisingly, we find that there are remarkable robustness discrepancies among
classes, leading to unbalance/unfair class-wise robustness in the robust
models. Furthermore, we keep investigating the relations between classes and
find that the unbalanced class-wise robustness is pretty consistent among
different attack and defense methods. Moreover, we observe that the stronger
attack methods in adversarial learning achieve performance improvement mainly
from a more successful attack on the vulnerable classes (i.e., classes with
less robustness). Inspired by these interesting findings, we design a simple
but effective attack method based on the traditional PGD attack, named
Temperature-PGD attack, which proposes to enlarge the robustness disparity
among classes with a temperature factor on the confidence distribution of each
image. Experiments demonstrate our method can achieve a higher attack rate than
the PGD attack. Furthermore, from the defense perspective, we also make some
modifications in the training and inference phases to improve the robustness of
the most vulnerable class, so as to mitigate the large difference in class-wise
robustness. We believe our work can contribute to a more comprehensive
understanding of adversarial training as well as rethinking the class-wise
properties in robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1"&gt;Kun Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1"&gt;Kelu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: Glare or Gloom, I Can Still See You -- End-to-End Multimodal Object Detection. (arXiv:2102.12319v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12319</id>
        <link href="http://arxiv.org/abs/2102.12319"/>
        <updated>2021-06-23T01:48:39.584Z</updated>
        <summary type="html"><![CDATA[Deep neural networks designed for vision tasks are often prone to failure
when they encounter environmental conditions not covered by the training data.
Single-modal strategies are insufficient when the sensor fails to acquire
information due to malfunction or its design limitations. Multi-sensor
configurations are known to provide redundancy, increase reliability, and are
crucial in achieving robustness against asymmetric sensor failures. To address
the issue of changing lighting conditions and asymmetric sensor degradation in
object detection, we develop a multi-modal 2D object detector, and propose
deterministic and stochastic sensor-aware feature fusion strategies. The
proposed fusion mechanisms are driven by the estimated sensor measurement
reliability values/weights. Reliable object detection in harsh lighting
conditions is essential for applications such as self-driving vehicles and
human-robot interaction. We also propose a new "r-blended" hybrid depth
modality for RGB-D sensors. Through extensive experimentation, we show that the
proposed strategies outperform the existing state-of-the-art methods on the
FLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We
additionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and
demonstrate that the proposed object detection methodologies are highly
effective for a variety of lighting conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazhar_O/0/1/0/all/0/1"&gt;Osama Mazhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babuska_R/0/1/0/all/0/1"&gt;Robert Babuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose Estimation and Tracking. (arXiv:2106.11589v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11589</id>
        <link href="http://arxiv.org/abs/2106.11589"/>
        <updated>2021-06-23T01:48:39.577Z</updated>
        <summary type="html"><![CDATA[This paper introduces an approach for multi-human 3D pose estimation and
tracking based on calibrated multi-view. The main challenge lies in finding the
cross-view and temporal correspondences correctly even when several human pose
estimations are noisy. Compare to previous solutions that construct 3D poses
from multiple views, our approach takes advantage of temporal consistency to
match the 2D poses estimated with previously constructed 3D skeletons in every
view. Therefore cross-view and temporal associations are accomplished
simultaneously. Since the performance suffers from mistaken association and
noisy predictions, we design two strategies for aiming better correspondences
and 3D reconstruction. Specifically, we propose a part-aware measurement for
2D-3D association and a filter that can cope with 2D outliers during
reconstruction. Our approach is efficient and effective comparing to
state-of-the-art methods; it achieves competitive results on two benchmarks:
96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus
evaluation frames to be more challenging and our proposal also reach
well-performed result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1"&gt;Hau Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jia-Hong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yao-Chih Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Ching-Hsien Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jia-Da Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chu-Song Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Ultrasound Tongue Image Reconstruction from Lip Images Using Self-supervised Learning and Attention Mechanism. (arXiv:2106.11769v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11769</id>
        <link href="http://arxiv.org/abs/2106.11769"/>
        <updated>2021-06-23T01:48:39.570Z</updated>
        <summary type="html"><![CDATA[Speech production is a dynamic procedure, which involved multi human organs
including the tongue, jaw and lips. Modeling the dynamics of the vocal tract
deformation is a fundamental problem to understand the speech, which is the
most common way for human daily communication. Researchers employ several
sensory streams to describe the process simultaneously, which are
incontrovertibly statistically related to other streams. In this paper, we
address the following question: given an observable image sequences of lips,
can we picture the corresponding tongue motion. We formulated this problem as
the self-supervised learning problem, and employ the two-stream convolutional
network and long-short memory network for the learning task, with the attention
mechanism. We evaluate the performance of the proposed method by leveraging the
unlabeled lip videos to predict an upcoming ultrasound tongue image sequence.
The results show that our model is able to generate images that close to the
real ultrasound tongue images, and results in the matching between two imaging
modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jihan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI. (arXiv:2106.11731v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11731</id>
        <link href="http://arxiv.org/abs/2106.11731"/>
        <updated>2021-06-23T01:48:39.561Z</updated>
        <summary type="html"><![CDATA[UK Biobank (UKB) is conducting a large-scale study of more than half a
million volunteers, collecting health-related information on genetics,
lifestyle, blood biochemistry, and more. Medical imaging furthermore targets
100,000 subjects, with 70,000 follow-up sessions, enabling measurements of
organs, muscle, and body composition. With up to 170,000 mounting MR images,
various methodologies are accordingly engaged in large-scale image analysis.
This work presents an experimental inference engine that can automatically
predict a comprehensive profile of subject metadata from UKB neck-to-knee body
MRI. In cross-validation, it accurately inferred baseline characteristics such
as age, height, weight, and sex, but also emulated measurements of body
composition by DXA, organ volumes, and abstract properties like grip strength,
pulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can
automatically analyze thousands of subjects within hours and provide individual
confidence intervals. The underlying methodology is based on convolutional
neural networks for image-based mean-variance regression on two-dimensional
representations of the MRI data. This work aims to make the proposed system
available for free to researchers, who can use it to obtain fast and
fully-automated estimates of 72 different measurements immediately upon release
of new UK Biobank image data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Langner_T/0/1/0/all/0/1"&gt;Taro Langner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mora_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9;s Mart&amp;#xed;nez Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Strand_R/0/1/0/all/0/1"&gt;Robin Strand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahlstrom_H/0/1/0/all/0/1"&gt;H&amp;#xe5;kan Ahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kullberg_J/0/1/0/all/0/1"&gt;Joel Kullberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning of Deep Spiking Neural Networks through Gradient Rewiring. (arXiv:2105.04916v3 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04916</id>
        <link href="http://arxiv.org/abs/2105.04916"/>
        <updated>2021-06-23T01:48:39.554Z</updated>
        <summary type="html"><![CDATA[Spiking Neural Networks (SNNs) have been attached great importance due to
their biological plausibility and high energy-efficiency on neuromorphic chips.
As these chips are usually resource-constrained, the compression of SNNs is
thus crucial along the road of practical use of SNNs. Most existing methods
directly apply pruning approaches in artificial neural networks (ANNs) to SNNs,
which ignore the difference between ANNs and SNNs, thus limiting the
performance of the pruned SNNs. Besides, these methods are only suitable for
shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination
in the neural system, we propose gradient rewiring (Grad R), a joint learning
algorithm of connectivity and weight for SNNs, that enables us to seamlessly
optimize network structure without retraining. Our key innovation is to
redefine the gradient to a new synaptic parameter, allowing better exploration
of network structures by taking full advantage of the competition between
pruning and regrowth of connections. The experimental results show that the
proposed method achieves minimal loss of SNNs' performance on MNIST and
CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under
unprecedented 0.73% connectivity, which reveals remarkable structure refining
capability in SNNs. Our work suggests that there exists extremely high
redundancy in deep SNNs. Our codes are available at
https://github.com/Yanqi-Chen/Gradient-Rewiring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Wei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tiejun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Backdoor in Deep Neural Networks via Intentional Adversarial Perturbations. (arXiv:2105.14259v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14259</id>
        <link href="http://arxiv.org/abs/2105.14259"/>
        <updated>2021-06-23T01:48:39.531Z</updated>
        <summary type="html"><![CDATA[Recent researches show that deep learning model is susceptible to backdoor
attacks. Many defenses against backdoor attacks have been proposed. However,
existing defense works require high computational overhead or backdoor attack
information such as the trigger size, which is difficult to satisfy in
realistic scenarios. In this paper, a novel backdoor detection method based on
adversarial examples is proposed. The proposed method leverages intentional
adversarial perturbations to detect whether an image contains a trigger, which
can be applied in both the training stage and the inference stage (sanitize the
training set in training stage and detect the backdoor instances in inference
stage). Specifically, given an untrusted image, the adversarial perturbation is
added to the image intentionally. If the prediction of the model on the
perturbed image is consistent with that on the unperturbed image, the input
image will be considered as a backdoor instance. Compared with most existing
defense works, the proposed adversarial perturbation based method requires low
computational resources and maintains the visual quality of the images.
Experimental results show that, the backdoor detection rate of the proposed
defense method is 99.63%, 99.76% and 99.91% on Fashion-MNIST, CIFAR-10 and
GTSRB datasets, respectively. Besides, the proposed method maintains the visual
quality of the image as the l2 norm of the added perturbation are as low as
2.8715, 3.0513 and 2.4362 on Fashion-MNIST, CIFAR-10 and GTSRB datasets,
respectively. In addition, it is also demonstrated that the proposed method can
achieve high defense performance against backdoor attacks under different
attack settings (trigger transparency, trigger size and trigger pattern).
Compared with the existing defense work (STRIP), the proposed method has better
detection performance on all the three datasets, and is more efficient than
STRIP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Mingfu Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yinghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yushu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GOO: A Dataset for Gaze Object Prediction in Retail Environments. (arXiv:2105.10793v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10793</id>
        <link href="http://arxiv.org/abs/2105.10793"/>
        <updated>2021-06-23T01:48:39.522Z</updated>
        <summary type="html"><![CDATA[One of the most fundamental and information-laden actions humans do is to
look at objects. However, a survey of current works reveals that existing
gaze-related datasets annotate only the pixel being looked at, and not the
boundaries of a specific object of interest. This lack of object annotation
presents an opportunity for further advancing gaze estimation research. To this
end, we present a challenging new task called gaze object prediction, where the
goal is to predict a bounding box for a person's gazed-at object. To train and
evaluate gaze networks on this task, we present the Gaze On Objects (GOO)
dataset. GOO is composed of a large set of synthetic images (GOO Synth)
supplemented by a smaller subset of real images (GOO-Real) of people looking at
objects in a retail environment. Our work establishes extensive baselines on
GOO by re-implementing and evaluating selected state-of-the art models on the
task of gaze following and domain adaptation. Code is available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomas_H/0/1/0/all/0/1"&gt;Henri Tomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1"&gt;Marcus Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dionido_R/0/1/0/all/0/1"&gt;Raimarc Dionido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ty_M/0/1/0/all/0/1"&gt;Mark Ty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirando_J/0/1/0/all/0/1"&gt;Jonric Mirando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casimiro_J/0/1/0/all/0/1"&gt;Joel Casimiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1"&gt;Rowel Atienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guinto_R/0/1/0/all/0/1"&gt;Richard Guinto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Adversarial Training against Universal Patches. (arXiv:2101.11453v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11453</id>
        <link href="http://arxiv.org/abs/2101.11453"/>
        <updated>2021-06-23T01:48:39.515Z</updated>
        <summary type="html"><![CDATA[Recently demonstrated physical-world adversarial attacks have exposed
vulnerabilities in perception systems that pose severe risks for
safety-critical applications such as autonomous driving. These attacks place
adversarial artifacts in the physical world that indirectly cause the addition
of a universal patch to inputs of a model that can fool it in a variety of
contexts. Adversarial training is the most effective defense against
image-dependent adversarial attacks. However, tailoring adversarial training to
universal patches is computationally expensive since the optimal universal
patch depends on the model weights which change during training. We propose
meta adversarial training (MAT), a novel combination of adversarial training
with meta-learning, which overcomes this challenge by meta-learning universal
patches along with model training. MAT requires little extra computation while
continuously adapting a large set of patches to the current model. MAT
considerably increases robustness against universal patch attacks on image
classification and traffic-light detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Metzen_J/0/1/0/all/0/1"&gt;Jan Hendrik Metzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finnie_N/0/1/0/all/0/1"&gt;Nicole Finnie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hutmacher_R/0/1/0/all/0/1"&gt;Robin Hutmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Reliable Probabilistic Face Embeddings in the Wild. (arXiv:2102.04075v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04075</id>
        <link href="http://arxiv.org/abs/2102.04075"/>
        <updated>2021-06-23T01:48:39.508Z</updated>
        <summary type="html"><![CDATA[Probabilistic Face Embeddings (PFE) can improve face recognition performance
in unconstrained scenarios by integrating data uncertainty into the feature
representation. However, existing PFE methods tend to be over-confident in
estimating uncertainty and is too slow to apply to large-scale face matching.
This paper proposes a regularized probabilistic face embedding method to
improve the robustness and speed of PFE. Specifically, the mutual likelihood
score (MLS) metric used in PFE is simplified to speedup the matching of face
feature pairs. Then, an output-constraint loss is proposed to penalize the
variance of the uncertainty output, which can regularize the output of the
neural network. In addition, an identification preserving loss is proposed to
improve the discriminative of the MLS metric, and a multi-layer feature fusion
module is proposed to improve the neural network's uncertainty estimation
ability. Comprehensive experiments show that the proposed method can achieve
comparable or better results in 9 benchmarks than the state-of-the-art methods,
and can improve the performance of risk-controlled face recognition. The code
of our work is publicly available in GitHub
(https://github.com/KaenChan/ProbFace).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1"&gt;Qi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_T/0/1/0/all/0/1"&gt;Taihe Yi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Points to Multi-Object 3D Reconstruction. (arXiv:2012.11575v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11575</id>
        <link href="http://arxiv.org/abs/2012.11575"/>
        <updated>2021-06-23T01:48:39.499Z</updated>
        <summary type="html"><![CDATA[We propose a method to detect and reconstruct multiple 3D objects from a
single RGB image. The key idea is to optimize for detection, alignment and
shape jointly over all objects in the RGB image, while focusing on realistic
and physically plausible reconstructions. To this end, we propose a keypoint
detector that localizes objects as center points and directly predicts all
object properties, including 9-DoF bounding boxes and 3D shapes -- all in a
single forward pass. The proposed method formulates 3D shape reconstruction as
a shape selection problem, i.e. it selects among exemplar shapes from a given
database. This makes it agnostic to shape representations, which enables a
lightweight reconstruction of realistic and visually-pleasing shapes based on
CAD-models, while the training objective is formulated around point clouds and
voxel representations. A collision-loss promotes non-intersecting objects,
further increasing the reconstruction realism. Given the RGB image, the
presented approach performs lightweight reconstruction in a single-stage, it is
real-time capable, fully differentiable and end-to-end trainable. Our
experiments compare multiple approaches for 9-DoF bounding box estimation,
evaluate the novel shape-selection mechanism and compare to recent methods in
terms of 3D bounding box estimation and 3D shape reconstruction quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1"&gt;Francis Engelmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1"&gt;Konstantinos Rematas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1"&gt;Vittorio Ferrari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Guided Radiology Report Generation. (arXiv:2106.10887v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10887</id>
        <link href="http://arxiv.org/abs/2106.10887"/>
        <updated>2021-06-23T01:48:39.477Z</updated>
        <summary type="html"><![CDATA[Medical imaging plays a pivotal role in diagnosis and treatment in clinical
practice. Inspired by the significant progress in automatic image captioning,
various deep learning (DL)-based architectures have been proposed for
generating radiology reports for medical images. However, model uncertainty
(i.e., model reliability/confidence on report generation) is still an
under-explored problem. In this paper, we propose a novel method to explicitly
quantify both the visual uncertainty and the textual uncertainty for the task
of radiology report generation. Such multi-modal uncertainties can sufficiently
capture the model confidence scores at both the report-level and the
sentence-level, and thus they are further leveraged to weight the losses for
achieving more comprehensive model optimization. Our experimental results have
demonstrated that our proposed method for model uncertainty characterization
and estimation can provide more reliable confidence scores for radiology report
generation, and our proposed uncertainty-weighted losses can achieve more
comprehensive model optimization and result in state-of-the-art performance on
a public radiology report dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zihao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1"&gt;Jiang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jianping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02638</id>
        <link href="http://arxiv.org/abs/2106.02638"/>
        <updated>2021-06-23T01:48:39.469Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process the matching and segmentation decoding of multiple
objects as efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),
while keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T
can maintain real-time multi-object speed on the above benchmarks. We ranked
1st in the 3rd Large-scale Video Object Segmentation Challenge. The code will
be publicly available at https://github.com/z-x-yang/AOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Reducing Labeling Cost in Deep Object Detection. (arXiv:2106.11921v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11921</id>
        <link href="http://arxiv.org/abs/2106.11921"/>
        <updated>2021-06-23T01:48:39.461Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have reached very high accuracy on object detection but
their success hinges on large amounts of labeled data. To reduce the dependency
on labels, various active-learning strategies have been proposed, typically
based on the confidence of the detector. However, these methods are biased
towards best-performing classes and can lead to acquired datasets that are not
good representatives of the data in the testing set. In this work, we propose a
unified framework for active learning, that considers both the uncertainty and
the robustness of the detector, ensuring that the network performs accurately
in all classes. Furthermore, our method is able to pseudo-label the very
confident predictions, suppressing a potential distribution drift while further
boosting the performance of the model. Experiments show that our method
comprehensively outperforms a wide range of active-learning methods on PASCAL
VOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%
reduction in labeling cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1"&gt;Ismail Elezi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Image Super-Resolution with Multi-scale Feature Interaction Network. (arXiv:2103.13028v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13028</id>
        <link href="http://arxiv.org/abs/2103.13028"/>
        <updated>2021-06-23T01:48:39.454Z</updated>
        <summary type="html"><![CDATA[Recently, the single image super-resolution (SISR) approaches with deep and
complex convolutional neural network structures have achieved promising
performance. However, those methods improve the performance at the cost of
higher memory consumption, which is difficult to be applied for some mobile
devices with limited storage and computing resources. To solve this problem, we
present a lightweight multi-scale feature interaction network (MSFIN). For
lightweight SISR, MSFIN expands the receptive field and adequately exploits the
informative features of the low-resolution observed images from various scales
and interactive connections. In addition, we design a lightweight recurrent
residual channel attention block (RRCAB) so that the network can benefit from
the channel attention mechanism while being sufficiently lightweight. Extensive
experiments on some benchmarks have confirmed that our proposed MSFIN can
achieve comparable performance against the state-of-the-arts with a more
lightweight model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengxue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_G/0/1/0/all/0/1"&gt;Guangwei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Juncheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1"&gt;Huimin Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Temporal Action Localization Through Local-Global Background Modeling. (arXiv:2106.11811v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11811</id>
        <link href="http://arxiv.org/abs/2106.11811"/>
        <updated>2021-06-23T01:48:39.447Z</updated>
        <summary type="html"><![CDATA[Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to
recognize and localize temporal starts and ends of action instances in an
untrimmed video with only video-level label supervision. Due to lack of
negative samples of background category, it is difficult for the network to
separate foreground and background, resulting in poor detection performance. In
this report, we present our 2021 HACS Challenge - Weakly-supervised Learning
Track solution that based on BaSNet to address above problem. Specifically, we
first adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to
get feature sequences. Then our proposed Local-Global Background Modeling
Network (LGBM-Net) is trained to localize instances by using only video-level
labels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple
models to get the final detection results and reach 22.45% mAP on the test set]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1"&gt;Zhiwu Qing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yutong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jianwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yuanjie Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03515</id>
        <link href="http://arxiv.org/abs/2012.03515"/>
        <updated>2021-06-23T01:48:39.424Z</updated>
        <summary type="html"><![CDATA[Few-shot learning methods offer pre-training techniques optimized for easier
later adaptation of the model to new classes (unseen during training) using one
or a few examples. This adaptivity to unseen classes is especially important
for many practical applications where the pre-trained label space cannot remain
fixed for effective use and the model needs to be "specialized" to support new
categories on the fly. One particularly interesting scenario, essentially
overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where
the training classes (e.g. animals) are of much `coarser granularity' than the
target (test) classes (e.g. breeds). A very practical example of C2FS is when
the target classes are sub-classes of the training classes. Intuitively, it is
especially challenging as (both regular and few-shot) supervised pre-training
tends to learn to ignore intra-class variability which is essential for
separating sub-classes. In this paper, we introduce a novel 'Angular
normalization' module that allows to effectively combine supervised and
self-supervised contrastive pre-training to approach the proposed C2FS task,
demonstrating significant gains in a broad study over multiple baselines and
datasets. We hope that this work will help to pave the way for future research
on this new, challenging, and very practical topic of C2FS classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1"&gt;Guy Bukchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1"&gt;Eli Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1"&gt;Ori Shahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in Point-Cloud Technology. (arXiv:2106.11902v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11902</id>
        <link href="http://arxiv.org/abs/2106.11902"/>
        <updated>2021-06-23T01:48:39.416Z</updated>
        <summary type="html"><![CDATA[With the advancement of deep neural networks and computer vision-based Human
Activity Recognition, employment of Point-Cloud Data technologies (LiDAR,
mmWave) has seen a lot interests due to its privacy preserving nature. Given
the high promise of accurate PCD technologies, we develop, PALMAR, a
multiple-inhabitant activity recognition system by employing efficient signal
processing and novel machine learning techniques to track individual person
towards developing an adaptive multi-inhabitant tracking and HAR system. More
specifically, we propose (i) a voxelized feature representation-based real-time
PCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive
Order Hidden Markov Model based multi-person tracking and crossover ambiguity
reduction techniques and (iii) novel adaptive deep learning-based domain
adaptation technique to improve the accuracy of HAR in presence of data
scarcity and diversity (device, location and population diversity). We
experimentally evaluate our framework and systems using (i) a real-time PCD
collected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants,
(ii) one publicly available 3D LiDAR activity data (28 participants) and (iii)
an embedded hardware prototype system which provided promising HAR performances
in multi-inhabitants (96%) scenario with a 63% improvement of multi-person
tracking than state-of-art framework without losing significant system
performances in the edge computing device.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Mohammad Arif Ul Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mahmudur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Widberg_J/0/1/0/all/0/1"&gt;Jared Q Widberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Resizing by Reconstruction from Deep Features. (arXiv:1904.08475v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.08475</id>
        <link href="http://arxiv.org/abs/1904.08475"/>
        <updated>2021-06-23T01:48:39.408Z</updated>
        <summary type="html"><![CDATA[Traditional image resizing methods usually work in pixel space and use
various saliency measures. The challenge is to adjust the image shape while
trying to preserve important content. In this paper we perform image resizing
in feature space where the deep layers of a neural network contain rich
important semantic information. We directly adjust the image feature maps,
extracted from a pre-trained classification network, and reconstruct the
resized image using a neural-network based optimization. This novel approach
leverages the hierarchical encoding of the network, and in particular, the
high-level discriminative power of its deeper layers, that recognizes semantic
objects and regions and allows maintaining their aspect ratio. Our use of
reconstruction from deep features diminishes the artifacts introduced by
image-space resizing operators. We evaluate our method on benchmarks, compare
to alternative approaches, and demonstrate its strength on challenging images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1"&gt;Moab Arar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1"&gt;Dov Danon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RUHSNet: 3D Object Detection Using Lidar Data in Real Time. (arXiv:2006.01250v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01250</id>
        <link href="http://arxiv.org/abs/2006.01250"/>
        <updated>2021-06-23T01:48:39.400Z</updated>
        <summary type="html"><![CDATA[In this work, we address the problem of 3D object detection from point cloud
data in real time. For autonomous vehicles to work, it is very important for
the perception component to detect the real world objects with both high
accuracy and fast inference. We propose a novel neural network architecture
along with the training and optimization details for detecting 3D objects in
point cloud data. We compare the results with different backbone architectures
including the standard ones like VGG, ResNet, Inception with our backbone. Also
we present the optimization and ablation studies including designing an
efficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking
and validating our results. Our work surpasses the state of the art in this
domain both in terms of average precision and speed running at > 30 FPS. This
makes it a feasible option to be deployed in real time applications including
self driving cars.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving. (arXiv:2106.11118v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11118</id>
        <link href="http://arxiv.org/abs/2106.11118"/>
        <updated>2021-06-23T01:48:39.391Z</updated>
        <summary type="html"><![CDATA[Aiming at facilitating a real-world, ever-evolving and scalable autonomous
driving system, we present a large-scale benchmark for standardizing the
evaluation of different self-supervised and semi-supervised approaches by
learning from raw data, which is the first and largest benchmark to date.
Existing autonomous driving systems heavily rely on `perfect' visual perception
models (e.g., detection) trained using extensive annotated data to ensure the
safety. However, it is unrealistic to elaborately label instances of all
scenarios and circumstances (e.g., night, extreme weather, cities) when
deploying a robust autonomous driving system. Motivated by recent powerful
advances of self-supervised and semi-supervised learning, a promising direction
is to learn a robust detection model by collaboratively exploiting large-scale
unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo)
either provides only a small amount of data or covers limited domains with full
annotation, hindering the exploration of large-scale pre-trained models. Here,
we release a Large-Scale Object Detection benchmark for Autonomous driving,
named as SODA10M, containing 10 million unlabeled images and 20K images labeled
with 6 representative object categories. To improve diversity, the images are
collected every ten seconds per frame within 32 different cities under
different weather conditions, periods and location scenes. We provide extensive
experiments and deep analyses of existing supervised state-of-the-art detection
models, popular self-supervised and semi-supervised approaches, and some
insights about how to develop future models. The data and more up-to-date
information have been released at https://soda-2d.github.io.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jianhua Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiwen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Confidence-Aware Learning for Camouflaged Object Detection. (arXiv:2106.11641v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11641</id>
        <link href="http://arxiv.org/abs/2106.11641"/>
        <updated>2021-06-23T01:48:39.371Z</updated>
        <summary type="html"><![CDATA[Confidence-aware learning is proven as an effective solution to prevent
networks becoming overconfident. We present a confidence-aware camouflaged
object detection framework using dynamic supervision to produce both accurate
camouflage map and meaningful "confidence" representing model awareness about
the current prediction. A camouflaged object detection network is designed to
produce our camouflage prediction. Then, we concatenate it with the input image
and feed it to the confidence estimation network to produce an one channel
confidence map.We generate dynamic supervision for the confidence estimation
network, representing the agreement of camouflage prediction with the ground
truth camouflage map. With the produced confidence map, we introduce
confidence-aware learning with the confidence map as guidance to pay more
attention to the hard/low-confidence pixels in the loss function. We claim
that, once trained, our confidence estimation network can evaluate pixel-wise
accuracy of the prediction without relying on the ground truth camouflage map.
Extensive results on four camouflaged object detection testing datasets
illustrate the superior performance of the proposed model in explaining the
camouflage prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Latent Transformer for Disentangled and Identity-Preserving Face Editing. (arXiv:2106.11895v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11895</id>
        <link href="http://arxiv.org/abs/2106.11895"/>
        <updated>2021-06-23T01:48:39.364Z</updated>
        <summary type="html"><![CDATA[High quality facial image editing is a challenging problem in the movie
post-production industry, requiring a high degree of control and identity
preservation. Previous works that attempt to tackle this problem may suffer
from the entanglement of facial attributes and the loss of the person's
identity. Furthermore, many algorithms are limited to a certain task. To tackle
these limitations, we propose to edit facial attributes via the latent space of
a StyleGAN generator, by training a dedicated latent transformation network and
incorporating explicit disentanglement and identity preservation terms in the
loss function. We further introduce a pipeline to generalize our face editing
to videos. Our model achieves a disentangled, controllable, and
identity-preserving facial attribute editing, even in the challenging case of
real (i.e., non-synthetic) images and videos. We conduct extensive experiments
on image and video datasets and show that our model outperforms other
state-of-the-art methods in visual quality and quantitative evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1"&gt;Alasdair Newson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1"&gt;Yann Gousseau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1"&gt;Pierre Hellier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Patch-level Classification of Deep Learning Methods on Transparent Images: from Convolutional Neural Networks to Visual Transformers. (arXiv:2106.11582v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11582</id>
        <link href="http://arxiv.org/abs/2106.11582"/>
        <updated>2021-06-23T01:48:39.357Z</updated>
        <summary type="html"><![CDATA[Nowadays, analysis of transparent images in the field of computer vision has
gradually become a hot spot. In this paper, we compare the classification
performance of different deep learning for the problem that transparent images
are difficult to analyze. We crop the transparent images into 8 * 8 and 224 *
224 pixels patches in the same proportion, and then divide the two different
pixels patches into foreground and background according to groundtruch. We also
use 4 types of convolutional neural networks and a novel ViT network model to
compare the foreground and background classification experiments. We conclude
that ViT performs the worst in classifying 8 * 8 pixels patches, but it
outperforms most convolutional neural networks in classifying 224 * 224.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11858</id>
        <link href="http://arxiv.org/abs/2106.11858"/>
        <updated>2021-06-23T01:48:39.349Z</updated>
        <summary type="html"><![CDATA[Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising image regions,
in each acquisition step. The problem is framed in an exploration-exploitation
framework by combining an embedding based on Uniform Manifold Approximation to
model representativeness with entropy as uncertainty measure to model
informativeness. We applied our proposed method to the challenging autonomous
driving data sets CamVid and Cityscapes and performed a quantitative comparison
with state-of-the-art methods. We find that our active learning method achieves
better performance on CamVid compared to other methods, while on Cityscapes,
the performance lift was negligible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1"&gt;Deepthi Sreenivasaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1"&gt;Thomas Wollmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Momentum Contrastive Learning for Few-Shot Classification. (arXiv:2101.11058v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11058</id>
        <link href="http://arxiv.org/abs/2101.11058"/>
        <updated>2021-06-23T01:48:39.341Z</updated>
        <summary type="html"><![CDATA[Few-shot learning aims to transfer information from one task to enable
generalization on novel tasks given a few examples. This information is present
both in the domain and the class labels. In this work we investigate the
complementary roles of these two sources of information by combining
instance-discriminative contrastive learning and supervised learning in a
single framework called Supervised Momentum Contrastive learning (SUPMOCO). Our
approach avoids a problem observed in supervised learning where information in
images not relevant to the task is discarded, which hampers their
generalization to novel tasks. We show that (self-supervised) contrastive
learning and supervised learning are mutually beneficial, leading to a new
state-of-the-art on the META-DATASET - a recently introduced benchmark for
few-shot learning. Our method is based on a simple modification of MOCO and
scales better than prior work on combining supervised and self-supervised
learning. This allows us to easily combine data from multiple domains leading
to further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_O/0/1/0/all/0/1"&gt;Orchid Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate High Resolution Images With Generative Variational Autoencoder. (arXiv:2008.10399v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10399</id>
        <link href="http://arxiv.org/abs/2008.10399"/>
        <updated>2021-06-23T01:48:39.320Z</updated>
        <summary type="html"><![CDATA[In this work, we present a novel neural network to generate high resolution
images. We replace the decoder of VAE with a discriminator while using the
encoder as it is. The encoder is fed data from a normal distribution while the
generator is fed from a gaussian distribution. The combination from both is
given to a discriminator which tells whether the generated image is correct or
not. We evaluate our network on 3 different datasets: MNIST, LSUN and CelebA
dataset. Our network beats the previous state of the art using MMD, SSIM, log
likelihood, reconstruction error, ELBO and KL divergence as the evaluation
metrics while generating much sharper images. This work is potentially very
exciting as we are able to combine the advantages of generative models and
inference models in a principled bayesian manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1"&gt;Abhinav Sagar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11795</id>
        <link href="http://arxiv.org/abs/2106.11795"/>
        <updated>2021-06-23T01:48:39.313Z</updated>
        <summary type="html"><![CDATA[Geometric Deep Learning has recently made striking progress with the advent
of continuous Deep Implicit Fields. They allow for detailed modeling of
watertight surfaces of arbitrary topology while not relying on a 3D Euclidean
grid, resulting in a learnable parameterization that is unlimited in
resolution. Unfortunately, these methods are often unsuitable for applications
that require an explicit mesh-based surface representation because converting
an implicit field to such a representation relies on the Marching Cubes
algorithm, which cannot be differentiated with respect to the underlying
implicit field. In this work, we remove this limitation and introduce a
differentiable way to produce explicit surface mesh representations from Deep
Implicit Fields. Our key insight is that by reasoning on how implicit field
perturbations impact local surface geometry, one can ultimately differentiate
the 3D location of surface samples with respect to the underlying deep implicit
field. We exploit this to define DeepMesh -- end-to-end differentiable mesh
representation that can vary its topology. We use two different applications to
validate our theoretical insight: Single view 3D Reconstruction via
Differentiable Rendering and Physically-Driven Shape Optimization. In both
cases our end-to-end differentiable parameterization gives us an edge over
state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1"&gt;Benoit Guillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1"&gt;Edoardo Remelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1"&gt;Artem Lukoianov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1"&gt;Stephan Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1"&gt;Timur Bagautdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1"&gt;Pierre Baque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hessian-Aware Pruning and Optimal Neural Implant. (arXiv:2101.08940v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08940</id>
        <link href="http://arxiv.org/abs/2101.08940"/>
        <updated>2021-06-23T01:48:39.305Z</updated>
        <summary type="html"><![CDATA[Pruning is an effective method to reduce the memory footprint and FLOPs
associated with neural network models. However, existing structured-pruning
methods often result in significant accuracy degradation for moderate pruning
levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP)
method coupled with a Neural Implant approach that uses second-order
sensitivity as a metric for structured pruning. The basic idea is to prune
insensitive components and to use a Neural Implant for moderately sensitive
components, instead of completely pruning them. For the latter approach, the
moderately sensitive components are replaced with with a low rank implant that
is smaller and less computationally expensive than the original component. We
use the relative Hessian trace to measure sensitivity, as opposed to the
magnitude based sensitivity metric commonly used in the literature. We test HAP
for both computer vision tasks and natural language tasks, and we achieve new
state-of-the-art results. Specifically, HAP achieves less than $0.1\%$/$0.5\%$
degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than
70\%/50\% of parameters pruned. Meanwhile, HAP also achieves significantly
better performance (up to 0.8\% with 60\% of parameters pruned) as compared to
gradient based method for head pruning on transformer-based models. The
framework has been open sourced and available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shixing Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhewei Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1"&gt;Amir Gholami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sehoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W Mahoney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.11756</id>
        <link href="http://arxiv.org/abs/2106.11756"/>
        <updated>2021-06-23T01:48:39.295Z</updated>
        <summary type="html"><![CDATA[We present a no-code Artificial Intelligence (AI) platform called Trinity
with the main design goal of enabling both machine learning researchers and
non-technical geospatial domain experts to experiment with domain-specific
signals and datasets for solving a variety of complex problems on their own.
This versatility to solve diverse problems is achieved by transforming complex
Spatio-temporal datasets to make them consumable by standard deep learning
models, in this case, Convolutional Neural Networks (CNNs), and giving the
ability to formulate disparate problems in a standard way, eg. semantic
segmentation. With an intuitive user interface, a feature store that hosts
derivatives of complex feature engineering, a deep learning kernel, and a
scalable data processing mechanism, Trinity provides a powerful platform for
domain experts to share the stage with scientists and engineers in solving
business-critical problems. It enables quick prototyping, rapid experimentation
and reduces the time to production by standardizing model building and
deployment. In this paper, we present our motivation behind Trinity and its
design along with showcasing sample applications to motivate the idea of
lowering the bar to using AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1"&gt;C.V.Krishnakumar Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1"&gt;Feili Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Henry Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yonghong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1"&gt;Kay Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1"&gt;Swetava Ganguli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1"&gt;Vipul Pandey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of a Region Proposal Architecture for Multi-task Document Layout Analysis. (arXiv:2106.11797v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11797</id>
        <link href="http://arxiv.org/abs/2106.11797"/>
        <updated>2021-06-23T01:48:39.287Z</updated>
        <summary type="html"><![CDATA[Automatically recognizing the layout of handwritten documents is an important
step towards useful extraction of information from those documents. The most
common application is to feed downstream applications such as automatic text
recognition and keyword spotting; however, the recognition of the layout also
helps to establish relationships between elements in the document which allows
to enrich the information that can be extracted. Most of the modern document
layout analysis systems are designed to address only one part of the document
layout problem, namely: baseline detection or region segmentation. In contrast,
we evaluate the effectiveness of the Mask-RCNN architecture to address the
problem of baseline detection and region segmentation in an integrated manner.
We present experimental results on two handwritten text datasets and one
handwritten music dataset. The analyzed architecture yields promising results,
outperforming state-of-the-art techniques in all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_L/0/1/0/all/0/1"&gt;Lorenzo Quir&amp;#xf3;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vidal_E/0/1/0/all/0/1"&gt;Enrique Vidal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Untrained networks for compressive lensless photography. (arXiv:2103.07609v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07609</id>
        <link href="http://arxiv.org/abs/2103.07609"/>
        <updated>2021-06-23T01:48:39.264Z</updated>
        <summary type="html"><![CDATA[Compressive lensless imagers enable novel applications in an extremely
compact device, requiring only a phase or amplitude mask placed close to the
sensor. They have been demonstrated for 2D and 3D microscopy, single-shot
video, and single-shot hyperspectral imaging; in each of these cases, a
compressive-sensing-based inverse problem is solved in order to recover a 3D
data-cube from a 2D measurement. Typically, this is accomplished using convex
optimization and hand-picked priors. Alternatively, deep learning-based
reconstruction methods offer the promise of better priors, but require many
thousands of ground truth training pairs, which can be difficult or impossible
to acquire. In this work, we propose the use of untrained networks for
compressive image recovery. Our approach does not require any labeled training
data, but instead uses the measurement itself to update the network weights. We
demonstrate our untrained approach on lensless compressive 2D imaging as well
as single-shot high-speed video recovery using the camera's rolling shutter,
and single-shot hyperspectral imaging. We provide simulation and experimental
verification, showing that our method results in improved image quality over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Monakhova_K/0/1/0/all/0/1"&gt;Kristina Monakhova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vi Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuo_G/0/1/0/all/0/1"&gt;Grace Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Waller_L/0/1/0/all/0/1"&gt;Laura Waller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhanced Separable Disentanglement for Unsupervised Domain Adaptation. (arXiv:2106.11915v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11915</id>
        <link href="http://arxiv.org/abs/2106.11915"/>
        <updated>2021-06-23T01:48:39.256Z</updated>
        <summary type="html"><![CDATA[Domain adaptation aims to mitigate the domain gap when transferring knowledge
from an existing labeled domain to a new domain. However, existing
disentanglement-based methods do not fully consider separation between
domain-invariant and domain-specific features, which means the domain-invariant
features are not discriminative. The reconstructed features are also not
sufficiently used during training. In this paper, we propose a novel enhanced
separable disentanglement (ESD) model. We first employ a disentangler to
distill domain-invariant and domain-specific features. Then, we apply feature
separation enhancement processes to minimize contamination between
domain-invariant and domain-specific features. Finally, our model reconstructs
complete feature vectors, which are used for further disentanglement during the
training phase. Extensive experiments from three benchmark datasets outperform
state-of-the-art methods, especially on challenging cross-domain tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youshan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1"&gt;Brian D. Davison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation. (arXiv:2106.11920v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11920</id>
        <link href="http://arxiv.org/abs/2106.11920"/>
        <updated>2021-06-23T01:48:39.242Z</updated>
        <summary type="html"><![CDATA[Analyzing the structure of proteins is a key part of understanding their
functions and thus their role in biology at the molecular level. In addition,
design new proteins in a methodical way is a major engineering challenge. In
this work, we introduce a joint geometric-neural networks approach for
comparing, deforming and generating 3D protein structures. Viewing protein
structures as 3D open curves, we adopt the Square Root Velocity Function (SRVF)
representation and leverage its suitable geometric properties along with Deep
Residual Networks (ResNets) for a joint registration and comparison. Our
ResNets handle better large protein deformations while being more
computationally efficient. On top of the mathematical framework, we further
design a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps
original, previously unseen structures, into a low-dimensional (latent)
hyper-sphere. Motivated by the spherical structure of the pre-shape space, we
naturally adopt the von Mises-Fisher (vMF) distribution to model our hidden
variables. We test the effectiveness of our models by generating novel protein
structures and predicting completions of corrupted protein structures.
Experimental results show that our method is able to generate plausible
structures, different from the structures in the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amor_B/0/1/0/all/0/1"&gt;Boulbaba Ben Amor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xichan Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yi Fang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:39.232Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation for Meta-Learning. (arXiv:2010.07092v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07092</id>
        <link href="http://arxiv.org/abs/2010.07092"/>
        <updated>2021-06-23T01:48:39.223Z</updated>
        <summary type="html"><![CDATA[Conventional image classifiers are trained by randomly sampling mini-batches
of images. To achieve state-of-the-art performance, practitioners use
sophisticated data augmentation schemes to expand the amount of training data
available for sampling. In contrast, meta-learning algorithms sample support
data, query data, and tasks on each training step. In this complex sampling
scenario, data augmentation can be used not only to expand the number of images
available per class, but also to generate entirely new classes/tasks. We
systematically dissect the meta-learning pipeline and investigate the distinct
ways in which data augmentation can be integrated at both the image and class
levels. Our proposed meta-specific data augmentation significantly improves the
performance of meta-learners on few-shot classification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1"&gt;Renkun Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1"&gt;Amr Sharaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EC-GAN: Low-Sample Classification using Semi-Supervised Algorithms and GANs. (arXiv:2012.15864v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15864</id>
        <link href="http://arxiv.org/abs/2012.15864"/>
        <updated>2021-06-23T01:48:39.202Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning has been gaining attention as it allows for
performing image analysis tasks such as classification with limited labeled
data. Some popular algorithms using Generative Adversarial Networks (GANs) for
semi-supervised classification share a single architecture for classification
and discrimination. However, this may require a model to converge to a separate
data distribution for each task, which may reduce overall performance. While
progress in semi-supervised learning has been made, less addressed are
small-scale, fully-supervised tasks where even unlabeled data is unavailable
and unattainable. We therefore, propose a novel GAN model namely External
Classifier GAN (EC-GAN), that utilizes GANs and semi-supervised algorithms to
improve classification in fully-supervised regimes. Our method leverages a GAN
to generate artificial data used to supplement supervised classification. More
specifically, we attach an external classifier, hence the name EC-GAN, to the
GAN's generator, as opposed to sharing an architecture with the discriminator.
Our experiments demonstrate that EC-GAN's performance is comparable to the
shared architecture method, far superior to the standard data augmentation and
regularization-based approach, and effective on a small, realistic dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1"&gt;Ayaan Haque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition. (arXiv:2106.11613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11613</id>
        <link href="http://arxiv.org/abs/2106.11613"/>
        <updated>2021-06-23T01:48:39.190Z</updated>
        <summary type="html"><![CDATA[Chinese character recognition has attracted much research interest due to its
wide applications. Although it has been studied for many years, some issues in
this field have not been completely resolved yet, e.g. the zero-shot problem.
Previous character-based and radical-based methods have not fundamentally
addressed the zero-shot problem since some characters or radicals in test sets
may not appear in training sets under a data-hungry condition. Inspired by the
fact that humans can generalize to know how to write characters unseen before
if they have learned stroke orders of some characters, we propose a
stroke-based method by decomposing each character into a sequence of strokes,
which are the most basic units of Chinese characters. However, we observe that
there is a one-to-many relationship between stroke sequences and Chinese
characters. To tackle this challenge, we employ a matching-based strategy to
transform the predicted stroke sequence to a specific character. We evaluate
the proposed method on handwritten characters, printed artistic characters, and
scene characters. The experimental results validate that the proposed method
outperforms existing methods on both character zero-shot and radical zero-shot
tasks. Moreover, the proposed method can be easily generalized to other
languages whose characters can be decomposed into strokes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingye Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xiangyang Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Hierarchy Preserving Deep Hashing for Large-scale Image Retrieval. (arXiv:1901.11259v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.11259</id>
        <link href="http://arxiv.org/abs/1901.11259"/>
        <updated>2021-06-23T01:48:39.176Z</updated>
        <summary type="html"><![CDATA[Deep hashing models have been proposed as an efficient method for large-scale
similarity search. However, most existing deep hashing methods only utilize
fine-level labels for training while ignoring the natural semantic hierarchy
structure. This paper presents an effective method that preserves the classwise
similarity of full-level semantic hierarchy for large-scale image retrieval.
Experiments on two benchmark datasets show that our method helps improve the
fine-level retrieval performance. Moreover, with the help of the semantic
hierarchy, it can produce significantly better binary codes for hierarchical
retrieval, which indicates its potential of providing more user-desired
retrieval results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1"&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_Yang_L/0/1/0/all/0/1"&gt;Le Ou-Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hong Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Hitchhiker's Guide to Prior-Shift Adaptation. (arXiv:2106.11695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11695</id>
        <link href="http://arxiv.org/abs/2106.11695"/>
        <updated>2021-06-23T01:48:39.170Z</updated>
        <summary type="html"><![CDATA[In many computer vision classification tasks, class priors at test time often
differ from priors on the training set. In the case of such prior shift,
classifiers must be adapted correspondingly to maintain close to optimal
performance. This paper analyzes methods for adaptation of probabilistic
classifiers to new priors and for estimating new priors on an unlabeled test
set. We propose a novel method to address a known issue of prior estimation
methods based on confusion matrices, where inconsistent estimates of decision
probabilities and confusion matrices lead to negative values in the estimated
priors. Experiments on fine-grained image classification datasets provide
insight into the best practice of prior shift estimation and classifier
adaptation and show that the proposed method achieves state-of-the-art results
in prior adaptation. Applying the best practice to two tasks with naturally
imbalanced priors, learning from web-crawled images and plant species
classification, increased the recognition accuracy by 1.1% and 3.4%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sipka_T/0/1/0/all/0/1"&gt;Tomas Sipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sulc_M/0/1/0/all/0/1"&gt;Milan Sulc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stealthy and Robust Fingerprinting Scheme for Generative Models. (arXiv:2106.11760v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.11760</id>
        <link href="http://arxiv.org/abs/2106.11760"/>
        <updated>2021-06-23T01:48:39.163Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel fingerprinting methodology for the Intellectual
Property protection of generative models. Prior solutions for discriminative
models usually adopt adversarial examples as the fingerprints, which give
anomalous inference behaviors and prediction results. Hence, these methods are
not stealthy and can be easily recognized by the adversary. Our approach
leverages the invisible backdoor technique to overcome the above limitation.
Specifically, we design verification samples, whose model outputs look normal
but can trigger a backdoor classifier to make abnormal predictions. We propose
a new backdoor embedding approach with Unique-Triplet Loss and fine-grained
categorization to enhance the effectiveness of our fingerprints. Extensive
evaluations show that this solution can outperform other strategies with higher
robustness, uniqueness and stealthiness for various GAN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guanlin_L/0/1/0/all/0/1"&gt;Li Guanlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shangwei_G/0/1/0/all/0/1"&gt;Guo Shangwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Run_W/0/1/0/all/0/1"&gt;Wang Run&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guowen_X/0/1/0/all/0/1"&gt;Xu Guowen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tianwei_Z/0/1/0/all/0/1"&gt;Zhang Tianwei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement. (arXiv:2106.11423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11423</id>
        <link href="http://arxiv.org/abs/2106.11423"/>
        <updated>2021-06-23T01:48:39.155Z</updated>
        <summary type="html"><![CDATA[We introduce a highly robust GAN-based framework for digitizing a normalized
3D avatar of a person from a single unconstrained photo. While the input image
can be of a smiling person or taken in extreme lighting conditions, our method
can reliably produce a high-quality textured model of a person's face in
neutral expression and skin textures under diffuse lighting condition.
Cutting-edge 3D face reconstruction methods use non-linear morphable face
models combined with GAN-based decoders to capture the likeness and details of
a person but fail to produce neutral head models with unshaded albedo textures
which is critical for creating relightable and animation-friendly avatars for
integration in virtual environments. The key challenges for existing methods to
work is the lack of training and ground truth data containing normalized 3D
faces. We propose a two-stage approach to address this problem. First, we adopt
a highly robust normalized 3D face generator by embedding a non-linear
morphable face model into a StyleGAN2 network. This allows us to generate
detailed but normalized facial assets. This inference is then followed by a
perceptual refinement step that uses the generated assets as regularization to
cope with the limited available training samples of normalized faces. We
further introduce a Normalized Face Dataset, which consists of a combination
photogrammetry scans, carefully selected photographs, and generated fake people
with neutral expressions in diffuse lighting conditions. While our prepared
dataset contains two orders of magnitude less subjects than cutting edge
GAN-based 3D facial reconstruction methods, we show that it is possible to
produce high-quality normalized face models for very challenging unconstrained
input images, and demonstrate superior performance to the current
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huiwen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1"&gt;Koki Nagano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1"&gt;Han-Wei Kung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldwhite_M/0/1/0/all/0/1"&gt;Mclean Goldwhite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qingguo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zejian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingyu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Liwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11810</id>
        <link href="http://arxiv.org/abs/2106.11810"/>
        <updated>2021-06-23T01:48:39.135Z</updated>
        <summary type="html"><![CDATA[In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1"&gt;Holger Caesar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1"&gt;Juraj Kabzan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1"&gt;Kok Seang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1"&gt;Whye Kit Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1"&gt;Eric Wolff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1"&gt;Alex Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1"&gt;Luke Fletcher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1"&gt;Oscar Beijbom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1"&gt;Sammy Omari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies from Single RGB Images. (arXiv:2106.11536v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11536</id>
        <link href="http://arxiv.org/abs/2106.11536"/>
        <updated>2021-06-23T01:48:39.127Z</updated>
        <summary type="html"><![CDATA[We introduce an approach that accurately reconstructs 3D human poses and
detailed 3D full-body geometric models from single images in realtime. The key
idea of our approach is a novel end-to-end multi-task deep learning framework
that uses single images to predict five outputs simultaneously: foreground
segmentation mask, 2D joints positions, semantic body partitions, 3D part
orientations and uv coordinates (uv map). The multi-task network architecture
not only generates more visual cues for reconstruction, but also makes each
individual prediction more accurate. The CNN regressor is further combined with
an optimization based algorithm for accurate kinematic pose reconstruction and
full-body shape modeling. We show that the realtime reconstruction reaches
accurate fitting that has not been seen before, especially for wild images. We
demonstrate the results of our realtime 3D pose and human body reconstruction
system on various challenging in-the-wild videos. We show the system advances
the frontier of 3D human body and pose reconstruction from single images by
quantitative evaluations and comparisons with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Liguo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Miaopeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Congyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Juntao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinguo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1"&gt;Jinxiang Chai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11776</id>
        <link href="http://arxiv.org/abs/2106.11776"/>
        <updated>2021-06-23T01:48:39.118Z</updated>
        <summary type="html"><![CDATA[Dietary-related problems such as obesity are a growing concern in todays
modern world. If the current trend continues, it is most likely that the
quality of life, in general, is significantly affected since obesity is
associated with other chronic diseases such as hypertension, irregular blood
sugar levels, and increased risk of heart attacks. The primary cause of these
problems is poor lifestyle choices and unhealthy dietary habits, with emphasis
on a select few food groups such as sugars, fats, and carbohydrates. In this
regard, computer-based food recognition offers automatic visual-based methods
to assess dietary intake and help people make healthier choices. Thus, the
following paper presents a brief review of visual-based methods for food
recognition, including their accuracy, performance, and the use of popular food
databases to evaluate existing models. The work further aims to highlight
future challenges in this area. New high-quality studies for developing
standard benchmarks and using continual learning methods for food recognition
are recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1"&gt;Ghalib Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1"&gt;Chu Kiong Loo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-Based Practical Light Field Image Compression Using A Disparity-Aware Model. (arXiv:2106.11558v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11558</id>
        <link href="http://arxiv.org/abs/2106.11558"/>
        <updated>2021-06-23T01:48:39.105Z</updated>
        <summary type="html"><![CDATA[Light field technology has increasingly attracted the attention of the
research community with its many possible applications. The lenslet array in
commercial plenoptic cameras helps capture both the spatial and angular
information of light rays in a single exposure. While the resulting high
dimensionality of light field data enables its superior capabilities, it also
impedes its extensive adoption. Hence, there is a compelling need for efficient
compression of light field images. Existing solutions are commonly composed of
several separate modules, some of which may not have been designed for the
specific structure and quality of light field data. This increases the
complexity of the codec and results in impractical decoding runtimes. We
propose a new learning-based, disparity-aided model for compression of 4D light
field images capable of parallel decoding. The model is end-to-end trainable,
eliminating the need for hand-tuning separate modules and allowing joint
learning of rate and distortion. The disparity-aided approach ensures the
structural integrity of the reconstructed light fields. Comparisons with the
state of the art show encouraging performance in terms of PSNR and MS-SSIM
metrics. Also, there is a notable gain in the encoding and decoding runtimes.
Source code is available at https://moha23.github.io/LFDAAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mohana Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rameshan_R/0/1/0/all/0/1"&gt;Renu M. Rameshan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Domain Adaptation in Ordinal Regression. (arXiv:2106.11576v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11576</id>
        <link href="http://arxiv.org/abs/2106.11576"/>
        <updated>2021-06-23T01:48:39.087Z</updated>
        <summary type="html"><![CDATA[We address the problem of universal domain adaptation (UDA) in ordinal
regression (OR), which attempts to solve classification problems in which
labels are not independent, but follow a natural order. We show that the UDA
techniques developed for classification and based on the clustering assumption,
under-perform in OR settings. We propose a method that complements the OR
classifier with an auxiliary task of order learning, which plays the double
role of discriminating between common and private instances, and expanding
class labels to the private target images via ranking. Combined with
adversarial domain discrimination, our model is able to address the closed set,
partial and open set configurations. We evaluate our method on three face age
estimation datasets, and show that it outperforms the baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boris_C/0/1/0/all/0/1"&gt;Chidlovskii Boris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1"&gt;Assem Sadek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1"&gt;Christian Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand-Drawn Electrical Circuit Recognition using Object Detection and Node Recognition. (arXiv:2106.11559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11559</id>
        <link href="http://arxiv.org/abs/2106.11559"/>
        <updated>2021-06-23T01:48:39.079Z</updated>
        <summary type="html"><![CDATA[With the recent developments in neural networks, there has been a resurgence
in algorithms for the automatic generation of simulation ready electronic
circuits from hand-drawn circuits. However, most of the approaches in
literature were confined to classify different types of electrical components
and only a few of those methods have shown a way to rebuild the circuit
schematic from the scanned image, which is extremely important for further
automation of netlist generation. This paper proposes a real-time algorithm for
the automatic recognition of hand-drawn electrical circuits based on object
detection and circuit node recognition. The proposed approach employs You Only
Look Once version 5 (YOLOv5) for detection of circuit components and a novel
Hough transform based approach for node recognition. Using YOLOv5 object
detection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in
detecting the components. The proposed method is also able to rebuild the
circuit schematic with 80% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1"&gt;Rachala Rohith Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panicker_M/0/1/0/all/0/1"&gt;Mahesh Raveendranatha Panicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-layered Semantic Representation Network for Multi-label Image Classification. (arXiv:2106.11596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11596</id>
        <link href="http://arxiv.org/abs/2106.11596"/>
        <updated>2021-06-23T01:48:39.072Z</updated>
        <summary type="html"><![CDATA[Multi-label image classification (MLIC) is a fundamental and practical task,
which aims to assign multiple possible labels to an image. In recent years,
many deep convolutional neural network (CNN) based approaches have been
proposed which model label correlations to discover semantics of labels and
learn semantic representations of images. This paper advances this research
direction by improving both the modeling of label correlations and the learning
of semantic representations. On the one hand, besides the local semantics of
each label, we propose to further explore global semantics shared by multiple
labels. On the other hand, existing approaches mainly learn the semantic
representations at the last convolutional layer of a CNN. But it has been noted
that the image representations of different layers of CNN capture different
levels or scales of features and have different discriminative abilities. We
thus propose to learn semantic representations at multiple convolutional
layers. To this end, this paper designs a Multi-layered Semantic Representation
Network (MSRN) which discovers both local and global semantics of labels
through modeling label correlations and utilizes the label semantics to guide
the semantic representations learning at multiple layers through an attention
mechanism. Extensive experiments on four benchmark datasets including VOC 2007,
COCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN
against state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1"&gt;Xiwen Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1"&gt;Hao Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Linchuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiao Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11485</id>
        <link href="http://arxiv.org/abs/2106.11485"/>
        <updated>2021-06-23T01:48:39.060Z</updated>
        <summary type="html"><![CDATA[High-resolution satellite imagery has proven useful for a broad range of
tasks, including measurement of global human population, local economic
livelihoods, and biodiversity, among many others. Unfortunately,
high-resolution imagery is both infrequently collected and expensive to
purchase, making it hard to efficiently and effectively scale these downstream
tasks over both time and space. We propose a new conditional pixel synthesis
model that uses abundant, low-cost, low-resolution imagery to generate accurate
high-resolution imagery at locations and times in which it is unavailable. We
show that our model attains photo-realistic sample quality and outperforms
competing baselines on a key downstream task -- object counting -- particularly
in geographic locations where conditions on the ground are changing rapidly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yutong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dingjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_N/0/1/0/all/0/1"&gt;Nicholas Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;William Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1"&gt;Chenlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1"&gt;Marshall Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1"&gt;David B. Lobell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Deep Semi-supervised Learning Approaches and Related Works. (arXiv:2106.11528v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11528</id>
        <link href="http://arxiv.org/abs/2106.11528"/>
        <updated>2021-06-23T01:48:39.053Z</updated>
        <summary type="html"><![CDATA[The author of this work proposes an overview of the recent semi-supervised
learning approaches and related works. Despite the remarkable success of neural
networks in various applications, there exist few formidable constraints
including the need for a large amount of labeled data. Therefore,
semi-supervised learning, which is a learning scheme in which the scarce labels
and a larger amount of unlabeled data are utilized to train models (e.g., deep
neural networks) is getting more important. Based on the key assumptions of
semi-supervised learning, which are the manifold assumption, cluster
assumption, and continuity assumption, the work reviews the recent
semi-supervised learning approaches. In particular, the methods in regard to
using deep neural networks in a semi-supervised learning setting are primarily
discussed. In addition, the existing works are first classified based on the
underlying idea and explained, and then the holistic approaches that unify the
aforementioned ideas are detailed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gyeongho Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:39.046Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Clustering with Sigmoid-based Regularization for Efficient Segmentation of Sequential Data. (arXiv:2106.11541v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11541</id>
        <link href="http://arxiv.org/abs/2106.11541"/>
        <updated>2021-06-23T01:48:39.039Z</updated>
        <summary type="html"><![CDATA[Kernel segmentation aims at partitioning a data sequence into several
non-overlapping segments that may have nonlinear and complex structures. In
general, it is formulated as a discrete optimization problem with combinatorial
constraints. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements.
Given that sequences in practice are too long, this algorithm is not a
practical approach. Although many heuristic algorithms have been proposed to
approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate
the aforementioned issues. First, we introduce a novel sigmoid-based
regularization to smoothly approximate the combinatorial constraints. Combining
it with objective of the balanced kernel clustering, we formulate a
differentiable model termed Kernel clustering with sigmoid-based regularization
(KCSR), where the gradient-based algorithm can be exploited to obtain the
optimal segmentation. Second, we develop a stochastic variant of the proposed
model. By using the stochastic gradient descent algorithm, which has much lower
time and space complexities, for optimization, the second model can perform
segmentation on overlong data sequences. Finally, for simultaneously segmenting
multiple data sequences, we slightly modify the sigmoid-based regularization to
further introduce an extended variant of the proposed model. Through extensive
experiments on various types of data sequences performances of our models are
evaluated and compared with those of the existing methods. The experimental
results validate advantages of the proposed models. Our Matlab source code is
available on github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doan_T/0/1/0/all/0/1"&gt;Tung Doan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takasu_A/0/1/0/all/0/1"&gt;Atsuhiro Takasu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Architecture Search Without Training Nor Labels: A Pruning Perspective. (arXiv:2106.11542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11542</id>
        <link href="http://arxiv.org/abs/2106.11542"/>
        <updated>2021-06-23T01:48:39.016Z</updated>
        <summary type="html"><![CDATA[With leveraging the weight-sharing and continuous relaxation to enable
gradient-descent to alternately optimize the supernet weights and the
architecture parameters through a bi-level optimization paradigm,
\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream
method in Neural Architecture Search (NAS) due to its simplicity and
efficiency. However, more recent works found that the performance of the
searched architecture barely increases with the optimization proceeding in
DARTS. In addition, several concurrent works show that the NAS could find more
competitive architectures without labels. The above observations reveal that
the supervision signal in DARTS may be a poor indicator for architecture
optimization, inspiring a foundational question: instead of using the
supervision signal to perform bi-level optimization, \textit{can we find
high-quality architectures \textbf{without any training nor labels}}? We
provide an affirmative answer by customizing the NAS as a network pruning at
initialization problem. By leveraging recent techniques on the network pruning
at initialization, we designed a FreeFlow proxy to score the importance of
candidate operations in NAS without any training nor labels, and proposed a
novel framework called \textit{training and label free neural architecture
search} (\textbf{FreeNAS}) accordingly. We show that, without any training nor
labels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS
baselines. More importantly, our framework is extremely efficient, which
completes the architecture search within only \textbf{3.6s} and \textbf{79s} on
a single GPU for the NAS-Bench-201 and DARTS search space, respectively. We
hope our work inspires more attempts in solving NAS from the perspective of
pruning at initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1"&gt;Gholamreza Haffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11562</id>
        <link href="http://arxiv.org/abs/2106.11562"/>
        <updated>2021-06-23T01:48:38.963Z</updated>
        <summary type="html"><![CDATA[We consider a class-incremental semantic segmentation (CISS) problem. While
some recently proposed algorithms utilized variants of knowledge distillation
(KD) technique to tackle the problem, they only partially addressed the key
additional challenges in CISS that causes the catastrophic forgetting; i.e.,
the semantic drift of the background class and multi-label prediction issue. To
better address these challenges, we propose a new method, dubbed as SSUL-M
(Semantic Segmentation with Unknown Label with Memory), by carefully combining
several techniques tailored for semantic segmentation. More specifically, we
make three main contributions; (1) modeling unknown class within the background
class to help learning future classes (help plasticity), (2) freezing backbone
network and past classifiers with binary cross-entropy loss and pseudo-labeling
to overcome catastrophic forgetting (help stability), and (3) utilizing tiny
exemplar memory for the first time in CISS to improve both plasticity and
stability. As a result, we show our method achieves significantly better
performance than the recent state-of-the-art baselines on the standard
benchmark datasets. Furthermore, we justify our contributions with thorough and
extensive ablation analyses and discuss different natures of the CISS problem
compared to the standard class-incremental learning for classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungmin Cha. Beomyoung Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1"&gt;Youngjoon Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1"&gt;Taesup Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creating A New Color Space utilizing PSO and FCM to Perform Skin Detection by using Neural Network and ANFIS. (arXiv:2106.11563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11563</id>
        <link href="http://arxiv.org/abs/2106.11563"/>
        <updated>2021-06-23T01:48:38.946Z</updated>
        <summary type="html"><![CDATA[Skin color detection is an essential required step in various applications
related to computer vision. These applications will include face detection,
finding pornographic images in movies and photos, finding ethnicity, age,
diagnosis, and so on. Therefore, proposing a proper skin detection method can
provide solution to several problems. In this study, first a new color space is
created using FCM and PSO algorithms. Then, skin classification has been
performed in the new color space utilizing linear and nonlinear modes.
Additionally, it has been done in RGB and LAB color spaces by using ANFIS and
neural network. Skin detection in RBG color space has been performed using
Mahalanobis distance and Euclidean distance algorithms. In comparison, this
method has 18.38% higher accuracy than the most accurate method on the same
database. Additionally, this method has achieved 90.05% in equal error rate
(1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan
dataset, which compared to the previous method on COMPAQ database, 1-EER has
increased by %0.87.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nazaria_K/0/1/0/all/0/1"&gt;Kobra Nazaria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazaheri_S/0/1/0/all/0/1"&gt;Samaneh Mazaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bigham_B/0/1/0/all/0/1"&gt;Bahram Sadeghi Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DocFormer: End-to-End Transformer for Document Understanding. (arXiv:2106.11539v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11539</id>
        <link href="http://arxiv.org/abs/2106.11539"/>
        <updated>2021-06-23T01:48:38.940Z</updated>
        <summary type="html"><![CDATA[We present DocFormer -- a multi-modal transformer based architecture for the
task of Visual Document Understanding (VDU). VDU is a challenging problem which
aims to understand documents in their varied formats (forms, receipts etc.) and
layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using
carefully designed tasks which encourage multi-modal interaction. DocFormer
uses text, vision and spatial features and combines them using a novel
multi-modal self-attention layer. DocFormer also shares learned spatial
embeddings across modalities which makes it easy for the model to correlate
text to visual tokens and vice versa. DocFormer is evaluated on 4 different
datasets each with strong baselines. DocFormer achieves state-of-the-art
results on all of them, sometimes beating models 4x its size (in no. of
parameters).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1"&gt;Srikar Appalaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasani_B/0/1/0/all/0/1"&gt;Bhavan Jasani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1"&gt;Bhargava Urala Kota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yusheng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1"&gt;R. Manmatha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gait analysis with curvature maps: A simulation study. (arXiv:2106.11466v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11466</id>
        <link href="http://arxiv.org/abs/2106.11466"/>
        <updated>2021-06-23T01:48:38.933Z</updated>
        <summary type="html"><![CDATA[Gait analysis is an important aspect of clinical investigation for detecting
neurological and musculoskeletal disorders and assessing the global health of a
patient. In this paper we propose to focus our attention on extracting relevant
curvature information from the body surface provided by a depth camera. We
assumed that the 3D mesh was made available in a previous step and demonstrated
how curvature maps could be useful to assess asymmetric anomalies with two
simple simulated abnormal gaits compared with a normal one. This research set
the grounds for the future development of a curvature-based gait analysis
system for healthcare professionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1"&gt;Khac Chinh Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daniel_M/0/1/0/all/0/1"&gt;Marc Daniel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meunier_J/0/1/0/all/0/1"&gt;Jean Meunier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal trajectory forecasting based on discrete heat map. (arXiv:2106.11467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11467</id>
        <link href="http://arxiv.org/abs/2106.11467"/>
        <updated>2021-06-23T01:48:38.925Z</updated>
        <summary type="html"><![CDATA[In Argoverse motion forecasting competition, the task is to predict the
probabilistic future trajectory distribution for the interested targets in the
traffic scene. We use vectorized lane map and 2 s targets' history trajectories
as input. Then the model outputs 6 forecasted trajectories with probability for
each target.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jingni Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yushi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Break-It-Fix-It: Unsupervised Learning for Program Repair. (arXiv:2106.06600v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06600</id>
        <link href="http://arxiv.org/abs/2106.06600"/>
        <updated>2021-06-23T01:48:38.902Z</updated>
        <summary type="html"><![CDATA[We consider repair tasks: given a critic (e.g., compiler) that assesses the
quality of an input, the goal is to train a fixer that converts a bad example
(e.g., code with syntax errors) into a good one (e.g., code with no syntax
errors). Existing works create training data consisting of (bad, good) pairs by
corrupting good examples using heuristics (e.g., dropping tokens). However,
fixers trained on this synthetically-generated data do not extrapolate well to
the real distribution of bad inputs. To bridge this gap, we propose a new
training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use
the critic to check a fixer's output on real bad inputs and add good (fixed)
outputs to the training data, and (ii) we train a breaker to generate realistic
bad code from good code. Based on these ideas, we iteratively update the
breaker and the fixer while using them in conjunction to generate more paired
data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new
dataset we introduce where the goal is to repair Python code with AST parse
errors; and DeepFix, where the goal is to repair C code with compiler errors.
BIFI outperforms existing methods, obtaining 90.5% repair accuracy on
GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not
require any labeled data; we hope it will be a strong starting point for
unsupervised learning of various repair tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1"&gt;Michihiro Yasunaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1"&gt;Percy Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation. (arXiv:2106.11401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11401</id>
        <link href="http://arxiv.org/abs/2106.11401"/>
        <updated>2021-06-23T01:48:38.895Z</updated>
        <summary type="html"><![CDATA[Moving objects have special importance for Autonomous Driving tasks.
Detecting moving objects can be posed as Moving Object Segmentation, by
segmenting the object pixels, or Moving Object Detection, by generating a
bounding box for the moving targets. In this paper, we present a Multi-Task
Learning architecture, based on Transformers, to jointly perform both tasks
through one network. Due to the importance of the motion features to the task,
the whole setup is based on a Spatio-Temporal aggregation. We evaluate the
performance of the individual tasks architecture versus the MTL setup, both
with early shared encoders, and late shared encoder-decoder transformers. For
the latter, we present a novel joint tasks query decoder transformer, that
enables us to have tasks dedicated heads out of the shared model. To evaluate
our approach, we use the KITTI MOD [29] data set. Results show1.5% mAP
improvement for Moving Object Detection, and 2%IoU improvement for Moving
Object Segmentation, over the individual tasks networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmed El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network. (arXiv:2106.11354v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11354</id>
        <link href="http://arxiv.org/abs/2106.11354"/>
        <updated>2021-06-23T01:48:38.887Z</updated>
        <summary type="html"><![CDATA[While working with fingerprint images acquired from crime scenes, mobile
cameras, or low-quality sensors, it becomes difficult for automated
identification systems to verify the identity due to image blur and distortion.
We propose a fingerprint deblurring model FDeblur-GAN, based on the conditional
Generative Adversarial Networks (cGANs) and multi-stage framework of the stack
GAN. Additionally, we integrate two auxiliary sub-networks into the model for
the deblurring task. The first sub-network is a ridge extractor model. It is
added to generate ridge maps to ensure that fingerprint information and
minutiae are preserved in the deblurring process and prevent the model from
generating erroneous minutiae. The second sub-network is a verifier that helps
the generator to preserve the ID information during the generation process.
Using a database of blurred fingerprints and corresponding ridge maps, the deep
network learns to deblur from the input blurry samples. We evaluate the
proposed method in combination with two different fingerprint matching
algorithms. We achieved an accuracy of 95.18% on our fingerprint database for
the task of matching deblurred and ground truth fingerprints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Amol S. Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1"&gt;Ali Dabouei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mapping Slums with Medium Resolution Satellite Imagery: a Comparative Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix Techniques. (arXiv:2106.11395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11395</id>
        <link href="http://arxiv.org/abs/2106.11395"/>
        <updated>2021-06-23T01:48:38.880Z</updated>
        <summary type="html"><![CDATA[The UN-Habitat estimates that over one billion people live in slums around
the world. However, state-of-the-art techniques to detect the location of slum
areas employ high-resolution satellite imagery, which is costly to obtain and
process. As a result, researchers have started to look at utilising free and
open-access medium resolution satellite imagery. Yet, there is no clear
consensus on which data preparation and machine learning approaches are the
most appropriate to use with such imagery data. In this paper, we evaluate two
techniques (multi-spectral data and grey-level co-occurrence matrix feature
extraction) on an open-access dataset consisting of labelled Sentinel-2 images
with a spatial resolution of 10 meters. Both techniques were paired with a
canonical correlation forests classifier. The results show that the grey-level
co-occurrence matrix performed better than multi-spectral data for all four
cities. It had an average accuracy for the slum class of 97% and a mean
intersection over union of 94%, while multi-spectral data had 75% and 64% for
the respective metrics. These results indicate that open-access satellite
imagery with a resolution of at least 10 meters may be suitable for keeping
track of development goals such as the detection of slums in cities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mattos_A/0/1/0/all/0/1"&gt;Agatha C. H. de Mattos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McArdle_G/0/1/0/all/0/1"&gt;Gavin McArdle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertolotto_M/0/1/0/all/0/1"&gt;Michela Bertolotto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification. (arXiv:2106.11486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11486</id>
        <link href="http://arxiv.org/abs/2106.11486"/>
        <updated>2021-06-23T01:48:38.872Z</updated>
        <summary type="html"><![CDATA[We propose unsupervised embedding adaptation for the downstream few-shot
classification task. Based on findings that deep neural networks learn to
generalize before memorizing, we develop Early-Stage Feature Reconstruction
(ESFR) -- a novel adaptation scheme with feature reconstruction and
dimensionality-driven early stopping that finds generalizable features.
Incorporating ESFR consistently improves the performance of baseline methods on
all standard settings, including the recently proposed transductive method.
ESFR used in conjunction with the transductive method further achieves
state-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB;
especially with 1.2%~2.0% improvements in accuracy over the previous best
performing method on 1-shot setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dong Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Sae-Young Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[f-Domain-Adversarial Learning: Theory and Algorithms. (arXiv:2106.11344v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11344</id>
        <link href="http://arxiv.org/abs/2106.11344"/>
        <updated>2021-06-23T01:48:38.801Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation is used in many machine learning applications
where, during training, a model has access to unlabeled data in the target
domain, and a related labeled dataset. In this paper, we introduce a novel and
general domain-adversarial framework. Specifically, we derive a novel
generalization bound for domain adaptation that exploits a new measure of
discrepancy between distributions based on a variational characterization of
f-divergences. It recovers the theoretical results from Ben-David et al.
(2010a) as a special case and supports divergences used in practice. Based on
this bound, we derive a new algorithmic framework that introduces a key
correction in the original adversarial training method of Ganin et al. (2016).
We show that many regularizers and ad-hoc objectives introduced over the last
years in this framework are then not required to achieve performance comparable
to (if not better than) state-of-the-art domain-adversarial methods.
Experimental analysis conducted on real-world natural language and computer
vision datasets show that our framework outperforms existing baselines, and
obtains the best results for f-divergences that were not considered previously
in domain-adversarial learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guojun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:38.772Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models. (arXiv:2106.06087v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06087</id>
        <link href="http://arxiv.org/abs/2106.06087"/>
        <updated>2021-06-23T01:48:38.764Z</updated>
        <summary type="html"><![CDATA[Targeted syntactic evaluations have demonstrated the ability of language
models to perform subject-verb agreement given difficult contexts. To elucidate
the mechanisms by which the models accomplish this behavior, this study applies
causal mediation analysis to pre-trained neural language models. We investigate
the magnitude of models' preferences for grammatical inflections, as well as
whether neurons process subject-verb agreement similarly across sentences with
different syntactic structures. We uncover similarities and differences across
architectures and model sizes -- notably, that larger models do not necessarily
learn stronger preferences. We also observe two distinct mechanisms for
producing subject-verb agreement depending on the syntactic structure of the
input sentence. Finally, we find that language models rely on similar sets of
neurons when given sentences with similar syntactic structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1"&gt;Matthew Finlayson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1"&gt;Aaron Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1"&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1"&gt;Stuart Shieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1"&gt;Tal Linzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1"&gt;Yonatan Belinkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding top-down attention using task-oriented ablation design. (arXiv:2106.11339v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11339</id>
        <link href="http://arxiv.org/abs/2106.11339"/>
        <updated>2021-06-23T01:48:38.755Z</updated>
        <summary type="html"><![CDATA[Top-down attention allows neural networks, both artificial and biological, to
focus on the information most relevant for a given task. This is known to
enhance performance in visual perception. But it remains unclear how attention
brings about its perceptual boost, especially when it comes to naturalistic
settings like recognising an object in an everyday scene. What aspects of a
visual task does attention help to deal with? We aim to answer this with a
computational experiment based on a general framework called task-oriented
ablation design. First we define a broad range of visual tasks and identify six
factors that underlie task variability. Then on each task we compare the
performance of two neural networks, one with top-down attention and one
without. These comparisons reveal the task-dependence of attention's perceptual
boost, giving a clearer idea of the role attention plays. Whereas many existing
cognitive accounts link attention to stimulus-level variables, such as visual
clutter and object scale, we find greater explanatory power in system-level
variables that capture the interaction between the model, the distribution of
training data and the task format. This finding suggests a shift in how
attention is studied could be fruitful. We make publicly available our code and
results, along with statistics relevant to ImageNet-based experiments beyond
this one. Our contribution serves to support the development of more human-like
vision models and the design of more informative machine-learning experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_F/0/1/0/all/0/1"&gt;Freddie Bickford Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roads_B/0/1/0/all/0/1"&gt;Brett D Roads&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiaoliang Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Love_B/0/1/0/all/0/1"&gt;Bradley C Love&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation. (arXiv:2104.08006v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08006</id>
        <link href="http://arxiv.org/abs/2104.08006"/>
        <updated>2021-06-23T01:48:38.731Z</updated>
        <summary type="html"><![CDATA[Now, the pre-training technique is ubiquitous in natural language processing
field. ProphetNet is a pre-training based natural language generation method
which shows powerful performance on English text summarization and question
generation tasks. In this paper, we extend ProphetNet into other domains and
languages, and present the ProphetNet family pre-training models, named
ProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We
pre-train a cross-lingual generation model ProphetNet-Multi, a Chinese
generation model ProphetNet-Zh, two open-domain dialog generation models
ProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG
(Programming Language Generation) model ProphetNet-Code to show the generation
performance besides NLG (Natural Language Generation) tasks. In our
experiments, ProphetNet-X models achieve new state-of-the-art performance on 10
benchmarks. All the models of ProphetNet-X share the same model structure,
which allows users to easily switch between different models. We make the code
and models publicly available, and we will keep updating more pre-training
models and finetuning scripts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1"&gt;Bolun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bartuer Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1"&gt;Biao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:38.724Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021. (arXiv:2106.00197v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00197</id>
        <link href="http://arxiv.org/abs/2106.00197"/>
        <updated>2021-06-23T01:48:38.717Z</updated>
        <summary type="html"><![CDATA[This paper describes the system submitted to the IWSLT 2021 Multilingual
Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified
transformer architecture for our MultiST model, so that the data from different
modalities (i.e., speech and text) and different tasks (i.e., Speech
Recognition, Machine Translation, and Speech Translation) can be exploited to
enhance the model's ability. Specifically, speech and text inputs are firstly
fed to different feature extractors to extract acoustic and textual features,
respectively. Then, these features are processed by a shared encoder--decoder
architecture. We apply several training techniques to improve the performance,
including multi-task learning, task-level curriculum learning, data
augmentation, etc. Our final system achieves significantly better results than
bilingual baselines on supervised language pairs and yields reasonable results
on zero-shot language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Architectures for Clinically Relevant Coronary Artery Segmentation. (arXiv:2106.11447v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11447</id>
        <link href="http://arxiv.org/abs/2106.11447"/>
        <updated>2021-06-23T01:48:38.709Z</updated>
        <summary type="html"><![CDATA[Coronary X-ray angiography is a crucial clinical procedure for the diagnosis
and treatment of coronary artery disease, which accounts for roughly 16% of
global deaths every year. However, the images acquired in these procedures have
low resolution and poor contrast, making lesion detection and assessment
challenging. Accurate coronary artery segmentation not only helps mitigate
these problems, but also allows the extraction of relevant anatomical features
for further analysis by quantitative methods. Although automated segmentation
of coronary arteries has been proposed before, previous approaches have used
non-optimal segmentation criteria, leading to less useful results. Most methods
either segment only the major vessel, discarding important information from the
remaining ones, or segment the whole coronary tree based mostly on contrast
information, producing a noisy output that includes vessels that are not
relevant for diagnosis. We adopt a better-suited clinical criterion and segment
vessels according to their clinical relevance. Additionally, we simultaneously
perform catheter segmentation, which may be useful for diagnosis due to the
scale factor provided by the catheter's known diameter, and is a task that has
not yet been performed with good results. To derive the optimal approach, we
conducted an extensive comparative study of encoder-decoder architectures
trained on a combination of focal loss and a variant of generalized dice loss.
Based on the EfficientNet and the UNet++ architectures, we propose a line of
efficient and high-performance segmentation models using a new decoder
architecture, the EfficientUNet++, whose best-performing version achieved
average dice scores of 0.8904 and 0.7526 for the artery and catheter classes,
respectively, and an average generalized dice score of 0.9234.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Louren&amp;#xe7;o Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Menezes_M/0/1/0/all/0/1"&gt;Miguel Nobre Menezes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodrigues_T/0/1/0/all/0/1"&gt;Tiago Rodrigues&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_B/0/1/0/all/0/1"&gt;Beatriz Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinto_F/0/1/0/all/0/1"&gt;Fausto J. Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Arlindo L. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MODETR: Moving Object Detection with Transformers. (arXiv:2106.11422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11422</id>
        <link href="http://arxiv.org/abs/2106.11422"/>
        <updated>2021-06-23T01:48:38.699Z</updated>
        <summary type="html"><![CDATA[Moving Object Detection (MOD) is a crucial task for the Autonomous Driving
pipeline. MOD is usually handled via 2-stream convolutional architectures that
incorporates both appearance and motion cues, without considering the
inter-relations between the spatial or motion features. In this paper, we
tackle this problem through multi-head attention mechanisms, both across the
spatial and motion streams. We propose MODETR; a Moving Object DEtection
TRansformer network, comprised of multi-stream transformer encoders for both
spatial and motion modalities, and an object transformer decoder that produces
the moving objects bounding boxes using set predictions. The whole architecture
is trained end-to-end using bi-partite loss. Several methods of incorporating
motion cues with the Transformer model are explored, including two-stream RGB
and Optical Flow (OF) methods, and multi-stream architectures that take
advantage of sequence information. To incorporate the temporal information, we
propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial
Positional Encoding(SPE) in DETR. We explore two architectural choices for
that, balancing between speed and time. To evaluate the our network, we perform
the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of
the Transformer network for MOD over the state-of-the art methods. Moreover,
the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1"&gt;Eslam Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Sallab_A/0/1/0/all/0/1"&gt;Ahmad El-Sallab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning Approach. (arXiv:2106.11549v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11549</id>
        <link href="http://arxiv.org/abs/2106.11549"/>
        <updated>2021-06-23T01:48:38.671Z</updated>
        <summary type="html"><![CDATA[Generic Event Boundary Detection (GEBD) is a newly introduced task that aims
to detect "general" event boundaries that correspond to natural human
perception. In this paper, we introduce a novel contrastive learning based
approach to deal with the GEBD. Our intuition is that the feature similarity of
the video snippet would significantly vary near the event boundaries, while
remaining relatively the same in the remaining part of the video. In our model,
Temporal Self-similarity Matrix (TSM) is utilized as an intermediate
representation which takes on a role as an information bottleneck. With our
model, we achieved significant performance boost compared to the given
baselines. Our code is available at
https://github.com/hello-jinwoo/LOVEU-CVPR2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyolim Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jinwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyungmin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taehyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seon Joo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEyond observation: an approach for ObjectNav. (arXiv:2106.11379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11379</id>
        <link href="http://arxiv.org/abs/2106.11379"/>
        <updated>2021-06-23T01:48:38.662Z</updated>
        <summary type="html"><![CDATA[With the rise of automation, unmanned vehicles became a hot topic both as
commercial products and as a scientific research topic. It composes a
multi-disciplinary field of robotics that encompasses embedded systems, control
theory, path planning, Simultaneous Localization and Mapping (SLAM), scene
reconstruction, and pattern recognition. In this work, we present our
exploratory research of how sensor data fusion and state-of-the-art machine
learning algorithms can perform the Embodied Artificial Intelligence (E-AI)
task called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation
(ObjectNav) consists of autonomous navigation using egocentric visual
observations to reach an object belonging to the target semantic class without
prior knowledge of the environment. Our method reached fourth place on the
Habitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard
Phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_D/0/1/0/all/0/1"&gt;Daniel V. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Todt_E/0/1/0/all/0/1"&gt;Eduardo Todt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photozilla: A Large-Scale Photography Dataset and Visual Embedding for 20 Photography Styles. (arXiv:2106.11359v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11359</id>
        <link href="http://arxiv.org/abs/2106.11359"/>
        <updated>2021-06-23T01:48:38.651Z</updated>
        <summary type="html"><![CDATA[The advent of social media platforms has been a catalyst for the development
of digital photography that engendered a boom in vision applications. With this
motivation, we introduce a large-scale dataset termed 'Photozilla', which
includes over 990k images belonging to 10 different photographic styles. The
dataset is then used to train 3 classification models to automatically classify
the images into the relevant style which resulted in an accuracy of ~96%. With
the rapid evolution of digital photography, we have seen new types of
photography styles emerging at an exponential rate. On that account, we present
a novel Siamese-based network that uses the trained classification models as
the base architecture to adapt and classify unseen styles with only 25 training
samples. We report an accuracy of over 68% for identifying 10 other distinct
types of photography styles. This dataset can be found at
https://trisha025.github.io/Photozilla/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1"&gt;Trisha Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1"&gt;Lucienne T. M. Blessing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1"&gt;Kwan Hui Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?. (arXiv:2104.10809v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10809</id>
        <link href="http://arxiv.org/abs/2104.10809"/>
        <updated>2021-06-23T01:48:38.643Z</updated>
        <summary type="html"><![CDATA[Language models trained on billions of tokens have recently led to
unprecedented results on many NLP tasks. This success raises the question of
whether, in principle, a system can ever ``understand'' raw text without access
to some form of grounding. We formally investigate the abilities of ungrounded
systems to acquire meaning. Our analysis focuses on the role of ``assertions'':
textual contexts that provide indirect clues about the underlying semantics. We
study whether assertions enable a system to emulate representations preserving
semantic relations like equivalence. We find that assertions enable semantic
emulation of languages that satisfy a strong notion of semantic transparency.
However, for classes of languages where the same expression can take different
values in different contexts, we show that emulation can become uncomputable.
Finally, we discuss differences between our formal model and natural language,
exploring how our results generalize to a modal setting and other semantic
relations. Together, our results suggest that assertions in code or language do
not provide sufficient signal to fully emulate semantic representations. We
formalize ways in which ungrounded language models appear to be fundamentally
limited in their ability to ``understand''.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1"&gt;William Merrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1"&gt;Roy Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals. (arXiv:2106.05544v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05544</id>
        <link href="http://arxiv.org/abs/2106.05544"/>
        <updated>2021-06-23T01:48:38.636Z</updated>
        <summary type="html"><![CDATA[Most previous studies integrate cognitive language processing signals (e.g.,
eye-tracking or EEG data) into neural models of natural language processing
(NLP) just by directly concatenating word embeddings with cognitive features,
ignoring the gap between the two modalities (i.e., textual vs. cognitive) and
noise in cognitive features. In this paper, we propose a CogAlign approach to
these issues, which learns to align textual neural representations to cognitive
features. In CogAlign, we use a shared encoder equipped with a modality
discriminator to alternatively encode textual and cognitive inputs to capture
their differences and commonalities. Additionally, a text-aware attention
mechanism is proposed to detect task-related information and to avoid using
noise in cognitive features. Experimental results on three NLP tasks, namely
named entity recognition, sentiment analysis and relation extraction, show that
CogAlign achieves significant improvements with multiple cognitive features
over state-of-the-art models on public datasets. Moreover, our model is able to
transfer cognitive information to other datasets that do not have any cognitive
processing signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yuqi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1"&gt;Deyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.11396</id>
        <link href="http://arxiv.org/abs/2106.11396"/>
        <updated>2021-06-23T01:48:38.628Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization recently has attracted increased interest in machine
learning due to its many applications such as hyper-parameter optimization and
policy optimization. Although some methods recently have been proposed to solve
the bilevel problems, these methods do not consider using adaptive learning
rates. To fill this gap, in the paper, we propose a class of fast and effective
adaptive methods for solving bilevel optimization problems that the outer
problem is possibly nonconvex and the inner problem is strongly-convex.
Specifically, we propose a fast single-loop BiAdam algorithm based on the basic
momentum technique, which achieves a sample complexity of
$\tilde{O}(\epsilon^{-4})$ for finding an $\epsilon$-stationary point. At the
same time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by
using variance reduced technique, which reaches the best known sample
complexity of $\tilde{O}(\epsilon^{-3})$. To further reduce computation in
estimating derivatives, we propose a fast single-loop stochastic approximated
BiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still
achieves a sample complexity of $\tilde{O}(\epsilon^{-4})$ without large
batches. We further present an accelerated version of saBiAdam algorithm
(VR-saBiAdam), which also reaches the best known sample complexity of
$\tilde{O}(\epsilon^{-3})$. We apply the unified adaptive matrices to our
methods as the SUPER-ADAM \citep{huang2021super}, which including many types of
adaptive learning rates. Moreover, our framework can flexibly use the momentum
and variance reduced techniques. In particular, we provide a useful convergence
analysis framework for both the constrained and unconstrained bilevel
optimization. To the best of our knowledge, we first study the adaptive bilevel
optimization methods with adaptive learning rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feihu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAIA: A Transfer Learning System of Object Detection that Fits Your Needs. (arXiv:2106.11346v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11346</id>
        <link href="http://arxiv.org/abs/2106.11346"/>
        <updated>2021-06-23T01:48:38.606Z</updated>
        <summary type="html"><![CDATA[Transfer learning with pre-training on large-scale datasets has played an
increasingly significant role in computer vision and natural language
processing recently. However, as there exist numerous application scenarios
that have distinctive demands such as certain latency constraints and
specialized data distributions, it is prohibitively expensive to take advantage
of large-scale pre-training for per-task requirements. In this paper, we focus
on the area of object detection and present a transfer learning system named
GAIA, which could automatically and efficiently give birth to customized
solutions according to heterogeneous downstream needs. GAIA is capable of
providing powerful pre-trained weights, selecting models that conform to
downstream demands such as latency constraints and specified data domains, and
collecting relevant data for practitioners who have very few datapoints for
their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open
Images, Caltech, CityPersons, and UODB which is a collection of datasets
including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as
an example, GAIA is able to efficiently produce models covering a wide range of
latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and
bells. To benefit every practitioner in the community of object detection, GAIA
is released at https://github.com/GAIA-vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bu_X/0/1/0/all/0/1"&gt;Xingyuan Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Junran Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhaoxiang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:38.597Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08653</id>
        <link href="http://arxiv.org/abs/2104.08653"/>
        <updated>2021-06-23T01:48:38.589Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) and Information Retrieval (IR) in the
judicial domain is an essential task. With the advent of availability
domain-specific data in electronic form and aid of different Artificial
intelligence (AI) technologies, automated language processing becomes more
comfortable, and hence it becomes feasible for researchers and developers to
provide various automated tools to the legal community to reduce human burden.
The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in
association with the International Conference on Artificial Intelligence and
Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined
four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to
provide few automated systems to the judicial system. The paper presents our
working note on the experiments carried out as a part of our participation in
all the sub-tasks defined in this shared task. We make use of different
Information Retrieval(IR) and deep learning based approaches to tackle these
problems. We obtain encouraging results in all these four sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08043</id>
        <link href="http://arxiv.org/abs/2106.08043"/>
        <updated>2021-06-23T01:48:38.581Z</updated>
        <summary type="html"><![CDATA[The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1"&gt;Arun S. Maiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-23T01:48:38.573Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image simulation for space applications with the SurRender software. (arXiv:2106.11322v1 [astro-ph.EP])]]></title>
        <id>http://arxiv.org/abs/2106.11322</id>
        <link href="http://arxiv.org/abs/2106.11322"/>
        <updated>2021-06-23T01:48:38.550Z</updated>
        <summary type="html"><![CDATA[Image Processing algorithms for vision-based navigation require reliable
image simulation capacities. In this paper we explain why traditional rendering
engines may present limitations that are potentially critical for space
applications. We introduce Airbus SurRender software v7 and provide details on
features that make it a very powerful space image simulator. We show how
SurRender is at the heart of the development processes of our computer vision
solutions and we provide a series of illustrations of rendered images for
various use cases ranging from Moon and Solar System exploration, to in orbit
rendezvous and planetary robotics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lebreton_J/0/1/0/all/0/1"&gt;J&amp;#xe9;r&amp;#xe9;my Lebreton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Brochard_R/0/1/0/all/0/1"&gt;Roland Brochard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Baudry_M/0/1/0/all/0/1"&gt;Matthieu Baudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jonniaux_G/0/1/0/all/0/1"&gt;Gr&amp;#xe9;gory Jonniaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Salah_A/0/1/0/all/0/1"&gt;Adrien Hadj Salah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kanani_K/0/1/0/all/0/1"&gt;Keyvan Kanani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Goff_M/0/1/0/all/0/1"&gt;Matthieu Le Goff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Masson_A/0/1/0/all/0/1"&gt;Aurore Masson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ollagnier_N/0/1/0/all/0/1"&gt;Nicolas Ollagnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Panicucci_P/0/1/0/all/0/1"&gt;Paolo Panicucci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Proag_A/0/1/0/all/0/1"&gt;Amsha Proag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Robin_C/0/1/0/all/0/1"&gt;Cyril Robin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal CT Images. (arXiv:2106.11330v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.11330</id>
        <link href="http://arxiv.org/abs/2106.11330"/>
        <updated>2021-06-23T01:48:38.542Z</updated>
        <summary type="html"><![CDATA[Accurate liver and lesion segmentation from computed tomography (CT) images
are highly demanded in clinical practice for assisting the diagnosis and
assessment of hepatic tumor disease. However, automatic liver and lesion
segmentation from contrast-enhanced CT volumes is extremely challenging due to
the diversity in contrast, resolution, and quality of images. Previous methods
based on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either
lack sufficient spatial contexts or suffer from high GPU computational cost,
which limits the performance. To tackle these issues, we propose a novel
context-aware PolyUNet for accurate liver and lesion segmentation. It jointly
explores structural diversity and consecutive t-adjacent slices to enrich
feature expressive power and spatial contextual information while avoiding the
overload of GPU memory consumption. In addition, we utilize zoom out/in and
two-stage refinement strategy to exclude the irrelevant contexts and focus on
the specific region for the fine-grained segmentation. Our method achieved very
competitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS)
Challenge among all tasks with a single model and ranked the $3^{rd}$,
$12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion
segmentation, lesion detection, and tumor burden estimation, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_S/0/1/0/all/0/1"&gt;Simon Chun-Ho Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation. (arXiv:2008.10875v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10875</id>
        <link href="http://arxiv.org/abs/2008.10875"/>
        <updated>2021-06-23T01:48:38.422Z</updated>
        <summary type="html"><![CDATA[Plug-and-play language models (PPLMs) enable topic-conditioned natural
language generation by pairing large pre-trained generators with attribute
models used to steer the predicted token distribution towards the selected
topic. Despite their computational efficiency, PPLMs require large amounts of
labeled texts to effectively balance generation fluency and proper
conditioning, making them unsuitable for low-resource settings. We present
ETC-NLG, an approach leveraging topic modeling annotations to enable
fully-unsupervised End-to-end Topic-Conditioned Natural Language Generation
over emergent topics in unlabeled document collections. We first test the
effectiveness of our approach in a low-resource setting for Italian, evaluating
the conditioning for both topic models and gold annotations. We then perform a
comparative evaluation of ETC-NLG for Italian and English using a parallel
corpus. Finally, we propose an automatic approach to estimate the effectiveness
of conditioning on the generated utterances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carbone_G/0/1/0/all/0/1"&gt;Ginevra Carbone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1"&gt;Gabriele Sarti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Evaluation of Machine Translation for Terminology Consistency. (arXiv:2106.11891v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11891</id>
        <link href="http://arxiv.org/abs/2106.11891"/>
        <updated>2021-06-23T01:48:38.396Z</updated>
        <summary type="html"><![CDATA[As neural machine translation (NMT) systems become an important part of
professional translator pipelines, a growing body of work focuses on combining
NMT with terminologies. In many scenarios and particularly in cases of domain
adaptation, one expects the MT output to adhere to the constraints provided by
a terminology. In this work, we propose metrics to measure the consistency of
MT output with regards to a domain terminology. We perform studies on the
COVID-19 domain over 5 languages, also performing terminology-targeted human
evaluation. We open-source the code for computing all proposed metrics:
https://github.com/mahfuzibnalam/terminology_evaluation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1"&gt;Md Mahfuz ibn Alam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1"&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1"&gt;James Cross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1"&gt;Matthias Gall&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1"&gt;Vassilina Nikoulina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Dialogue Generation via Multi-Level Contrastive Learning. (arXiv:2009.09147v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09147</id>
        <link href="http://arxiv.org/abs/2009.09147"/>
        <updated>2021-06-23T01:48:38.347Z</updated>
        <summary type="html"><![CDATA[Most of the existing works for dialogue generation are data-driven models
trained directly on corpora crawled from websites. They mainly focus on
improving the model architecture to produce better responses but pay little
attention to considering the quality of the training data contrastively. In
this paper, we propose a multi-level contrastive learning paradigm to model the
fine-grained quality of the responses with respect to the query. A Rank-aware
Calibration (RC) network is designed to construct the multi-level contrastive
optimization objectives. Since these objectives are calculated based on the
sentence level, which may erroneously encourage/suppress the generation of
uninformative/informative words. To tackle this incidental issue, on one hand,
we design an exquisite token-level strategy for estimating the instance loss
more accurately. On the other hand, we build a Knowledge Inference (KI)
component to capture the keyword knowledge from the reference during training
and exploit such information to encourage the generation of informative words.
We evaluate the proposed model on a carefully annotated dialogue dataset and
the results suggest that our model can generate more relevant and diverse
responses compared to the baseline models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaojiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11796</id>
        <link href="http://arxiv.org/abs/2106.11796"/>
        <updated>2021-06-23T01:48:38.337Z</updated>
        <summary type="html"><![CDATA[Current task-oriented dialog (TOD) systems mostly manage structured knowledge
(e.g. databases and tables) to guide the goal-oriented conversations. However,
they fall short of handling dialogs which also involve unstructured knowledge
(e.g. reviews and documents). In this paper, we formulate a task of modeling
TOD grounded on a fusion of structured and unstructured knowledge. To address
this task, we propose a TOD system with semi-structured knowledge management,
SeKnow, which extends the belief state to manage knowledge with both structured
and unstructured contents. Furthermore, we introduce two implementations of
SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained
language model, respectively. Both implementations use the end-to-end manner to
jointly optimize dialog modeling grounded on structured and unstructured
knowledge. We conduct experiments on the modified version of MultiWOZ 2.1
dataset, where dialogs are processed to involve semi-structured knowledge.
Experimental results show that SeKnow has strong performances in both
end-to-end dialog and intermediate knowledge management, compared to existing
TOD systems and their extensions with pipeline knowledge management schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Silin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech. (arXiv:2106.11783v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11783</id>
        <link href="http://arxiv.org/abs/2106.11783"/>
        <updated>2021-06-23T01:48:38.327Z</updated>
        <summary type="html"><![CDATA[Tackling online hatred using informed textual responses - called counter
narratives - has been brought under the spotlight recently. Accordingly, a
research line has emerged to automatically generate counter narratives in order
to facilitate the direct intervention in the hate discussion and to prevent
hate content from further spreading. Still, current neural approaches tend to
produce generic/repetitive responses and lack grounded and up-to-date evidence
such as facts, statistics, or examples. Moreover, these models can create
plausible but not necessarily true arguments. In this paper we present the
first complete knowledge-bound counter narrative generation pipeline, grounded
in an external knowledge repository that can provide more informative content
to fight online hatred. Together with our approach, we present a series of
experiments that show its feasibility to produce suitable and informative
counter narratives in in-domain and cross-domain settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1"&gt;Yi-Ling Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tekiroglu_S/0/1/0/all/0/1"&gt;Serra Sinem Tekiroglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1"&gt;Marco Guerini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How well do you know your summarization datasets?. (arXiv:2106.11388v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11388</id>
        <link href="http://arxiv.org/abs/2106.11388"/>
        <updated>2021-06-23T01:48:38.283Z</updated>
        <summary type="html"><![CDATA[State-of-the-art summarization systems are trained and evaluated on massive
datasets scraped from the web. Despite their prevalence, we know very little
about the underlying characteristics (data noise, summarization complexity,
etc.) of these datasets, and how these affect system performance and the
reliability of automatic metrics like ROUGE. In this study, we manually analyze
600 samples from three popular summarization datasets. Our study is driven by a
six-class typology which captures different noise types (missing facts,
entities) and degrees of summarization difficulty (extractive, abstractive). We
follow with a thorough analysis of 27 state-of-the-art summarization models and
5 popular metrics, and report our key insights: (1) Datasets have distinct data
quality and complexity distributions, which can be traced back to their
collection process. (2) The performance of models and reliability of metrics is
dependent on sample complexity. (3) Faithful summaries often receive low scores
because of the poor diversity of references. We release the code, annotated
data and model outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tejaswin_P/0/1/0/all/0/1"&gt;Priyam Tejaswin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1"&gt;Dhruv Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error-Aware Interactive Semantic Parsing of OpenStreetMap. (arXiv:2106.11739v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11739</id>
        <link href="http://arxiv.org/abs/2106.11739"/>
        <updated>2021-06-23T01:48:38.270Z</updated>
        <summary type="html"><![CDATA[In semantic parsing of geographical queries against real-world databases such
as OpenStreetMap (OSM), unique correct answers do not necessarily exist.
Instead, the truth might be lying in the eye of the user, who needs to enter an
interactive setup where ambiguities can be resolved and parsing mistakes can be
corrected. Our work presents an approach to interactive semantic parsing where
an explicit error detection is performed, and a clarification question is
generated that pinpoints the suspected source of ambiguity or error and
communicates it to the human user. Our experimental results show that a
combination of entropy-based uncertainty detection and beam search, together
with multi-source training on clarification question, initial parse, and user
answer, results in improvements of 1.2% F1 score on a parser that already
performs at 90.26% on the NLMaps dataset for OSM semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Staniek_M/0/1/0/all/0/1"&gt;Michael Staniek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1"&gt;Stefan Riezler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Language Models Perform Generalizable Commonsense Inference?. (arXiv:2106.11533v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11533</id>
        <link href="http://arxiv.org/abs/2106.11533"/>
        <updated>2021-06-23T01:48:38.262Z</updated>
        <summary type="html"><![CDATA[Inspired by evidence that pretrained language models (LMs) encode commonsense
knowledge, recent work has applied LMs to automatically populate commonsense
knowledge graphs (CKGs). However, there is a lack of understanding on their
generalization to multiple CKGs, unseen relations, and novel entities. This
paper analyzes the ability of LMs to perform generalizable commonsense
inference, in terms of knowledge capacity, transferability, and induction. Our
experiments with these three aspects show that: (1) LMs can adapt to different
schemas defined by multiple CKGs but fail to reuse the knowledge to generalize
to new relations. (2) Adapted LMs generalize well to unseen subjects, but less
so on novel objects. Future work should investigate how to improve the
transferability and induction of commonsense mining from LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1"&gt;Filip Ilievski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SENT: Sentence-level Distant Relation Extraction via Negative Training. (arXiv:2106.11566v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11566</id>
        <link href="http://arxiv.org/abs/2106.11566"/>
        <updated>2021-06-23T01:48:38.253Z</updated>
        <summary type="html"><![CDATA[Distant supervision for relation extraction provides uniform bag labels for
each sentence inside the bag, while accurate sentence labels are important for
downstream applications that need the exact relation type. Directly using bag
labels for sentence-level training will introduce much noise, thus severely
degrading performance. In this work, we propose the use of negative training
(NT), in which a model is trained using complementary labels regarding that
``the instance does not belong to these complementary labels". Since the
probability of selecting a true label as a complementary label is low, NT
provides less noisy information. Furthermore, the model trained with NT is able
to separate the noisy data from the training data. Based on NT, we propose a
sentence-level framework, SENT, for distant relation extraction. SENT not only
filters the noisy data to construct a cleaner dataset, but also performs a
re-labeling process to transform the noisy data into useful training data, thus
further benefiting the model's performance. Experimental results show the
significant improvement of the proposed method over previous methods on
sentence-level evaluation and de-noise effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1"&gt;Ruotian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1"&gt;Tao Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yaqian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Exploration of Pre-training Language Models. (arXiv:2106.11483v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11483</id>
        <link href="http://arxiv.org/abs/2106.11483"/>
        <updated>2021-06-23T01:48:38.245Z</updated>
        <summary type="html"><![CDATA[Recently, the development of pre-trained language models has brought natural
language processing (NLP) tasks to the new state-of-the-art. In this paper we
explore the efficiency of various pre-trained language models. We pre-train a
list of transformer-based models with the same amount of text and the same
training steps. The experimental results shows that the most improvement upon
the origin BERT is adding the RNN-layer to capture more contextual information
for the transformer-encoder layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tong Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11791</id>
        <link href="http://arxiv.org/abs/2106.11791"/>
        <updated>2021-06-23T01:48:38.223Z</updated>
        <summary type="html"><![CDATA[The majority of existing methods for empathetic response generation rely on
the emotion of the context to generate empathetic responses. However, empathy
is much more than generating responses with an appropriate emotion. It also
often entails subtle expressions of understanding and personal resonance with
the situation of the other interlocutor. Unfortunately, such qualities are
difficult to quantify and the datasets lack the relevant annotations. To
address this issue, in this paper we propose an approach that relies on
exemplars to cue the generative model on fine stylistic properties that signal
empathy to the interlocutor. To this end, we employ dense passage retrieval to
extract relevant exemplary responses from the training set. Three elements of
human communication -- emotional presence, interpretation, and exploration, and
sentiment are additionally introduced using synthetic labels to guide the
generation towards empathy. The human evaluation is also extended by these
elements of human communication. We empirically show that these approaches
yield significant improvements in empathetic response quality in terms of both
automated and human-evaluated metrics. The implementation is available at
https://github.com/declare-lab/exemplary-empathy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Routing between Capsules. (arXiv:2106.11531v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11531</id>
        <link href="http://arxiv.org/abs/2106.11531"/>
        <updated>2021-06-23T01:48:38.211Z</updated>
        <summary type="html"><![CDATA[Routing methods in capsule networks often learn a hierarchical relationship
for capsules in successive layers, but the intra-relation between capsules in
the same layer is less studied, while this intra-relation is a key factor for
the semantic understanding in text data. Therefore, in this paper, we introduce
a new capsule network with graph routing to learn both relationships, where
capsules in each layer are treated as the nodes of a graph. We investigate
strategies to yield adjacency and degree matrix with three different distances
from a layer of capsules, and propose the graph routing mechanism between those
capsules. We validate our approach on five text classification datasets, and
our findings suggest that the approach combining bottom-up routing and top-down
attention performs the best. Such an approach demonstrates generalization
capability across datasets. Compared to the state-of-the-art routing methods,
the improvements in accuracy in the five datasets we used were 0.82, 0.39,
0.07, 1.01, and 0.02, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Suhang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers. (arXiv:2106.11455v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11455</id>
        <link href="http://arxiv.org/abs/2106.11455"/>
        <updated>2021-06-23T01:48:38.072Z</updated>
        <summary type="html"><![CDATA[The goal of database question answering is to enable natural language
querying of real-life relational databases in diverse application domains.
Recently, large-scale datasets such as Spider and WikiSQL facilitated novel
modeling techniques for text-to-SQL parsing, improving zero-shot generalization
to unseen databases. In this work, we examine the challenges that still prevent
these techniques from practical deployment. First, we present KaggleDBQA, a new
cross-domain evaluation dataset of real Web databases, with domain-specific
data types, original formatting, and unrestricted questions. Second, we
re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in
real-life settings. Finally, we augment our in-domain evaluation task with
database documentation, a naturally occurring source of implicit domain
knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art
zero-shot parsers but a more realistic evaluation setting and creative use of
associated database documentation boosts their accuracy by over 13.2%, doubling
their performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chia-Hsuan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1"&gt;Oleksandr Polozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1"&gt;Matthew Richardson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11520</id>
        <link href="http://arxiv.org/abs/2106.11520"/>
        <updated>2021-06-23T01:48:38.063Z</updated>
        <summary type="html"><![CDATA[A wide variety of NLP applications, such as machine translation,
summarization, and dialog, involve text generation. One major challenge for
these applications is how to evaluate whether such generated texts are actually
fluent, accurate, or effective. In this work, we conceptualize the evaluation
of generated text as a text generation problem, modeled using pre-trained
sequence-to-sequence models. The general idea is that models trained to convert
the generated text to/from a reference output or the source text will achieve
higher scores when the generated text is better. We operationalize this idea
using BART, an encoder-decoder based pre-trained model, and propose a metric
BARTScore with a number of variants that can be flexibly applied in an
unsupervised fashion to evaluation of text from different perspectives (e.g.
informativeness, fluency, or factuality). BARTScore is conceptually simple and
empirically effective. It can outperform existing top-scoring metrics in 16 of
22 test settings, covering evaluation of 16 datasets (e.g., machine
translation, text summarization) and 7 different perspectives (e.g.,
informativeness, factuality). Code to calculate BARTScore is available at
https://github.com/neulab/BARTScore, and we have released an interactive
leaderboard for meta-evaluation at
this http URL on the ExplainaBoard
platform, which allows us to interactively understand the strengths,
weaknesses, and complementarity of each metric.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LV-BERT: Exploiting Layer Variety for BERT. (arXiv:2106.11740v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11740</id>
        <link href="http://arxiv.org/abs/2106.11740"/>
        <updated>2021-06-23T01:48:38.047Z</updated>
        <summary type="html"><![CDATA[Modern pre-trained language models are mostly built upon backbones stacking
self-attention and feed-forward layers in an interleaved order. In this paper,
beyond this stereotyped layer pattern, we aim to improve pre-trained models by
exploiting layer variety from two aspects: the layer type set and the layer
order. Specifically, besides the original self-attention and feed-forward
layers, we introduce convolution into the layer type set, which is
experimentally found beneficial to pre-trained models. Furthermore, beyond the
original interleaved order, we explore more layer orders to discover more
powerful architectures. However, the introduced layer variety leads to a large
architecture space of more than billions of candidates, while training a single
candidate model from scratch already requires huge computation cost, making it
not affordable to search such a space by directly training large amounts of
candidate models. To solve this problem, we first pre-train a supernet from
which the weights of all candidate models can be inherited, and then adopt an
evolutionary algorithm guided by pre-training accuracy to find the optimal
architecture. Extensive experiments show that LV-BERT model obtained by our
method outperforms BERT and its variants on various downstream tasks. For
example, LV-BERT-small achieves 78.8 on the GLUE testing set, 1.8 higher than
the strong baseline ELECTRA-small.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weihao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qibin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Tuning of a Voice Assistant System for Dysfluent Speech. (arXiv:2106.11759v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.11759</id>
        <link href="http://arxiv.org/abs/2106.11759"/>
        <updated>2021-06-23T01:48:38.037Z</updated>
        <summary type="html"><![CDATA[Dysfluencies and variations in speech pronunciation can severely degrade
speech recognition performance, and for many individuals with
moderate-to-severe speech disorders, voice operated systems do not work.
Current speech recognition systems are trained primarily with data from fluent
speakers and as a consequence do not generalize well to speech with
dysfluencies such as sound or word repetitions, sound prolongations, or audible
blocks. The focus of this work is on quantitative analysis of a consumer speech
recognition system on individuals who stutter and production-oriented
approaches for improving performance for common voice assistant tasks (i.e.,
"what is the weather?"). At baseline, this system introduces a significant
number of insertion and substitution errors resulting in intended speech Word
Error Rates (isWER) that are 13.64\% worse (absolute) for individuals with
fluency disorders. We show that by simply tuning the decoding parameters in an
existing hybrid speech recognition system one can improve isWER by 24\%
(relative) for individuals with fluency disorders. Tuning these parameters
translates to 3.6\% better domain recognition and 1.7\% better intent
recognition relative to the default setup for the 18 study participants across
all stuttering severities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_V/0/1/0/all/0/1"&gt;Vikramjit Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zifang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lea_C/0/1/0/all/0/1"&gt;Colin Lea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tooley_L/0/1/0/all/0/1"&gt;Lauren Tooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1"&gt;Sarah Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botten_D/0/1/0/all/0/1"&gt;Darren Botten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palekar_A/0/1/0/all/0/1"&gt;Ashwini Palekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thelapurath_S/0/1/0/all/0/1"&gt;Shrinath Thelapurath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Georgiou_P/0/1/0/all/0/1"&gt;Panayiotis Georgiou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kajarekar_S/0/1/0/all/0/1"&gt;Sachin Kajarekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jefferey Bigham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Deep Neural Network Learning using Classification Confidence Thresholding. (arXiv:2106.11437v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11437</id>
        <link href="http://arxiv.org/abs/2106.11437"/>
        <updated>2021-06-23T01:48:38.013Z</updated>
        <summary type="html"><![CDATA[Most modern neural networks for classification fail to take into account the
concept of the unknown. Trained neural networks are usually tested in an
unrealistic scenario with only examples from a closed set of known classes. In
an attempt to develop a more realistic model, the concept of working in an open
set environment has been introduced. This in turn leads to the concept of
incremental learning where a model with its own architecture and initial
trained set of data can identify unknown classes during the testing phase and
autonomously update itself if evidence of a new class is detected. Some
problems that arise in incremental learning are inefficient use of resources to
retrain the classifier repeatedly and the decrease of classification accuracy
as multiple classes are added over time. This process of instantiating new
classes is repeated as many times as necessary, accruing errors. To address
these problems, this paper proposes the Classification Confidence Threshold
approach to prime neural networks for incremental learning to keep accuracies
high by limiting forgetting. A lean method is also used to reduce resources
used in the retraining of the neural network. The proposed method is based on
the idea that a network is able to incrementally learn a new class even when
exposed to a limited number samples associated with the new class. This method
can be applied to most existing neural networks with minimal changes to network
architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leo_J/0/1/0/all/0/1"&gt;Justin Leo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-lingual Adaptation for Sequence Tagging and Beyond. (arXiv:2010.12405v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12405</id>
        <link href="http://arxiv.org/abs/2010.12405"/>
        <updated>2021-06-23T01:48:38.000Z</updated>
        <summary type="html"><![CDATA[Cross-lingual adaptation with multilingual pre-trained language models
(mPTLMs) mainly consists of two lines of works: zero-shot approach and
translation-based approach, which have been studied extensively on the
sequence-level tasks. We further verify the efficacy of these cross-lingual
adaptation approaches by evaluating their performances on more fine-grained
sequence tagging tasks. After re-examining their strengths and drawbacks, we
propose a novel framework to consolidate the zero-shot approach and the
translation-based approach for better adaptation performance. Instead of simply
augmenting the source data with the machine-translated data, we tailor-make a
warm-up mechanism to quickly update the mPTLMs with the gradients estimated on
a few translated data. Then, the adaptation approach is applied to the refined
parameters and the cross-lingual transfer is performed in a warm-start way. The
experimental results on nine target languages demonstrate that our method is
beneficial to the cross-lingual adaptation of various sequence tagging tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1"&gt;Lidong Bing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenxuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dive into Deep Learning. (arXiv:2106.11342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.11342</id>
        <link href="http://arxiv.org/abs/2106.11342"/>
        <updated>2021-06-23T01:48:37.985Z</updated>
        <summary type="html"><![CDATA[This open-source book represents our attempt to make deep learning
approachable, teaching readers the concepts, the context, and the code. The
entire book is drafted in Jupyter notebooks, seamlessly integrating exposition
figures, math, and interactive examples with self-contained code. Our goal is
to offer a resource that could (i) be freely available for everyone; (ii) offer
sufficient technical depth to provide a starting point on the path to actually
becoming an applied machine learning scientist; (iii) include runnable code,
showing readers how to solve problems in practice; (iv) allow for rapid
updates, both by us and also by the community at large; (v) be complemented by
a forum for interactive discussion of technical details and to answer
questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.11517</id>
        <link href="http://arxiv.org/abs/2106.11517"/>
        <updated>2021-06-23T01:48:37.975Z</updated>
        <summary type="html"><![CDATA[In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1"&gt;Shamane Siriwardhana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1"&gt;Rivindu Weerasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Elliott Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1"&gt;Suranga Nanayakkara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03335</id>
        <link href="http://arxiv.org/abs/2103.03335"/>
        <updated>2021-06-23T01:48:37.967Z</updated>
        <summary type="html"><![CDATA[Due to high annotation costs making the best use of existing human-created
training data is an important research direction. We, therefore, carry out a
systematic evaluation of transferability of BERT-based neural ranking models
across five English datasets. Previous studies focused primarily on zero-shot
and few-shot transfer from a large dataset to a dataset with a small number of
queries. In contrast, each of our collections has a substantial number of
queries, which enables a full-shot evaluation mode and improves reliability of
our results. Furthermore, since source datasets licences often prohibit
commercial use, we compare transfer learning to training on pseudo-labels
generated by a BM25 scorer. We find that training on pseudo-labels -- possibly
with subsequent fine-tuning using a modest number of annotated queries -- can
produce a competitive or better model compared to transfer learning. Yet, it is
necessary to improve the stability and/or effectiveness of the few-shot
training, which, sometimes, can degrade performance of a pretrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1"&gt;Iurii Mokrii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1"&gt;Leonid Boytsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1"&gt;Pavel Braslavski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Race, Racism, and Anti-Racism in NLP. (arXiv:2106.11410v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11410</id>
        <link href="http://arxiv.org/abs/2106.11410"/>
        <updated>2021-06-23T01:48:37.958Z</updated>
        <summary type="html"><![CDATA[Despite inextricable ties between race and language, little work has
considered race in NLP research and development. In this work, we survey 79
papers from the ACL anthology that mention race. These papers reveal various
types of race-related bias in all stages of NLP model development, highlighting
the need for proactive consideration of how NLP systems can uphold racial
hierarchies. However, persistent gaps in research on race and NLP remain: race
has been siloed as a niche topic and remains ignored in many NLP tasks; most
work operationalizes race as a fixed single-dimensional variable with a
ground-truth label, which risks reinforcing differences produced by historical
racism; and the voices of historically marginalized people are nearly absent in
NLP literature. By identifying where and how NLP literature has and has not
considered race, especially in comparison to related fields, our work calls for
inclusion and racial justice in NLP research practices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1"&gt;Anjalie Field&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1"&gt;Su Lin Blodgett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11403</id>
        <link href="http://arxiv.org/abs/2106.11403"/>
        <updated>2021-06-23T01:48:37.932Z</updated>
        <summary type="html"><![CDATA[Objective: The objective of this study is to develop a deep learning pipeline
to detect signals on dietary supplement-related adverse events (DS AEs) from
Twitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to
2018 that mentioned both DS and AE. We annotated biomedical entities and
relations on 2,000 randomly selected tweets. For the concept extraction task,
we compared the performance of traditional word embeddings with SVM, CRF and
LSTM-CRF classifiers to BERT models. For the relation extraction task, we
compared GloVe vectors with CNN classifiers to BERT models. We chose the best
performing models in each task to assemble an end-to-end deep learning pipeline
to detect DS AE signals and compared the results to the known DS AEs from a DS
knowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models
outperformed traditional word embeddings. The best performing concept
extraction model is the BioBERT model that can identify supplement, symptom,
and body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,
respectively. The best performing relation extraction model is the BERT model
that can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,
respectively. The end-to-end pipeline was able to extract DS indication and DS
AEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the
iDISK, we could find both known and novel DS-AEs. Conclusion: We have
demonstrated the feasibility of detecting DS AE signals from Twitter with a
BioBERT-based deep learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yefeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunpeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference on Word Embedding and Beyond. (arXiv:2106.11384v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11384</id>
        <link href="http://arxiv.org/abs/2106.11384"/>
        <updated>2021-06-23T01:48:37.921Z</updated>
        <summary type="html"><![CDATA[In the text processing context, most ML models are built on word embeddings.
These embeddings are themselves trained on some datasets, potentially
containing sensitive data. In some cases this training is done independently,
in other cases, it occurs as part of training a larger, task-specific model. In
either case, it is of interest to consider membership inference attacks based
on the embedding layer as a way of understanding sensitive information leakage.
But, somewhat surprisingly, membership inference attacks on word embeddings and
their effect in other natural language processing (NLP) tasks that use these
embeddings, have remained relatively unexplored.

In this work, we show that word embeddings are vulnerable to black-box
membership inference attacks under realistic assumptions. Furthermore, we show
that this leakage persists through two other major NLP applications:
classification and text-generation, even when the embedding layer is not
exposed to the attacker. We show that our MI attack achieves high attack
accuracy against a classifier model and an LSTM-based language model. Indeed,
our attack is a cheaper membership inference attack on text-generative models,
which does not require the knowledge of the target model or any expensive
training of text-generative models as shadow models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1"&gt;Saeed Mahloujifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1"&gt;Huseyin A. Inan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chase_M/0/1/0/all/0/1"&gt;Melissa Chase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_E/0/1/0/all/0/1"&gt;Esha Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_M/0/1/0/all/0/1"&gt;Marcello Hasegawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Phrase-level Active Learning for Neural Machine Translation. (arXiv:2106.11375v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11375</id>
        <link href="http://arxiv.org/abs/2106.11375"/>
        <updated>2021-06-23T01:48:37.904Z</updated>
        <summary type="html"><![CDATA[Neural machine translation (NMT) is sensitive to domain shift. In this paper,
we address this problem in an active learning setting where we can spend a
given budget on translating in-domain data, and gradually fine-tune a
pre-trained out-of-domain NMT model on the newly translated data. Existing
active learning methods for NMT usually select sentences based on uncertainty
scores, but these methods require costly translation of full sentences even
when only one or two key phrases within the sentence are informative. To
address this limitation, we re-examine previous work from the phrase-based
machine translation (PBMT) era that selected not full sentences, but rather
individual phrases. However, while incorporating these phrases into PBMT
systems was relatively simple, it is less trivial for NMT systems, which need
to be trained on full sequences to capture larger structural properties of
sentences unique to the new domain. To overcome these hurdles, we propose to
select both full sentences and individual phrases from unlabelled data in the
new domain for routing to human translators. In a German-English translation
task, our active learning approach achieves consistent improvements over
uncertainty-based sentence selection methods, improving up to 1.2 BLEU score
over strong active learning baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Junjie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering. (arXiv:2106.11575v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11575</id>
        <link href="http://arxiv.org/abs/2106.11575"/>
        <updated>2021-06-23T01:48:37.891Z</updated>
        <summary type="html"><![CDATA[One of the main challenges in conversational question answering (CQA) is to
resolve the conversational dependency, such as anaphora and ellipsis. However,
existing approaches do not explicitly train QA models on how to resolve the
dependency, and thus these models are limited in understanding human dialogues.
In this paper, we propose a novel framework, ExCorD (Explicit guidance on how
to resolve Conversational Dependency) to enhance the abilities of QA models in
comprehending conversational context. ExCorD first generates self-contained
questions that can be understood without the conversation history, then trains
a QA model with the pairs of original and self-contained questions using a
consistency-based regularizer. In our experiments, we demonstrate that ExCorD
significantly improves the QA models' performance by up to 1.2 F1 on QuAC, and
5.2 F1 on CANARD, while addressing the limitations of the existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1"&gt;Gangwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunjae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jungsoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the US. (arXiv:2105.10880v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10880</id>
        <link href="http://arxiv.org/abs/2105.10880"/>
        <updated>2021-06-23T01:48:37.718Z</updated>
        <summary type="html"><![CDATA[Climate change has largely impacted our daily lives. As one of its
consequences, we are experiencing more wildfires. In the year 2020, wildfires
burned a record number of 8,888,297 acres in the US. To awaken people's
attention to climate change, and to visualize the current risk of wildfires, We
developed RtFPS, "Real-Time Fire Prediction System". It provides a real-time
prediction visualization of wildfire risk at specific locations base on a
Machine Learning model. It also provides interactive map features that show the
historical wildfire events with environmental info.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1"&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhiyin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_D/0/1/0/all/0/1"&gt;Desmond Chan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tracking Instances as Queries. (arXiv:2106.11963v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11963</id>
        <link href="http://arxiv.org/abs/2106.11963"/>
        <updated>2021-06-23T01:48:37.697Z</updated>
        <summary type="html"><![CDATA[Recently, query based deep networks catch lots of attention owing to their
end-to-end pipeline and competitive results on several fundamental computer
vision tasks, such as object detection, semantic segmentation, and instance
segmentation. However, how to establish a query based video instance
segmentation (VIS) framework with elegant architecture and strong performance
remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e.,
tracking instances as queries), a unified query based VIS framework fully
leveraging the intrinsic one-to-one correspondence between instances and
queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on
YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS
Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single
scale testing \& modest amount of training data}. We also provide
QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 dataset as references
for the VIS community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shusheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1"&gt;Ying Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1"&gt;Bin Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What is all this new MeSH about? Exploring the semantic provenance of new descriptors in the MeSH thesaurus. (arXiv:2101.08293v2 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08293</id>
        <link href="http://arxiv.org/abs/2101.08293"/>
        <updated>2021-06-23T01:48:37.688Z</updated>
        <summary type="html"><![CDATA[The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary
widely used in biomedical knowledge systems, particularly for semantic indexing
of scientific literature. As the MeSH hierarchy evolves through annual version
updates, some new descriptors are introduced that were not previously
available. This paper explores the conceptual provenance of these new
descriptors. In particular, we investigate whether such new descriptors have
been previously covered by older descriptors and what is their current relation
to them. To this end, we propose a framework to categorize new descriptors
based on their current relation to older descriptors. Based on the proposed
classification scheme, we quantify, analyse and present the different types of
new descriptors introduced in MeSH during the last fifteen years. The results
show that only about 25% of new MeSH descriptors correspond to new emerging
concepts, whereas the rest were previously covered by one or more existing
descriptors, either implicitly or explicitly. Most of them were covered by a
single existing descriptor and they usually end up as descendants of it in the
current hierarchy, gradually leading towards a more fine-grained MeSH
vocabulary. These insights about the dynamics of the thesaurus are useful for
the retrospective study of scientific articles annotated with MeSH, but could
also be used to inform the policy of updating the thesaurus in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Turing Award elites revisited: patterns of productivity, collaboration, authorship and impact. (arXiv:2106.11534v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.11534</id>
        <link href="http://arxiv.org/abs/2106.11534"/>
        <updated>2021-06-23T01:48:37.677Z</updated>
        <summary type="html"><![CDATA[The Turing Award is recognized as the most influential and prestigious award
in the field of computer science(CS). With the rise of the science of science
(SciSci), a large amount of bibliographic data has been analyzed in an attempt
to understand the hidden mechanism of scientific evolution. These include the
analysis of the Nobel Prize, including physics, chemistry, medicine, etc. In
this article, we extract and analyze the data of 72 Turing Award laureates from
the complete bibliographic data, fill the gap in the lack of Turing Award
analysis, and discover the development characteristics of computer science as
an independent discipline. First, we show most Turing Award laureates have
long-term and high-quality educational backgrounds, and more than 61% of them
have a degree in mathematics, which indicates that mathematics has played a
significant role in the development of computer science. Secondly, the data
shows that not all scholars have high productivity and high h-index; that is,
the number of publications and h-index is not the leading indicator for
evaluating the Turing Award. Third, the average age of awardees has increased
from 40 to around 70 in recent years. This may be because new breakthroughs
take longer, and some new technologies need time to prove their influence.
Besides, we have also found that in the past ten years, international
collaboration has experienced explosive growth, showing a new paradigm in the
form of collaboration. It is also worth noting that in recent years, the
emergence of female winners has also been eye-catching. Finally, by analyzing
the personal publication records, we find that many people are more likely to
publish high-impact articles during their high-yield periods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yinyu Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Sha Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhou Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hall_W/0/1/0/all/0/1"&gt;Wendy Hall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models. (arXiv:2103.03335v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03335</id>
        <link href="http://arxiv.org/abs/2103.03335"/>
        <updated>2021-06-23T01:48:37.666Z</updated>
        <summary type="html"><![CDATA[Due to high annotation costs making the best use of existing human-created
training data is an important research direction. We, therefore, carry out a
systematic evaluation of transferability of BERT-based neural ranking models
across five English datasets. Previous studies focused primarily on zero-shot
and few-shot transfer from a large dataset to a dataset with a small number of
queries. In contrast, each of our collections has a substantial number of
queries, which enables a full-shot evaluation mode and improves reliability of
our results. Furthermore, since source datasets licences often prohibit
commercial use, we compare transfer learning to training on pseudo-labels
generated by a BM25 scorer. We find that training on pseudo-labels -- possibly
with subsequent fine-tuning using a modest number of annotated queries -- can
produce a competitive or better model compared to transfer learning. Yet, it is
necessary to improve the stability and/or effectiveness of the few-shot
training, which, sometimes, can degrade performance of a pretrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokrii_I/0/1/0/all/0/1"&gt;Iurii Mokrii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boytsov_L/0/1/0/all/0/1"&gt;Leonid Boytsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braslavski_P/0/1/0/all/0/1"&gt;Pavel Braslavski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for Day-Night Place Recognition. (arXiv:2106.11481v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.11481</id>
        <link href="http://arxiv.org/abs/2106.11481"/>
        <updated>2021-06-23T01:48:37.598Z</updated>
        <summary type="html"><![CDATA[Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Sourav Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1"&gt;Michael Milford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating abstractive summaries of Lithuanian news articles using a transformer model. (arXiv:2105.03279v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03279</id>
        <link href="http://arxiv.org/abs/2105.03279"/>
        <updated>2021-06-23T01:48:37.576Z</updated>
        <summary type="html"><![CDATA[In this work, we train the first monolingual Lithuanian transformer model on
a relatively large corpus of Lithuanian news articles and compare various
output decoding algorithms for abstractive news summarization. We achieve an
average ROUGE-2 score 0.163, generated summaries are coherent and look
impressive at first glance. However, some of them contain misleading
information that is not so easy to spot. We describe all the technical details
and share our trained model and accompanying code in an online open-source
repository, as well as some characteristic samples of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1"&gt;Lukas Stankevi&amp;#x10d;ius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1"&gt;Mantas Luko&amp;#x161;evi&amp;#x10d;ius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Models in Detection of Dietary Supplement Adverse Event Signals from Twitter. (arXiv:2106.11403v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.11403</id>
        <link href="http://arxiv.org/abs/2106.11403"/>
        <updated>2021-06-23T01:48:37.549Z</updated>
        <summary type="html"><![CDATA[Objective: The objective of this study is to develop a deep learning pipeline
to detect signals on dietary supplement-related adverse events (DS AEs) from
Twitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to
2018 that mentioned both DS and AE. We annotated biomedical entities and
relations on 2,000 randomly selected tweets. For the concept extraction task,
we compared the performance of traditional word embeddings with SVM, CRF and
LSTM-CRF classifiers to BERT models. For the relation extraction task, we
compared GloVe vectors with CNN classifiers to BERT models. We chose the best
performing models in each task to assemble an end-to-end deep learning pipeline
to detect DS AE signals and compared the results to the known DS AEs from a DS
knowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models
outperformed traditional word embeddings. The best performing concept
extraction model is the BioBERT model that can identify supplement, symptom,
and body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,
respectively. The best performing relation extraction model is the BERT model
that can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,
respectively. The end-to-end pipeline was able to extract DS indication and DS
AEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the
iDISK, we could find both known and novel DS-AEs. Conclusion: We have
demonstrated the feasibility of detecting DS AE signals from Twitter with a
BioBERT-based deep learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yefeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunpeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Query-Driven Topic Model. (arXiv:2106.07346v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07346</id>
        <link href="http://arxiv.org/abs/2106.07346"/>
        <updated>2021-06-23T01:48:37.521Z</updated>
        <summary type="html"><![CDATA[Topic modeling is an unsupervised method for revealing the hidden semantic
structure of a corpus. It has been increasingly widely adopted as a tool in the
social sciences, including political science, digital humanities and
sociological research in general. One desirable property of topic models is to
allow users to find topics describing a specific aspect of the corpus. A
possible solution is to incorporate domain-specific knowledge into topic
modeling, but this requires a specification from domain experts. We propose a
novel query-driven topic model that allows users to specify a simple query in
words or phrases and return query-related topics, thus avoiding tedious work
from domain experts. Our proposed approach is particularly attractive when the
user-specified query has a low occurrence in a text corpus, making it difficult
for traditional topic models built on word cooccurrence patterns to identify
relevant topics. Experimental results demonstrate the effectiveness of our
model in comparison with both classical topic models and neural topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1"&gt;Rob Procter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT. (arXiv:2104.08653v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08653</id>
        <link href="http://arxiv.org/abs/2104.08653"/>
        <updated>2021-06-23T01:48:37.499Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) and Information Retrieval (IR) in the
judicial domain is an essential task. With the advent of availability
domain-specific data in electronic form and aid of different Artificial
intelligence (AI) technologies, automated language processing becomes more
comfortable, and hence it becomes feasible for researchers and developers to
provide various automated tools to the legal community to reduce human burden.
The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in
association with the International Conference on Artificial Intelligence and
Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined
four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to
provide few automated systems to the judicial system. The paper presents our
working note on the experiments carried out as a part of our participation in
all the sub-tasks defined in this shared task. We make use of different
Information Retrieval(IR) and deep learning based approaches to tackle these
problems. We obtain encouraging results in all these four sub-tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1"&gt;Baban Gain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1"&gt;Dibyanayan Bandyopadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1"&gt;Tanik Saikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-23T01:48:37.480Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying the Impact of Human Capital, Job History, and Language Factors on Job Seniority with a Large-scale Analysis of Resumes. (arXiv:2106.11846v1 [econ.GN])]]></title>
        <id>http://arxiv.org/abs/2106.11846</id>
        <link href="http://arxiv.org/abs/2106.11846"/>
        <updated>2021-06-23T01:48:37.435Z</updated>
        <summary type="html"><![CDATA[As job markets worldwide have become more competitive and applicant selection
criteria have become more opaque, and different (and sometimes contradictory)
information and advice is available for job seekers wishing to progress in
their careers, it has never been more difficult to determine which factors in a
r\'esum\'e most effectively help career progression. In this work we present a
novel, large scale dataset of over half a million r\'esum\'es with preliminary
analysis to begin to answer empirically which factors help or hurt people
wishing to transition to more senior roles as they progress in their career. We
find that previous experience forms the most important factor, outweighing
other aspects of human capital, and find which language factors in a r\'esum\'e
have significant effects. This lays the groundwork for future inquiry in career
trajectories using large scale data analysis and natural language processing
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Wright_A/0/1/0/all/0/1"&gt;Austin P Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Ziems_C/0/1/0/all/0/1"&gt;Caleb Ziems&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Park_H/0/1/0/all/0/1"&gt;Haekyu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Saad_Falcon_J/0/1/0/all/0/1"&gt;Jon Saad-Falcon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Tomprou_M/0/1/0/all/0/1"&gt;Maria Tomprou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Mathematical Objects of Interest -- A Study of Mathematical Notations. (arXiv:2002.02712v3 [cs.DL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02712</id>
        <link href="http://arxiv.org/abs/2002.02712"/>
        <updated>2021-06-23T01:48:37.249Z</updated>
        <summary type="html"><![CDATA[Mathematical notation, i.e., the writing system used to communicate concepts
in mathematics, encodes valuable information for a variety of information
search and retrieval systems. Yet, mathematical notations remain mostly
unutilized by today's systems. In this paper, we present the first in-depth
study on the distributions of mathematical notation in two large scientific
corpora: the open access arXiv (2.5B mathematical objects) and the mathematical
reviewing service for pure and applied mathematics zbMATH (61M mathematical
objects). Our study lays a foundation for future research projects on
mathematical information retrieval for large scientific corpora. Further, we
demonstrate the relevance of our results to a variety of use-cases. For
example, to assist semantic extraction systems, to improve scientific search
engines, and to facilitate specialized math recommendation systems. The
contributions of our presented research are as follows: (1) we present the
first distributional analysis of mathematical formulae on arXiv and zbMATH; (2)
we retrieve relevant mathematical objects for given textual search queries
(e.g., linking $P_{n}^{(\alpha, \beta)}\!\left(x\right)$ with `Jacobi
polynomial'); (3) we extend zbMATH's search engine by providing relevant
mathematical formulae; and (4) we exemplify the applicability of the results by
presenting auto-completion for math inputs as the first contribution to math
recommendation systems. To expedite future research projects, we have made
available our source code and data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greiner_Petter_A/0/1/0/all/0/1"&gt;Andre Greiner-Petter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubotz_M/0/1/0/all/0/1"&gt;Moritz Schubotz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_F/0/1/0/all/0/1"&gt;Fabian Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Breitinger_C/0/1/0/all/0/1"&gt;Corinna Breitinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohl_H/0/1/0/all/0/1"&gt;Howard S. Cohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1"&gt;Bela Gipp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering. (arXiv:2106.11517v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.11517</id>
        <link href="http://arxiv.org/abs/2106.11517"/>
        <updated>2021-06-23T01:48:37.203Z</updated>
        <summary type="html"><![CDATA[In this paper, we illustrate how to fine-tune the entire Retrieval Augment
Generation (RAG) architecture in an end-to-end manner. We highlighted the main
engineering challenges that needed to be addressed to achieve this objective.
We also compare how end-to-end RAG architecture outperforms the original RAG
architecture for the task of question answering. We have open-sourced our
implementation in the HuggingFace Transformers library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siriwardhana_S/0/1/0/all/0/1"&gt;Shamane Siriwardhana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weerasekera_R/0/1/0/all/0/1"&gt;Rivindu Weerasekera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1"&gt;Elliott Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nanayakkara_S/0/1/0/all/0/1"&gt;Suranga Nanayakkara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certification of Iterative Predictions in Bayesian Neural Networks. (arXiv:2105.10134v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10134</id>
        <link href="http://arxiv.org/abs/2105.10134"/>
        <updated>2021-06-22T01:57:13.990Z</updated>
        <summary type="html"><![CDATA[We consider the problem of computing reach-avoid probabilities for iterative
predictions made with Bayesian neural network (BNN) models. Specifically, we
leverage bound propagation techniques and backward recursion to compute lower
bounds for the probability that trajectories of the BNN model reach a given set
of states while avoiding a set of unsafe states. We use the lower bounds in the
context of control and reinforcement learning to provide safety certification
for given control policies, as well as to synthesize control policies that
improve the certification bounds. On a set of benchmarks, we demonstrate that
our framework can be employed to certify policies over BNNs predictions for
problems of more than $10$ dimensions, and to effectively synthesize policies
that significantly increase the lower bound on the satisfaction probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wicker_M/0/1/0/all/0/1"&gt;Matthew Wicker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laurenti_L/0/1/0/all/0/1"&gt;Luca Laurenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patane_A/0/1/0/all/0/1"&gt;Andrea Patane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paoletti_N/0/1/0/all/0/1"&gt;Nicola Paoletti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1"&gt;Alessandro Abate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1"&gt;Marta Kwiatkowska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basic and Depression Specific Emotion Identification in Tweets: Multi-label Classification Experiments. (arXiv:2105.12364v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12364</id>
        <link href="http://arxiv.org/abs/2105.12364"/>
        <updated>2021-06-22T01:57:13.943Z</updated>
        <summary type="html"><![CDATA[In this paper, we present empirical analysis on basic and depression specific
multi-emotion mining in Tweets with the help of state of the art multi-label
classifiers. We choose our basic emotions from a hybrid emotion model
consisting of the common emotions from four highly regarded psychological
models of emotions. Moreover, we augment that emotion model with new emotion
categories because of their importance in the analysis of depression. Most of
those additional emotions have not been used in previous emotion mining
research. Our experimental analyses show that a cost sensitive RankSVM
algorithm and a Deep Learning model are both robust, measured by both Macro
F-measures and Micro F-measures. This suggests that these algorithms are
superior in addressing the widely known data imbalance problem in multi-label
learning. Moreover, our application of Deep Learning performs the best, giving
it an edge in modeling deep semantic features of our extended emotional
categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farruque_N/0/1/0/all/0/1"&gt;Nawshad Farruque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chenyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1"&gt;Osmar Zaiane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goebel_R/0/1/0/all/0/1"&gt;Randy Goebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00676</id>
        <link href="http://arxiv.org/abs/2106.00676"/>
        <updated>2021-06-22T01:57:13.921Z</updated>
        <summary type="html"><![CDATA[Classifying the core textual components of a scientific paper-title, author,
body text, etc.-is a critical first step in automated scientific document
understanding. Previous work has shown how using elementary layout information,
i.e., each token's 2D position on the page, leads to more accurate
classification. We introduce new methods for incorporating VIsual LAyout (VILA)
structures, e.g., the grouping of page texts into text lines or text blocks,
into language models to further improve performance. We show that the I-VILA
approach, which simply adds special tokens denoting the boundaries of layout
structures into model inputs, can lead to 1.9% Macro F1 improvements for token
classification. Moreover, we design a hierarchical model, H-VILA, that encodes
the text based on layout structures and record an up-to 47% inference time
reduction with less than 1.5% Macro F1 loss for the text classification models.
Experiments are conducted on a newly curated evaluation suite, S2-VLUE, with a
novel metric measuring classification uniformity within visual groups and a new
dataset of gold annotations covering papers from 19 scientific disciplines.
Pre-trained weights, benchmark datasets, and source code will be available at
https://github.com/allenai/VILA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zejiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lucy Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1"&gt;Doug Downey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis. (arXiv:2106.07524v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07524</id>
        <link href="http://arxiv.org/abs/2106.07524"/>
        <updated>2021-06-22T01:57:13.916Z</updated>
        <summary type="html"><![CDATA[Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist
medical specialists in vital circumstances. Deep learning methodologies
constitute a main approach for chest CT scan analysis and disease prediction.
However, large annotated databases are necessary for developing deep learning
models that are able to provide COVID-19 diagnosis across various medical
environments in different countries. Due to privacy issues, publicly available
COVID-19 CT datasets are highly difficult to obtain, which hinders the research
and development of AI-enabled diagnosis methods of COVID-19 based on CT scans.
In this paper we present the COV19-CT-DB database which is annotated for
COVID-19, consisting of about 5,000 3-D CT scans, We have split the database in
training, validation and test datasets. The former two datasets can be used for
training and validation of machine learning models, while the latter will be
used for evaluation of the developed models. We also present a deep learning
approach, based on a CNN-RNN network and report its performance on the
COVID19-CT-DB database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1"&gt;Dimitrios Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1"&gt;Anastasios Arsenos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soukissian_L/0/1/0/all/0/1"&gt;Levon Soukissian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1"&gt;Stefanos Kollias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Autonomous Road Vehicle Integrated Approaches to an Emergency Obstacle Avoidance Maneuver. (arXiv:2105.09446v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09446</id>
        <link href="http://arxiv.org/abs/2105.09446"/>
        <updated>2021-06-22T01:57:13.911Z</updated>
        <summary type="html"><![CDATA[As passenger vehicle technologies have advanced, so have their capabilities
to avoid obstacles, especially with developments in tires, suspensions,
steering, as well as safety technologies like ABS, ESC, and more recently, ADAS
systems. However, environments around passenger vehicles have also become more
complex, and dangerous. There have previously been studies that outline driver
tendencies and performance capabilities when attempting to avoid obstacles
while driving passenger vehicles. Now that autonomous vehicles are being
developed with obstacle avoidance capabilities, it is important to target
performance that meets or exceeds that of human drivers. This manuscript
highlights systems that are crucial for an emergency obstacle avoidance
maneuver (EOAM) and identifies the state-of-the-art for each of the related
systems, while considering the nuances of traveling at highway speeds. Some of
the primary EOAM-related systems/areas that are discussed in this review are:
general path planning methods, system hierarchies, decision-making, trajectory
generation, and trajectory-tracking control methods. After concluding remarks,
suggestions for future work which could lead to an ideal EOAM development, are
discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowe_E/0/1/0/all/0/1"&gt;Evan Lowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guvenc_L/0/1/0/all/0/1"&gt;Levent Guven&amp;#xe7;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-06-22T01:57:13.906Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Rate-Distortion-Perception Representations for Lossy Compression. (arXiv:2106.10311v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.10311</id>
        <link href="http://arxiv.org/abs/2106.10311"/>
        <updated>2021-06-22T01:57:13.900Z</updated>
        <summary type="html"><![CDATA[In the context of lossy compression, Blau & Michaeli (2019) adopt a
mathematical notion of perceptual quality and define the information
rate-distortion-perception function, generalizing the classical rate-distortion
tradeoff. We consider the notion of universal representations in which one may
fix an encoder and vary the decoder to achieve any point within a collection of
distortion and perception constraints. We prove that the corresponding
information-theoretic universal rate-distortion-perception function is
operationally achievable in an approximate sense. Under MSE distortion, we show
that the entire distortion-perception tradeoff of a Gaussian source can be
achieved by a single encoder of the same rate asymptotically. We then
characterize the achievable distortion-perception region for a fixed
representation in the case of arbitrary distributions, identify conditions
under which the aforementioned results continue to hold approximately, and
study the case when the rate is not fixed in advance. This motivates the study
of practical constructions that are approximately universal across the RDP
tradeoff, thereby alleviating the need to design a new encoder for each
objective. We provide experimental results on MNIST and SVHN suggesting that on
image compression tasks, the operational tradeoffs achieved by machine learning
models with a fixed encoder suffer only a small penalty when compared to their
variable encoder counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;George Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jingjing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khisti_A/0/1/0/all/0/1"&gt;Ashish Khisti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-domain Multi-relational Link Prediction. (arXiv:2106.06171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06171</id>
        <link href="http://arxiv.org/abs/2106.06171"/>
        <updated>2021-06-22T01:57:13.886Z</updated>
        <summary type="html"><![CDATA[Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Phuc_L/0/1/0/all/0/1"&gt;Luu Huu Phuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koh Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1"&gt;Seiji Okajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1"&gt;Arseny Tolmachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1"&gt;Tomoyoshi Takebayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1"&gt;Koji Maruhashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTC: Multiresolution Tensor Completion from Partial and Coarse Observations. (arXiv:2106.07135v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07135</id>
        <link href="http://arxiv.org/abs/2106.07135"/>
        <updated>2021-06-22T01:57:13.868Z</updated>
        <summary type="html"><![CDATA[Existing tensor completion formulation mostly relies on partial observations
from a single tensor. However, tensors extracted from real-world data are often
more complex due to: (i) Partial observation: Only a small subset (e.g., 5%) of
tensor elements are available. (ii) Coarse observation: Some tensor modes only
present coarse and aggregated patterns (e.g., monthly summary instead of daily
reports). In this paper, we are given a subset of the tensor and some
aggregated/coarse observations (along one or more modes) and seek to recover
the original fine-granular tensor with low-rank factorization. We formulate a
coupled tensor completion problem and propose an efficient Multi-resolution
Tensor Completion model (MTC) to solve the problem. Our MTC model explores
tensor mode properties and leverages the hierarchy of resolutions to
recursively initialize an optimization setup, and optimizes on the coupled
system using alternating least squares. MTC ensures low computational and space
complexity. We evaluate our model on two COVID-19 related spatio-temporal
tensors. The experiments show that MTC could provide 65.20% and 75.79%
percentage of fitness (PoF) in tensor completion with only 5% fine granular
observations, which is 27.96% relative improvement over the best baseline. To
evaluate the learned low-rank factors, we also design a tensor prediction task
for daily and cumulative disease case predictions, where MTC achieves 50% in
PoF and 30% relative improvements over the best baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chaoqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navjot Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Cao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Qian_C/0/1/0/all/0/1"&gt;Cheng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Solomonik_E/0/1/0/all/0/1"&gt;Edgar Solomonik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependency Structure Misspecification in Multi-Source Weak Supervision Models. (arXiv:2106.10302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10302</id>
        <link href="http://arxiv.org/abs/2106.10302"/>
        <updated>2021-06-22T01:57:13.856Z</updated>
        <summary type="html"><![CDATA[Data programming (DP) has proven to be an attractive alternative to costly
hand-labeling of data.

In DP, users encode domain knowledge into \emph{labeling functions} (LF),
heuristics that label a subset of the data noisily and may have complex
dependencies. A label model is then fit to the LFs to produce an estimate of
the unknown class label.

The effects of label model misspecification on test set performance of a
downstream classifier are understudied. This presents a serious awareness gap
to practitioners, in particular since the dependency structure among LFs is
frequently ignored in field applications of DP.

We analyse modeling errors due to structure over-specification.

We derive novel theoretical bounds on the modeling error and empirically show
that this error can be substantial, even when modeling a seemingly sensible
structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cachay_S/0/1/0/all/0/1"&gt;Salva R&amp;#xfc;hling Cachay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boecking_B/0/1/0/all/0/1"&gt;Benedikt Boecking&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubrawski_A/0/1/0/all/0/1"&gt;Artur Dubrawski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory compression and thermal efficiency of quantum implementations of non-deterministic hidden Markov models. (arXiv:2105.06285v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06285</id>
        <link href="http://arxiv.org/abs/2105.06285"/>
        <updated>2021-06-22T01:57:13.839Z</updated>
        <summary type="html"><![CDATA[Stochastic modelling is an essential component of the quantitative sciences,
with hidden Markov models (HMMs) often playing a central role. Concurrently,
the rise of quantum technologies promises a host of advantages in computational
problems, typically in terms of the scaling of requisite resources such as time
and memory. HMMs are no exception to this, with recent results highlighting
quantum implementations of deterministic HMMs exhibiting superior memory and
thermal efficiency relative to their classical counterparts. In many contexts
however, non-deterministic HMMs are viable alternatives; compared to them the
advantages of current quantum implementations do not always hold. Here, we
provide a systematic prescription for constructing quantum implementations of
non-deterministic HMMs that re-establish the quantum advantages against this
broader class. Crucially, we show that whenever the classical implementation
suffers from thermal dissipation due to its need to process information in a
time-local manner, our quantum implementations will both mitigate some of this
dissipation, and achieve an advantage in memory compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Elliott_T/0/1/0/all/0/1"&gt;Thomas J. Elliott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08850</id>
        <link href="http://arxiv.org/abs/2102.08850"/>
        <updated>2021-06-22T01:57:13.719Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Steffen Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory. (arXiv:2105.03361v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03361</id>
        <link href="http://arxiv.org/abs/2105.03361"/>
        <updated>2021-06-22T01:57:13.714Z</updated>
        <summary type="html"><![CDATA[We develop a variational framework to understand the properties of functions
learned by deep neural networks with ReLU activation functions fit to data. We
propose a new function space, which is reminiscent of classical bounded
variation spaces, that captures the compositional structure associated with
deep neural networks. We derive a representer theorem showing that deep ReLU
networks are solutions to regularized data fitting problems in this function
space. The function space consists of compositions of functions from the
(non-reflexive) Banach spaces of second-order bounded variation in the Radon
domain. These are Banach spaces with sparsity-promoting norms, giving insight
into the role of sparsity in deep neural networks. The neural network solutions
have skip connections and rank bounded weight matrices, providing new
theoretical support for these common architectural choices. The variational
problem we study can be recast as a finite-dimensional neural network training
problem with regularization schemes related to the notions of weight decay and
path-norm regularization. Finally, our analysis builds on techniques from
variational spline theory, providing new connections between deep neural
networks and splines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Parhi_R/0/1/0/all/0/1"&gt;Rahul Parhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1"&gt;Robert D. Nowak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection. (arXiv:2103.05354v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05354</id>
        <link href="http://arxiv.org/abs/2103.05354"/>
        <updated>2021-06-22T01:57:13.708Z</updated>
        <summary type="html"><![CDATA[Security-sensitive applications that rely on Deep Neural Networks (DNNs) are
vulnerable to small perturbations that are crafted to generate Adversarial
Examples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify
them. Many defense and detection techniques have been proposed. Model's
confidences and Dropout, as a popular way to estimate the model's uncertainty,
have been used for AE detection but they showed limited success against black-
and gray-box attacks. Moreover, the state-of-the-art detection techniques have
been designed for specific attacks or broken by others, need knowledge about
the attacks, are not consistent, increase model parameters overhead, are
time-consuming, or have latency in inference time. To trade off these factors,
we revisit the model's uncertainty and confidences and propose a novel
unsupervised ensemble AE detection mechanism that 1) uses the uncertainty
method called SelectiveNet, 2) processes model layers outputs, i.e.feature
maps, to generate new confidence probabilities. The detection method is called
Selective and Feature based Adversarial Detection (SFAD). Experimental results
show that the proposed approach achieves better performance against black- and
gray-box attacks than the state-of-the-art methods and achieves comparable
performance against white-box attacks. Moreover, results show that SFAD is
fully robust against High Confidence Attacks (HCAs) for MNIST and partially
robust for CIFAR10 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1"&gt;Ahmed Aldahdooh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier D&amp;#xe9;forges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Black Hole Properties from Astronomical Multivariate Time Series with Bayesian Attentive Neural Processes. (arXiv:2106.01450v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01450</id>
        <link href="http://arxiv.org/abs/2106.01450"/>
        <updated>2021-06-22T01:57:13.703Z</updated>
        <summary type="html"><![CDATA[Among the most extreme objects in the Universe, active galactic nuclei (AGN)
are luminous centers of galaxies where a black hole feeds on surrounding
matter. The variability patterns of the light emitted by an AGN contain
information about the physical properties of the underlying black hole.
Upcoming telescopes will observe over 100 million AGN in multiple broadband
wavelengths, yielding a large sample of multivariate time series with long gaps
and irregular sampling. We present a method that reconstructs the AGN time
series and simultaneously infers the posterior probability density distribution
(PDF) over the physical quantities of the black hole, including its mass and
luminosity. We apply this method to a simulated dataset of 11,000 AGN and
report precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole
mass. This work is the first to address probabilistic time series
reconstruction and parameter inference for AGN in an end-to-end fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Park_J/0/1/0/all/0/1"&gt;Ji Won Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Villar_A/0/1/0/all/0/1"&gt;Ashley Villar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yan-Fei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1"&gt;Joshua Yao-Yu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Marshall_P/0/1/0/all/0/1"&gt;Philip J. Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Roodman_A/0/1/0/all/0/1"&gt;Aaron Roodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing Differential Privacy Guarantees for Heterogeneous Compositions Using FFT. (arXiv:2102.12412v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12412</id>
        <link href="http://arxiv.org/abs/2102.12412"/>
        <updated>2021-06-22T01:57:13.688Z</updated>
        <summary type="html"><![CDATA[The recently proposed Fast Fourier Transform (FFT)-based accountant for
evaluating $(\varepsilon,\delta)$-differential privacy guarantees using the
privacy loss distribution formalism has been shown to give tighter bounds than
commonly used methods such as R\'enyi accountants when applied to homogeneous
compositions, i.e., to compositions of identical mechanisms. In this paper, we
extend this approach to heterogeneous compositions. We carry out a full error
analysis that allows choosing the parameters of the algorithm such that a
desired accuracy is obtained. The analysis also extends previous results by
taking into account all the parameters of the algorithm. Using the error
analysis, we also give a bound for the computational complexity in terms of the
error which is analogous to and slightly tightens the one given by Murtagh and
Vadhan (2018). We also show how to speed up the evaluation of tight privacy
guarantees using the Plancherel theorem at the cost of increased
pre-computation and memory usage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koskela_A/0/1/0/all/0/1"&gt;Antti Koskela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1"&gt;Antti Honkela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03824</id>
        <link href="http://arxiv.org/abs/2105.03824"/>
        <updated>2021-06-22T01:57:13.683Z</updated>
        <summary type="html"><![CDATA[We show that Transformer encoder architectures can be massively sped up, with
limited accuracy costs, by replacing the self-attention sublayers with simple
linear transformations that "mix" input tokens. These linear transformations,
along with standard nonlinearities in feed-forward layers, prove competent at
modeling semantic relationships in several text classification tasks. Most
surprisingly, we find that replacing the self-attention sublayer in a
Transformer encoder with a standard, unparameterized Fourier Transform achieves
92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains
nearly seven times faster on GPUs and twice as fast on TPUs. The resulting
model, FNet, also scales very efficiently to long inputs. Specifically, when
compared to the "efficient" Transformers on the Long Range Arena benchmark,
FNet matches the accuracy of the most accurate models, but is faster than the
fastest models across all sequence lengths on GPUs (and across relatively
shorter lengths on TPUs). Finally, FNet has a light memory footprint and is
particularly efficient at smaller model sizes: for a fixed speed and accuracy
budget, small FNet models outperform Transformer counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1"&gt;James Lee-Thorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1"&gt;Ilya Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Ontanon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological obstructions in neural networks learning. (arXiv:2012.15834v1 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2012.15834</id>
        <link href="http://arxiv.org/abs/2012.15834"/>
        <updated>2021-06-22T01:57:13.676Z</updated>
        <summary type="html"><![CDATA[We apply methods of topological data analysis to loss functions to gain
insights on learning of deep neural networks and their generalization
properties. We study global properties of the loss function gradient flow. We
use topological data analysis of the loss function and its Morse complex to
relate local behavior along gradient trajectories with global properties of the
loss surface. We define neural network Topological Obstructions score,
TO-score, with help of robust topological invariants, barcodes of loss
function, that quantify the badness of local minima for gradient-based
optimization. We have made several experiments for computing these invariants,
for small neural networks, and for fully connected, convolutional and
ResNet-like neural networks on different datasets: MNIST, Fashion MNIST,
CIFAR10, SVHN. Our two principal observations are as follows. Firstly, the
neural network barcode and TO-score decrease with the increase of the neural
network depth and width. Secondly, there is an intriguing connection between
the length of minima segments in the barcode and the minima generalization
error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1"&gt;Serguei Barannikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1"&gt;Grigorii Sotnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1"&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive-Control-Oriented Meta-Learning for Nonlinear Systems. (arXiv:2103.04490v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04490</id>
        <link href="http://arxiv.org/abs/2103.04490"/>
        <updated>2021-06-22T01:57:13.669Z</updated>
        <summary type="html"><![CDATA[Real-time adaptation is imperative to the control of robots operating in
complex, dynamic environments. Adaptive control laws can endow even nonlinear
systems with good trajectory tracking performance, provided that any uncertain
dynamics terms are linearly parameterizable with known nonlinear features.
However, it is often difficult to specify such features a priori, such as for
aerodynamic disturbances on rotorcraft or interaction forces between a
manipulator arm and various objects. In this paper, we turn to data-driven
modeling with neural networks to learn, offline from past data, an adaptive
controller with an internal parametric model of these nonlinear features. Our
key insight is that we can better prepare the controller for deployment with
control-oriented meta-learning of features in closed-loop simulation, rather
than regression-oriented meta-learning of features to fit input-output data.
Specifically, we meta-learn the adaptive controller with closed-loop tracking
simulation as the base-learner and the average tracking error as the
meta-objective. With a nonlinear planar rotorcraft subject to wind, we
demonstrate that our adaptive controller outperforms other controllers trained
with regression-oriented meta-learning when deployed in closed-loop for
trajectory tracking control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Richards_S/0/1/0/all/0/1"&gt;Spencer M. Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizan_N/0/1/0/all/0/1"&gt;Navid Azizan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slotine_J/0/1/0/all/0/1"&gt;Jean-Jacques Slotine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the intrinsic robustness to noise of some leading classifiers and symmetric loss function -- an empirical evaluation. (arXiv:2010.13570v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13570</id>
        <link href="http://arxiv.org/abs/2010.13570"/>
        <updated>2021-06-22T01:57:13.663Z</updated>
        <summary type="html"><![CDATA[In some industrial applications such as fraud detection, the performance of
common supervision techniques may be affected by the poor quality of the
available labels : in actual operational use-cases, these labels may be weak in
quantity, quality or trustworthiness. We propose a benchmark to evaluate the
natural robustness of different algorithms taken from various paradigms on
artificially corrupted datasets, with a focus on noisy labels. This paper
studies the intrinsic robustness of some leading classifiers. The algorithms
under scrutiny include SVM, logistic regression, random forests, XGBoost,
Khiops. Furthermore, building on results from recent literature, the study is
supplemented with an investigation into the opportunity to enhance some
algorithms with symmetric loss functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baher_H/0/1/0/all/0/1"&gt;Hugo Le Baher&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1"&gt;Vincent Lemaire&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Trinquart_R/0/1/0/all/0/1"&gt;Romain Trinquart&lt;/a&gt; (2) ((1) Polytech Nantes (France), (2) Orange Labs (France))</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplicative Reweighting for Robust Neural Network Optimization. (arXiv:2102.12192v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12192</id>
        <link href="http://arxiv.org/abs/2102.12192"/>
        <updated>2021-06-22T01:57:13.648Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are widespread due to their powerful performance. Yet,
they suffer from degraded performance in the presence of noisy labels at train
time or adversarial examples during inference. Inspired by the setting of
learning with expert advice, where multiplicative weights (MW) updates were
recently shown to be robust to moderate adversarial corruptions, we propose to
use MW for reweighting examples during neural networks optimization. We
establish the convergence of our method when used with gradient descent and
show its advantage in two simple examples. We then validate empirically our
findings by demonstrating that MW improve networks accuracy in the presence of
label noise on CIFAR-10, CIFAR-100 and Clothing1M, and leads to better
robustness to adversarial attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bar_N/0/1/0/all/0/1"&gt;Noga Bar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00666</id>
        <link href="http://arxiv.org/abs/2106.00666"/>
        <updated>2021-06-22T01:57:13.643Z</updated>
        <summary type="html"><![CDATA[Can Transformer perform $2\mathrm{D}$ object-level recognition from a pure
sequence-to-sequence perspective with minimal knowledge about the $2\mathrm{D}$
spatial structure? To answer this question, we present You Only Look at One
Sequence (YOLOS), a series of object detection models based on the na\"ive
Vision Transformer with the fewest possible modifications as well as inductive
biases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset
only can already achieve competitive object detection performance on COCO,
\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$
box AP. We also discuss the impacts as well as limitations of current pre-train
schemes and model scaling strategies for Transformer in vision through object
detection. Code and model weights are available at
\url{https://github.com/hustvl/YOLOS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Bencheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jiyang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1"&gt;Jianwei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-22T01:57:13.637Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement learning for pursuit and evasion of microswimmers at low Reynolds number. (arXiv:2106.08609v1 [physics.flu-dyn] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.08609</id>
        <link href="http://arxiv.org/abs/2106.08609"/>
        <updated>2021-06-22T01:57:13.631Z</updated>
        <summary type="html"><![CDATA[Aquatic organisms can use hydrodynamic cues to navigate, find their preys and
escape from predators. We consider a model of two competing microswimmers
engaged in a pursue-evasion task while immersed in a low-Reynolds-number
environment. The players have limited abilities: they can only sense
hydrodynamic disturbances, which provide some cue about the opponent's
position, and perform simple manoeuvres. The goal of the pursuer is to
capturethe evader in the shortest possible time. Conversely the evader aims at
deferring capture as much as possible. We show that by means of Reinforcement
Learning the players find efficient and physically explainable strategies which
non-trivially exploit the hydrodynamic environment. This Letter offers a
proof-of-concept for the use of Reinforcement Learning to discover
prey-predator strategies in aquatic environments, with potential applications
to underwater robotics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Borra_F/0/1/0/all/0/1"&gt;Francesco Borra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Biferale_L/0/1/0/all/0/1"&gt;Luca Biferale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cencini_M/0/1/0/all/0/1"&gt;Massimo Cencini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Celani_A/0/1/0/all/0/1"&gt;Antonio Celani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Represent Action Values as a Hypergraph on the Action Vertices. (arXiv:2010.14680v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14680</id>
        <link href="http://arxiv.org/abs/2010.14680"/>
        <updated>2021-06-22T01:57:13.625Z</updated>
        <summary type="html"><![CDATA[Action-value estimation is a critical component of many reinforcement
learning (RL) methods whereby sample complexity relies heavily on how fast a
good estimator for action value can be learned. By viewing this problem through
the lens of representation learning, good representations of both state and
action can facilitate action-value estimation. While advances in deep learning
have seamlessly driven progress in learning state representations, given the
specificity of the notion of agency to RL, little attention has been paid to
learning action representations. We conjecture that leveraging the
combinatorial structure of multi-dimensional action spaces is a key ingredient
for learning good representations of action. To test this, we set forth the
action hypergraph networks framework -- a class of functions for learning
action representations in multi-dimensional discrete action spaces with a
structural inductive bias. Using this framework we realise an agent class based
on a combination with deep Q-networks, which we dub hypergraph Q-networks. We
show the effectiveness of our approach on a myriad of domains: illustrative
prediction problems under minimal confounding effects, Atari 2600 games, and
discretised physical control benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavakoli_A/0/1/0/all/0/1"&gt;Arash Tavakoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatemi_M/0/1/0/all/0/1"&gt;Mehdi Fatemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kormushev_P/0/1/0/all/0/1"&gt;Petar Kormushev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07464</id>
        <link href="http://arxiv.org/abs/2105.07464"/>
        <updated>2021-06-22T01:57:13.611Z</updated>
        <summary type="html"><![CDATA[Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengjun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Topological Framework for Deep Learning. (arXiv:2008.13697v13 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.13697</id>
        <link href="http://arxiv.org/abs/2008.13697"/>
        <updated>2021-06-22T01:57:13.603Z</updated>
        <summary type="html"><![CDATA[We utilize classical facts from topology to show that the classification
problem in machine learning is always solvable under very mild conditions.
Furthermore, we show that a softmax classification network acts on an input
topological space by a finite sequence of topological moves to achieve the
classification task. Moreover, given a training dataset, we show how
topological formalism can be used to suggest the appropriate architectural
choices for neural networks designed to be trained as classifiers on the data.
Finally, we show how the architecture of a neural network cannot be chosen
independently from the shape of the underlying data. To demonstrate these
results, we provide example datasets and show how they are acted upon by neural
nets from this topological perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Istvan_K/0/1/0/all/0/1"&gt;Kyle Istvan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Invariant Adversarial Learning. (arXiv:2104.00322v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00322</id>
        <link href="http://arxiv.org/abs/2104.00322"/>
        <updated>2021-06-22T01:57:13.597Z</updated>
        <summary type="html"><![CDATA[The phenomenon of adversarial examples illustrates one of the most basic
vulnerabilities of deep neural networks. Among the variety of techniques
introduced to surmount this inherent weakness, adversarial training has emerged
as the most common and efficient strategy to achieve robustness. Typically,
this is achieved by balancing robust and natural objectives. In this work, we
aim to achieve better trade-off between robust and natural performances by
enforcing a domain-invariant feature representation. We present a new
adversarial training method, Domain Invariant Adversarial Learning (DIAL),
which learns a feature representation which is both robust and domain
invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on
the natural domain and its corresponding adversarial domain. In a case where
the source domain consists of natural examples and the target domain is the
adversarially perturbed examples, our method learns a feature representation
constrained not to discriminate between the natural and adversarial examples,
and can therefore achieve a more robust representation. Our experiments
indicate that our method improves both robustness and natural accuracy, when
compared to current state-of-the-art adversarial training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1"&gt;Matan Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1"&gt;Idan Attias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1"&gt;Aryeh Kontorovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Large Margin Learning. (arXiv:2103.13598v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13598</id>
        <link href="http://arxiv.org/abs/2103.13598"/>
        <updated>2021-06-22T01:57:13.592Z</updated>
        <summary type="html"><![CDATA[This paper serves as a survey of recent advances in large margin training and
its theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)
that are probably the most prominent machine learning models for large-scale
data in the community over the past decade. We generalize the formulation of
classification margins from classical research to latest DNNs, summarize
theoretical connections between the margin, network generalization, and
robustness, and introduce recent efforts in enlarging the margins for DNNs
comprehensively. Since the viewpoint of different methods is discrepant, we
categorize them into groups for ease of comparison and discussion in the paper.
Hopefully, our discussions and overview inspire new research work in the
community that aim to improve the performance of DNNs, and we also point to
directions where the large margin principle can be verified to provide
theoretical evidence why certain regularizations for DNNs function well in
practice. We managed to shorten the paper such that the crucial spirit of large
margin learning and related methods are better emphasized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yiwen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02803</id>
        <link href="http://arxiv.org/abs/2011.02803"/>
        <updated>2021-06-22T01:57:13.590Z</updated>
        <summary type="html"><![CDATA[Contrastive loss and its variants have become very popular recently for
learning visual representations without supervision. In this work, we study
three intriguing properties of contrastive learning. We first generalize the
standard contrastive loss to a broader family of losses, and we find that
various instantiations of the generalized loss perform similarly under the
presence of a multi-layer non-linear projection head. We then study if
instance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,
which are based on global image representation) can learn well on images with
multiple objects present. We find that meaningful hierarchical local features
can be learned despite the fact that these objectives operate on global
instance-level features.

Finally, we study an intriguing phenomenon of feature suppression among
competing features shared across augmented views, such as "color distribution"
vs "object class". We construct datasets with explicit and controllable
competing features, and show that, for contrastive learning, a few bits of
easy-to-learn shared features can suppress, and even fully prevent, the
learning of other sets of competing features. In scenarios where there are
multiple objects in an image, the dominant object would suppress the learning
of smaller objects. Existing contrastive learning methods critically rely on
data augmentation to favor certain sets of features over others, and face
potential limitation for scenarios where existing augmentations cannot fully
address the feature suppression. This poses open challenges to existing
contrastive learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Calvin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lala Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why flatness does and does not correlate with generalization for deep neural networks. (arXiv:2103.06219v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06219</id>
        <link href="http://arxiv.org/abs/2103.06219"/>
        <updated>2021-06-22T01:57:13.585Z</updated>
        <summary type="html"><![CDATA[The intuition that local flatness of the loss landscape is correlated with
better generalization for deep neural networks (DNNs) has been explored for
decades, spawning many different flatness measures. Recently, this link with
generalization has been called into question by a demonstration that many
measures of flatness are vulnerable to parameter re-scaling which arbitrarily
changes their value without changing neural network outputs.

Here we show that, in addition, some popular variants of SGD such as Adam and
Entropy-SGD, can also break the flatness-generalization correlation. As an
alternative to flatness measures, we use a function based picture and propose
using the log of Bayesian prior upon initialization, $\log P(f)$, as a
predictor of the generalization when a DNN converges on function $f$ after
training to zero error. The prior is directly proportional to the Bayesian
posterior for functions that give zero error on a test set. For the case of
image classification, we show that $\log P(f)$ is a significantly more robust
predictor of generalization than flatness measures are.

Whilst local flatness measures fail under parameter re-scaling, the
prior/posterior, which is global quantity, remains invariant under re-scaling.
Moreover, the correlation with generalization as a function of data complexity
remains good for different variants of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuofeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Isaac Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1"&gt;Guillermo Valle P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1"&gt;Ard Louis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining Inference Queries with Bayesian Optimization. (arXiv:2102.05308v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05308</id>
        <link href="http://arxiv.org/abs/2102.05308"/>
        <updated>2021-06-22T01:57:13.570Z</updated>
        <summary type="html"><![CDATA[Obtaining an explanation for an SQL query result can enrich the analysis
experience, reveal data errors, and provide deeper insight into the data.
Inference query explanation seeks to explain unexpected aggregate query results
on inference data; such queries are challenging to explain because an
explanation may need to be derived from the source, training, or inference data
in an ML pipeline. In this paper, we model an objective function as a black-box
function and propose BOExplain, a novel framework for explaining inference
queries using Bayesian optimization (BO). An explanation is a predicate
defining the input tuples that should be removed so that the query result of
interest is significantly affected. BO - a technique for finding the global
optimum of a black-box function - is used to find the best predicate. We
develop two new techniques (individual contribution encoding and warm start) to
handle categorical variables. We perform experiments showing that the
predicates found by BOExplain have a higher degree of explanation compared to
those found by the state-of-the-art query explanation engines. We also show
that BOExplain is effective at deriving explanations for inference queries from
source and training data on a variety of real-world datasets. BOExplain is
open-sourced as a Python package at https://github.com/sfu-db/BOExplain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lockhart_B/0/1/0/all/0/1"&gt;Brandon Lockhart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinglin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weiyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiannan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1"&gt;Eugene Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness in Credit Scoring: Assessment, Implementation and Profit Implications. (arXiv:2103.01907v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01907</id>
        <link href="http://arxiv.org/abs/2103.01907"/>
        <updated>2021-06-22T01:57:13.564Z</updated>
        <summary type="html"><![CDATA[The rise of algorithmic decision-making has spawned much research on fair
machine learning (ML). Financial institutions use ML for building risk
scorecards that support a range of credit-related decisions. Yet, the
literature on fair ML in credit scoring is scarce. The paper makes three
contributions. First, we revisit statistical fairness criteria and examine
their adequacy for credit scoring. Second, we catalog algorithmic options for
incorporating fairness goals in the ML model development pipeline. Last, we
empirically compare different fairness processors in a profit-oriented credit
scoring context using real-world data. The empirical results substantiate the
evaluation of fairness measures, identify suitable options to implement fair
credit scoring, and clarify the profit-fairness trade-off in lending decisions.
We find that multiple fairness criteria can be approximately satisfied at once
and recommend separation as a proper criterion for measuring the fairness of a
scorecard. We also find fair in-processors to deliver a good balance between
profit and fairness and show that algorithmic discrimination can be reduced to
a reasonable level at a relatively low cost. The codes corresponding to the
paper are available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kozodoi_N/0/1/0/all/0/1"&gt;Nikita Kozodoi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jacob_J/0/1/0/all/0/1"&gt;Johannes Jacob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lessmann_S/0/1/0/all/0/1"&gt;Stefan Lessmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data. (arXiv:2104.02005v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02005</id>
        <link href="http://arxiv.org/abs/2104.02005"/>
        <updated>2021-06-22T01:57:13.559Z</updated>
        <summary type="html"><![CDATA[Recently, sound-based COVID-19 detection studies have shown great promise to
achieve scalable and prompt digital pre-screening. However, there are still two
unsolved issues hindering the practice. First, collected datasets for model
training are often imbalanced, with a considerably smaller proportion of users
tested positive, making it harder to learn representative and robust features.
Second, deep learning models are generally overconfident in their predictions.
Clinically, false predictions aggravate healthcare costs. Estimation of the
uncertainty of screening would aid this. To handle these issues, we propose an
ensemble framework where multiple deep learning models for sound-based COVID-19
detection are developed from different but balanced subsets from original data.
As such, data are utilized more effectively compared to traditional up-sampling
and down-sampling approaches: an AUC of 0.74 with a sensitivity of 0.68 and a
specificity of 0.69 is achieved. Simultaneously, we estimate uncertainty from
the disagreement across multiple models. It is shown that false predictions
often yield higher uncertainty, enabling us to suggest the users with certainty
higher than a threshold to repeat the audio test on their phones or to take
clinical tests if digital diagnosis still fails. This study paves the way for a
more robust sound-based COVID-19 automated screening system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qendro_L/0/1/0/all/0/1"&gt;Lorena Qendro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Ting Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1"&gt;Cecilia Mascolo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Input Gradients Highlight Discriminative Features?. (arXiv:2102.12781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12781</id>
        <link href="http://arxiv.org/abs/2102.12781"/>
        <updated>2021-06-22T01:57:13.553Z</updated>
        <summary type="html"><![CDATA[Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,
Smilkov et al., 2017] that provide instance-specific explanations of model
predictions are often based on assumption (A): magnitude of input gradients --
gradients of logits with respect to input -- noisily highlight discriminative
task-relevant features. In this work, we test the validity of assumption (A)
using a three-pronged approach. First, we develop an evaluation framework,
DiffROAR, to test assumption (A) on four image classification benchmarks. Our
results suggest that (i) input gradients of standard models (i.e., trained on
original data) may grossly violate (A), whereas (ii) input gradients of
adversarially robust models satisfy (A). Second, we then introduce BlockMNIST,
an MNIST-based semi-real dataset, that by design encodes a priori knowledge of
discriminative features. Our analysis on BlockMNIST leverages this information
to validate as well as characterize differences between input gradient
attributions of standard and robust models. Finally, we theoretically prove
that our empirical findings hold on a simplified version of the BlockMNIST
dataset. Specifically, we prove that input gradients of standard
one-hidden-layer MLPs trained on this dataset do not highlight
instance-specific signal coordinates, thus grossly violating assumption (A).
Our findings motivate the need to formalize and test common assumptions in
interpretability in a falsifiable manner [Leavitt and Morcos, 2020].
Additionally, we believe that the DiffROAR evaluation framework and
BlockMNIST-based datasets can serve as sanity checks to audit instance-specific
interpretability methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1"&gt;Harshay Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04353</id>
        <link href="http://arxiv.org/abs/2102.04353"/>
        <updated>2021-06-22T01:57:13.547Z</updated>
        <summary type="html"><![CDATA[There has recently been significant interest in training reinforcement
learning (RL) agents in vision-based environments. This poses many challenges,
such as high dimensionality and potential for observational overfitting through
spurious correlations. A promising approach to solve both of these problems is
a self-attention bottleneck, which provides a simple and effective framework
for learning high performing policies, even in the presence of distractions.
However, due to poor scalability of attention architectures, these methods do
not scale beyond low resolution visual inputs, using large patches (thus small
attention matrices). In this paper we make use of new efficient attention
algorithms, recently shown to be highly effective for Transformers, and
demonstrate that these new techniques can be applied in the RL setting. This
allows our attention-based controllers to scale to larger visual inputs, and
facilitate the use of smaller patches, even individual pixels, improving
generalization. In addition, we propose a new efficient algorithm approximating
softmax attention with what we call hybrid random features, leveraging the
theory of angular kernels. We show theoretically and empirically that hybrid
random features is a promising approach when using attention for vision-based
RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1"&gt;Anirban Santara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04174</id>
        <link href="http://arxiv.org/abs/2103.04174"/>
        <updated>2021-06-22T01:57:13.533Z</updated>
        <summary type="html"><![CDATA[A video prediction model that generalizes to diverse scenes would enable
intelligent agents such as robots to perform a variety of tasks via planning
with the model. However, while existing video prediction models have produced
promising results on small datasets, they suffer from severe underfitting when
trained on large and diverse datasets. To address this underfitting challenge,
we first observe that the ability to train larger video prediction models is
often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep
hierarchical latent variable models can produce higher quality predictions by
capturing the multi-level stochasticity of future observations, but end-to-end
optimization of such models is notably difficult. Our key insight is that
greedy and modular optimization of hierarchical autoencoders can simultaneously
address both the memory constraints and the optimization challenges of
large-scale video prediction. We introduce Greedy Hierarchical Variational
Autoencoders (GHVAEs), a method that learns high-fidelity video predictions by
greedily training each level of a hierarchical autoencoder. In comparison to
state-of-the-art models, GHVAEs provide 17-55% gains in prediction performance
on four video datasets, a 35-40% higher success rate on real robot tasks, and
can improve performance monotonically by simply adding more modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bohan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Martin-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-06-22T01:57:13.527Z</updated>
        <summary type="html"><![CDATA[Topological data analysis, including persistent homology, has undergone
significant development in recent years. However, one outstanding challenge is
to build a coherent statistical inference procedure on persistent diagrams. The
paired dependent data structure, which are the births and deaths in persistent
diagrams, adds complexity to statistical inference. In this paper, we present a
new lattice path representation for persistent diagrams. A new exact
statistical inference procedure is developed for lattice paths via
combinatorial enumerations. The proposed lattice path method is applied to
study the topological characterization of the protein structures of the
COVID-19 virus. We demonstrate that there are topological changes during the
conformational change of spike proteins, a necessary step in infecting host
cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Characterizing GAN Convergence Through Proximal Duality Gap. (arXiv:2105.04801v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04801</id>
        <link href="http://arxiv.org/abs/2105.04801"/>
        <updated>2021-06-22T01:57:13.521Z</updated>
        <summary type="html"><![CDATA[Despite the accomplishments of Generative Adversarial Networks (GANs) in
modeling data distributions, training them remains a challenging task. A
contributing factor to this difficulty is the non-intuitive nature of the GAN
loss curves, which necessitates a subjective evaluation of the generated output
to infer training progress. Recently, motivated by game theory, duality gap has
been proposed as a domain agnostic measure to monitor GAN training. However, it
is restricted to the setting when the GAN converges to a Nash equilibrium. But
GANs need not always converge to a Nash equilibrium to model the data
distribution. In this work, we extend the notion of duality gap to proximal
duality gap that is applicable to the general context of training GANs where
Nash equilibria may not exist. We show theoretically that the proximal duality
gap is capable of monitoring the convergence of GANs to a wider spectrum of
equilibria that subsumes Nash equilibria. We also theoretically establish the
relationship between the proximal duality gap and the divergence between the
real and generated data distributions for different GAN formulations. Our
results provide new insights into the nature of GAN convergence. Finally, we
validate experimentally the usefulness of proximal duality gap for monitoring
and influencing GAN training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1"&gt;Sahil Sidheekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1"&gt;Aroof Aimen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1"&gt;Narayanan C. Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Bayesian Optimization. (arXiv:2006.05109v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05109</id>
        <link href="http://arxiv.org/abs/2006.05109"/>
        <updated>2021-06-22T01:57:13.515Z</updated>
        <summary type="html"><![CDATA[Given the increasing importance of machine learning (ML) in our lives,
several algorithmic fairness techniques have been proposed to mitigate biases
in the outcomes of the ML models. However, most of these techniques are
specialized to cater to a single family of ML models and a specific definition
of fairness, limiting their adaptibility in practice. We introduce a general
constrained Bayesian optimization (BO) framework to optimize the performance of
any ML model while enforcing one or multiple fairness constraints. BO is a
model-agnostic optimization method that has been successfully applied to
automatically tune the hyperparameters of ML models. We apply BO with fairness
constraints to a range of popular models, including random forests, gradient
boosting, and neural networks, showing that we can obtain accurate and fair
solutions by acting solely on the hyperparameters. We also show empirically
that our approach is competitive with specialized techniques that enforce
model-specific fairness constraints, and outperforms preprocessing methods that
learn fair representations of the input data. Moreover, our method can be used
in synergy with such specialized fairness techniques to tune their
hyperparameters. Finally, we study the relationship between fairness and the
hyperparameters selected by BO. We observe a correlation between regularization
and unbiased models, explaining why acting on the hyperparameters leads to ML
models that generalize well and are fair.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perrone_V/0/1/0/all/0/1"&gt;Valerio Perrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zafar_M/0/1/0/all/0/1"&gt;Muhammad Bilal Zafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmucker_R/0/1/0/all/0/1"&gt;Robin Schmucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs. (arXiv:2105.05911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05911</id>
        <link href="http://arxiv.org/abs/2105.05911"/>
        <updated>2021-06-22T01:57:13.509Z</updated>
        <summary type="html"><![CDATA[In recent years, algorithms and neural architectures based on the
Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism
problem, emerged as a powerful tool for (supervised) machine learning with
graphs and relational data. Here, we give a comprehensive overview of the
algorithm's use in a machine learning setting. We discuss the theoretical
background, show how to use it for supervised graph- and node classification,
discuss recent extensions, and its connection to neural architectures.
Moreover, we give an overview of current applications and future directions to
stimulate research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1"&gt;Christopher Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fey_M/0/1/0/all/0/1"&gt;Matthias Fey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kriege_N/0/1/0/all/0/1"&gt;Nils M. Kriege&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings. (arXiv:2012.15484v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15484</id>
        <link href="http://arxiv.org/abs/2012.15484"/>
        <updated>2021-06-22T01:57:13.494Z</updated>
        <summary type="html"><![CDATA[Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,
requires a QA-system to include facts from a diverse knowledge graph (KG) in
its reasoning process to produce an answer. Large KGs, especially common-sense
KGs, are known to be incomplete, i.e., not all non-existent facts are always
incorrect. Therefore, being able to reason over incomplete KGs for QA is a
critical requirement in real-world applications that has not been addressed
extensively in the literature. We develop a novel QA architecture that allows
us to reason over incomplete KGs, something current FVQA state-of-the-art
(SOTA) approaches lack due to their critical reliance on fact retrieval. We use
KG Embeddings, a technique widely used for KG completion, for the downstream
task of FVQA. We also employ a new image representation technique we call
'Image-as-Knowledge' to enable this capability, alongside a simple one-step
CoAttention mechanism to attend to text and image during QA. Our FVQA
architecture is faster during inference time, being O(m), as opposed to
existing FVQA SOTA methods which are O(N log N), where m = number of vertices,
N = number of edges = O(m^2). KG embeddings are shown to hold complementary
information to word embeddings: a combination of both metrics permits
performance comparable to SOTA methods in the standard answer retrieval task,
and significantly better (26% absolute) in the proposed missing-edge reasoning
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramnath_K/0/1/0/all/0/1"&gt;Kiran Ramnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1"&gt;Mark Hasegawa-Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[State Entropy Maximization with Random Encoders for Efficient Exploration. (arXiv:2102.09430v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09430</id>
        <link href="http://arxiv.org/abs/2102.09430"/>
        <updated>2021-06-22T01:57:13.494Z</updated>
        <summary type="html"><![CDATA[Recent exploration methods have proven to be a recipe for improving
sample-efficiency in deep reinforcement learning (RL). However, efficient
exploration in high-dimensional observation spaces still remains a challenge.
This paper presents Random Encoders for Efficient Exploration (RE3), an
exploration method that utilizes state entropy as an intrinsic reward. In order
to estimate state entropy in environments with high-dimensional observations,
we utilize a k-nearest neighbor entropy estimator in the low-dimensional
representation space of a convolutional encoder. In particular, we find that
the state entropy can be estimated in a stable and compute-efficient manner by
utilizing a randomly initialized encoder, which is fixed throughout training.
Our experiments show that RE3 significantly improves the sample-efficiency of
both model-free and model-based RL methods on locomotion and navigation tasks
from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3
allows learning diverse behaviors without extrinsic rewards, effectively
improving sample-efficiency in downstream tasks. Source code and videos are
available at https://sites.google.com/view/re3-rl.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1"&gt;Younggyo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lili Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability. (arXiv:2103.00065v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00065</id>
        <link href="http://arxiv.org/abs/2103.00065"/>
        <updated>2021-06-22T01:57:13.494Z</updated>
        <summary type="html"><![CDATA[We empirically demonstrate that full-batch gradient descent on neural network
training objectives typically operates in a regime we call the Edge of
Stability. In this regime, the maximum eigenvalue of the training loss Hessian
hovers just above the numerical value $2 / \text{(step size)}$, and the
training loss behaves non-monotonically over short timescales, yet consistently
decreases over long timescales. Since this behavior is inconsistent with
several widespread presumptions in the field of optimization, our findings
raise questions as to whether these presumptions are relevant to neural network
training. We hope that our findings will inspire future efforts aimed at
rigorously understanding optimization at the Edge of Stability. Code is
available at https://github.com/locuslab/edge-of-stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Jeremy M. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1"&gt;Simran Kaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1"&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations. (arXiv:2011.12854v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12854</id>
        <link href="http://arxiv.org/abs/2011.12854"/>
        <updated>2021-06-22T01:57:13.493Z</updated>
        <summary type="html"><![CDATA[Most explanation methods in deep learning map importance estimates for a
model's prediction back to the original input space. These "visual"
explanations are often insufficient, as the model's actual concept remains
elusive. Moreover, without insights into the model's semantic concept, it is
difficult -- if not impossible -- to intervene on the model's behavior via its
explanations, called Explanatory Interactive Learning. Consequently, we propose
to intervene on a Neuro-Symbolic scene representation, which allows one to
revise the model on the semantic level, e.g. "never focus on the color to make
your decision". We compiled a novel confounded visual scene data set, the
CLEVR-Hans data set, capturing complex compositions of different objects. The
results of our experiments on CLEVR-Hans demonstrate that our semantic
explanations, i.e. compositional explanations at a per-object level, can
identify confounders that are not identifiable using "visual" explanations
only. More importantly, feedback on this semantic level makes it possible to
revise the model from focusing on these factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1"&gt;Wolfgang Stammer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1"&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defense Against Reward Poisoning Attacks in Reinforcement Learning. (arXiv:2102.05776v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05776</id>
        <link href="http://arxiv.org/abs/2102.05776"/>
        <updated>2021-06-22T01:57:13.490Z</updated>
        <summary type="html"><![CDATA[We study defense strategies against reward poisoning attacks in reinforcement
learning. As a threat model, we consider attacks that minimally alter rewards
to make the attacker's target policy uniquely optimal under the poisoned
rewards, with the optimality gap specified by an attack parameter. Our goal is
to design agents that are robust against such attacks in terms of the
worst-case utility w.r.t. the true, unpoisoned, rewards while computing their
policies under the poisoned rewards. We propose an optimization framework for
deriving optimal defense policies, both when the attack parameter is known and
unknown. Moreover, we show that defense policies that are solutions to the
proposed optimization problems have provable performance guarantees. In
particular, we provide the following bounds with respect to the true,
unpoisoned, rewards: a) lower bounds on the expected return of the defense
policies, and b) upper bounds on how suboptimal these defense policies are
compared to the attacker's target policy. We conclude the paper by illustrating
the intuitions behind our formal results, and showing that the derived bounds
are non-trivial.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banihashem_K/0/1/0/all/0/1"&gt;Kiarash Banihashem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1"&gt;Adish Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1"&gt;Goran Radanovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-06-22T01:57:13.485Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08440</id>
        <link href="http://arxiv.org/abs/2104.08440"/>
        <updated>2021-06-22T01:57:13.479Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (RL) techniques can benefit greatly from
leveraging prior experience, which can be either self-generated or acquired
from other entities. Action advising is a framework that provides a flexible
way to transfer such knowledge in the form of actions between teacher-student
peers. However, due to the realistic concerns, the number of these interactions
is limited with a budget; therefore, it is crucial to perform these in the most
appropriate moments. There have been several promising studies recently that
address this problem setting especially from the student's perspective. Despite
their success, they have some shortcomings when it comes to the practical
applicability and integrity as an overall solution to the learning from advice
challenge. In this paper, we extend the idea of advice reusing via teacher
imitation to construct a unified approach that addresses both advice collection
and advice utilisation problems. We also propose a method to automatically tune
the relevant hyperparameters of these components on-the-fly to make it able to
adapt to any task with minimal human intervention. The experiments we performed
in $5$ different Atari games verify that our algorithm either surpasses or
performs on-par with its top competitors while being far simpler to be
employed. Furthermore, its individual components are also found to be providing
significant advantages alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1"&gt;Ercument Ilhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1"&gt;Jeremy Gow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1"&gt;Diego Perez-Liebana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03287</id>
        <link href="http://arxiv.org/abs/2105.03287"/>
        <updated>2021-06-22T01:57:13.472Z</updated>
        <summary type="html"><![CDATA[By computing the rank correlation between attention weights and
feature-additive explanation methods, previous analyses either invalidate or
support the role of attention-based explanations as a faithful and plausible
measure of salience. To investigate whether this approach is appropriate, we
compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and
attention-based explanations, applied to two neural architectures trained on
single- and pair-sequence language tasks. In most cases, we find that none of
our chosen methods agree. Based on our empirical observations and theoretical
objections, we conclude that rank correlation does not measure the quality of
feature-additive methods. Practitioners should instead use the numerous and
rigorous diagnostic methods proposed by the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1"&gt;Michael Neely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1"&gt;Stefan F. Schouten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1"&gt;Maurits J. R. Bleeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1"&gt;Ana Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-22T01:57:13.467Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration. (arXiv:2102.08946v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08946</id>
        <link href="http://arxiv.org/abs/2102.08946"/>
        <updated>2021-06-22T01:57:13.449Z</updated>
        <summary type="html"><![CDATA[Previous studies dominantly target at self-supervised learning on real-valued
networks and have achieved many promising results. However, on the more
challenging binary neural networks (BNNs), this task has not yet been fully
explored in the community. In this paper, we focus on this more difficult
scenario: learning networks where both weights and activations are binary,
meanwhile, without any human annotated labels. We observe that the commonly
used contrastive objective is not satisfying on BNNs for competitive accuracy,
since the backbone network contains relatively limited capacity and
representation ability. Hence instead of directly applying existing
self-supervised methods, which cause a severe decline in performance, we
present a novel guided learning paradigm from real-valued to distill binary
networks on the final prediction distribution, to minimize the loss and obtain
desirable accuracy. Our proposed method can boost the simple contrastive
learning baseline by an absolute gain of 5.5~15% on BNNs. We further reveal
that it is difficult for BNNs to recover the similar predictive distributions
as real-valued models when training without labels. Thus, how to calibrate them
is key to address the degradation in performance. Extensive experiments are
conducted on the large-scale ImageNet and downstream datasets. Our method
achieves substantial improvement over the simple contrastive learning baseline,
and is even comparable to many mainstream supervised BNN methods. Code is
available at https://github.com/szq0214/S2-BNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zechun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1"&gt;Marios Savvides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixture of Robust Experts (MoRE). (arXiv:2104.10586v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10586</id>
        <link href="http://arxiv.org/abs/2104.10586"/>
        <updated>2021-06-22T01:57:13.437Z</updated>
        <summary type="html"><![CDATA[To tackle the susceptibility of deep neural networks to examples, the
adversarial training has been proposed which provides a notion of robust
through an inner maximization problem presenting the first-order embedded
within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1"&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldhahn_R/0/1/0/all/0/1"&gt;Ryan Goldhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks. (arXiv:2105.02968v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02968</id>
        <link href="http://arxiv.org/abs/2105.02968"/>
        <updated>2021-06-22T01:57:13.431Z</updated>
        <summary type="html"><![CDATA[Deep neural networks that yield human interpretable decisions by
architectural design have lately become an increasingly popular alternative to
post hoc interpretation of traditional black-box models. Among these networks,
the arguably most widespread approach is so-called prototype learning, where
similarities to learned latent prototypes serve as the basis of classifying an
unseen data point. In this work, we point to an important shortcoming of such
approaches. Namely, there is a semantic gap between similarity in latent space
and similarity in input space, which can corrupt interpretability. We design
two experiments that exemplify this issue on the so-called ProtoPNet.
Specifically, we find that this network's interpretability mechanism can be led
astray by intentionally crafted or even JPEG compression artefacts, which can
produce incomprehensible decisions. We argue that practitioners ought to have
this shortcoming in mind when deploying prototype-based models in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_A/0/1/0/all/0/1"&gt;Adrian Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1"&gt;Claudio Fanconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1"&gt;Rahul Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas Kohler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis. (arXiv:2106.07524v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07524</id>
        <link href="http://arxiv.org/abs/2106.07524"/>
        <updated>2021-06-22T01:57:13.425Z</updated>
        <summary type="html"><![CDATA[Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist
medical specialists in vital circumstances. Deep learning methodologies
constitute a main approach for chest CT scan analysis and disease prediction.
However, large annotated databases are necessary for developing deep learning
models that are able to provide COVID-19 diagnosis across various medical
environments in different countries. Due to privacy issues, publicly available
COVID-19 CT datasets are highly difficult to obtain, which hinders the research
and development of AI-enabled diagnosis methods of COVID-19 based on CT scans.
In this paper we present the COV19-CT-DB database which is annotated for
COVID-19, consisting of about 5,000 3-D CT scans, We have split the database in
training, validation and test datasets. The former two datasets can be used for
training and validation of machine learning models, while the latter will be
used for evaluation of the developed models. We also present a deep learning
approach, based on a CNN-RNN network and report its performance on the
COVID19-CT-DB database.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1"&gt;Dimitrios Kollias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1"&gt;Anastasios Arsenos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soukissian_L/0/1/0/all/0/1"&gt;Levon Soukissian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1"&gt;Stefanos Kollias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variance-Dependent Best Arm Identification. (arXiv:2106.10417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10417</id>
        <link href="http://arxiv.org/abs/2106.10417"/>
        <updated>2021-06-22T01:57:13.419Z</updated>
        <summary type="html"><![CDATA[We study the problem of identifying the best arm in a stochastic multi-armed
bandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is
associated with an unknown reward distribution supported on $[0,1]$ with mean
$\theta_i$ and variance $\sigma_i^2$. Assume $\theta_1 > \theta_2 \geq \cdots
\geq\theta_n$. We propose an adaptive algorithm which explores the gaps and
variances of the rewards of the arms and makes future decisions based on the
gathered information using a novel approach called \textit{grouped median
elimination}. The proposed algorithm guarantees to output the best arm with
probability $(1-\delta)$ and uses at most $O \left(\sum_{i = 1}^n
\left(\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i}\right)(\ln \delta^{-1}
+ \ln \ln \Delta_i^{-1})\right)$ samples, where $\Delta_i$ ($i \geq 2$) denotes
the reward gap between arm $i$ and the best arm and we define $\Delta_1 =
\Delta_2$. This achieves a significant advantage over the variance-independent
algorithms in some favorable scenarios and is the first result that removes the
extra $\ln n$ factor on the best arm compared with the state-of-the-art. We
further show that $\Omega \left( \sum_{i = 1}^n \left(
\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i} \right) \ln \delta^{-1}
\right)$ samples are necessary for an algorithm to achieve the same goal,
thereby illustrating that our algorithm is optimal up to doubly logarithmic
terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pinyan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chao Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaojin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Designing Interpretable Approximations to Deep Reinforcement Learning. (arXiv:2010.14785v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14785</id>
        <link href="http://arxiv.org/abs/2010.14785"/>
        <updated>2021-06-22T01:57:13.404Z</updated>
        <summary type="html"><![CDATA[In an ever expanding set of research and application areas, deep neural
networks (DNNs) set the bar for algorithm performance. However, depending upon
additional constraints such as processing power and execution time limits, or
requirements such as verifiable safety guarantees, it may not be feasible to
actually use such high-performing DNNs in practice. Many techniques have been
developed in recent years to compress or distill complex DNNs into smaller,
faster or more understandable models and controllers. This work seeks to
identify reduced models that not only preserve a desired performance level, but
also, for example, succinctly explain the latent knowledge represented by a
DNN. We illustrate the effectiveness of the proposed approach on the evaluation
of decision tree variants and kernel machines in the context of benchmark
reinforcement learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dahlin_N/0/1/0/all/0/1"&gt;Nathan Dahlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalagarla_K/0/1/0/all/0/1"&gt;Krishna Chaitanya Kalagarla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1"&gt;Nikhil Naik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1"&gt;Rahul Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nuzzo_P/0/1/0/all/0/1"&gt;Pierluigi Nuzzo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDDA: Explanation-driven Data Augmentation to Improve Model and Explanation Alignment. (arXiv:2105.14162v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14162</id>
        <link href="http://arxiv.org/abs/2105.14162"/>
        <updated>2021-06-22T01:57:13.398Z</updated>
        <summary type="html"><![CDATA[Recent years have seen the introduction of a range of methods for post-hoc
explainability of image classifier predictions. However, these post-hoc
explanations may not always align perfectly with classifier predictions, which
poses a significant challenge when attempting to debug models based on such
explanations. To this end, we seek a methodology that can improve alignment
between model predictions and explanation method that is both agnostic to the
model and explanation classes and which does not require ground truth
explanations. We achieve this through a novel explanation-driven data
augmentation (EDDA) method that augments the training data with occlusions of
existing data stemming from model-explanations; this is based on the simple
motivating principle that occluding salient regions for the model prediction
should decrease the model confidence in the prediction, while occluding
non-salient regions should not change the prediction -- if the model and
explainer are aligned. To verify that this augmentation method improves model
and explainer alignment, we evaluate the methodology on a variety of datasets,
image classification models, and explanation methods. We verify in all cases
that our explanation-driven data augmentation method improves alignment of the
model and explanation in comparison to no data augmentation and non-explanation
driven data augmentation methods. In conclusion, this approach provides a novel
model- and explainer-agnostic methodology for improving alignment between model
predictions and explanations, which we see as a critical step forward for
practical deployment and debugging of image classification models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruiwen Li&lt;/a&gt; (co-first author), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhibo Zhang&lt;/a&gt; (co-first author), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiani Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1"&gt;Scott Sanner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jongseong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yeonjeong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1"&gt;Dongsub Shim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proper Value Equivalence. (arXiv:2106.10316v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.10316</id>
        <link href="http://arxiv.org/abs/2106.10316"/>
        <updated>2021-06-22T01:57:13.392Z</updated>
        <summary type="html"><![CDATA[One of the main challenges in model-based reinforcement learning (RL) is to
decide which aspects of the environment should be modeled. The
value-equivalence (VE) principle proposes a simple answer to this question: a
model should capture the aspects of the environment that are relevant for
value-based planning. Technically, VE distinguishes models based on a set of
policies and a set of functions: a model is said to be VE to the environment if
the Bellman operators it induces for the policies yield the correct result when
applied to the functions. As the number of policies and functions increase, the
set of VE models shrinks, eventually collapsing to a single point corresponding
to a perfect model. A fundamental question underlying the VE principle is thus
how to select the smallest sets of policies and functions that are sufficient
for planning. In this paper we take an important step towards answering this
question. We start by generalizing the concept of VE to order-$k$ counterparts
defined with respect to $k$ applications of the Bellman operator. This leads to
a family of VE classes that increase in size as $k \rightarrow \infty$. In the
limit, all functions become value functions, and we have a special
instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE
class may contain multiple models even in the limit when all value functions
are used. Crucially, all these models are sufficient for planning, meaning that
they will yield an optimal policy despite the fact that they may ignore many
aspects of the environment. We construct a loss function for learning PVE
models and argue that popular algorithms such as MuZero and Muesli can be
understood as minimizing an upper bound for this loss. We leverage this
connection to propose a modification to MuZero and show that it can lead to
improved performance in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grimm_C/0/1/0/all/0/1"&gt;Christopher Grimm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farquhar_G/0/1/0/all/0/1"&gt;Gregory Farquhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1"&gt;David Silver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02190</id>
        <link href="http://arxiv.org/abs/2106.02190"/>
        <updated>2021-06-22T01:57:13.386Z</updated>
        <summary type="html"><![CDATA[We developed Distilled Graph Attention Policy Networks (DGAPNs), a
curiosity-driven reinforcement learning model to generate novel
graph-structured chemical representations that optimize user-defined objectives
by efficiently navigating a physically constrained domain. The framework is
examined on the task of generating molecules that are designed to bind,
noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial
Graph Attention Network (sGAT) that leverages self-attention over both node and
edge attributes as well as encoding spatial structure -- this capability is of
considerable interest in areas such as molecular and synthetic biology and drug
discovery. An attentional policy network is then introduced to learn decision
rules for a dynamic, fragment-based chemical environment, and state-of-the-art
policy gradient techniques are employed to train the network with enhanced
stability. Exploration is efficiently encouraged by incorporating innovation
reward bonuses learned and proposed by random network distillation. In
experiments, our framework achieved outstanding results compared to
state-of-the-art algorithms, while increasing the diversity of proposed
molecules and reducing the complexity of paths to chemical synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1"&gt;Nicholas Choma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1"&gt;Mikaela Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1"&gt;&amp;#xc9;rica T. Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Manesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica G. Melesse Vergara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas S. Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1"&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1"&gt;Martha S. Head&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1"&gt;Peter Nugent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1"&gt;Daniel A. Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Self Reported Symptoms Predict Daily COVID-19 Cases?. (arXiv:2105.08321v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08321</id>
        <link href="http://arxiv.org/abs/2105.08321"/>
        <updated>2021-06-22T01:57:13.372Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic has impacted lives and economies across the globe,
leading to many deaths. While vaccination is an important intervention, its
roll-out is slow and unequal across the globe. Therefore, extensive testing
still remains one of the key methods to monitor and contain the virus. Testing
on a large scale is expensive and arduous. Hence, we need alternate methods to
estimate the number of cases. Online surveys have been shown to be an effective
method for data collection amidst the pandemic. In this work, we develop
machine learning models to estimate the prevalence of COVID-19 using
self-reported symptoms. Our best model predicts the daily cases with a mean
absolute error (MAE) of 226.30 (normalized MAE of 27.09%) per state, which
demonstrates the possibility of predicting the actual number of confirmed cases
by utilizing self-reported symptoms. The models are developed at two levels of
data granularity - local models, which are trained at the state level, and a
single global model which is trained on the combined data aggregated across all
states. Our results indicate a lower error on the local models as opposed to
the global model. In addition, we also show that the most important symptoms
(features) vary considerably from state to state. This work demonstrates that
the models developed on crowd-sourced data, curated via online platforms, can
complement the existing epidemiological surveillance infrastructure in a
cost-effective manner. The code is publicly available at
https://github.com/parthpatwa/Can-Self-Reported-Symptoms-Predict-Daily-COVID-19-Cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_V/0/1/0/all/0/1"&gt;Viswanatha Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sukumaran_R/0/1/0/all/0/1"&gt;Rohan Sukumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+TV_S/0/1/0/all/0/1"&gt;Sethuraman TV&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nashnoush_E/0/1/0/all/0/1"&gt;Eptehal Nashnoush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1"&gt;Sheshank Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1"&gt;Rishemjit Kaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Transfer Learning for Reinforcement Learning in Order Dispatching. (arXiv:2105.13218v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13218</id>
        <link href="http://arxiv.org/abs/2105.13218"/>
        <updated>2021-06-22T01:57:13.366Z</updated>
        <summary type="html"><![CDATA[Order dispatch is one of the central problems to ride-sharing platforms.
Recently, value-based reinforcement learning algorithms have shown promising
performance on this problem. However, in real-world applications, the
non-stationarity of the demand-supply system poses challenges to re-utilizing
data generated in different time periods to learn the value function. In this
work, motivated by the fact that the relative relationship between the values
of some states is largely stable across various environments, we propose a
pattern transfer learning framework for value-based reinforcement learning in
the order dispatch problem. Our method efficiently captures the value patterns
by incorporating a concordance penalty. The superior performance of the
proposed method is supported by experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1"&gt;Runzhe Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Sheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shikai Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearly Minimax Optimal Adversarial Imitation Learning with Known and Unknown Transitions. (arXiv:2106.10424v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10424</id>
        <link href="http://arxiv.org/abs/2106.10424"/>
        <updated>2021-06-22T01:57:13.360Z</updated>
        <summary type="html"><![CDATA[This paper is dedicated to designing provably efficient adversarial imitation
learning (AIL) algorithms that directly optimize policies from expert
demonstrations. Firstly, we develop a transition-aware AIL algorithm named TAIL
with an expert sample complexity of $\tilde{O}(H^{3/2} |S|/\varepsilon)$ under
the known transition setting, where $H$ is the planning horizon, $|S|$ is the
state space size and $\varepsilon$ is desired policy value gap. This improves
upon the previous best bound of $\tilde{O}(H^2 |S| / \varepsilon^2)$ for AIL
methods and matches the lower bound of $\tilde{\Omega} (H^{3/2}
|S|/\varepsilon)$ in [Rajaraman et al., 2021] up to a logarithmic factor. The
key ingredient of TAIL is a fine-grained estimator for expert state-action
distribution, which explicitly utilizes the transition function information.
Secondly, considering practical settings where the transition functions are
usually unknown but environment interaction is allowed, we accordingly develop
a model-based transition-aware AIL algorithm named MB-TAIL. In particular,
MB-TAIL builds an empirical transition model by interacting with the
environment and performs imitation under the recovered empirical model. The
interaction complexity of MB-TAIL is $\tilde{O} (H^3 |S|^2 |A| /
\varepsilon^2)$, which improves the best known result of $\tilde{O} (H^4 |S|^2
|A| / \varepsilon^2)$ in [Shani et al., 2021]. Finally, our theoretical results
are supported by numerical evaluation and detailed analysis on two challenging
MDPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziniu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How COVID-19 Have Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v1 [cs.CY] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.09981</id>
        <link href="http://arxiv.org/abs/2106.09981"/>
        <updated>2021-06-22T01:57:13.355Z</updated>
        <summary type="html"><![CDATA[While the long-term effects of COVID-19 are yet to be determined, its
immediate impact on crowdfunding is nonetheless significant. This study takes a
computational approach to more deeply comprehend this change. Using a unique
data set of all the campaigns published over the past two years on GoFundMe, we
explore the factors that have led to the successful funding of a crowdfunding
project. In particular, we study a corpus of crowdfunded projects, analyzing
cover images and other variables commonly present on crowdfunding sites.
Furthermore, we construct a classifier and a regression model to assess the
significance of features based on XGBoost. In addition, we employ
counterfactual analysis to investigate the causality between features and the
success of crowdfunding. More importantly, sentiment analysis and the paired
sample t-test are performed to examine the differences in crowdfunding
campaigns before and after the COVID-19 outbreak that started in March 2020.
First, we note that there is significant racial disparity in crowdfunding
success. Second, we find that sad emotion expressed through the campaign's
description became significant after the COVID-19 outbreak. Considering all
these factors, our findings shed light on the impact of COVID-19 on
crowdfunding campaigns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junda Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xupin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling. (arXiv:2106.03156v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03156</id>
        <link href="http://arxiv.org/abs/2106.03156"/>
        <updated>2021-06-22T01:57:13.349Z</updated>
        <summary type="html"><![CDATA[We develop a new method of online inference for a vector of parameters
estimated by the Polyak-Ruppert averaging procedure of stochastic gradient
descent (SGD) algorithms. We leverage insights from time series regression in
econometrics and construct asymptotically pivotal statistics via random
scaling. Our approach is fully operational with online data and is rigorously
underpinned by a functional central limit theorem. Our proposed inference
method has a couple of key advantages over the existing methods. First, the
test statistic is computed in an online fashion with only SGD iterates and the
critical values can be obtained without any resampling methods, thereby
allowing for efficient implementation suitable for massive online data. Second,
there is no need to estimate the asymptotic variance and our inference method
is shown to be robust to changes in the tuning parameters for SGD algorithms in
simulation experiments with synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sokbae Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Seo_M/0/1/0/all/0/1"&gt;Myung Hwan Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Youngki Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Outbreak Prediction and Analysis using Self Reported Symptoms. (arXiv:2101.10266v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10266</id>
        <link href="http://arxiv.org/abs/2101.10266"/>
        <updated>2021-06-22T01:57:13.344Z</updated>
        <summary type="html"><![CDATA[It is crucial for policymakers to understand the community prevalence of
COVID-19 so combative resources can be effectively allocated and prioritized
during the COVID-19 pandemic. Traditionally, community prevalence has been
assessed through diagnostic and antibody testing data. However, despite the
increasing availability of COVID-19 testing, the required level has not been
met in most parts of the globe, introducing a need for an alternative method
for communities to determine disease prevalence. This is further complicated by
the observation that COVID-19 prevalence and spread varies across different
spatial, temporal, and demographics. In this study, we understand trends in the
spread of COVID-19 by utilizing the results of self-reported COVID-19 symptoms
surveys as an alternative to COVID-19 testing reports. This allows us to assess
community disease prevalence, even in areas with low COVID-19 testing ability.
Using individually reported symptom data from various populations, our method
predicts the likely percentage of the population that tested positive for
COVID-19. We do so with a Mean Absolute Error (MAE) of 1.14 and Mean Relative
Error (MRE) of 60.40\% with 95\% confidence interval as (60.12, 60.67). This
implies that our model predicts +/- 1140 cases than the original in a
population of 1 million. In addition, we forecast the location-wise percentage
of the population testing positive for the next 30 days using self-reported
symptoms data from previous days. The MAE for this method is as low as 0.15
(MRE of 23.61\% with 95\% confidence interval as (23.6, 13.7)) for New York. We
present an analysis of these results, exposing various clinical attributes of
interest across different demographics. Lastly, we qualitatively analyze how
various policy enactments (testing, curfew) affect the prevalence of COVID-19
in a community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sukumaran_R/0/1/0/all/0/1"&gt;Rohan Sukumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patwa_P/0/1/0/all/0/1"&gt;Parth Patwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethuraman_T/0/1/0/all/0/1"&gt;T V Sethuraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shankar_S/0/1/0/all/0/1"&gt;Sheshank Shankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanaparti_R/0/1/0/all/0/1"&gt;Rishank Kanaparti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1"&gt;Joseph Bae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_Y/0/1/0/all/0/1"&gt;Yash Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Abhishek Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1"&gt;Ayush Chopra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myungsun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramaswamy_P/0/1/0/all/0/1"&gt;Priya Ramaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1"&gt;Ramesh Raskar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Honey, I Shrunk The Actor: A Case Study on Preserving Performance with Smaller Actors in Actor-Critic RL. (arXiv:2102.11893v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11893</id>
        <link href="http://arxiv.org/abs/2102.11893"/>
        <updated>2021-06-22T01:57:13.337Z</updated>
        <summary type="html"><![CDATA[Actors and critics in actor-critic reinforcement learning algorithms are
functionally separate, yet they often use the same network architectures. This
case study explores the performance impact of network sizes when considering
actor and critic architectures independently. By relaxing the assumption of
architectural symmetry, it is often possible for smaller actors to achieve
comparable policy performance to their symmetric counterparts. Our experiments
show up to 99% reduction in the number of network weights with an average
reduction of 77% over multiple actor-critic algorithms on 9 independent tasks.
Given that reducing actor complexity results in a direct reduction of run-time
inference cost, we believe configurations of actors and critics are aspects of
actor-critic design that deserve to be considered independently, particularly
in resource-constrained applications or when deploying multiple actors
simultaneously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1"&gt;Siddharth Mysore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mabsout_B/0/1/0/all/0/1"&gt;Bassel Mabsout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancuso_R/0/1/0/all/0/1"&gt;Renato Mancuso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal learning for Audio-Visual Video Parsing. (arXiv:2104.04598v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04598</id>
        <link href="http://arxiv.org/abs/2104.04598"/>
        <updated>2021-06-22T01:57:13.237Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel approach to the audio-visual video parsing
(AVVP) task that demarcates events from a video separately for audio and visual
modalities. The proposed parsing approach simultaneously detects the temporal
boundaries in terms of start and end times of such events. We show how AVVP can
benefit from the following techniques geared towards effective cross-modal
learning: (i) adversarial training and skip connections (ii) global context
aware attention and, (iii) self-supervised pretraining using an audio-video
grounding objective to obtain cross-modal audio-video representations. We
present extensive experimental evaluations on the Look, Listen, and Parse (LLP)
dataset and show that we outperform the state-of-the-art Hybrid Attention
Network (HAN) on all five metrics proposed for AVVP. We also present several
ablations to validate the effect of pretraining, global attention and
adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamba_J/0/1/0/all/0/1"&gt;Jatin Lamba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek/0/1/0/all/0/1"&gt;Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akula_J/0/1/0/all/0/1"&gt;Jayaprakash Akula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1"&gt;Rishabh Dabral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-06-22T01:57:13.231Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-06-22T01:57:13.223Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08930</id>
        <link href="http://arxiv.org/abs/2008.08930"/>
        <updated>2021-06-22T01:57:13.214Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been widely applied in different
scenarios thanks to the development of deep neural networks. The original GAN
was proposed based on the non-parametric assumption of the infinite capacity of
networks. However, it is still unknown whether GANs can generate realistic
samples without any prior information. Due to the overconfident assumption,
many issues remain unaddressed in GANs' training, such as non-convergence, mode
collapses, gradient vanishing. Regularization and normalization are common
methods of introducing prior information to stabilize training and improve
discrimination. Although a handful number of regularization and normalization
methods have been proposed for GANs, to the best of our knowledge, there exists
no comprehensive survey which primarily focuses on objectives and development
of these methods, apart from some in-comprehensive and limited scope studies.
In this work, we conduct a comprehensive survey on the regularization and
normalization techniques from different perspectives of GANs training. First,
we systematically describe different perspectives of GANs training and thus
obtain the different objectives of regularization and normalization. Based on
these objectives, we propose a new taxonomy. Furthermore, we compare the
performance of the mainstream methods on different datasets and investigate the
regularization and normalization techniques that have been frequently employed
in SOTA GANs. Finally, we highlight potential future directions of research in
this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xintian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Rentuo Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1"&gt;Pengfei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting the Olympic medal distribution during a pandemic: a socio-economic machine learning model. (arXiv:2012.04378v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04378</id>
        <link href="http://arxiv.org/abs/2012.04378"/>
        <updated>2021-06-22T01:57:13.201Z</updated>
        <summary type="html"><![CDATA[Forecasting the number of Olympic medals for each nation is highly relevant
for different stakeholders: Ex ante, sports betting companies can determine the
odds while sponsors and media companies can allocate their resources to
promising teams. Ex post, sports politicians and managers can benchmark the
performance of their teams and evaluate the drivers of success. To
significantly increase the Olympic medal forecasting accuracy, we apply machine
learning, more specifically a two-staged Random Forest, thus outperforming more
traditional na\"ive forecast for three previous Olympics held between 2008 and
2016 for the first time. Regarding the Tokyo 2020 Games in 2021, our model
suggests that the United States will lead the Olympic medal table, winning 120
medals, followed by China (87) and Great Britain (74). Intriguingly, we predict
that the current COVID-19 pandemic will not significantly alter the medal count
as all countries suffer from the pandemic to some extent (data inherent) and
limited historical data points on comparable diseases (model inherent).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlembach_C/0/1/0/all/0/1"&gt;Christoph Schlembach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1"&gt;Sascha L. Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schreyer_D/0/1/0/all/0/1"&gt;Dominik Schreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wunderlich_L/0/1/0/all/0/1"&gt;Linus Wunderlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Universal Consistency of Decision Trees in High Dimensions. (arXiv:2104.13881v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13881</id>
        <link href="http://arxiv.org/abs/2104.13881"/>
        <updated>2021-06-22T01:57:13.177Z</updated>
        <summary type="html"><![CDATA[This paper shows that decision trees constructed with Classification and
Regression Trees (CART) methodology are universally consistent in an additive
model context, even when the number of predictor variables scales exponentially
with the sample size, under certain $1$-norm sparsity constraints. The
consistency is universal in the sense that there are no a priori assumptions on
the distribution of the predictor variables. Amazingly, this adaptivity to
(approximate or exact) sparsity is achieved with a single tree, as opposed to
what might be expected for an ensemble. Finally, we show that these qualitative
properties of individual trees are inherited by Breiman's random forests.
Another surprise is that consistency holds even when the "mtry" tuning
parameter vanishes as a fraction of the number of predictor variables, thus
speeding up computation of the forest. A key step in the analysis is the
establishment of an oracle inequality, which precisely characterizes the
goodness-of-fit and complexity tradeoff for a misspecified model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1"&gt;Jason M. Klusowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic State Space Model for Joint Inference from Differential Equations and Data. (arXiv:2103.10153v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10153</id>
        <link href="http://arxiv.org/abs/2103.10153"/>
        <updated>2021-06-22T01:57:13.171Z</updated>
        <summary type="html"><![CDATA[Mechanistic models with differential equations are a key component of
scientific applications of machine learning. Inference in such models is
usually computationally demanding, because it involves repeatedly solving the
differential equation. The main problem here is that the numerical solver is
hard to combine with standard inference techniques. Recent work in
probabilistic numerics has developed a new class of solvers for ordinary
differential equations (ODEs) that phrase the solution process directly in
terms of Bayesian filtering. We here show that this allows such methods to be
combined very directly, with conceptual and numerical ease, with latent force
models in the ODE itself. It then becomes possible to perform approximate
Bayesian inference on the latent force as well as the ODE solution in a single,
linear complexity pass of an extended Kalman filter / smoother - that is, at
the cost of computing a single ODE solution. We demonstrate the expressiveness
and performance of the algorithm by training, among others, a non-parametric
SIRD model on data from the COVID-19 outbreak.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_J/0/1/0/all/0/1"&gt;Jonathan Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kramer_N/0/1/0/all/0/1"&gt;Nicholas Kr&amp;#xe4;mer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Learning Based Denoising Autoencoder. (arXiv:2101.07937v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07937</id>
        <link href="http://arxiv.org/abs/2101.07937"/>
        <updated>2021-06-22T01:57:13.164Z</updated>
        <summary type="html"><![CDATA[This letter introduces a new denoiser that modifies the structure of
denoising autoencoder (DAE), namely noise learning based DAE (nlDAE). The
proposed nlDAE learns the noise of the input data. Then, the denoising is
performed by subtracting the regenerated noise from the noisy input. Hence,
nlDAE is more effective than DAE when the noise is simpler to regenerate than
the original data. To validate the performance of nlDAE, we provide three case
studies: signal restoration, symbol demodulation, and precise localization.
Numerical results suggest that nlDAE requires smaller latent space dimension
and smaller training dataset compared to DAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Woong-Hee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozger_M/0/1/0/all/0/1"&gt;Mustafa Ozger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Challita_U/0/1/0/all/0/1"&gt;Ursula Challita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_K/0/1/0/all/0/1"&gt;Ki Won Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge. (arXiv:2012.11696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11696</id>
        <link href="http://arxiv.org/abs/2012.11696"/>
        <updated>2021-06-22T01:57:13.156Z</updated>
        <summary type="html"><![CDATA[Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1"&gt;Pierre Dognin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1"&gt;Igor Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1"&gt;Inkit Padhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1"&gt;Mattia Rigotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1"&gt;Jarret Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1"&gt;Richard A. Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1"&gt;Brian Belgodere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Framelets Enhance Graph Neural Networks. (arXiv:2102.06986v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06986</id>
        <link href="http://arxiv.org/abs/2102.06986"/>
        <updated>2021-06-22T01:57:13.150Z</updated>
        <summary type="html"><![CDATA[This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xuebin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bingxin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Lio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Ming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Montufar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capturing Label Characteristics in VAEs. (arXiv:2006.10102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10102</id>
        <link href="http://arxiv.org/abs/2006.10102"/>
        <updated>2021-06-22T01:57:13.134Z</updated>
        <summary type="html"><![CDATA[We present a principled approach to incorporating labels in VAEs that
captures the rich characteristic information associated with those labels.
While prior work has typically conflated these by learning latent variables
that directly correspond to label values, we argue this is contrary to the
intended effect of supervision in VAEs-capturing rich label characteristics
with the latents. For example, we may want to capture the characteristics of a
face that make it look young, rather than just the age of the person. To this
end, we develop the CCVAE, a novel VAE model and concomitant variational
objective which captures label characteristics explicitly in the latent space,
eschewing direct correspondences between label values and latents. Through
judicious structuring of mappings between such characteristic latents and
labels, we show that the CCVAE can effectively learn meaningful representations
of the characteristics of interest across a variety of supervision schemes. In
particular, we show that the CCVAE allows for more effective and more general
interventions to be performed, such as smooth traversals within the
characteristics for a given label, diverse conditional generation, and
transferring characteristics across datapoints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1"&gt;Tom Joy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1"&gt;Sebastian M. Schmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H. S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1"&gt;N. Siddharth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Explanations using Minimal Forcing Subsets. (arXiv:2011.00639v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00639</id>
        <link href="http://arxiv.org/abs/2011.00639"/>
        <updated>2021-06-22T01:57:13.127Z</updated>
        <summary type="html"><![CDATA[How can we find a subset of training samples that are most responsible for a
specific prediction made by a complex black-box machine learning model? More
generally, how can we explain the model's decisions to end-users in a
transparent way? We propose a new model-agnostic algorithm to identify a
minimal set of training samples that are indispensable for a given model's
decision at a particular test point, i.e., the model's decision would have
changed upon the removal of this subset from the training dataset. Our
algorithm identifies such a set of "indispensable" samples iteratively by
solving a constrained optimization problem. Further, we speed up the algorithm
through efficient approximations and provide theoretical justification for its
performance. To demonstrate the applicability and effectiveness of our
approach, we apply it to a variety of tasks including data poisoning detection,
training set debugging and understanding loan decisions. The results show that
our algorithm is an effective and easy-to-comprehend tool that helps to better
understand local model behavior, and therefore facilitates the adoption of
machine learning in domains where such understanding is a requisite.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_J/0/1/0/all/0/1"&gt;Joydeep Ghosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learned Factor Graphs for Inference from Stationary Time Sequences. (arXiv:2006.03258v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03258</id>
        <link href="http://arxiv.org/abs/2006.03258"/>
        <updated>2021-06-22T01:57:13.121Z</updated>
        <summary type="html"><![CDATA[The design of methods for inference from time sequences has traditionally
relied on statistical models that describe the relation between a latent
desired sequence and the observed one. A broad family of model-based algorithms
have been derived to carry out inference at controllable complexity using
recursive computations over the factor graph representing the underlying
distribution. An alternative model-agnostic approach utilizes machine learning
(ML) methods. Here we propose a framework that combines model-based algorithms
and data-driven ML tools for stationary time sequences. In the proposed
approach, neural networks are developed to separately learn specific components
of a factor graph describing the distribution of the time sequence, rather than
the complete inference task. By exploiting stationary properties of this
distribution, the resulting approach can be applied to sequences of varying
temporal duration. Learned factor graph can be realized using compact neural
networks that are trainable using small training sets, or alternatively, be
used to improve upon existing deep inference systems. We present an inference
algorithm based on learned stationary factor graphs, which learns to implement
the sum-product scheme from labeled data, and can be applied to sequences of
different lengths. Our experimental results demonstrate the ability of the
proposed learned factor graphs to learn to carry out accurate inference from
small training sets for sleep stage detection using the Sleep-EDF dataset, as
well as for symbol detection in digital communications with unknown channels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shlezinger_N/0/1/0/all/0/1"&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farsad_N/0/1/0/all/0/1"&gt;Nariman Farsad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1"&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldsmith_A/0/1/0/all/0/1"&gt;Andrea J. Goldsmith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure. (arXiv:2010.07488v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07488</id>
        <link href="http://arxiv.org/abs/2010.07488"/>
        <updated>2021-06-22T01:57:13.115Z</updated>
        <summary type="html"><![CDATA[Glaucoma is the leading cause of irreversible blindness in the world,
affecting over 70 million people. The cumbersome Standard Automated Perimetry
(SAP) test is most frequently used to detect visual loss due to glaucoma. Due
to the SAP test's innate difficulty and its high test-retest variability, we
propose the RetiNerveNet, a deep convolutional recursive neural network for
obtaining estimates of the SAP visual field. RetiNerveNet uses information from
the more objective Spectral-Domain Optical Coherence Tomography (SDOCT).
RetiNerveNet attempts to trace-back the arcuate convergence of the retinal
nerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness
around the optic disc, to estimate individual age-corrected 24-2 SAP values.
Recursive passes through the proposed network sequentially yield estimates of
the visual locations progressively farther from the optic disc. While all the
methods used for our experiments exhibit lower performance for the advanced
disease group, the proposed network is observed to be more accurate than all
the baselines for estimating the individual visual field values. We further
augment RetiNerveNet to additionally predict the SAP Mean Deviation values and
also create an ensemble of RetiNerveNets that further improves the performance,
by increasingly weighting-up underrepresented parts of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Shounak Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mariottoni_E/0/1/0/all/0/1"&gt;Eduardo B. Mariottoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dov_D/0/1/0/all/0/1"&gt;David Dov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jammal_A/0/1/0/all/0/1"&gt;Alessandro A. Jammal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medeiros_F/0/1/0/all/0/1"&gt;Felipe A. Medeiros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressive Sensing and Neural Networks from a Statistical Learning Perspective. (arXiv:2010.15658v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15658</id>
        <link href="http://arxiv.org/abs/2010.15658"/>
        <updated>2021-06-22T01:57:13.110Z</updated>
        <summary type="html"><![CDATA[Various iterative reconstruction algorithms for inverse problems can be
unfolded as neural networks. Empirically, this approach has often led to
improved results, but theoretical guarantees are still scarce. While some
progress on generalization properties of neural networks have been made, great
challenges remain. In this chapter, we discuss and combine these topics to
present a generalization error analysis for a class of neural networks suitable
for sparse reconstruction from few linear measurements. The hypothesis class
considered is inspired by the classical iterative soft-thresholding algorithm
(ISTA). The neural networks in this class are obtained by unfolding iterations
of ISTA and learning some of the weights. Based on training samples, we aim at
learning the optimal network parameters via empirical risk minimization and
thereby the optimal network that reconstructs signals from their compressive
linear measurements. In particular, we may learn a sparsity basis that is
shared by all of the iterations/layers and thereby obtain a new approach for
dictionary learning. For this class of networks, we present a generalization
bound, which is based on bounding the Rademacher complexity of hypothesis
classes consisting of such deep networks via Dudley's integral. Remarkably,
under realistic conditions, the generalization error scales only
logarithmically in the number of layers, and at most linear in number of
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Behboodi_A/0/1/0/all/0/1"&gt;Arash Behboodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rauhut_H/0/1/0/all/0/1"&gt;Holger Rauhut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Schnoor_E/0/1/0/all/0/1"&gt;Ekkehard Schnoor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tight Differential Privacy for Discrete-Valued Mechanisms and for the Subsampled Gaussian Mechanism Using FFT. (arXiv:2006.07134v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07134</id>
        <link href="http://arxiv.org/abs/2006.07134"/>
        <updated>2021-06-22T01:57:13.091Z</updated>
        <summary type="html"><![CDATA[We propose a numerical accountant for evaluating the tight
$(\varepsilon,\delta)$-privacy loss for algorithms with discrete one
dimensional output. The method is based on the privacy loss distribution
formalism and it uses the recently introduced fast Fourier transform based
accounting technique. We carry out an error analysis of the method in terms of
moment bounds of the privacy loss distribution which leads to rigorous lower
and upper bounds for the true $(\varepsilon,\delta)$-values. As an application,
we present a novel approach to accurate privacy accounting of the subsampled
Gaussian mechanism. This completes the previously proposed analysis by giving
strict lower and upper bounds for the privacy parameters. We demonstrate the
performance of the accountant on the binomial mechanism and show that our
approach allows decreasing noise variance up to 75 percent at equal privacy
compared to existing bounds in the literature. We also illustrate how to
compute tight bounds for the exponential mechanism applied to counting queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Koskela_A/0/1/0/all/0/1"&gt;Antti Koskela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jalko_J/0/1/0/all/0/1"&gt;Joonas J&amp;#xe4;lk&amp;#xf6;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Prediger_L/0/1/0/all/0/1"&gt;Lukas Prediger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Honkela_A/0/1/0/all/0/1"&gt;Antti Honkela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12135</id>
        <link href="http://arxiv.org/abs/2006.12135"/>
        <updated>2021-06-22T01:57:13.085Z</updated>
        <summary type="html"><![CDATA[Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model's robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mostly Harmless Machine Learning: Learning Optimal Instruments in Linear IV Models. (arXiv:2011.06158v3 [econ.EM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06158</id>
        <link href="http://arxiv.org/abs/2011.06158"/>
        <updated>2021-06-22T01:57:13.080Z</updated>
        <summary type="html"><![CDATA[We offer straightforward theoretical results that justify incorporating
machine learning in the standard linear instrumental variable setting. The key
idea is to use machine learning, combined with sample-splitting, to predict the
treatment variable from the instrument and any exogenous covariates, and then
use this predicted treatment and the covariates as technical instruments to
recover the coefficients in the second-stage. This allows the researcher to
extract non-linear co-variation between the treatment and instrument that may
dramatically improve estimation precision and robustness by boosting instrument
strength. Importantly, we constrain the machine-learned predictions to be
linear in the exogenous covariates, thus avoiding spurious identification
arising from non-linear relationships between the treatment and the covariates.
We show that this approach delivers consistent and asymptotically normal
estimates under weak conditions and that it may be adapted to be
semiparametrically efficient (Chamberlain, 1992). Our method preserves standard
intuitions and interpretations of linear instrumental variable methods,
including under weak identification, and provides a simple, user-friendly
upgrade to the applied economics toolbox. We illustrate our method with an
example in law and criminal justice, examining the causal effect of appellate
court reversals on district court sentencing decisions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiafeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Chen_D/0/1/0/all/0/1"&gt;Daniel L. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Lewis_G/0/1/0/all/0/1"&gt;Greg Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks. (arXiv:2102.08574v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08574</id>
        <link href="http://arxiv.org/abs/2102.08574"/>
        <updated>2021-06-22T01:57:13.074Z</updated>
        <summary type="html"><![CDATA[We propose firefly neural architecture descent, a general framework for
progressively and dynamically growing neural networks to jointly optimize the
networks' parameters and architectures. Our method works in a steepest descent
fashion, which iteratively finds the best network within a functional
neighborhood of the original network that includes a diverse set of candidate
network structures. By using Taylor approximation, the optimal network
structure in the neighborhood can be found with a greedy selection procedure.
We show that firefly descent can flexibly grow networks both wider and deeper,
and can be applied to learn accurate but resource-efficient neural
architectures that avoid catastrophic forgetting in continual learning.
Empirically, firefly descent achieves promising results on both neural
architecture search and continual learning. In particular, on a challenging
continual image classification task, it learns networks that are smaller in
size but have higher average accuracy than those learned by the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lemeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1"&gt;Peter Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IGANI: Iterative Generative Adversarial Networks for Imputation with Application to Traffic Data. (arXiv:2008.04847v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04847</id>
        <link href="http://arxiv.org/abs/2008.04847"/>
        <updated>2021-06-22T01:57:13.068Z</updated>
        <summary type="html"><![CDATA[Increasing use of sensor data in intelligent transportation systems calls for
accurate imputation algorithms that can enable reliable traffic management in
the occasional absence of data. As one of the effective imputation approaches,
generative adversarial networks (GANs) are implicit generative models that can
be used for data imputation, which is formulated as an unsupervised learning
problem. This work introduces a novel iterative GAN architecture, called
Iterative Generative Adversarial Networks for Imputation (IGANI), for data
imputation. IGANI imputes data in two steps and maintains the invertibility of
the generative imputer, which will be shown to be a sufficient condition for
the convergence of the proposed GAN-based imputation. The performance of our
proposed method is evaluated on (1) the imputation of traffic speed data
collected in the city of Guangzhou in China, and the training of short-term
traffic prediction models using imputed data, and (2) the imputation of
multi-variable traffic data of highways in Portland-Vancouver metropolitan
region which includes volume, occupancy, and speed with different missing rates
for each of them. It is shown that our proposed algorithm mostly produces more
accurate results compared to those of previous GAN-based imputation
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Amir Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meidani_H/0/1/0/all/0/1"&gt;Hadi Meidani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10836</id>
        <link href="http://arxiv.org/abs/2106.10836"/>
        <updated>2021-06-22T01:57:13.052Z</updated>
        <summary type="html"><![CDATA[When dealing with deep neural network (DNN) applications on edge devices,
continuously updating the model is important. Although updating a model with
real incoming data is ideal, using all of them is not always feasible due to
limits, such as labeling and communication costs. Thus, it is necessary to
filter and select the data to use for training (i.e., active learning) on the
device. In this paper, we formalize a practical active learning problem for
DNNs on edge devices and propose a general task-agnostic framework to tackle
this problem, which reduces it to a stream submodular maximization. This
framework is light enough to be run with low computational resources, yet
provides solutions whose quality is theoretically guaranteed thanks to the
submodular property. Through this framework, we can configure data selection
criteria flexibly, including using methods proposed in previous active learning
studies. We evaluate our approach on both classification and object detection
tasks in a practical setting to simulate a real-life scenario. The results of
our study show that the proposed framework outperforms all other methods in
both tasks, while running at a practical speed on real devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Senzaki_Y/0/1/0/all/0/1"&gt;Yuya Senzaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamelain_C/0/1/0/all/0/1"&gt;Christian Hamelain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation. (arXiv:2106.10796v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10796</id>
        <link href="http://arxiv.org/abs/2106.10796"/>
        <updated>2021-06-22T01:57:13.045Z</updated>
        <summary type="html"><![CDATA[Communication overhead is the key challenge for distributed training.
Gradient compression is a widely used approach to reduce communication traffic.
When combining with parallel communication mechanism method like pipeline,
gradient compression technique can greatly alleviate the impact of
communication overhead. However, there exists two problems of gradient
compression technique to be solved. Firstly, gradient compression brings in
extra computation cost, which will delay the next training iteration. Secondly,
gradient compression usually leads to the decrease of convergence accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1"&gt;Enda Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Dezun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yemao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1"&gt;Shuo Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiangke Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pointwise Binary Classification with Pairwise Confidence Comparisons. (arXiv:2010.01875v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01875</id>
        <link href="http://arxiv.org/abs/2010.01875"/>
        <updated>2021-06-22T01:57:13.040Z</updated>
        <summary type="html"><![CDATA[To alleviate the data requirement for training effective binary classifiers
in binary classification, many weakly supervised learning settings have been
proposed. Among them, some consider using pairwise but not pointwise labels,
when pointwise labels are not accessible due to privacy, confidentiality, or
security reasons. However, as a pairwise label denotes whether or not two data
points share a pointwise label, it cannot be easily collected if either point
is equally likely to be positive or negative. Thus, in this paper, we propose a
novel setting called pairwise comparison (Pcomp) classification, where we have
only pairs of unlabeled data that we know one is more likely to be positive
than the other. Firstly, we give a Pcomp data generation process, derive an
unbiased risk estimator (URE) with theoretical guarantee, and further improve
URE using correction functions. Secondly, we link Pcomp classification to
noisy-label learning to develop a progressive URE and improve it by imposing
consistency regularization. Finally, we demonstrate by experiments the
effectiveness of our methods, which suggests Pcomp is a valuable and
practically useful type of pairwise supervision besides the pairwise label.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_S/0/1/0/all/0/1"&gt;Senlin Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1"&gt;Nan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Miao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL. (arXiv:2012.08005v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08005</id>
        <link href="http://arxiv.org/abs/2012.08005"/>
        <updated>2021-06-22T01:57:13.030Z</updated>
        <summary type="html"><![CDATA[Several practical applications of reinforcement learning involve an agent
learning from past data without the possibility of further exploration. Often
these applications require us to 1) identify a near optimal policy or to 2)
estimate the value of a target policy. For both tasks we derive
\emph{exponential} information-theoretic lower bounds in discounted infinite
horizon MDPs with a linear function representation for the action value
function even if 1) \emph{realizability} holds, 2) the batch algorithm observes
the exact reward and transition \emph{functions}, and 3) the batch algorithm is
given the \emph{best} a priori data distribution for the problem class. Our
work introduces a new `oracle + batch algorithm' framework to prove lower
bounds that hold for every distribution. The work shows an exponential
separation between batch and online reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1"&gt;Andrea Zanette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Squared: A Meta AutoML System. (arXiv:2012.05390v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05390</id>
        <link href="http://arxiv.org/abs/2012.05390"/>
        <updated>2021-06-22T01:57:13.024Z</updated>
        <summary type="html"><![CDATA[There are currently many barriers that prevent non-experts from exploiting
machine learning solutions ranging from the lack of intuition on statistical
learning techniques to the trickiness of hyperparameter tuning. Such barriers
have led to an explosion of interest in automated machine learning (AutoML),
whereby an off-the-shelf system can take care of many of the steps for
end-users without the need for expertise in machine learning. This paper
presents Ensemble Squared (Ensemble$^2$), an AutoML system that ensembles the
results of state-of-the-art open-source AutoML systems. Ensemble$^2$ exploits
the diversity of existing AutoML systems by leveraging the differences in their
model search space and heuristics. Empirically, we show that diversity of each
AutoML system is sufficient to justify ensembling at the AutoML system level.
In demonstrating this, we also establish new state-of-the-art AutoML results on
the OpenML tabular classification benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Jason Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_T/0/1/0/all/0/1"&gt;Tony Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yung_D/0/1/0/all/0/1"&gt;Dylan Yung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasseri_S/0/1/0/all/0/1"&gt;S. Ali Nasseri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1"&gt;Frank Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04884</id>
        <link href="http://arxiv.org/abs/2101.04884"/>
        <updated>2021-06-22T01:57:13.004Z</updated>
        <summary type="html"><![CDATA[Can a computer determine a piano player's skill level? Is it preferable to
base this assessment on visual analysis of the player's performance or should
we trust our ears over our eyes? Since current CNNs have difficulty processing
long video videos, how can shorter clips be sampled to best reflect the players
skill level? In this work, we collect and release a first-of-its-kind dataset
for multimodal skill assessment focusing on assessing piano player's skill
level, answer the asked questions, initiate work in automated evaluation of
piano playing skills and provide baselines for future work. Dataset is
available from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1"&gt;Paritosh Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1"&gt;Jaiden Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1"&gt;Brendan Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Rough Differential Equations for Long Time Series. (arXiv:2009.08295v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08295</id>
        <link href="http://arxiv.org/abs/2009.08295"/>
        <updated>2021-06-22T01:57:12.998Z</updated>
        <summary type="html"><![CDATA[Neural controlled differential equations (CDEs) are the continuous-time
analogue of recurrent neural networks, as Neural ODEs are to residual networks,
and offer a memory-efficient continuous-time way to model functions of
potentially irregular time series. Existing methods for computing the forward
pass of a Neural CDE involve embedding the incoming time series into path
space, often via interpolation, and using evaluations of this path to drive the
hidden state. Here, we use rough path theory to extend this formulation.
Instead of directly embedding into path space, we instead represent the input
signal over small time intervals through its \textit{log-signature}, which are
statistics describing how the signal drives a CDE. This is the approach for
solving \textit{rough differential equations} (RDEs), and correspondingly we
describe our main contribution as the introduction of Neural RDEs. This
extension has a purpose: by generalising the Neural CDE approach to a broader
class of driving signals, we demonstrate particular advantages for tackling
long time series. In this regime, we demonstrate efficacy on problems of length
up to 17k observations and observe significant training speed-ups, improvements
in model performance, and reduced memory requirements compared to existing
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morrill_J/0/1/0/all/0/1"&gt;James Morrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salvi_C/0/1/0/all/0/1"&gt;Cristopher Salvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kidger_P/0/1/0/all/0/1"&gt;Patrick Kidger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1"&gt;James Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1"&gt;Terry Lyons&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Catastrophic Forgetting in Few-Shot Problems. (arXiv:2005.00146v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00146</id>
        <link href="http://arxiv.org/abs/2005.00146"/>
        <updated>2021-06-22T01:57:12.992Z</updated>
        <summary type="html"><![CDATA[Neural networks are known to suffer from catastrophic forgetting when trained
on sequential datasets. While there have been numerous attempts to solve this
problem in large-scale supervised classification, little has been done to
overcome catastrophic forgetting in few-shot classification problems. We
demonstrate that the popular gradient-based model-agnostic meta-learning
algorithm (MAML) indeed suffers from catastrophic forgetting and introduce a
Bayesian online meta-learning framework that tackles this problem. Our
framework utilises Bayesian online learning and meta-learning along with
Laplace approximation and variational inference to overcome catastrophic
forgetting in few-shot classification problems. The experimental evaluations
demonstrate that our framework can effectively achieve this goal in comparison
with various baselines. As an additional utility, we also demonstrate
empirically that our framework is capable of meta-learning on sequentially
arriving few-shot tasks from a stationary task distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1"&gt;Pauching Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritter_H/0/1/0/all/0/1"&gt;Hippolyt Ritter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_D/0/1/0/all/0/1"&gt;David Barber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Nonconvex Framework for Structured Dynamic Covariance Recovery. (arXiv:2011.05601v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05601</id>
        <link href="http://arxiv.org/abs/2011.05601"/>
        <updated>2021-06-22T01:57:12.986Z</updated>
        <summary type="html"><![CDATA[We propose a flexible yet interpretable model for high-dimensional data with
time-varying second order statistics, motivated and applied to functional
neuroimaging data. Motivated by the neuroscience literature, we factorize the
covariances into sparse spatial and smooth temporal components. While this
factorization results in both parsimony and domain interpretability, the
resulting estimation problem is nonconvex. To this end, we design a two-stage
optimization scheme with a carefully tailored spectral initialization, combined
with iteratively refined alternating projected gradient descent. We prove a
linear convergence rate up to a nontrivial statistical error for the proposed
descent scheme and establish sample complexity guarantees for the estimator. We
further quantify the statistical error for the multivariate Gaussian case.
Empirical results using simulated and real brain imaging data illustrate that
our approach outperforms existing baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tsai_K/0/1/0/all/0/1"&gt;Katherine Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kolar_M/0/1/0/all/0/1"&gt;Mladen Kolar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koyejo_O/0/1/0/all/0/1"&gt;Oluwasanmi Koyejo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Contrastive Learning for Few-Shot Classification. (arXiv:2012.13831v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13831</id>
        <link href="http://arxiv.org/abs/2012.13831"/>
        <updated>2021-06-22T01:57:12.980Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore contrastive learning for few-shot classification,
in which we propose to use it as an additional auxiliary training objective
acting as a data-dependent regularizer to promote more general and transferable
features. In particular, we present a novel attention-based spatial contrastive
objective to learn locally discriminative and class-agnostic features. As a
result, our approach overcomes some of the limitations of the cross-entropy
loss, such as its excessive discrimination towards seen classes, which reduces
the transferability of features to unseen classes. With extensive experiments,
we show that the proposed method outperforms state-of-the-art approaches,
confirming the importance of learning good and transferable embeddings for
few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouali_Y/0/1/0/all/0/1"&gt;Yassine Ouali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A non-alternating graph hashing algorithm for large scale image search. (arXiv:2012.13138v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13138</id>
        <link href="http://arxiv.org/abs/2012.13138"/>
        <updated>2021-06-22T01:57:12.975Z</updated>
        <summary type="html"><![CDATA[In the era of big data, methods for improving memory and computational
efficiency have become crucial for successful deployment of technologies.
Hashing is one of the most effective approaches to deal with computational
limitations that come with big data. One natural way for formulating this
problem is spectral hashing that directly incorporates affinity to learn binary
codes. However, due to binary constraints, the optimization becomes
intractable. To mitigate this challenge, different relaxation approaches have
been proposed to reduce the computational load of obtaining binary codes and
still attain a good solution. The problem with all existing relaxation methods
is resorting to one or more additional auxiliary variables to attain high
quality binary codes while relaxing the problem. The existence of auxiliary
variables leads to coordinate descent approach which increases the
computational complexity. We argue that introducing these variables is
unnecessary. To this end, we propose a novel relaxed formulation for spectral
hashing that adds no additional variables to the problem. Furthermore, instead
of solving the problem in original space where number of variables is equal to
the data points, we solve the problem in a much smaller space and retrieve the
binary codes from this solution. This trick reduces both the memory and
computational complexity at the same time. We apply two optimization
techniques, namely projected gradient and optimization on manifold, to obtain
the solution. Using comprehensive experiments on four public datasets, we show
that the proposed efficient spectral hashing (ESH) algorithm achieves highly
competitive retrieval performance compared with state of the art at low
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1"&gt;Sobhan Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdizavareh_M/0/1/0/all/0/1"&gt;Mohammad Hadi Mehdizavareh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenouri_S/0/1/0/all/0/1"&gt;Shojaeddin Chenouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;Hamid R Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PIVEN: A Deep Neural Network for Prediction Intervals with Specific Value Prediction. (arXiv:2006.05139v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05139</id>
        <link href="http://arxiv.org/abs/2006.05139"/>
        <updated>2021-06-22T01:57:12.958Z</updated>
        <summary type="html"><![CDATA[Improving the robustness of neural nets in regression tasks is key to their
application in multiple domains. Deep learning-based approaches aim to achieve
this goal either by improving their prediction of specific values (i.e., point
prediction), or by producing prediction intervals (PIs) that quantify
uncertainty. We present PIVEN, a deep neural network for producing both a PI
and a value prediction. Our loss function expresses the value prediction as a
function of the upper and lower bounds, thus ensuring that it falls within the
interval without increasing model complexity. Moreover, our approach makes no
assumptions regarding data distribution within the PI, making its value
prediction more effective for various real-world problems. Experiments and
ablation tests on known benchmarks show that our approach produces tighter
uncertainty bounds than the current state-of-the-art approaches for producing
PIs, while maintaining comparable performance to the state-of-the-art approach
for value-prediction. Additionally, we go beyond previous work and include
large image datasets in our evaluation, where PIVEN is combined with modern
neural nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Simhayev_E/0/1/0/all/0/1"&gt;Eli Simhayev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1"&gt;Gilad Katz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rokach_L/0/1/0/all/0/1"&gt;Lior Rokach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation in shift-invariant spaces with deep ReLU neural networks. (arXiv:2005.11949v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11949</id>
        <link href="http://arxiv.org/abs/2005.11949"/>
        <updated>2021-06-22T01:57:12.953Z</updated>
        <summary type="html"><![CDATA[We study the expressive power of deep ReLU neural networks for approximating
functions in dilated shift-invariant spaces, which are widely used in signal
processing, image processing, communications and so on. Approximation error
bounds are estimated with respect to the width and depth of neural networks.
The network construction is based on the bit extraction and data-fitting
capacity of deep neural networks. As applications of our main results, the
approximation rates of classical function spaces such as Sobolev spaces and
Besov spaces are obtained. We also give lower bounds of the $L^p (1\le p \le
\infty)$ approximation error for Sobolev spaces, which show that our
construction of neural network is asymptotically optimal up to a logarithmic
factor.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Libraries: A Deep Learning Framework Designed from Engineers' Perspectives. (arXiv:2102.06725v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06725</id>
        <link href="http://arxiv.org/abs/2102.06725"/>
        <updated>2021-06-22T01:57:12.947Z</updated>
        <summary type="html"><![CDATA[While there exist a plethora of deep learning tools and frameworks, the
fast-growing complexity of the field brings new demands and challenges, such as
more flexible network design, speedy computation on distributed setting, and
compatibility between different tools. In this paper, we introduce Neural
Network Libraries (https://nnabla.org), a deep learning framework designed from
engineer's perspective, with emphasis on usability and compatibility as its
core design principles. We elaborate on each of our design principles and its
merits, and validate our attempts via experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1"&gt;Takuya Narihira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alonsogarcia_J/0/1/0/all/0/1"&gt;Javier Alonsogarcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1"&gt;Fabien Cardinaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayakawa_A/0/1/0/all/0/1"&gt;Akio Hayakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1"&gt;Masato Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwaki_K/0/1/0/all/0/1"&gt;Kazunori Iwaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1"&gt;Thomas Kemp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_Y/0/1/0/all/0/1"&gt;Yoshiyuki Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1"&gt;Lukas Mauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1"&gt;Akira Nakamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obuchi_Y/0/1/0/all/0/1"&gt;Yukio Obuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_A/0/1/0/all/0/1"&gt;Andrew Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1"&gt;Kenji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiedmann_S/0/1/0/all/0/1"&gt;Stephen Tiedmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhlich_S/0/1/0/all/0/1"&gt;Stefan Uhlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yashima_T/0/1/0/all/0/1"&gt;Takuya Yashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshiyama_K/0/1/0/all/0/1"&gt;Kazuki Yoshiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Graph Neural Networks. (arXiv:2006.02684v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02684</id>
        <link href="http://arxiv.org/abs/2006.02684"/>
        <updated>2021-06-22T01:57:12.934Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) model nonlinear representations in graph data
with applications in distributed agent coordination, control, and planning
among others. Current GNN architectures assume ideal scenarios and ignore link
fluctuations that occur due to environment, human factors, or external attacks.
In these situations, the GNN fails to address its distributed task if the
topological randomness is not considered accordingly. To overcome this issue,
we put forth the stochastic graph neural network (SGNN) model: a GNN where the
distributed graph convolution module accounts for the random network changes.
Since stochasticity brings in a new learning paradigm, we conduct a statistical
analysis on the SGNN output variance to identify conditions the learned filters
should satisfy for achieving robust transference to perturbed scenarios,
ultimately revealing the explicit impact of random link losses. We further
develop a stochastic gradient descent (SGD) based learning process for the SGNN
and derive conditions on the learning rate under which this learning process
converges to a stationary point. Numerical results corroborate our theoretical
findings and compare the benefits of SGNN robust transference with a
conventional GNN that ignores graph perturbations during learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Isufi_E/0/1/0/all/0/1"&gt;Elvin Isufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoreGen: Contextualized Code Representation Learning for Commit Message Generation. (arXiv:2007.06934v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06934</id>
        <link href="http://arxiv.org/abs/2007.06934"/>
        <updated>2021-06-22T01:57:12.929Z</updated>
        <summary type="html"><![CDATA[Automatic generation of high-quality commit messages for code commits can
substantially facilitate software developers' works and coordination. However,
the semantic gap between source code and natural language poses a major
challenge for the task. Several studies have been proposed to alleviate the
challenge but none explicitly involves code contextual information during
commit message generation. Specifically, existing research adopts static
embedding for code tokens, which maps a token to the same vector regardless of
its context. In this paper, we propose a novel Contextualized code
representation learning strategy for commit message Generation (CoreGen).
CoreGen first learns contextualized code representations which exploit the
contextual information behind code commit sequences. The learned
representations of code commits built upon Transformer are then fine-tuned for
downstream commit message generation. Experiments on the benchmark dataset
demonstrate the superior effectiveness of our model over the baseline models
with at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also
highlight the future opportunities in training contextualized code
representations on larger code corpus as a solution to low-resource tasks and
adapting the contextualized code representation framework to other code-to-text
generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Lun Yiu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Cuiyun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhicong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Inference of the Value Function for Reinforcement Learning in Infinite Horizon Settings. (arXiv:2001.04515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.04515</id>
        <link href="http://arxiv.org/abs/2001.04515"/>
        <updated>2021-06-22T01:57:12.913Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is a general technique that allows an agent to learn
an optimal policy and interact with an environment in sequential decision
making problems. The goodness of a policy is measured by its value function
starting from some initial state. The focus of this paper is to construct
confidence intervals (CIs) for a policy's value in infinite horizon settings
where the number of decision points diverges to infinity. We propose to model
the action-value state function (Q-function) associated with a policy based on
series/sieve method to derive its confidence interval. When the target policy
depends on the observed data as well, we propose a SequentiAl Value Evaluation
(SAVE) method to recursively update the estimated policy and its value
estimator. As long as either the number of trajectories or the number of
decision points diverges to infinity, we show that the proposed CI achieves
nominal coverage even in cases where the optimal policy is not unique.
Simulation studies are conducted to back up our theoretical findings. We apply
the proposed method to a dataset from mobile health studies and find that
reinforcement learning algorithms could help improve patient's health status. A
Python implementation of the proposed procedure is available at
https://github.com/shengzhang37/SAVE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;C. Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_S/0/1/0/all/0/1"&gt;S. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1"&gt;W. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Song_R/0/1/0/all/0/1"&gt;R. Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed-Privacy Forgetting in Deep Networks. (arXiv:2012.13431v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13431</id>
        <link href="http://arxiv.org/abs/2012.13431"/>
        <updated>2021-06-22T01:57:12.907Z</updated>
        <summary type="html"><![CDATA[We show that the influence of a subset of the training samples can be removed
-- or "forgotten" -- from the weights of a network trained on large-scale image
classification tasks, and we provide strong computable bounds on the amount of
remaining information after forgetting. Inspired by real-world applications of
forgetting techniques, we introduce a novel notion of forgetting in
mixed-privacy setting, where we know that a "core" subset of the training
samples does not need to be forgotten. While this variation of the problem is
conceptually simple, we show that working in this setting significantly
improves the accuracy and guarantees of forgetting methods applied to vision
classification tasks. Moreover, our method allows efficient removal of all
information contained in non-core data by simply setting to zero a subset of
the weights with minimal loss in performance. We achieve these results by
replacing a standard deep network with a suitable linear approximation. With
opportune changes to the network architecture and training procedure, we show
that such linear approximation achieves comparable performance to the original
network and that the forgetting problem becomes quadratic and can be solved
efficiently even for large models. Unlike previous forgetting methods on deep
networks, ours can achieve close to the state-of-the-art accuracy on large
scale vision tasks. In particular, we show that our method allows forgetting
without having to trade off the model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1"&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Demonstration of Panda: A Weakly Supervised Entity Matching System. (arXiv:2106.10821v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.10821</id>
        <link href="http://arxiv.org/abs/2106.10821"/>
        <updated>2021-06-22T01:57:12.901Z</updated>
        <summary type="html"><![CDATA[Entity matching (EM) refers to the problem of identifying tuple pairs in one
or more relations that refer to the same real world entities. Supervised
machine learning (ML) approaches, and deep learning based approaches in
particular, typically achieve state-of-the-art matching results. However, these
approaches require many labeled examples, in the form of matching and
non-matching pairs, which are expensive and time-consuming to label. In this
paper, we introduce Panda, a weakly supervised system specifically designed for
EM. Panda uses the same labeling function abstraction as Snorkel, where
labeling functions (LF) are user-provided programs that can generate large
amounts of (somewhat noisy) labels quickly and cheaply, which can then be
combined via a labeling model to generate accurate final predictions. To
support users developing LFs for EM, Panda provides an integrated development
environment (IDE) that lives in a modern browser architecture. Panda's IDE
facilitates the development, debugging, and life-cycle management of LFs in the
context of EM tasks, similar to how IDEs such as Visual Studio or Eclipse excel
in general-purpose programming. Panda's IDE includes many novel features
purpose-built for EM, such as smart data sampling, a builtin library of EM
utility functions, automatically generated LFs, visual debugging of LFs, and
finally, an EM-specific labeling model. We show in this demo that Panda IDE can
greatly accelerate the development of high-quality EM solutions using weak
supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Renzhi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakala_P/0/1/0/all/0/1"&gt;Prem Sakala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1"&gt;Xu Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constraint-Based Regularization of Neural Networks. (arXiv:2006.10114v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10114</id>
        <link href="http://arxiv.org/abs/2006.10114"/>
        <updated>2021-06-22T01:57:12.895Z</updated>
        <summary type="html"><![CDATA[We propose a method for efficiently incorporating constraints into a
stochastic gradient Langevin framework for the training of deep neural
networks. Constraints allow direct control of the parameter space of the model.
Appropriately designed, they reduce the vanishing/exploding gradient problem,
control weight magnitudes and stabilize deep neural networks and thus improve
the robustness of training algorithms and the generalization capabilities of
the trained neural network. We present examples of constrained training methods
motivated by orthogonality preservation for weight matrices and explicit weight
normalizations. We describe the methods in the overdamped formulation of
Langevin dynamics and the underdamped form, in which momenta help to improve
sampling efficiency. The methods are explored in test examples in image
classification and natural language processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1"&gt;Benedict Leimkuhler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchon_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Pouchon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1"&gt;Tiffany Vlaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Neural Network Verification via Shadow Prices. (arXiv:1902.07247v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.07247</id>
        <link href="http://arxiv.org/abs/1902.07247"/>
        <updated>2021-06-22T01:57:12.888Z</updated>
        <summary type="html"><![CDATA[To use neural networks in safety-critical settings it is paramount to provide
assurances on their runtime operation. Recent work on ReLU networks has sought
to verify whether inputs belonging to a bounded box can ever yield some
undesirable output. Input-splitting procedures, a particular type of
verification mechanism, do so by recursively partitioning the input set into
smaller sets. The efficiency of these methods is largely determined by the
number of splits the box must undergo before the property can be verified. In
this work, we propose a new technique based on shadow prices that fully
exploits the information of the problem yielding a more efficient generation of
splits than the state-of-the-art. Results on the Airborne Collision Avoidance
System (ACAS) benchmark verification tasks show a considerable reduction in the
partitions generated which substantially reduces computation times. These
results open the door to improved verification methods for a wide variety of
machine learning applications including vision and control.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rubies_Royo_V/0/1/0/all/0/1"&gt;Vicenc Rubies-Royo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stipanovic_D/0/1/0/all/0/1"&gt;Dusan M. Stipanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomlin_C/0/1/0/all/0/1"&gt;Claire Tomlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10811</id>
        <link href="http://arxiv.org/abs/2106.10811"/>
        <updated>2021-06-22T01:57:12.869Z</updated>
        <summary type="html"><![CDATA[Neural shape representations have recently shown to be effective in shape
analysis and reconstruction tasks. Existing neural network methods require
point coordinates and corresponding normal vectors to learn the implicit level
sets of the shape. Normal vectors are often not provided as raw data,
therefore, approximation and reorientation are required as pre-processing
stages, both of which can introduce noise. In this paper, we propose a
divergence guided shape representation learning approach that does not require
normal vectors as input. We show that incorporating a soft constraint on the
divergence of the distance function favours smooth solutions that reliably
orients gradients to match the unknown normal at each point, in some cases even
better than approaches that use ground truth normal vectors directly.
Additionally, we introduce a novel geometric initialization method for
sinusoidal shape representation networks that further improves convergence to
the desired solution. We evaluate the effectiveness of our approach on the task
of surface reconstruction and show state-of-the-art performance compared to
other unoriented methods and on-par performance compared to oriented methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1"&gt;Yizhak Ben-Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1"&gt;Chamin Hewa Koneputugodage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12391</id>
        <link href="http://arxiv.org/abs/2007.12391"/>
        <updated>2021-06-22T01:57:12.863Z</updated>
        <summary type="html"><![CDATA[This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1"&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1"&gt;David Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive Experiments and a Paradox Concerning Logging Policy. (arXiv:2010.03792v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03792</id>
        <link href="http://arxiv.org/abs/2010.03792"/>
        <updated>2021-06-22T01:57:12.858Z</updated>
        <summary type="html"><![CDATA[The doubly robust (DR) estimator, which consists of two nuisance parameters,
the conditional mean outcome and the logging policy (the probability of
choosing an action), is crucial in causal inference. This paper proposes a DR
estimator for dependent samples obtained from adaptive experiments. To obtain
an asymptotically normal semiparametric estimator from dependent samples with
non-Donsker nuisance estimators, we propose adaptive-fitting as a variant of
sample-splitting. We also report an empirical paradox that our proposed DR
estimator tends to show better performances compared to other estimators
utilizing the true logging policy. While a similar phenomenon is known for
estimators with i.i.d. samples, traditional explanations based on asymptotic
efficiency cannot elucidate our case with dependent samples. We confirm this
hypothesis through simulation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1"&gt;Shota Yasui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAlinn_K/0/1/0/all/0/1"&gt;Kenichiro McAlinn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On predicting research grants productivity. (arXiv:2106.10700v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.10700</id>
        <link href="http://arxiv.org/abs/2106.10700"/>
        <updated>2021-06-22T01:57:12.853Z</updated>
        <summary type="html"><![CDATA[Understanding the reasons associated with successful proposals is of
paramount importance to improve evaluation processes. In this context, we
analyzed whether bibliometric features are able to predict the success of
research grants. We extracted features aiming at characterizing the academic
history of Brazilian researchers, including research topics, affiliations,
number of publications and visibility. The extracted features were then used to
predict grants productivity via machine learning in three major research areas,
namely Medicine, Dentistry and Veterinary Medicine. We found that research
subject and publication history play a role in predicting productivity. In
addition, institution-based features turned out to be relevant when combined
with other features. While the best results outperformed text-based attributes,
the evaluated features were not highly discriminative. Our findings indicate
that predicting grants success, at least with the considered set of
bibliometric features, is not a trivial task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tohalino_J/0/1/0/all/0/1"&gt;Jorge A. V. Tohalino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amancio_D/0/1/0/all/0/1"&gt;Diego R. Amancio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting. (arXiv:2003.10392v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10392</id>
        <link href="http://arxiv.org/abs/2003.10392"/>
        <updated>2021-06-22T01:57:12.848Z</updated>
        <summary type="html"><![CDATA[Developing efficient and principled neural architecture optimization methods
is a critical challenge of modern deep learning. Recently, Liu et al.[19]
proposed a splitting steepest descent (S2D) method that jointly optimizes the
neural parameters and architectures based on progressively growing network
structures by splitting neurons into multiple copies in a steepest descent
fashion. However, S2D suffers from a local optimality issue when all the
neurons become "splitting stable", a concept akin to local stability in
parametric optimization. In this work, we develop a significant and surprising
extension of the splitting descent framework that addresses the local
optimality issue. The idea is to observe that the original S2D is unnecessarily
restricted to splitting neurons into positive weighted copies. By simply
allowing both positive and negative weights during splitting, we can eliminate
the appearance of splitting stability in S2D and hence escape the local optima
to obtain better performance. By incorporating signed splittings, we
significantly extend the optimization power of splitting steepest descent both
theoretically and empirically. We verify our method on various challenging
benchmarks such as CIFAR-100, ImageNet and ModelNet40, on which we outperform
S2D and other advanced methods on learning accurate and energy-efficient neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lemeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1"&gt;Mao Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1"&gt;Qi Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Urdu Caption Generation using Attention based LSTM. (arXiv:2008.01663v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01663</id>
        <link href="http://arxiv.org/abs/2008.01663"/>
        <updated>2021-06-22T01:57:12.829Z</updated>
        <summary type="html"><![CDATA[Recent advancements in deep learning have created many opportunities to solve
real-world problems that remained unsolved for more than a decade. Automatic
caption generation is a major research field, and the research community has
done a lot of work on it in most common languages like English. Urdu is the
national language of Pakistan and also much spoken and understood in the
sub-continent region of Pakistan-India, and yet no work has been done for Urdu
language caption generation. Our research aims to fill this gap by developing
an attention-based deep learning model using techniques of sequence modeling
specialized for the Urdu language. We have prepared a dataset in the Urdu
language by translating a subset of the "Flickr8k" dataset containing 700 'man'
images. We evaluate our proposed technique on this dataset and show that it can
achieve a BLEU score of 0.83 in the Urdu language. We improve on the previous
state-of-the-art by using better CNN architectures and optimization techniques.
Furthermore, we provide a discussion on how the generated captions can be made
correct grammar-wise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilahi_I/0/1/0/all/0/1"&gt;Inaam Ilahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zia_H/0/1/0/all/0/1"&gt;Hafiz Muhammad Abdullah Zia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1"&gt;Muhammad Ahtazaz Ahsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabassam_R/0/1/0/all/0/1"&gt;Rauf Tabassam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Armaghan Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Backdoor Attack on Deep Neural Networks. (arXiv:2002.12162v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.12162</id>
        <link href="http://arxiv.org/abs/2002.12162"/>
        <updated>2021-06-22T01:57:12.822Z</updated>
        <summary type="html"><![CDATA[Although deep neural networks (DNNs) have achieved a great success in various
computer vision tasks, it is recently found that they are vulnerable to
adversarial attacks. In this paper, we focus on the so-called \textit{backdoor
attack}, which injects a backdoor trigger to a small portion of training data
(also known as data poisoning) such that the trained DNN induces
misclassification while facing examples with this trigger. To be specific, we
carefully study the effect of both real and synthetic backdoor attacks on the
internal response of vanilla and backdoored DNNs through the lens of Gard-CAM.
Moreover, we show that the backdoor attack induces a significant bias in neuron
activation in terms of the $\ell_\infty$ norm of an activation map compared to
its $\ell_1$ and $\ell_2$ norm. Spurred by our results, we propose the
\textit{$\ell_\infty$-based neuron pruning} to remove the backdoor from the
backdoored DNN. Experiments show that our method could effectively decrease the
attack success rate, and also hold a high classification accuracy for clean
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kaidi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xue Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic heterogeneous quantization of deep neural networks for low-latency inference on the edge for particle detectors. (arXiv:2006.10159v3 [physics.ins-det] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10159</id>
        <link href="http://arxiv.org/abs/2006.10159"/>
        <updated>2021-06-22T01:57:12.817Z</updated>
        <summary type="html"><![CDATA[Although the quest for more accurate solutions is pushing deep learning
research towards larger and more complex algorithms, edge devices demand
efficient inference and therefore reduction in model size, latency and energy
consumption. One technique to limit model size is quantization, which implies
using fewer bits to represent weights and biases. Such an approach usually
results in a decline in performance. Here, we introduce a method for designing
optimally heterogeneously quantized versions of deep neural network models for
minimum-energy, high-accuracy, nanosecond inference and fully automated
deployment on chip. With a per-layer, per-parameter type automatic quantization
procedure, sampling from a wide range of quantizers, model energy consumption
and size are minimized while high accuracy is maintained. This is crucial for
the event selection procedure in proton-proton collisions at the CERN Large
Hadron Collider, where resources are strictly limited and a latency of
${\mathcal O}(1)~\mu$s is required. Nanosecond inference and a resource
consumption reduced by a factor of 50 when implemented on field-programmable
gate array hardware are achieved.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Coelho_C/0/1/0/all/0/1"&gt;Claudionor N. Coelho Jr.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kuusela_A/0/1/0/all/0/1"&gt;Aki Kuusela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Li_S/0/1/0/all/0/1"&gt;Shan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhuang_H/0/1/0/all/0/1"&gt;Hao Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Aarrestad_T/0/1/0/all/0/1"&gt;Thea Aarrestad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Loncar_V/0/1/0/all/0/1"&gt;Vladimir Loncar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ngadiuba_J/0/1/0/all/0/1"&gt;Jennifer Ngadiuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1"&gt;Maurizio Pierini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pol_A/0/1/0/all/0/1"&gt;Adrian Alan Pol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Summers_S/0/1/0/all/0/1"&gt;Sioni Summers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Estimation of Causal Effects in the Presence of Unobserved Confounding in the Alzheimer's Continuum. (arXiv:2006.13135v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13135</id>
        <link href="http://arxiv.org/abs/2006.13135"/>
        <updated>2021-06-22T01:57:12.801Z</updated>
        <summary type="html"><![CDATA[Studying the relationship between neuroanatomy and cognitive decline due to
Alzheimer's has been a major research focus in the last decade. However, to
infer cause-effect relationships rather than simple associations from
observational data, we need to (i) express the causal relationships leading to
cognitive decline in a graphical model, and (ii) ensure the causal effect of
interest is identifiable from the collected data. We derive a causal graph from
the current clinical knowledge on cause and effect in the Alzheimer's disease
continuum, and show that identifiability of the causal effect requires all
confounders to be known and measured. However, in complex neuroimaging studies,
we neither know all potential confounders nor do we have data on them. To
alleviate this requirement, we leverage the dependencies among multiple causes
by deriving a substitute confounder via a probabilistic latent factor model. In
our theoretical analysis, we prove that using the substitute confounder enables
identifiability of the causal effect of neuroanatomy on cognition. We
quantitatively evaluate the effectiveness of our approach on semi-synthetic
data, where we know the true causal effects, and illustrate its use on real
data on the Alzheimer's disease continuum, where it reveals important causes
that otherwise would have been missed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Polsterl_S/0/1/0/all/0/1"&gt;Sebastian P&amp;#xf6;lsterl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wachinger_C/0/1/0/all/0/1"&gt;Christian Wachinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multilayered Block Network Model to Forecast Large Dynamic Transportation Graphs: an Application to US Air Transport. (arXiv:1911.13136v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.13136</id>
        <link href="http://arxiv.org/abs/1911.13136"/>
        <updated>2021-06-22T01:57:12.772Z</updated>
        <summary type="html"><![CDATA[Dynamic transportation networks have been analyzed for years by means of
static graph-based indicators in order to study the temporal evolution of
relevant network components, and to reveal complex dependencies that would not
be easily detected by a direct inspection of the data. This paper presents a
state-of-the-art latent network model to forecast multilayer dynamic graphs
that are increasingly common in transportation and proposes a community-based
extension to reduce the computational burden. Flexible time series analysis is
obtained by modeling the probability of edges between vertices through latent
Gaussian processes. The models and Bayesian inference are illustrated on a
sample of 10-year data from four major airlines within the US air
transportation system. Results show how the estimated latent parameters from
the models are related to the airline's connectivity dynamics, and their
ability to project the multilayer graph into the future for out-of-sample full
network forecasts, while stochastic blockmodeling allows for the identification
of relevant communities. Reliable network predictions would allow policy-makers
to better understand the dynamics of the transport system, and help in their
planning on e.g. route development, or the deployment of new regulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rodriguez_Deniz_H/0/1/0/all/0/1"&gt;Hector Rodriguez-Deniz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Villani_M/0/1/0/all/0/1"&gt;Mattias Villani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Voltes_Dorta_A/0/1/0/all/0/1"&gt;Augusto Voltes-Dorta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Deep ODE-Nets using Basis Function Expansions. (arXiv:2106.10820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10820</id>
        <link href="http://arxiv.org/abs/2106.10820"/>
        <updated>2021-06-22T01:57:12.747Z</updated>
        <summary type="html"><![CDATA[The recently-introduced class of ordinary differential equation networks
(ODE-Nets) establishes a fruitful connection between deep learning and
dynamical systems. In this work, we reconsider formulations of the weights as
continuous-depth functions using linear combinations of basis functions. This
perspective allows us to compress the weights through a change of basis,
without retraining, while maintaining near state-of-the-art performance. In
turn, both inference time and the memory footprint are reduced, enabling quick
and rigorous adaptation between computational environments. Furthermore, our
framework enables meaningful continuous-in-time batch normalization layers
using function projections. The performance of basis function compression is
demonstrated by applying continuous-depth models to (a) image classification
tasks using convolutional units and (b) sentence-tagging tasks using
transformer encoder units.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Queiruga_A/0/1/0/all/0/1"&gt;Alejandro Queiruga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erichson_N/0/1/0/all/0/1"&gt;N. Benjamin Erichson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analytical confidence intervals for the number of different objects in data streams. (arXiv:1909.11564v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.11564</id>
        <link href="http://arxiv.org/abs/1909.11564"/>
        <updated>2021-06-22T01:57:12.742Z</updated>
        <summary type="html"><![CDATA[This paper develops a new mathematical-statistical approach to analyze a
class of Flajolet-Martin algorithms (FMa), and provides analytical confidence
intervals for the number F0 of distinct elements in a stream, based on Chernoff
bounds. The class of FMa has reached a significant popularity in bigdata stream
learning, and the attention of the literature has mainly been based on
algorithmic aspects, basically complexity optimality, while the statistical
analysis of these class of algorithms has been often faced heuristically. The
analysis provided here shows deep connections with mathematical special
functions and with extreme value theory. The latter connection may help in
explaining heuristic considerations, while the first opens many numerical
issues, faced at the end of the present paper. Finally, the algorithms are
tested on an anonymized real data stream and MonteCarlo simulations are
provided to support our analytical choice in this context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Aletti_G/0/1/0/all/0/1"&gt;Giacomo Aletti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in the Face of Adaptivity: A Bayesian Perspective. (arXiv:2106.10761v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10761</id>
        <link href="http://arxiv.org/abs/2106.10761"/>
        <updated>2021-06-22T01:57:12.738Z</updated>
        <summary type="html"><![CDATA[Repeated use of a data sample via adaptively chosen queries can rapidly lead
to overfitting, wherein the issued queries yield answers on the sample that
differ wildly from the values of those queries on the underlying data
distribution. Differential privacy provides a tool to ensure generalization
despite adaptively-chosen queries, but its worst-case nature means that it
cannot, for example, yield improved results for low-variance queries. In this
paper, we give a simple new characterization that illuminates the core problem
of adaptive data analysis. We show explicitly that the harms of adaptivity come
from the covariance between the behavior of future queries and a Bayes
factor-based measure of how much information about the data sample was encoded
in the responses given to past queries. We leverage this intuition to introduce
a new stability notion; we then use it to prove new generalization results for
the most basic noise-addition mechanisms (Laplace and Gaussian noise addition),
with guarantees that scale with the variance of the queries rather than the
square of their range. Our characterization opens the door to new insights and
new algorithms for the fundamental problem of achieving generalization in
adaptive data analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenfeld_M/0/1/0/all/0/1"&gt;Moshe Shenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ligett_K/0/1/0/all/0/1"&gt;Katrina Ligett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Examples Make Strong Poisons. (arXiv:2106.10807v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10807</id>
        <link href="http://arxiv.org/abs/2106.10807"/>
        <updated>2021-06-22T01:57:12.732Z</updated>
        <summary type="html"><![CDATA[The adversarial machine learning literature is largely partitioned into
evasion attacks on testing data and poisoning attacks on training data. In this
work, we show that adversarial examples, originally intended for attacking
pre-trained models, are even more effective for data poisoning than recent
methods designed specifically for poisoning. Our findings indicate that
adversarial examples, when assigned the original label of their natural base
image, cannot be used to train a classifier for natural images. Furthermore,
when adversarial examples are assigned their adversarial class label, they are
useful for training. This suggests that adversarial examples contain useful
semantic content, just with the ``wrong'' labels (according to a network, but
not a human). Our method, adversarial poisoning, is substantially more
effective than existing poisoning methods for secure dataset release, and we
release a poisoned version of ImageNet, ImageNet-P, to encourage research into
the strength of this form of data obfuscation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1"&gt;Liam Fowl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1"&gt;Ping-yeh Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1"&gt;Jonas Geiping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czaja_W/0/1/0/all/0/1"&gt;Wojtek Czaja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Cryptographic Hardness of Learning Single Periodic Neurons. (arXiv:2106.10744v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10744</id>
        <link href="http://arxiv.org/abs/2106.10744"/>
        <updated>2021-06-22T01:57:12.717Z</updated>
        <summary type="html"><![CDATA[We show a simple reduction which demonstrates the cryptographic hardness of
learning a single periodic neuron over isotropic Gaussian distributions in the
presence of noise. More precisely, our reduction shows that any polynomial-time
algorithm (not necessarily gradient-based) for learning such functions under
small noise implies a polynomial-time quantum algorithm for solving worst-case
lattice problems, whose hardness form the foundation of lattice-based
cryptography. Our core hard family of functions, which are well-approximated by
one-layer neural networks, take the general form of a univariate periodic
function applied to an affine projection of the data. These functions have
appeared in previous seminal works which demonstrate their hardness against
gradient-based (Shamir'18), and Statistical Query (SQ) algorithms (Song et
al.'17). We show that if (polynomially) small noise is added to the labels, the
intractability of learning these functions applies to all polynomial-time
algorithms under the aforementioned cryptographic assumptions.

Moreover, we demonstrate the necessity of noise in the hardness result by
designing a polynomial-time algorithm for learning certain families of such
functions under exponentially small adversarial noise. Our proposed algorithm
is not a gradient-based or an SQ algorithm, but is rather based on the
celebrated Lenstra-Lenstra-Lov\'asz (LLL) lattice basis reduction algorithm.
Furthermore, in the absence of noise, this algorithm can be directly applied to
solve CLWE detection (Bruna et al.'21) and phase retrieval with an optimal
sample complexity of $d+1$ samples. In the former case, this improves upon the
quadratic-in-$d$ sample complexity required in (Bruna et al.'21). In the latter
case, this improves upon the state-of-the-art AMP-based algorithm, which
requires approximately $1.128d$ samples (Barbier et al.'19).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Min Jae Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadik_I/0/1/0/all/0/1"&gt;Ilias Zadik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1"&gt;Joan Bruna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multirate Training of Neural Networks. (arXiv:2106.10771v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10771</id>
        <link href="http://arxiv.org/abs/2106.10771"/>
        <updated>2021-06-22T01:57:12.688Z</updated>
        <summary type="html"><![CDATA[We propose multirate training of neural networks: partitioning neural network
parameters into "fast" and "slow" parts which are trained simultaneously using
different learning rates. By choosing appropriate partitionings we can obtain
large computational speed-ups for transfer learning tasks. We show that for
various transfer learning applications in vision and NLP we can fine-tune deep
neural networks in almost half the time, without reducing the generalization
performance of the resulting model. We also discuss other splitting choices for
the neural network parameters which are beneficial in enhancing generalization
performance in settings where neural networks are trained from scratch.
Finally, we propose an additional multirate technique which can learn different
features present in the data by training the full network on different time
scales simultaneously. The benefits of using this approach are illustrated for
ResNet architectures on image data. Our paper unlocks the potential of using
multirate techniques for neural network training and provides many starting
points for future work in this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1"&gt;Tiffany Vlaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1"&gt;Benedict Leimkuhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction. (arXiv:2106.10786v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10786</id>
        <link href="http://arxiv.org/abs/2106.10786"/>
        <updated>2021-06-22T01:57:12.652Z</updated>
        <summary type="html"><![CDATA[Natural reading orders of words are crucial for information extraction from
form-like documents. Despite recent advances in Graph Convolutional Networks
(GCNs) on modeling spatial layout patterns of documents, they have limited
ability to capture reading orders of given word-level node representations in a
graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new
positional encoding technique designed to apprehend the sequential presentation
of words in documents. ROPE generates unique reading order codes for
neighboring words relative to the target word given a word-level graph
connectivity. We study two fundamental document entity extraction tasks
including word labeling and word grouping on the public FUNSD dataset and a
large-scale payment dataset. We show that ROPE consistently improves existing
GCNs with a margin up to 8.4% F1-score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renshen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1"&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1"&gt;Siyang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1"&gt;Ashok Popat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I-MAD: Interpretable Malware Detector Using Galaxy Transformer. (arXiv:1909.06865v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06865</id>
        <link href="http://arxiv.org/abs/1909.06865"/>
        <updated>2021-06-22T01:57:12.646Z</updated>
        <summary type="html"><![CDATA[Malware currently presents a number of serious threats to computer users.
Signature-based malware detection methods are limited in detecting new malware
samples that are significantly different from known ones. Therefore, machine
learning-based methods have been proposed, but there are two challenges these
methods face. The first is to model the full semantics behind the assembly code
of malware. The second challenge is to provide interpretable results while
keeping excellent detection performance. In this paper, we propose an
Interpretable MAlware Detector (I-MAD) that outperforms state-of-the-art static
malware detection models regarding accuracy with excellent interpretability. To
improve the detection performance, I-MAD incorporates a novel network component
called the Galaxy Transformer network that can understand assembly code at the
basic block, function, and executable levels. It also incorporates our proposed
interpretable feed-forward neural network to provide interpretations for its
detection results by quantifying the impact of each feature with respect to the
prediction. Experiment results show that our model significantly outperforms
existing state-of-the-art static malware detection models and presents
meaningful interpretations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Miles Q. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_B/0/1/0/all/0/1"&gt;Benjamin C. M. Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charland_P/0/1/0/all/0/1"&gt;Philippe Charland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Steven H.H. Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Machine Learning: Fad or Future?. (arXiv:2106.10714v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.10714</id>
        <link href="http://arxiv.org/abs/2106.10714"/>
        <updated>2021-06-22T01:57:12.632Z</updated>
        <summary type="html"><![CDATA[For the last few decades, classical machine learning has allowed us to
improve the lives of many through automation, natural language processing,
predictive analytics and much more. However, a major concern is the fact that
we're fast approach the threshold of the maximum possible computational
capacity available to us by the means of classical computing devices including
CPUs, GPUs and Application Specific Integrated Circuits (ASICs). This is due to
the exponential increase in model sizes which now have parameters in the
magnitude of billions and trillions, requiring a significant amount of
computing resources across a significant amount of time, just to converge one
single model. To observe the efficacy of using quantum computing for certain
machine learning tasks and explore the improved potential of convergence, error
reduction and robustness to noisy data, this paper will look forth to test and
verify the aspects in which quantum machine learning can help improve over
classical machine learning approaches while also shedding light on the likely
limitations that have prevented quantum approaches to become the mainstream. A
major focus will be to recreate the work by Farhi et al and conduct experiments
using their theory of performing machine learning in a quantum context, with
assistance from the Tensorflow Quantum documentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ishtiaq_A/0/1/0/all/0/1"&gt;Arhum Ishtiaq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mahmood_S/0/1/0/all/0/1"&gt;Sara Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning audio sequence representations for acoustic event classification. (arXiv:1707.08729v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1707.08729</id>
        <link href="http://arxiv.org/abs/1707.08729"/>
        <updated>2021-06-22T01:57:12.625Z</updated>
        <summary type="html"><![CDATA[Acoustic Event Classification (AEC) has become a significant task for
machines to perceive the surrounding auditory scene. However, extracting
effective representations that capture the underlying characteristics of the
acoustic events is still challenging. Previous methods mainly focused on
designing the audio features in a `hand-crafted' manner. Interestingly,
data-learnt features have been recently reported to show better performance. Up
to now, these were only considered on the frame level. In this article, we
propose an unsupervised learning framework to learn a vector representation of
an audio sequence for AEC. This framework consists of a Recurrent Neural
Network (RNN) encoder and an RNN decoder, which respectively transforms the
variable-length audio sequence into a fixed-length vector and reconstructs the
input sequence on the generated vector. After training the encoder-decoder, we
feed the audio sequences to the encoder and then take the learnt vectors as the
audio sequence representations. Compared with previous methods, the proposed
method can not only deal with the problem of arbitrary-lengths of audio
streams, but also learn the salient information of the sequence. Extensive
evaluation on a large-size acoustic event database is performed, and the
empirical results demonstrate that the learnt audio sequence representation
yields a significant performance improvement by a large margin compared with
other state-of-the-art hand-crafted sequence features for AEC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zixing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Ding Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1"&gt;Kun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Bayesian Meta-learning via Weighted Free Energy Minimization. (arXiv:2106.10711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10711</id>
        <link href="http://arxiv.org/abs/2106.10711"/>
        <updated>2021-06-22T01:57:12.619Z</updated>
        <summary type="html"><![CDATA[Meta-learning optimizes the hyperparameters of a training procedure, such as
its initialization, kernel, or learning rate, based on data sampled from a
number of auxiliary tasks. A key underlying assumption is that the auxiliary
tasks, known as meta-training tasks, share the same generating distribution as
the tasks to be encountered at deployment time, known as meta-test tasks. This
may, however, not be the case when the test environment differ from the
meta-training conditions. To address shifts in task generating distribution
between meta-training and meta-testing phases, this paper introduces weighted
free energy minimization (WFEM) for transfer meta-learning. We instantiate the
proposed approach for non-parametric Bayesian regression and classification via
Gaussian Processes (GPs). The method is validated on a toy sinusoidal
regression problem, as well as on classification using miniImagenet and CUB
data sets, through comparison with standard meta-learning of GP priors as
implemented by PACOH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_S/0/1/0/all/0/1"&gt;Sharu Theresa Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Signal Representations for EEG Cross-Subject Channel Selection and Trial Classification. (arXiv:2106.10633v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.10633</id>
        <link href="http://arxiv.org/abs/2106.10633"/>
        <updated>2021-06-22T01:57:12.614Z</updated>
        <summary type="html"><![CDATA[EEG technology finds applications in several domains. Currently, most EEG
systems require subjects to wear several electrodes on the scalp to be
effective. However, several channels might include noisy information, redundant
signals, induce longer preparation times and increase computational times of
any automated system for EEG decoding. One way to reduce the signal-to-noise
ratio and improve classification accuracy is to combine channel selection with
feature extraction, but EEG signals are known to present high inter-subject
variability. In this work we introduce a novel algorithm for
subject-independent channel selection of EEG recordings. Considering
multi-channel trial recordings as statistical units and the EEG decoding task
as the class of reference, the algorithm (i) exploits channel-specific
1D-Convolutional Neural Networks (1D-CNNs) as feature extractors in a
supervised fashion to maximize class separability; (ii) it reduces a high
dimensional multi-channel trial representation into a unique trial vector by
concatenating the channels' embeddings and (iii) recovers the complex
inter-channel relationships during channel selection, by exploiting an ensemble
of AutoEncoders (AE) to identify from these vectors the most relevant channels
to perform classification. After training, the algorithm can be exploited by
transferring only the parametrized subgroup of selected channel-specific
1D-CNNs to new signals from new subjects and obtain low-dimensional and highly
informative trial vectors to be fed to any classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Massi_M/0/1/0/all/0/1"&gt;Michela C. Massi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ieva_F/0/1/0/all/0/1"&gt;Francesca Ieva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast PDN Impedance Prediction Using Deep Learning. (arXiv:2106.10693v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10693</id>
        <link href="http://arxiv.org/abs/2106.10693"/>
        <updated>2021-06-22T01:57:12.608Z</updated>
        <summary type="html"><![CDATA[Modeling and simulating a power distribution network (PDN) for printed
circuit boards (PCBs) with irregular board shapes and multi-layer stackup is
computationally inefficient using full-wave simulations. This paper presents a
new concept of using deep learning for PDN impedance prediction. A boundary
element method (BEM) is applied to efficiently calculate the impedance for
arbitrary board shape and stackup. Then over one million boards with different
shapes, stackup, IC location, and decap placement are randomly generated to
train a deep neural network (DNN). The trained DNN can predict the impedance
accurately for new board configurations that have not been used for training.
The consumed time using the trained DNN is only 0.1 seconds, which is over 100
times faster than the BEM method and 5000 times faster than full-wave
simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Ling Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juang_J/0/1/0/all/0/1"&gt;Jack Juang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiguradze_Z/0/1/0/all/0/1"&gt;Zurab Kiguradze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_B/0/1/0/all/0/1"&gt;Bo Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1"&gt;Shuai Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Songping Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiping Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1"&gt;Chulsoon Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging directed causal discovery to detect latent common causes. (arXiv:1910.10174v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.10174</id>
        <link href="http://arxiv.org/abs/1910.10174"/>
        <updated>2021-06-22T01:57:12.603Z</updated>
        <summary type="html"><![CDATA[The discovery of causal relationships is a fundamental problem in science and
medicine. In recent years, many elegant approaches to discovering causal
relationships between two variables from observational data have been proposed.
However, most of these deal only with purely directed causal relationships and
cannot detect latent common causes. Here, we devise a general heuristic which
takes a causal discovery algorithm that can only distinguish purely directed
causal relations and modifies it to also detect latent common causes. We apply
our method to two directed causal discovery algorithms, the Information
Geometric Causal Inference of (Daniusis et al., 2010) and the Kernel
Conditional Deviance for Causal Inference of (Mitrovic, Sejdinovic, & Teh,
2018), and extensively test on synthetic data -- detecting latent common causes
in additive, multiplicative and complex noise regimes -- and on real data,
where we are able to detect known common causes. In addition to detecting
latent common causes, our experiments demonstrate that both the modified
algorithms preserve the performance of the original in distinguishing directed
causal relations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lee_C/0/1/0/all/0/1"&gt;Ciar&amp;#xe1;n M. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hart_C/0/1/0/all/0/1"&gt;Christopher Hart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Richens_J/0/1/0/all/0/1"&gt;Jonathan G. Richens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Johri_S/0/1/0/all/0/1"&gt;Saurabh Johri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representations and Strategies for Transferable Machine Learning Models in Chemical Discovery. (arXiv:2106.10768v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.10768</id>
        <link href="http://arxiv.org/abs/2106.10768"/>
        <updated>2021-06-22T01:57:12.589Z</updated>
        <summary type="html"><![CDATA[Strategies for machine-learning(ML)-accelerated discovery that are general
across materials composition spaces are essential, but demonstrations of ML
have been primarily limited to narrow composition variations. By addressing the
scarcity of data in promising regions of chemical space for challenging targets
like open-shell transition-metal complexes, general representations and
transferable ML models that leverage known relationships in existing data will
accelerate discovery. Over a large set (ca. 1000) of isovalent transition-metal
complexes, we quantify evident relationships for different properties (i.e.,
spin-splitting and ligand dissociation) between rows of the periodic table
(i.e., 3d/4d metals and 2p/3p ligands). We demonstrate an extension to
graph-based revised autocorrelation (RAC) representation (i.e., eRAC) that
incorporates the effective nuclear charge alongside the nuclear charge
heuristic that otherwise overestimates dissimilarity of isovalent complexes. To
address the common challenge of discovery in a new space where data is limited,
we introduce a transfer learning approach in which we seed models trained on a
large amount of data from one row of the periodic table with a small number of
data points from the additional row. We demonstrate the synergistic value of
the eRACs alongside this transfer learning strategy to consistently improve
model performance. Analysis of these models highlights how the approach
succeeds by reordering the distances between complexes to be more consistent
with the periodic table, a property we expect to be broadly useful for other
materials domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Harper_D/0/1/0/all/0/1"&gt;Daniel R. Harper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Aditya Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Arunachalam_N/0/1/0/all/0/1"&gt;Naveen Arunachalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenru Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Janet_J/0/1/0/all/0/1"&gt;Jon Paul Janet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kulik_H/0/1/0/all/0/1"&gt;Heather J. Kulik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IG-RL: Inductive Graph Reinforcement Learning for Massive-Scale Traffic Signal Control. (arXiv:2003.05738v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.05738</id>
        <link href="http://arxiv.org/abs/2003.05738"/>
        <updated>2021-06-22T01:57:12.584Z</updated>
        <summary type="html"><![CDATA[Scaling adaptive traffic-signal control involves dealing with combinatorial
state and action spaces. Multi-agent reinforcement learning attempts to address
this challenge by distributing control to specialized agents. However,
specialization hinders generalization and transferability, and the
computational graphs underlying neural-networks architectures -- dominating in
the multi-agent setting -- do not offer the flexibility to handle an arbitrary
number of entities which changes both between road networks, and over time as
vehicles traverse the network. We introduce Inductive Graph Reinforcement
Learning (IG-RL) based on graph-convolutional networks which adapts to the
structure of any road network, to learn detailed representations of
traffic-controllers and their surroundings. Our decentralized approach enables
learning of a transferable-adaptive-traffic-signal-control policy. After being
trained on an arbitrary set of road networks, our model can generalize to new
road networks, traffic distributions, and traffic regimes, with no additional
training and a constant number of parameters, enabling greater scalability
compared to prior methods. Furthermore, our approach can exploit the
granularity of available data by capturing the (dynamic) demand at both the
lane and the vehicle levels. The proposed method is tested on both road
networks and traffic settings never experienced during training. We compare
IG-RL to multi-agent reinforcement learning and domain-specific baselines. In
both synthetic road networks and in a larger experiment involving the control
of the 3,971 traffic signals of Manhattan, we show that different
instantiations of IG-RL outperform baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devailly_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois-Xavier Devailly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larocque_D/0/1/0/all/0/1"&gt;Denis Larocque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1"&gt;Laurent Charlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossy Compression for Lossless Prediction. (arXiv:2106.10800v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10800</id>
        <link href="http://arxiv.org/abs/2106.10800"/>
        <updated>2021-06-22T01:57:12.577Z</updated>
        <summary type="html"><![CDATA[Most data is automatically collected and only ever "seen" by algorithms. Yet,
data compressors preserve perceptual fidelity rather than just the information
needed by algorithms performing downstream tasks. In this paper, we
characterize the bit-rate required to ensure high performance on all predictive
tasks that are invariant under a set of transformations, such as data
augmentations. Based on our theory, we design unsupervised objectives for
training neural compressors. Using these objectives, we train a generic image
compressor that achieves substantial rate savings (more than $1000\times$ on
ImageNet) compared to JPEG on 8 datasets, without decreasing downstream
classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1"&gt;Yann Dubois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bloem_Reddy_B/0/1/0/all/0/1"&gt;Benjamin Bloem-Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullrich_K/0/1/0/all/0/1"&gt;Karen Ullrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Shapley Value fair? Improving Client Selection for Mavericks in Federated Learning. (arXiv:2106.10734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10734</id>
        <link href="http://arxiv.org/abs/2106.10734"/>
        <updated>2021-06-22T01:57:12.564Z</updated>
        <summary type="html"><![CDATA[Shapley Value is commonly adopted to measure and incentivize client
participation in federated learning. In this paper, we show -- theoretically
and through simulations -- that Shapley Value underestimates the contribution
of a common type of client: the Maverick. Mavericks are clients that differ
both in data distribution and data quantity and can be the sole owners of
certain types of data. Selecting the right clients at the right moment is
important for federated learning to reduce convergence times and improve
accuracy. We propose FedEMD, an adaptive client selection strategy based on the
Wasserstein distance between the local and global data distributions. As FedEMD
adapts the selection probability such that Mavericks are preferably selected
when the model benefits from improvement on rare classes, it consistently
ensures the fast convergence in the presence of different types of Mavericks.
Compared to existing strategies, including Shapley Value-based ones, FedEMD
improves the convergence of neural network classifiers by at least 26.9% for
FedAvg aggregation compared with the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1"&gt;Chi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1"&gt;Stefanie Roos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Training using Weight-Constrained Stochastic Dynamics. (arXiv:2106.10704v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10704</id>
        <link href="http://arxiv.org/abs/2106.10704"/>
        <updated>2021-06-22T01:57:12.559Z</updated>
        <summary type="html"><![CDATA[We employ constraints to control the parameter space of deep neural networks
throughout training. The use of customized, appropriately designed constraints
can reduce the vanishing/exploding gradients problem, improve smoothness of
classification boundaries, control weight magnitudes and stabilize deep neural
networks, and thus enhance the robustness of training algorithms and the
generalization capabilities of neural networks. We provide a general approach
to efficiently incorporate constraints into a stochastic gradient Langevin
framework, allowing enhanced exploration of the loss landscape. We also present
specific examples of constrained training methods motivated by orthogonality
preservation for weight matrices and explicit weight normalizations.
Discretization schemes are provided both for the overdamped formulation of
Langevin dynamics and the underdamped form, in which momenta further improve
sampling efficiency. These optimization schemes can be used directly, without
needing to adapt neural network architecture design choices or to modify the
objective with regularization terms, and see performance improvements in
classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leimkuhler_B/0/1/0/all/0/1"&gt;Benedict Leimkuhler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1"&gt;Tiffany Vlaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchon_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Pouchon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1"&gt;Amos Storkey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming Catastrophic Forgetting by Generative Regularization. (arXiv:1912.01238v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01238</id>
        <link href="http://arxiv.org/abs/1912.01238"/>
        <updated>2021-06-22T01:57:12.553Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a new method to overcome catastrophic forgetting by
adding generative regularization to Bayesian inference framework. Bayesian
method provides a general framework for continual learning. We could further
construct a generative regularization term for all given classification models
by leveraging energy-based models and Langevin-dynamic sampling to enrich the
features learned in each task. By combining discriminative and generative loss
together, we empirically show that the proposed method outperforms
state-of-the-art methods on a variety of tasks, avoiding catastrophic
forgetting in continual learning. In particular, the proposed method
outperforms baseline methods over 15% on the Fashion-MNIST dataset and 10% on
the CUB dataset]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Patrick H. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Strategies for Decision Theoretic Online Learning. (arXiv:2106.10717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10717</id>
        <link href="http://arxiv.org/abs/2106.10717"/>
        <updated>2021-06-22T01:57:12.548Z</updated>
        <summary type="html"><![CDATA[We extend the drifting games analysis to continuous time and show that the
optimal adversary, if the value function has strictly positive derivative up to
fourth order is bronian motion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1"&gt;Yoav Freund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10698</id>
        <link href="http://arxiv.org/abs/2106.10698"/>
        <updated>2021-06-22T01:57:12.543Z</updated>
        <summary type="html"><![CDATA[One of the important and tedious task in agricultural practices is the
detection of the disease on crops. It requires huge time as well as skilled
labor. This paper proposes a smart and efficient technique for detection of
crop disease which uses computer vision and machine learning techniques. The
proposed system is able to detect 20 different diseases of 5 common plants with
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1"&gt;Pranesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1"&gt;Atharva Karwande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1"&gt;Tejas Kolhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1"&gt;Soham Kamble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Akshay Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1"&gt;Medha Wyawahare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients. (arXiv:2106.10784v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10784</id>
        <link href="http://arxiv.org/abs/2106.10784"/>
        <updated>2021-06-22T01:57:12.537Z</updated>
        <summary type="html"><![CDATA[\textit{Differentiable ARchiTecture Search} (DARTS) has recently become the
mainstream of neural architecture search (NAS) due to its efficiency and
simplicity. With a gradient-based bi-level optimization, DARTS alternately
optimizes the inner model weights and the outer architecture parameter in a
weight-sharing supernet. A key challenge to the scalability and quality of the
learned architectures is the need for differentiating through the inner-loop
optimisation. While much has been discussed about several potentially fatal
factors in DARTS, the architecture gradient, a.k.a. hypergradient, has received
less attention. In this paper, we tackle the hypergradient computation in DARTS
based on the implicit function theorem, making it only depends on the obtained
solution to the inner-loop optimization and agnostic to the optimization path.
To further reduce the computational requirements, we formulate a stochastic
hypergradient approximation for differentiable NAS, and theoretically show that
the architecture optimization with the proposed method, named iDARTS, is
expected to converge to a stationary point. Comprehensive experiments on two
NAS benchmark search spaces and the common NAS search space verify the
effectiveness of our proposed method. It leads to architectures outperforming,
with large margins, those learned by the baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Miao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1"&gt;Steven Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1"&gt;Ehsan Abbasnejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haffari_R/0/1/0/all/0/1"&gt;Reza Haffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Network Embedding in Apache Spark. (arXiv:2106.10620v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.10620</id>
        <link href="http://arxiv.org/abs/2106.10620"/>
        <updated>2021-06-22T01:57:12.532Z</updated>
        <summary type="html"><![CDATA[Network embedding has been widely used in social recommendation and network
analysis, such as recommendation systems and anomaly detection with graphs.
However, most of previous approaches cannot handle large graphs efficiently,
due to that (i) computation on graphs is often costly and (ii) the size of
graph or the intermediate results of vectors could be prohibitively large,
rendering it difficult to be processed on a single machine. In this paper, we
propose an efficient and effective distributed algorithm for network embedding
on large graphs using Apache Spark, which recursively partitions a graph into
several small-sized subgraphs to capture the internal and external structural
information of nodes, and then computes the network embedding for each subgraph
in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain
the embeddings of nodes in a linear cost. After that, we demonstrate in various
experiments that our proposed approach is able to handle graphs with billions
of edges within a few hours and is at least 4 times faster than the
state-of-the-art approaches. Besides, it achieves up to $4.25\%$ and $4.27\%$
improvements on link prediction and node classification tasks respectively. In
the end, we deploy the proposed algorithms in two online games of Tencent with
the applications of friend recommendation and item recommendation, which
improve the competitors by up to $91.11\%$ in running time and up to $12.80\%$
in the corresponding evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wenqing Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep splitting method for parabolic PDEs. (arXiv:1907.03452v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.03452</id>
        <link href="http://arxiv.org/abs/1907.03452"/>
        <updated>2021-06-22T01:57:12.526Z</updated>
        <summary type="html"><![CDATA[In this paper we introduce a numerical method for nonlinear parabolic PDEs
that combines operator splitting with deep learning. It divides the PDE
approximation problem into a sequence of separate learning problems. Since the
computational graph for each of the subproblems is comparatively small, the
approach can handle extremely high-dimensional PDEs. We test the method on
different examples from physics, stochastic control and mathematical finance.
In all cases, it yields very good results in up to 10,000 dimensions with short
run times.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Beck_C/0/1/0/all/0/1"&gt;Christian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Becker_S/0/1/0/all/0/1"&gt;Sebastian Becker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Cheridito_P/0/1/0/all/0/1"&gt;Patrick Cheridito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1"&gt;Arnulf Jentzen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Neufeld_A/0/1/0/all/0/1"&gt;Ariel Neufeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Generalization Bounds of Group Invariant / Equivariant Deep Networks via Quotient Feature Spaces. (arXiv:1910.06552v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.06552</id>
        <link href="http://arxiv.org/abs/1910.06552"/>
        <updated>2021-06-22T01:57:12.520Z</updated>
        <summary type="html"><![CDATA[Numerous invariant (or equivariant) neural networks have succeeded in
handling invariant data such as point clouds and graphs. However, a
generalization theory for the neural networks has not been well developed,
because several essential factors for the theory, such as network size and
margin distribution, are not deeply connected to the invariance and
equivariance. In this study, we develop a novel generalization error bound for
invariant and equivariant deep neural networks. To describe the effect of
invariance and equivariance on generalization, we develop a notion of a
\textit{quotient feature space}, which measures the effect of group actions for
the properties. Our main result proves that the volume of quotient feature
spaces can describe the generalization error. Furthermore, the bound shows that
the invariance and equivariance significantly improve the leading term of the
bound. We apply our result to specific invariant and equivariant networks, such
as DeepSets (Zaheer et al. (2017)), and show that their generalization bound is
considerably improved by $\sqrt{n!}$, where $n!$ is the number of permutations.
We also discuss the expressive power of invariant DNNs and show that they can
achieve an optimal approximation rate. Our experimental result supports our
theoretical claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sannai_A/0/1/0/all/0/1"&gt;Akiyoshi Sannai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Imaizumi_M/0/1/0/all/0/1"&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kawano_M/0/1/0/all/0/1"&gt;Makoto Kawano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densities of Almost Surely Terminating Probabilistic Programs are Differentiable Almost Everywhere. (arXiv:2004.03924v2 [cs.LO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03924</id>
        <link href="http://arxiv.org/abs/2004.03924"/>
        <updated>2021-06-22T01:57:12.496Z</updated>
        <summary type="html"><![CDATA[We study the differential properties of higher-order statistical
probabilistic programs with recursion and conditioning. Our starting point is
an open problem posed by Hongseok Yang: what class of statistical probabilistic
programs have densities that are differentiable almost everywhere? To formalise
the problem, we consider Statistical PCF (SPCF), an extension of call-by-value
PCF with real numbers, and constructs for sampling and conditioning. We give
SPCF a sampling-style operational semantics a la Borgstrom et al., and study
the associated weight (commonly referred to as the density) function and value
function on the set of possible execution traces. Our main result is that
almost-surely terminating SPCF programs, generated from a set of primitive
functions (e.g. the set of analytic functions) satisfying mild closure
properties, have weight and value functions that are almost-everywhere
differentiable. We use a stochastic form of symbolic execution to reason about
almost-everywhere differentiability. A by-product of this work is that
almost-surely terminating deterministic (S)PCF programs with real parameters
denote functions that are almost-everywhere differentiable. Our result is of
practical interest, as almost-everywhere differentiability of the density
function is required to hold for the correctness of major gradient-based
inference algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1"&gt;Carol Mak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_C/0/1/0/all/0/1"&gt;C.-H. Luke Ong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paquet_H/0/1/0/all/0/1"&gt;Hugo Paquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1"&gt;Dominik Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Regression via Model Based Methods. (arXiv:2106.10759v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10759</id>
        <link href="http://arxiv.org/abs/2106.10759"/>
        <updated>2021-06-22T01:57:12.473Z</updated>
        <summary type="html"><![CDATA[The mean squared error loss is widely used in many applications, including
auto-encoders, multi-target regression, and matrix factorization, to name a
few. Despite computational advantages due to its differentiability, it is not
robust to outliers. In contrast, l_p norms are known to be robust, but cannot
be optimized via, e.g., stochastic gradient descent, as they are
non-differentiable. We propose an algorithm inspired by so-called model-based
optimization (MBO) [35, 36], which replaces a non-convex objective with a
convex model function and alternates between optimizing the model function and
updating the solution. We apply this to robust regression, proposing SADM, a
stochastic variant of the Online Alternating Direction Method of Multipliers
(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges
with the rate O(log T/T). Finally, we demonstrate experimentally (a) the
robustness of l_p norms to outliers and (b) the efficiency of our proposed
model-based algorithms in comparison with gradient methods on autoencoders and
multi-target regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1"&gt;Armin Moharrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1"&gt;Khashayar Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1"&gt;Edmund Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1"&gt;Stratis Ioannidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A compressive multi-kernel method for privacy-preserving machine learning. (arXiv:2106.10671v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10671</id>
        <link href="http://arxiv.org/abs/2106.10671"/>
        <updated>2021-06-22T01:57:12.464Z</updated>
        <summary type="html"><![CDATA[As the analytic tools become more powerful, and more data are generated on a
daily basis, the issue of data privacy arises. This leads to the study of the
design of privacy-preserving machine learning algorithms. Given two objectives,
namely, utility maximization and privacy-loss minimization, this work is based
on two previously non-intersecting regimes -- Compressive Privacy and
multi-kernel method. Compressive Privacy is a privacy framework that employs
utility-preserving lossy-encoding scheme to protect the privacy of the data,
while multi-kernel method is a kernel based machine learning regime that
explores the idea of using multiple kernels for building better predictors. The
compressive multi-kernel method proposed consists of two stages -- the
compression stage and the multi-kernel stage. The compression stage follows the
Compressive Privacy paradigm to provide the desired privacy protection. Each
kernel matrix is compressed with a lossy projection matrix derived from the
Discriminant Component Analysis (DCA). The multi-kernel stage uses the
signal-to-noise ratio (SNR) score of each kernel to non-uniformly combine
multiple compressive kernels. The proposed method is evaluated on two
mobile-sensing datasets -- MHEALTH and HAR -- where activity recognition is
defined as utility and person identification is defined as privacy. The results
show that the compression regime is successful in privacy preservation as the
privacy classification accuracies are almost at the random-guess level in all
experiments. On the other hand, the novel SNR-based multi-kernel shows utility
classification accuracy improvement upon the state-of-the-art in both datasets.
These results indicate a promising direction for research in privacy-preserving
machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chanyaswad_T/0/1/0/all/0/1"&gt;Thee Chanyaswad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1"&gt;J. Morris Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1"&gt;S.Y. Kung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Outlier Detection and Spatial Analysis Algorithms. (arXiv:2106.10669v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10669</id>
        <link href="http://arxiv.org/abs/2106.10669"/>
        <updated>2021-06-22T01:57:12.456Z</updated>
        <summary type="html"><![CDATA[Outlier detection is a significant area in data mining. It can be either used
to pre-process the data prior to an analysis or post the processing phase
(before visualization) depending on the effectiveness of the outlier and its
importance. Outlier detection extends to several fields such as detection of
credit card fraud, network intrusions, machine failure prediction, potential
terrorist attacks, and so on. Outliers are those data points with
characteristics considerably different. They deviate from the data set causing
inconsistencies, noise and anomalies during analysis and result in modification
of the original points However, a common misconception is that outliers have to
be immediately eliminated or replaced from the data set. Such points could be
considered useful if analyzed separately as they could be obtained from a
separate mechanism entirely making it important to the research question. This
study surveys the different methods of outlier detection for spatial analysis.
Spatial data or geospatial data are those that exhibit geographic properties or
attributes such as position or areas. An example would be weather data such as
precipitation, temperature, wind velocity, and so on collected for a defined
region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+John_J/0/1/0/all/0/1"&gt;Jacob John&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency. (arXiv:2106.10649v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10649</id>
        <link href="http://arxiv.org/abs/2106.10649"/>
        <updated>2021-06-22T01:57:12.447Z</updated>
        <summary type="html"><![CDATA[Backpropagation image saliency aims at explaining model predictions by
estimating model-centric importance of individual pixels in the input. However,
class-insensitivity of the earlier layers in a network only allows saliency
computation with low resolution activation maps of the deeper layers, resulting
in compromised image saliency. Remedifying this can lead to sanity failures. We
propose CAMERAS, a technique to compute high-fidelity backpropagation saliency
maps without requiring any external priors and preserving the map sanity. Our
method systematically performs multi-scale accumulation and fusion of the
activation maps and backpropagated gradients to compute precise saliency maps.
From accurate image saliency to articulation of relative importance of input
features for different models, and precise discrimination between model
perception of visually similar objects, our high-resolution mapping offers
multiple novel insights into the black-box deep visual models, which are
presented in the paper. We also demonstrate the utility of our saliency maps in
adversarial setup by drastically reducing the norm of attack signals by
focusing them on the precise regions identified by our maps. Our method also
inspires new evaluation metrics and a sanity check for this developing research
direction. Code is available here https://github.com/VisMIL/CAMERAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1"&gt;Mohammad A. A. K. Jalwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighborhood Contrastive Learning for Novel Class Discovery. (arXiv:2106.10731v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10731</id>
        <link href="http://arxiv.org/abs/2106.10731"/>
        <updated>2021-06-22T01:57:12.431Z</updated>
        <summary type="html"><![CDATA[In this paper, we address Novel Class Discovery (NCD), the task of unveiling
new classes in a set of unlabeled samples given a labeled dataset with known
classes. We exploit the peculiarities of NCD to build a new framework, named
Neighborhood Contrastive Learning (NCL), to learn discriminative
representations that are important to clustering performance. Our contribution
is twofold. First, we find that a feature extractor trained on the labeled set
generates representations in which a generic query sample and its neighbors are
likely to share the same class. We exploit this observation to retrieve and
aggregate pseudo-positive pairs with contrastive learning, thus encouraging the
model to learn more discriminative representations. Second, we notice that most
of the instances are easily discriminated by the network, contributing less to
the contrastive loss. To overcome this issue, we propose to generate hard
negatives by mixing labeled and unlabeled samples in the feature space. We
experimentally demonstrate that these two ingredients significantly contribute
to clustering performance and lead our model to outperform state-of-the-art
methods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%
on ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Subhankar Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhiming Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opportunities and challenges in partitioning the graph measure space of real-world networks. (arXiv:2106.10753v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10753</id>
        <link href="http://arxiv.org/abs/2106.10753"/>
        <updated>2021-06-22T01:57:12.406Z</updated>
        <summary type="html"><![CDATA[Based on a large dataset containing thousands of real-world networks ranging
from genetic, protein interaction, and metabolic networks to brain, language,
ecology, and social networks we search for defining structural measures of the
different complex network domains (CND). We calculate 208 measures for all
networks and using a comprehensive and scrupulous workflow of statistical and
machine learning methods we investigated the limitations and possibilities of
identifying the key graph measures of CNDs. Our approach managed to identify
well distinguishable groups of network domains and confer their relevant
features. These features turn out to be CND specific and not unique even at the
level of individual CNDs. The presented methodology may be applied to other
similar scenarios involving highly unbalanced and skewed datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jozsa_M/0/1/0/all/0/1"&gt;M&amp;#xe1;t&amp;#xe9; J&amp;#xf3;zsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazar_A/0/1/0/all/0/1"&gt;Alp&amp;#xe1;r S. L&amp;#xe1;z&amp;#xe1;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazar_Z/0/1/0/all/0/1"&gt;Zsolt I. L&amp;#xe1;z&amp;#xe1;r&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Attack on Graph Neural Networks as An Influence Maximization Problem. (arXiv:2106.10785v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10785</id>
        <link href="http://arxiv.org/abs/2106.10785"/>
        <updated>2021-06-22T01:57:12.400Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have attracted increasing interests. With broad
deployments of GNNs in real-world applications, there is an urgent need for
understanding the robustness of GNNs under adversarial attacks, especially in
realistic setups. In this work, we study the problem of attacking GNNs in a
restricted and realistic setup, by perturbing the features of a small set of
nodes, with no access to model parameters and model predictions. Our formal
analysis draws a connection between this type of attacks and an influence
maximization problem on the graph. This connection not only enhances our
understanding on the problem of adversarial attack on GNNs, but also allows us
to propose a group of effective and practical attack strategies. Our
experiments verify that the proposed attack strategies significantly degrade
the performance of three popular GNN models and outperform baseline adversarial
attack strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1"&gt;Junwei Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1"&gt;Qiaozhu Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cogradient Descent for Dependable Learning. (arXiv:2106.10617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10617</id>
        <link href="http://arxiv.org/abs/2106.10617"/>
        <updated>2021-06-22T01:57:12.373Z</updated>
        <summary type="html"><![CDATA[Conventional gradient descent methods compute the gradients for multiple
variables through the partial derivative. Treating the coupled variables
independently while ignoring the interaction, however, leads to an insufficient
optimization for bilinear models. In this paper, we propose a dependable
learning based on Cogradient Descent (CoGD) algorithm to address the bilinear
optimization problem, providing a systematic way to coordinate the gradients of
coupling variables based on a kernelized projection function. CoGD is
introduced to solve bilinear problems when one variable is with sparsity
constraint, as often occurs in modern learning paradigms. CoGD can also be used
to decompose the association of features and weights, which further generalizes
our method to better train convolutional neural networks (CNNs) and improve the
model capacity. CoGD is applied in representative bilinear problems, including
image reconstruction, image inpainting, network pruning and CNN training.
Extensive experiments show that CoGD improves the state-of-the-arts by
significant margins. Code is available at
{https://github.com/bczhangbczhang/CoGD}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baochang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1"&gt;Li&amp;#x27;an Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning in the social and health sciences. (arXiv:2106.10716v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10716</id>
        <link href="http://arxiv.org/abs/2106.10716"/>
        <updated>2021-06-22T01:57:12.368Z</updated>
        <summary type="html"><![CDATA[The uptake of machine learning (ML) approaches in the social and health
sciences has been rather slow, and research using ML for social and health
research questions remains fragmented. This may be due to the separate
development of research in the computational/data versus social and health
sciences as well as a lack of accessible overviews and adequate training in ML
techniques for non data science researchers. This paper provides a meta-mapping
of research questions in the social and health sciences to appropriate ML
approaches, by incorporating the necessary requirements to statistical analysis
in these disciplines. We map the established classification into description,
prediction, and causal inference to common research goals, such as estimating
prevalence of adverse health or social outcomes, predicting the risk of an
event, and identifying risk factors or causes of adverse outcomes. This
meta-mapping aims at overcoming disciplinary barriers and starting a fluid
dialogue between researchers from the social and health sciences and
methodologically trained researchers. Such mapping may also help to fully
exploit the benefits of ML while considering domain-specific aspects relevant
to the social and health sciences, and hopefully contribute to the acceleration
of the uptake of ML applications to advance both basic and applied social and
health sciences research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leist_A/0/1/0/all/0/1"&gt;Anja K. Leist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klee_M/0/1/0/all/0/1"&gt;Matthias Klee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jung Hyun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehkopf_D/0/1/0/all/0/1"&gt;David H. Rehkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bordas_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane P. A. Bordas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muniz_Terrera_G/0/1/0/all/0/1"&gt;Graciela Muniz-Terrera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wade_S/0/1/0/all/0/1"&gt;Sara Wade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedXGBoost: Privacy-Preserving XGBoost for Federated Learning. (arXiv:2106.10662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10662</id>
        <link href="http://arxiv.org/abs/2106.10662"/>
        <updated>2021-06-22T01:57:12.362Z</updated>
        <summary type="html"><![CDATA[Federated learning is the distributed machine learning framework that enables
collaborative training across multiple parties while ensuring data privacy.
Practical adaptation of XGBoost, the state-of-the-art tree boosting framework,
to federated learning remains limited due to high cost incurred by conventional
privacy-preserving methods. To address the problem, we propose two variants of
federated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP.
Our first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication
method to preserve privacy with lossless accuracy and lower overhead than
encryption-based techniques. Developed independently, the second protocol
FedXGBoost-LDP is heuristically designed with noise perturbation for local
differential privacy, and empirically evaluated on real-world and synthetic
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Nhan Khanh Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quang Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingchen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fangzhou Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Quanwei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1"&gt;Sandra Hirche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10587</id>
        <link href="http://arxiv.org/abs/2106.10587"/>
        <updated>2021-06-22T01:57:12.347Z</updated>
        <summary type="html"><![CDATA[Existing computer vision research in categorization struggles with
fine-grained attributes recognition due to the inherently high intra-class
variances and low inter-class variances. SOTA methods tackle this challenge by
locating the most informative image regions and rely on them to classify the
complete image. The most recent work, Vision Transformer (ViT), shows its
strong performance in both traditional and fine-grained classification tasks.
In this work, we propose a multi-stage ViT framework for fine-grained image
classification tasks, which localizes the informative image regions without
requiring architectural changes using the inherent multi-head self-attention
mechanism. We also introduce attention-guided augmentations for improving the
model's capabilities. We demonstrate the value of our approach by experimenting
with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,
Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's
interpretability via qualitative results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1"&gt;Kerem Turgutlu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-06-22T01:57:11.808Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combinatorial Semi-Bandit in the Non-Stationary Environment. (arXiv:2002.03580v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03580</id>
        <link href="http://arxiv.org/abs/2002.03580"/>
        <updated>2021-06-22T01:57:11.803Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the non-stationary combinatorial semi-bandit
problem, both in the switching case and in the dynamic case. In the general
case where (a) the reward function is non-linear, (b) arms may be
probabilistically triggered, and (c) only approximate offline oracle exists
\cite{wang2017improving}, our algorithm achieves
$\tilde{\mathcal{O}}(\sqrt{\mathcal{S} T})$ distribution-dependent regret in
the switching case, and $\tilde{\mathcal{O}}(\mathcal{V}^{1/3}T^{2/3})$ in the
dynamic case, where $\mathcal S$ is the number of switchings and $\mathcal V$
is the sum of the total ``distribution changes''. The regret bounds in both
scenarios are nearly optimal, but our algorithm needs to know the parameter
$\mathcal S$ or $\mathcal V$ in advance.

We further show that by employing another technique, our algorithm no longer
needs to know the parameters $\mathcal S$ or $\mathcal V$ but the regret bounds
could become suboptimal.

In a special case where the reward function is linear and we have an exact
oracle, we design a parameter-free algorithm that achieves nearly optimal
regret both in the switching case and in the dynamic case without knowing the
parameters in advance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kai Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10681</id>
        <link href="http://arxiv.org/abs/2106.10681"/>
        <updated>2021-06-22T01:57:11.798Z</updated>
        <summary type="html"><![CDATA[Visual information extraction (VIE) has attracted increasing attention in
recent years. The existing methods usually first organized optical character
recognition (OCR) results into plain texts and then utilized token-level entity
annotations as supervision to train a sequence tagging model. However, it
expends great annotation costs and may be exposed to label confusion, and the
OCR errors will also significantly affect the final performance. In this paper,
we propose a unified weakly-supervised learning framework called TCPN (Tag,
Copy or Predict Network), which introduces 1) an efficient encoder to
simultaneously model the semantic and layout information in 2D OCR results; 2)
a weakly-supervised training strategy that utilizes only key information
sequences as supervision; and 3) a flexible and switchable decoder which
contains two inference modes: one (Copy or Predict Mode) is to output key
information sequences of different categories by copying a token from the input
or predicting one in each time step, and the other (Tag Mode) is to directly
tag the input sequence in a single forward pass. Our method shows new
state-of-the-art performance on several public benchmarks, which fully proves
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weihong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kai Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yichao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation. (arXiv:2106.10783v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10783</id>
        <link href="http://arxiv.org/abs/2106.10783"/>
        <updated>2021-06-22T01:57:11.793Z</updated>
        <summary type="html"><![CDATA[We consider the offline reinforcement learning (RL) setting where the agent
aims to optimize the policy solely from the data without further environment
interactions. In offline RL, the distributional shift becomes the primary
source of difficulty, which arises from the deviation of the target policy
being optimized from the behavior policy used for data collection. This
typically causes overestimation of action values, which poses severe problems
for model-free algorithms that use bootstrapping. To mitigate the problem,
prior offline RL algorithms often used sophisticated techniques that encourage
underestimation of action values, which introduces an additional set of
hyperparameters that need to be tuned properly. In this paper, we present an
offline RL algorithm that prevents overestimation in a more principled way. Our
algorithm, OptiDICE, directly estimates the stationary distribution corrections
of the optimal policy and does not rely on policy-gradients, unlike previous
offline RL algorithms. Using an extensive set of benchmark datasets for offline
RL, we show that OptiDICE performs competitively with the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jongmin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_W/0/1/0/all/0/1"&gt;Wonseok Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Byung-Jun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kee-Eung Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling. (arXiv:2106.10673v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.10673</id>
        <link href="http://arxiv.org/abs/2106.10673"/>
        <updated>2021-06-22T01:57:11.780Z</updated>
        <summary type="html"><![CDATA[Human personality traits are the key drivers behind our decision-making,
influencing our life path on a daily basis. Inference of personality traits,
such as Myers-Briggs Personality Type, as well as an understanding of
dependencies between personality traits and users' behavior on various social
media platforms is of crucial importance to modern research and industry
applications. The emergence of diverse and cross-purpose social media avenues
makes it possible to perform user personality profiling automatically and
efficiently based on data represented across multiple data modalities. However,
the research efforts on personality profiling from multi-source multi-modal
social media data are relatively sparse, and the level of impact of different
social network data on machine learning performance has yet to be
comprehensively evaluated. Furthermore, there is not such dataset in the
research community to benchmark. This study is one of the first attempts
towards bridging such an important research gap. Specifically, in this work, we
infer the Myers-Briggs Personality Type indicators, by applying a novel
multi-view fusion framework, called "PERS" and comparing the performance
results not just across data modalities but also with respect to different
social network data sources. Our experimental results demonstrate the PERS's
ability to learn from multi-view data for personality profiling by efficiently
leveraging on the significantly different data arriving from diverse social
multimedia sources. We have also found that the selection of a machine learning
approach is of crucial importance when choosing social network data sources and
that people tend to reveal multiple facets of their personality in different
social media avenues. Our released social multimedia dataset facilitates future
research on this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farseev_A/0/1/0/all/0/1"&gt;Aleksandr Farseev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Spectral Marked Point Processes. (arXiv:2106.10773v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10773</id>
        <link href="http://arxiv.org/abs/2106.10773"/>
        <updated>2021-06-22T01:57:11.774Z</updated>
        <summary type="html"><![CDATA[Self- and mutually-exciting point processes are popular models in machine
learning and statistics for dependent discrete event data. To date, most
existing models assume stationary kernels (including the classical Hawkes
processes) and simple parametric models. Modern applications with complex event
data require more general point process models that can incorporate contextual
information of the events, called marks, besides the temporal and location
information. Moreover, such applications often require non-stationary models to
capture more complex spatio-temporal dependence. To tackle these challenges, a
key question is to devise a versatile influence kernel in the point process
model. In this paper, we introduce a novel and general neural network-based
non-stationary influence kernel with high expressiveness for handling complex
discrete events data while providing theoretical performance guarantees. We
demonstrate the superior performance of our proposed method compared with the
state-of-the-art on synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shixiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xiuyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Reach, Swim, Walk and Fly in One Trial: Data-Driven Control with Scarce Data and Side Information. (arXiv:2106.10533v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.10533</id>
        <link href="http://arxiv.org/abs/2106.10533"/>
        <updated>2021-06-22T01:57:11.767Z</updated>
        <summary type="html"><![CDATA[We develop a learning-based control algorithm for unknown dynamical systems
under very severe data limitations. Specifically, the algorithm has access to
streaming data only from a single and ongoing trial. Despite the scarcity of
data, we show -- through a series of examples -- that the algorithm can provide
performance comparable to reinforcement learning algorithms trained over
millions of environment interactions. It accomplishes such performance by
effectively leveraging various forms of side information on the dynamics to
reduce the sample complexity. Such side information typically comes from
elementary laws of physics and qualitative properties of the system. More
precisely, the algorithm approximately solves an optimal control problem
encoding the system's desired behavior. To this end, it constructs and refines
a differential inclusion that contains the unknown vector field of the
dynamics. The differential inclusion, used in an interval Taylor-based method,
enables to over-approximate the set of states the system may reach.
Theoretically, we establish a bound on the suboptimality of the approximate
solution with respect to the case of known dynamics. We show that the longer
the trial or the more side information is available, the tighter the bound.
Empirically, experiments in a high-fidelity F-16 aircraft simulator and
MuJoCo's environments such as the Reacher, Swimmer, and Cheetah illustrate the
algorithm's effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Djeumou_F/0/1/0/all/0/1"&gt;Franck Djeumou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Stein Variational Neural Network Ensembles. (arXiv:2106.10760v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10760</id>
        <link href="http://arxiv.org/abs/2106.10760"/>
        <updated>2021-06-22T01:57:11.762Z</updated>
        <summary type="html"><![CDATA[Ensembles of deep neural networks have achieved great success recently, but
they do not offer a proper Bayesian justification. Moreover, while they allow
for averaging of predictions over several hypotheses, they do not provide any
guarantees for their diversity, leading to redundant solutions in function
space. In contrast, particle-based inference methods, such as Stein variational
gradient descent (SVGD), offer a Bayesian framework, but rely on the choice of
a kernel to measure the similarity between ensemble members. In this work, we
study different SVGD methods operating in the weight space, function space, and
in a hybrid setting. % Defining the kernel directly on the neural network
functions seems promising to overcome the limitations of deep ensembles. %
However, ensuring diversity in function space while maintaining SVGD's
theoretical guarantees is not trivial. % In this work, we provide an overview
over different ensembling and SVGD methods in weight space and function space
and propose new and assess their theoretical and empirical properties on
synthetic and real-world tasks. We compare the SVGD approaches to other
ensembling-based methods in terms of their theoretical properties and assess
their empirical performance on synthetic and real-world tasks. We find that
SVGD using functional and hybrid kernels can overcome the limitations of deep
ensembles. It improves on functional diversity and uncertainty estimation and
approaches the true Bayesian posterior more closely. Moreover, we show that
using stochastic SVGD updates, as opposed to the standard deterministic ones,
can further improve the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAngelo_F/0/1/0/all/0/1"&gt;Francesco D&amp;#x27;Angelo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1"&gt;Florian Wenzel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Perturbation-based Saliency Maps for Explaining Atari Agents. (arXiv:2101.07312v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07312</id>
        <link href="http://arxiv.org/abs/2101.07312"/>
        <updated>2021-06-22T01:57:11.756Z</updated>
        <summary type="html"><![CDATA[Recent years saw a plethora of work on explaining complex intelligent agents.
One example is the development of several algorithms that generate saliency
maps which show how much each pixel attributed to the agents' decision.
However, most evaluations of such saliency maps focus on image classification
tasks. As far as we know, there is no work that thoroughly compares different
saliency maps for Deep Reinforcement Learning agents. This paper compares four
perturbation-based approaches to create saliency maps for Deep Reinforcement
Learning agents trained on four different Atari 2600 games. All four approaches
work by perturbing parts of the input and measuring how much this affects the
agent's output. The approaches are compared using three computational metrics:
dependence on the learned parameters of the agent (sanity checks), faithfulness
to the agent's reasoning (input degradation), and run-time. In particular,
during the sanity checks we find issues with two approaches and propose a
solution to fix one of those issues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Limmer_B/0/1/0/all/0/1"&gt;Benedikt Limmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust M-estimation-based Tensor Ring Completion: a Half-quadratic Minimization Approach. (arXiv:2106.10422v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10422</id>
        <link href="http://arxiv.org/abs/2106.10422"/>
        <updated>2021-06-22T01:57:11.741Z</updated>
        <summary type="html"><![CDATA[Tensor completion is the problem of estimating the missing values of
high-order data from partially observed entries. Among several definitions of
tensor rank, tensor ring rank affords the flexibility and accuracy needed to
model tensors of different orders, which motivated recent efforts on
tensor-ring completion. However, data corruption due to prevailing outliers
poses major challenges to existing algorithms. In this paper, we develop a
robust approach to tensor ring completion that uses an M-estimator as its error
statistic, which can significantly alleviate the effect of outliers. Leveraging
a half-quadratic (HQ) method, we reformulate the problem as one of weighted
tensor completion. We present two HQ-based algorithms based on truncated
singular value decomposition and matrix factorization along with their
convergence and complexity analysis. Extendibility of the proposed approach to
alternative definitions of tensor rank is also discussed. The experimental
results demonstrate the superior performance of the proposed approach over
state-of-the-art robust algorithms for tensor completion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yicong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1"&gt;George K. Atia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Animal ID Problem: Continual Curation. (arXiv:2106.10377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10377</id>
        <link href="http://arxiv.org/abs/2106.10377"/>
        <updated>2021-06-22T01:57:11.736Z</updated>
        <summary type="html"><![CDATA[Hoping to stimulate new research in individual animal identification from
images, we propose to formulate the problem as the human-machine Continual
Curation of images and animal identities. This is an open world recognition
problem, where most new animals enter the system after its algorithms are
initially trained and deployed. Continual Curation, as defined here, requires
(1) an improvement in the effectiveness of current recognition methods, (2) a
pairwise verification algorithm that allows the possibility of no decision, and
(3) an algorithmic decision mechanism that seeks human input to guide the
curation process. Error metrics must evaluate the ability of recognition
algorithms to identify not only animals that have been seen just once or twice
but also recognize new animals not in the database. An important measure of
overall system performance is accuracy as a function of the amount of human
input required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1"&gt;Charles V. Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parham_J/0/1/0/all/0/1"&gt;Jason R. Parham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmberg_J/0/1/0/all/0/1"&gt;Jason Holmberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1"&gt;Tanya Y. Berger-Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Which Parts Determine the Impression of the Font?. (arXiv:2103.14216v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14216</id>
        <link href="http://arxiv.org/abs/2103.14216"/>
        <updated>2021-06-22T01:57:11.731Z</updated>
        <summary type="html"><![CDATA[Various fonts give different impressions, such as legible, rough, and
comic-text.This paper aims to analyze the correlation between the local shapes,
or parts, and the impression of fonts. By focusing on local shapes instead of
the whole letter shape, we can realize letter-shape independent and more
general analysis. The analysis is performed by newly combining SIFT and
DeepSets, to extract an arbitrary number of essential parts from a particular
font and aggregate them to infer the font impressions by nonlinear regression.
Our qualitative and quantitative analyses prove that (1)fonts with similar
parts have similar impressions, (2)many impressions, such as legible and rough,
largely depend on specific parts, (3)several impressions are very irrelevant to
parts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masaya Ueda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo. (arXiv:2106.10558v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10558</id>
        <link href="http://arxiv.org/abs/2106.10558"/>
        <updated>2021-06-22T01:57:11.726Z</updated>
        <summary type="html"><![CDATA[Variational Monte Carlo (VMC) is an approach for computing ground-state
wavefunctions that has recently become more powerful due to the introduction of
neural network-based wavefunction parametrizations. However, efficiently
training neural wavefunctions to converge to an energy minimum remains a
difficult problem. In this work, we analyze optimization and sampling methods
used in VMC and introduce alterations to improve their performance. First,
based on theoretical convergence analysis in a noiseless setting, we motivate a
new optimizer that we call the Rayleigh-Gauss-Newton method, which can improve
upon gradient descent and natural gradient descent to achieve superlinear
convergence. Second, in order to realize this favorable comparison in the
presence of stochastic noise, we analyze the effect of sampling error on VMC
parameter updates and experimentally demonstrate that it can be reduced by the
parallel tempering method. In particular, we demonstrate that RGN can be made
robust to energy spikes that occur when new regions of configuration space
become available to the sampler over the course of optimization. Finally,
putting theory into practice, we apply our enhanced optimization and sampling
methods to the transverse-field Ising and XXZ models on large lattices,
yielding ground-state energy estimates with remarkably high accuracy after just
200-500 parameter updates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Webber_R/0/1/0/all/0/1"&gt;Robert J. Webber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lindsey_M/0/1/0/all/0/1"&gt;Michael Lindsey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffMG: Differentiable Meta Graph Search for Heterogeneous Graph Neural Networks. (arXiv:2010.03250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03250</id>
        <link href="http://arxiv.org/abs/2010.03250"/>
        <updated>2021-06-22T01:57:11.721Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel framework to automatically utilize
task-dependent semantic information which is encoded in heterogeneous
information networks (HINs). Specifically, we search for a meta graph, which
can capture more complex semantic relations than a meta path, to determine how
graph neural networks (GNNs) propagate messages along different types of edges.
We formalize the problem within the framework of neural architecture search
(NAS) and then perform the search in a differentiable manner. We design an
expressive search space in the form of a directed acyclic graph (DAG) to
represent candidate meta graphs for a HIN, and we propose task-dependent type
constraint to filter out those edge types along which message passing has no
effect on the representations of nodes that are related to the downstream task.
The size of the search space we define is huge, so we further propose a novel
and efficient search algorithm to make the total search cost on a par with
training a single GNN once. Compared with existing popular NAS algorithms, our
proposed search algorithm improves the search efficiency. We conduct extensive
experiments on different HINs and downstream tasks to evaluate our method, and
experimental results show that our method can outperform state-of-the-art
heterogeneous GNNs and also improves efficiency compared with those methods
which can implicitly learn meta paths.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuhui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Huan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Learning via Schr\"{o}dinger Bridge. (arXiv:2106.10410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10410</id>
        <link href="http://arxiv.org/abs/2106.10410"/>
        <updated>2021-06-22T01:57:11.705Z</updated>
        <summary type="html"><![CDATA[We propose to learn a generative model via entropy interpolation with a
Schr\"{o}dinger Bridge. The generative learning task can be formulated as
interpolating between a reference distribution and a target distribution based
on the Kullback-Leibler divergence. At the population level, this entropy
interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift
term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by
plugging the drift term estimated by a deep score estimator and a deep density
ratio estimator into the Euler-Maruyama method. Under some mild smoothness
assumptions of the target distribution, we prove the consistency of both the
score estimator and the density ratio estimator, and then establish the
consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical
results guarantee that the distribution learned by our approach converges to
the target distribution. Experimental results on multimodal synthetic data and
benchmark data support our theoretical findings and indicate that the
generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art
GANs, suggesting a new formulation of generative learning. We demonstrate its
usefulness in image interpolation and image inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gefei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Can Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Model Adversarial Training for Deep Compressed Sensing. (arXiv:2106.10696v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10696</id>
        <link href="http://arxiv.org/abs/2106.10696"/>
        <updated>2021-06-22T01:57:11.700Z</updated>
        <summary type="html"><![CDATA[Deep compressed sensing assumes the data has sparse representation in a
latent space, i.e., it is intrinsically of low-dimension. The original data is
assumed to be mapped from a low-dimensional space through a
low-to-high-dimensional generator. In this work, we propound how to design such
a low-to-high dimensional deep learning-based generator suiting for compressed
sensing, while satisfying robustness to universal adversarial perturbations in
the latent domain. We also justify why the noise is considered in the latent
space. The work is also buttressed with theoretical analysis on the robustness
of the trained generator to adversarial perturbations. Experiments on
real-world datasets are provided to substantiate the efficacy of the proposed
\emph{generative model adversarial training for deep compressed sensing.}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Esmaeili_A/0/1/0/all/0/1"&gt;Ashkan Esmaeili&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Generalization in Overparameterized Normalizing Flows. (arXiv:2106.10535v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10535</id>
        <link href="http://arxiv.org/abs/2106.10535"/>
        <updated>2021-06-22T01:57:11.694Z</updated>
        <summary type="html"><![CDATA[In supervised learning, it is known that overparameterized neural networks
with one hidden layer provably and efficiently learn and generalize, when
trained using stochastic gradient descent with sufficiently small learning rate
and suitable initialization. In contrast, the benefit of overparameterization
in unsupervised learning is not well understood. Normalizing flows (NFs)
constitute an important class of models in unsupervised learning for sampling
and density estimation. In this paper, we theoretically and empirically analyze
these models when the underlying neural network is one-hidden-layer
overparameterized network. Our main contributions are two-fold: (1) On the one
hand, we provide theoretical and empirical evidence that for a class of NFs
containing most of the existing NF models, overparametrization hurts training.
(2) On the other hand, we prove that unconstrained NFs, a recently introduced
model, can efficiently learn any reasonable data distribution under minimal
assumptions when the underlying network is overparametrized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_K/0/1/0/all/0/1"&gt;Kulin Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Amit Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Navin Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid approach to detecting symptoms of depression in social media entries. (arXiv:2106.10485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10485</id>
        <link href="http://arxiv.org/abs/2106.10485"/>
        <updated>2021-06-22T01:57:11.689Z</updated>
        <summary type="html"><![CDATA[Sentiment and lexical analyses are widely used to detect depression or
anxiety disorders. It has been documented that there are significant
differences in the language used by a person with emotional disorders in
comparison to a healthy individual. Still, the effectiveness of these lexical
approaches could be improved further because the current analysis focuses on
what the social media entries are about, and not how they are written. In this
study, we focus on aspects in which these short texts are similar to each
other, and how they were created. We present an innovative approach to the
depression screening problem by applying Collgram analysis, which is a known
effective method of obtaining linguistic information from texts. We compare
these results with sentiment analysis based on the BERT architecture. Finally,
we create a hybrid model achieving a diagnostic accuracy of 71%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolk_A/0/1/0/all/0/1"&gt;Agnieszka Wo&amp;#x142;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chlasta_K/0/1/0/all/0/1"&gt;Karol Chlasta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holas_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Holas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trainable Class Prototypes for Few-Shot Learning. (arXiv:2106.10846v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10846</id>
        <link href="http://arxiv.org/abs/2106.10846"/>
        <updated>2021-06-22T01:57:11.683Z</updated>
        <summary type="html"><![CDATA[Metric learning is a widely used method for few shot learning in which the
quality of prototypes plays a key role in the algorithm. In this paper we
propose the trainable prototypes for distance measure instead of the artificial
ones within the meta-training and task-training framework. Also to avoid the
disadvantages that the episodic meta-training brought, we adopt non-episodic
meta-training based on self-supervised learning. Overall we solve the few-shot
tasks in two phases: meta-training a transferable feature extractor via
self-supervised learning and training the prototypes for metric classification.
In addition, the simple attention mechanism is used in both meta-training and
task-training. Our method achieves state-of-the-art performance in a variety of
established few-shot tasks on the standard few-shot visual classification
dataset, with about 20% increase compared to the available unsupervised
few-shot learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Augmented Optimizers for Deep Learning. (arXiv:2106.10708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10708</id>
        <link href="http://arxiv.org/abs/2106.10708"/>
        <updated>2021-06-22T01:57:11.667Z</updated>
        <summary type="html"><![CDATA[Popular approaches for minimizing loss in data-driven learning often involve
an abstraction or an explicit retention of the history of gradients for
efficient parameter updates. The aggregated history of gradients nudges the
parameter updates in the right direction even when the gradients at any given
step are not informative. Although the history of gradients summarized in
meta-parameters or explicitly stored in memory has been shown effective in
theory and practice, the question of whether $all$ or only a subset of the
gradients in the history are sufficient in deciding the parameter updates
remains unanswered. In this paper, we propose a framework of memory-augmented
gradient descent optimizers that retain a limited view of their gradient
history in their internal memory. Such optimizers scale well to large real-life
datasets, and our experiments show that the memory augmented extensions of
standard optimizers enjoy accelerated convergence and improved performance on a
majority of computer vision and language tasks that we considered.
Additionally, we prove that the proposed class of optimizers with fixed-size
memory converge under assumptions of strong convexity, regardless of which
gradients are selected or how they are linearly combined to form the update
step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+McRae_P/0/1/0/all/0/1"&gt;Paul-Aymeric McRae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TD-GEN: Graph Generation With Tree Decomposition. (arXiv:2106.10656v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10656</id>
        <link href="http://arxiv.org/abs/2106.10656"/>
        <updated>2021-06-22T01:57:11.646Z</updated>
        <summary type="html"><![CDATA[We propose TD-GEN, a graph generation framework based on tree decomposition,
and introduce a reduced upper bound on the maximum number of decisions needed
for graph generation. The framework includes a permutation invariant tree
generation model which forms the backbone of graph generation. Tree nodes are
supernodes, each representing a cluster of nodes in the graph. Graph nodes and
edges are incrementally generated inside the clusters by traversing the tree
supernodes, respecting the structure of the tree decomposition, and following
node sharing decisions between the clusters. Finally, we discuss the
shortcomings of standard evaluation criteria based on statistical properties of
the generated graphs as performance measures. We propose to compare the
performance of models based on likelihood. Empirical results on a variety of
standard graph generation datasets demonstrate the superior performance of our
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shirzad_H/0/1/0/all/0/1"&gt;Hamed Shirzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajimirsadeghi_H/0/1/0/all/0/1"&gt;Hossein Hajimirsadeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdi_A/0/1/0/all/0/1"&gt;Amir H. Abdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1"&gt;Greg Mori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Policy Evaluation: Learning Adversarial Environments with Adaptive Importance Sampling. (arXiv:2106.10566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10566</id>
        <link href="http://arxiv.org/abs/2106.10566"/>
        <updated>2021-06-22T01:57:11.641Z</updated>
        <summary type="html"><![CDATA[The evaluation of rare but high-stakes events remains one of the main
difficulties in obtaining reliable policies from intelligent agents, especially
in large or continuous state/action spaces where limited scalability enforces
the use of a prohibitively large number of testing iterations. On the other
hand, a biased or inaccurate policy evaluation in a safety-critical system
could potentially cause unexpected catastrophic failures during deployment. In
this paper, we propose the Accelerated Policy Evaluation (APE) method, which
simultaneously uncovers rare events and estimates the rare event probability in
Markov decision processes. The APE method treats the environment nature as an
adversarial agent and learns towards, through adaptive importance sampling, the
zero-variance sampling distribution for the policy evaluation. Moreover, APE is
scalable to large discrete or continuous spaces by incorporating function
approximators. We investigate the convergence properties of proposed algorithms
under suitable regularity conditions. Our empirical studies show that APE
estimates rare event probability with a smaller variance while only using
orders of magnitude fewer samples compared to baseline methods in both
multi-agent and single-agent environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengdi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Peide Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fengpei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jiacheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xuewei Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1"&gt;Kentaro Oguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhiyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1"&gt;Henry Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Ding Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attack to Fool and Explain Deep Networks. (arXiv:2106.10606v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10606</id>
        <link href="http://arxiv.org/abs/2106.10606"/>
        <updated>2021-06-22T01:57:11.635Z</updated>
        <summary type="html"><![CDATA[Deep visual models are susceptible to adversarial perturbations to inputs.
Although these signals are carefully crafted, they still appear noise-like
patterns to humans. This observation has led to the argument that deep visual
representation is misaligned with human perception. We counter-argue by
providing evidence of human-meaningful patterns in adversarial perturbations.
We first propose an attack that fools a network to confuse a whole category of
objects (source class) with a target label. Our attack also limits the
unintended fooling by samples from non-sources classes, thereby circumscribing
human-defined semantic notions for network fooling. We show that the proposed
attack not only leads to the emergence of regular geometric patterns in the
perturbations, but also reveals insightful information about the decision
boundaries of deep models. Exploring this phenomenon further, we alter the
`adversarial' objective of our attack to use it as a tool to `explain' deep
visual representation. We show that by careful channeling and projection of the
perturbations computed by our method, we can visualize a model's understanding
of human-defined semantic notions. Finally, we exploit the explanability
properties of our perturbations to perform image generation, inpainting and
interactive image manipulation by attacking adversarialy robust
`classifiers'.In all, our major contribution is a novel pragmatic adversarial
attack that is subsequently transformed into a tool to interpret the visual
models. The article also makes secondary contributions in terms of establishing
the utility of our attack beyond the adversarial objective with multiple
interesting applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1"&gt;Muhammad A. A. K. Jalwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyML: Analysis of Xtensa LX6 microprocessor for Neural Network Applications by ESP32 SoC. (arXiv:2106.10652v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10652</id>
        <link href="http://arxiv.org/abs/2106.10652"/>
        <updated>2021-06-22T01:57:11.628Z</updated>
        <summary type="html"><![CDATA[In recent decades, Machine Learning (ML) has become extremely important for
many computing applications. The pervasiveness of ultra-low-power embedded
devices such as ESP32 or ESP32 Cam with tiny Machine Learning (tinyML)
applications will enable the mass proliferation of Artificial Intelligent
powered Embedded IoT Devices. In the last few years, the microcontroller device
(Espressif ESP32) became powerful enough to be used for small/tiny machine
learning (tinyML) tasks. The ease of use of platforms like Arduino IDE,
MicroPython and TensorFlow Lite (TF) with tinyML application make it an
indispensable topic of research for mobile robotics, modern computer science
and electrical engineering. The goal of this paper is to analyze the speed of
the Xtensa dual core 32-bit LX6 microprocessor by running a neural network
application. The different number of inputs (9, 36, 144 and 576) inputted
through the different number of neurons in neural networks with one and two
hidden layers. Xtensa LX6 microprocessor has been analyzed because it comes
inside with Espressif ESP32 and ESP32 Cam which are very easy to use, plug and
play IoT device. In this paper speed of the Xtensa LX6 microprocessor in
feed-forward mode has been analyzed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zim_M/0/1/0/all/0/1"&gt;Md Ziaul Haque Zim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithm Unrolling for Massive Access via Deep Neural Network with Theoretical Guarantee. (arXiv:2106.10426v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.10426</id>
        <link href="http://arxiv.org/abs/2106.10426"/>
        <updated>2021-06-22T01:57:11.614Z</updated>
        <summary type="html"><![CDATA[Massive access is a critical design challenge of Internet of Things (IoT)
networks. In this paper, we consider the grant-free uplink transmission of an
IoT network with a multiple-antenna base station (BS) and a large number of
single-antenna IoT devices. Taking into account the sporadic nature of IoT
devices, we formulate the joint activity detection and channel estimation
(JADCE) problem as a group-sparse matrix estimation problem. This problem can
be solved by applying the existing compressed sensing techniques, which however
either suffer from high computational complexities or lack of algorithm
robustness. To this end, we propose a novel algorithm unrolling framework based
on the deep neural network to simultaneously achieve low computational
complexity and high robustness for solving the JADCE problem. Specifically, we
map the original iterative shrinkage thresholding algorithm (ISTA) into an
unrolled recurrent neural network (RNN), thereby improving the convergence rate
and computational efficiency through end-to-end training. Moreover, the
proposed algorithm unrolling approach inherits the structure and domain
knowledge of the ISTA, thereby maintaining the algorithm robustness, which can
handle non-Gaussian preamble sequence matrix in massive access. With rigorous
theoretical analysis, we further simplify the unrolled network structure by
reducing the redundant training parameters. Furthermore, we prove that the
simplified unrolled deep neural network structures enjoy a linear convergence
rate. Extensive simulations based on various preamble signatures show that the
proposed unrolled networks outperform the existing methods in terms of the
convergence rate, robustness and estimation accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yandong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hayoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuanming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural network interpretability for forecasting of aggregated renewable generatiion. (arXiv:2106.10476v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10476</id>
        <link href="http://arxiv.org/abs/2106.10476"/>
        <updated>2021-06-22T01:57:11.609Z</updated>
        <summary type="html"><![CDATA[With the rapid growth of renewable energy, lots of small photovoltaic (PV)
prosumers emerge. Due to the uncertainty of solar power generation, there is a
need for aggregated prosumers to predict solar power generation and whether
solar power generation will be larger than load. This paper presents two
interpretable neural networks to solve the problem: one binary classification
neural network and one regression neural network. The neural networks are built
using TensorFlow. The global feature importance and local feature contributions
are examined by three gradient-based methods: Integrated Gradients, Expected
Gradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions
might fail by estimating the prediction uncertainty using Bayesian neural
networks. Neural networks, which are interpreted by gradient-based methods and
complemented with uncertainty estimation, provide robust and explainable
forecasting for decision-makers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murzakhanov_I/0/1/0/all/0/1"&gt;Ilgiz Murzakhanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Offline Reinforcement Learning with Residual Generative Modeling. (arXiv:2106.10411v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10411</id>
        <link href="http://arxiv.org/abs/2106.10411"/>
        <updated>2021-06-22T01:57:11.604Z</updated>
        <summary type="html"><![CDATA[Offline reinforcement learning (RL) tries to learn the near-optimal policy
with recorded offline experience without online exploration. Current offline RL
research includes: 1) generative modeling, i.e., approximating a policy using
fixed data; and 2) learning the state-action value function. While most
research focuses on the state-action function part through reducing the
bootstrapping error in value function approximation induced by the distribution
shift of training data, the effects of error propagation in generative modeling
have been neglected. In this paper, we analyze the error in generative
modeling. We propose AQL (action-conditioned Q-learning), a residual generative
model to reduce policy approximation error for offline RL. We show that our
method can learn more accurate policy approximations in different benchmark
datasets. In addition, we show that the proposed offline RL method can learn
more competitive AI agents in complex control tasks under the multiplayer
online battle arena (MOBA) game Honor of Kings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Hua Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1"&gt;Deheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1"&gt;Qiang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhenhui/0/1/0/all/0/1"&gt;Zhenhui&lt;/a&gt; (Jessie)Li</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Particle Filtering without Modifying the Forward Pass. (arXiv:2106.10314v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10314</id>
        <link href="http://arxiv.org/abs/2106.10314"/>
        <updated>2021-06-22T01:57:11.599Z</updated>
        <summary type="html"><![CDATA[In recent years particle filters have being used as components in systems
optimized end-to-end with gradient descent. However, the resampling step in a
particle filter is not differentiable, which biases gradients and interferes
with optimization. To remedy this problem, several differentiable variants of
resampling have been proposed, all of which modify the behavior of the particle
filter in significant and potentially undesirable ways. In this paper, we show
how to obtain unbiased estimators of the gradient of the marginal likelihood by
only modifying messages used in backpropagation, leaving the standard forward
pass of a particle filter unchanged. Our method is simple to implement, has a
low computational overhead, does not introduce additional hyperparameters, and
extends to derivatives of higher orders. We call it stop-gradient resampling,
since it can easily be implemented with automatic differentiation libraries
using the stop-gradient operator instead of explicitly modifying the backward
messages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scibior_A/0/1/0/all/0/1"&gt;Adam &amp;#x15a;cibior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Masrani_V/0/1/0/all/0/1"&gt;Vaden Masrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wood_F/0/1/0/all/0/1"&gt;Frank Wood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Review on Non-Neural Networks Collaborative Filtering Recommendation Systems. (arXiv:2106.10679v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10679</id>
        <link href="http://arxiv.org/abs/2106.10679"/>
        <updated>2021-06-22T01:57:11.565Z</updated>
        <summary type="html"><![CDATA[Over the past two decades, recommender systems have attracted a lot of
interest due to the explosion in the amount of data in online applications. A
particular attention has been paid to collaborative filtering, which is the
most widely used in applications that involve information recommendations.
Collaborative filtering (CF) uses the known preference of a group of users to
make predictions and recommendations about the unknown preferences of other
users (recommendations are made based on the past behavior of users). First
introduced in the 1990s, a wide variety of increasingly successful models have
been proposed. Due to the success of machine learning techniques in many areas,
there has been a growing emphasis on the application of such algorithms in
recommendation systems. In this article, we present an overview of the CF
approaches for recommender systems, their two main categories, and their
evaluation metrics. We focus on the application of classical Machine Learning
algorithms to CF recommender systems by presenting their evolution from their
first use-cases to advanced Machine Learning models. We attempt to provide a
comprehensive and comparative overview of CF systems (with python
implementations) that can serve as a guideline for research and practice in
this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wenga_C/0/1/0/all/0/1"&gt;Carmel Wenga&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fansi_M/0/1/0/all/0/1"&gt;Majirus Fansi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Chabrier_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Chabrier&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mari_J/0/1/0/all/0/1"&gt;Jean-Martial Mari&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gabillon_A/0/1/0/all/0/1"&gt;Alban Gabillon&lt;/a&gt; (1) ((1) University of French Polynesia, (2) NzhinuSoft)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Assessment of Generalization Performance Robustness for Deep Networks via Contrastive Examples. (arXiv:2106.10653v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10653</id>
        <link href="http://arxiv.org/abs/2106.10653"/>
        <updated>2021-06-22T01:57:11.527Z</updated>
        <summary type="html"><![CDATA[Training images with data transformations have been suggested as contrastive
examples to complement the testing set for generalization performance
evaluation of deep neural networks (DNNs). In this work, we propose a practical
framework ContRE (The word "contre" means "against" or "versus" in French.)
that uses Contrastive examples for DNN geneRalization performance Estimation.
Specifically, ContRE follows the assumption in contrastive learning that robust
DNN models with good generalization performance are capable of extracting a
consistent set of features and making consistent predictions from the same
image under varying data transformations. Incorporating with a set of
randomized strategies for well-designed data transformations over the training
set, ContRE adopts classification errors and Fisher ratios on the generated
contrastive examples to assess and analyze the generalization performance of
deep models in complement with a testing set. To show the effectiveness and the
efficiency of ContRE, extensive experiments have been done using various DNN
models on three open source benchmark datasets with thorough ablation studies
and applicability analyses. Our experiment results confirm that (1) behaviors
of deep models on contrastive examples are strongly correlated to what on the
testing set, and (2) ContRE is a robust measure of generalization performance
complementing to the testing set in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xuanyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization. (arXiv:2106.10575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10575</id>
        <link href="http://arxiv.org/abs/2106.10575"/>
        <updated>2021-06-22T01:57:11.509Z</updated>
        <summary type="html"><![CDATA[Gradient-based meta-learning and hyperparameter optimization have seen
significant progress recently, enabling practical end-to-end training of neural
networks together with many hyperparameters. Nevertheless, existing approaches
are relatively expensive as they need to compute second-order derivatives and
store a longer computational graph. This cost prevents scaling them to larger
network architectures. We present EvoGrad, a new approach to meta-learning that
draws upon evolutionary techniques to more efficiently compute hypergradients.
EvoGrad estimates hypergradient with respect to hyperparameters without
calculating second-order gradients, or storing a longer computational graph,
leading to significant improvements in efficiency. We evaluate EvoGrad on two
substantial recent meta-learning applications, namely cross-domain few-shot
learning with feature-wise transformations and noisy label learning with
MetaWeightNet. The results show that EvoGrad significantly improves efficiency
and enables scaling meta-learning to bigger CNN architectures such as from
ResNet18 to ResNet34.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1"&gt;Ondrej Bohdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1"&gt;Timothy Hospedales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-rank Characteristic Tensor Density Estimation Part II: Compression and Latent Density Estimation. (arXiv:2106.10591v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10591</id>
        <link href="http://arxiv.org/abs/2106.10591"/>
        <updated>2021-06-22T01:57:11.504Z</updated>
        <summary type="html"><![CDATA[Learning generative probabilistic models is a core problem in machine
learning, which presents significant challenges due to the curse of
dimensionality. This paper proposes a joint dimensionality reduction and
non-parametric density estimation framework, using a novel estimator that can
explicitly capture the underlying distribution of appropriate reduced-dimension
representations of the input data. The idea is to jointly design a nonlinear
dimensionality reducing auto-encoder to model the training data in terms of a
parsimonious set of latent random variables, and learn a canonical low-rank
tensor model of the joint distribution of the latent variables in the Fourier
domain. The proposed latent density model is non-parametric and universal, as
opposed to the predefined prior that is assumed in variational auto-encoders.
Joint optimization of the auto-encoder and the latent density estimator is
pursued via a formulation which learns both by minimizing a combination of the
negative log-likelihood in the latent domain and the auto-encoder
reconstruction loss. We demonstrate that the proposed model achieves very
promising results on toy, tabular, and image datasets on regression tasks,
sampling, and anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amiridi_M/0/1/0/all/0/1"&gt;Magda Amiridi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kargas_N/0/1/0/all/0/1"&gt;Nikos Kargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sidiropoulos_N/0/1/0/all/0/1"&gt;Nicholas D. Sidiropoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the benefits of maximum likelihood estimation for Regression and Forecasting. (arXiv:2106.10370v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10370</id>
        <link href="http://arxiv.org/abs/2106.10370"/>
        <updated>2021-06-22T01:57:11.499Z</updated>
        <summary type="html"><![CDATA[We advocate for a practical Maximum Likelihood Estimation (MLE) approach for
regression and forecasting, as an alternative to the typical approach of
Empirical Risk Minimization (ERM) for a specific target metric. This approach
is better suited to capture inductive biases such as prior domain knowledge in
datasets, and can output post-hoc estimators at inference time that can
optimize different types of target metrics. We present theoretical results to
demonstrate that our approach is always competitive with any estimator for the
target metric under some general conditions, and in many practical settings
(such as Poisson Regression) can actually be much superior to ERM. We
demonstrate empirically that our method instantiated with a well-designed
general purpose mixture likelihood family can obtain superior performance over
ERM for a variety of tasks across time-series forecasting and regression
datasets with different data distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Das_A/0/1/0/all/0/1"&gt;Abhimanyu Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sen_R/0/1/0/all/0/1"&gt;Rajat Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified View of Algorithms for Path Planning Using Probabilistic Inference on Factor Graphs. (arXiv:2106.10442v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10442</id>
        <link href="http://arxiv.org/abs/2106.10442"/>
        <updated>2021-06-22T01:57:11.494Z</updated>
        <summary type="html"><![CDATA[Even if path planning can be solved using standard techniques from dynamic
programming and control, the problem can also be approached using probabilistic
inference. The algorithms that emerge using the latter framework bear some
appealing characteristics that qualify the probabilistic approach as a powerful
alternative to the more traditional control formulations. The idea of using
estimation on stochastic models to solve control problems is not new and the
inference approach considered here falls under the rubric of Active Inference
(AI) and Control as Inference (CAI). In this work, we look at the specific
recursions that arise from various cost functions that, although they may
appear similar in scope, bear noticeable differences, at least when applied to
typical path planning problems. We start by posing the path planning problem on
a probabilistic factor graph, and show how the various algorithms translate
into specific message composition rules. We then show how this unified
approach, presented both in probability space and in log space, provides a very
general framework that includes the Sum-product, the Max-product, Dynamic
programming and mixed Reward/Entropy criteria-based algorithms. The framework
also expands algorithmic design options for smoother or sharper policy
distributions, including generalized Sum/Max-product algorithm, a Smooth
Dynamic programming algorithm and modified versions of the Reward/Entropy
recursions. We provide a comprehensive table of recursions and a comparison
through simulations, first on a synthetic small grid with a single goal with
obstacles, and then on a grid extrapolated from a real-world scene with
multiple goals and a semantic map.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palmieri_F/0/1/0/all/0/1"&gt;Francesco A.N. Palmieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pattipati_K/0/1/0/all/0/1"&gt;Krishna R. Pattipati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gennaro_G/0/1/0/all/0/1"&gt;Giovanni Di Gennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretti_G/0/1/0/all/0/1"&gt;Giovanni Fioretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verolla_F/0/1/0/all/0/1"&gt;Francesco Verolla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buonanno_A/0/1/0/all/0/1"&gt;Amedeo Buonanno&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signal Processing Based Deep Learning for Blind Symbol Decoding and Modulation Classification. (arXiv:2106.10543v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.10543</id>
        <link href="http://arxiv.org/abs/2106.10543"/>
        <updated>2021-06-22T01:57:11.481Z</updated>
        <summary type="html"><![CDATA[Blindly decoding a signal requires estimating its unknown transmit
parameters, compensating for the wireless channel impairments, and identifying
the modulation type. While deep learning can solve complex problems, digital
signal processing (DSP) is interpretable and can be more computationally
efficient. To combine both, we propose the dual path network (DPN). It consists
of a signal path of DSP operations that recover the signal, and a feature path
of neural networks that estimate the unknown transmit parameters. By
interconnecting the paths over several recovery stages, later stages benefit
from the recovered signals and reuse all the previously extracted features. The
proposed design is demonstrated to provide 5% improvement in modulation
classification compared to alternative designs lacking either feature sharing
or access to recovered signals. The estimation results of DPN along with its
blind decoding performance are shown to outperform a blind signal processing
algorithm for BPSK and QPSK on a simulated dataset. An over-the-air
software-defined-radio capture was used to verify DPN results at high SNRs. DPN
design can process variable length inputs and is shown to outperform relying on
fixed length inputs with prediction averaging on longer signals by up to 15% in
modulation classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hanna_S/0/1/0/all/0/1"&gt;Samer Hanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dick_C/0/1/0/all/0/1"&gt;Chris Dick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cabric_D/0/1/0/all/0/1"&gt;Danijela Cabric&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Max-Min Entropy Framework for Reinforcement Learning. (arXiv:2106.10517v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10517</id>
        <link href="http://arxiv.org/abs/2106.10517"/>
        <updated>2021-06-22T01:57:11.476Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a max-min entropy framework for reinforcement
learning (RL) to overcome the limitation of the maximum entropy RL framework in
model-free sample-based learning. Whereas the maximum entropy RL framework
guides learning for policies to reach states with high entropy in the future,
the proposed max-min entropy framework aims to learn to visit states with low
entropy and maximize the entropy of these low-entropy states to promote
exploration. For general Markov decision processes (MDPs), an efficient
algorithm is constructed under the proposed max-min entropy framework based on
disentanglement of exploration and exploitation. Numerical results show that
the proposed algorithm yields drastic performance improvement over the current
state-of-the-art RL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Seungyul Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1"&gt;Youngchul Sung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10437</id>
        <link href="http://arxiv.org/abs/2106.10437"/>
        <updated>2021-06-22T01:57:11.469Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) is a one-to-many task with multiple possible solutions.
However, previous works were not concerned about this characteristic. For a
one-to-many pipeline, the generator should be able to generate multiple
estimates of the reconstruction, and not be penalized for generating similar
and equally realistic images. To achieve this, we propose adding weighted
pixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable
the generator to generate various images. We modify the strict content loss to
not penalize the stochastic variation in reconstructed images as long as it has
consistent content. Additionally, we observe that there are out-of-focus
regions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We
filter blurry regions in the training data using the method of [10]. Finally,
we modify the discriminator to receive the low-resolution image as a reference
image along with the target image to provide better feedback to the generator.
Using our proposed methods, we were able to improve the performance of ESRGAN
in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16
perceptual extreme SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sieun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Classifier as Mutual Information Evaluator. (arXiv:2106.10471v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10471</id>
        <link href="http://arxiv.org/abs/2106.10471"/>
        <updated>2021-06-22T01:57:11.464Z</updated>
        <summary type="html"><![CDATA[Cross-entropy loss with softmax output is a standard choice to train neural
network classifiers. We give a new view of neural network classifiers with
softmax and cross-entropy as mutual information evaluators. We show that when
the dataset is balanced, training a neural network with cross-entropy maximises
the mutual information between inputs and labels through a variational form of
mutual information. Thereby, we develop a new form of softmax that also
converts a classifier to a mutual information evaluator when the dataset is
imbalanced. Experimental results show that the new form leads to better
classification accuracy, in particular for imbalanced datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Multi-task Learning with Expert Diversity. (arXiv:2106.10595v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10595</id>
        <link href="http://arxiv.org/abs/2106.10595"/>
        <updated>2021-06-22T01:57:11.460Z</updated>
        <summary type="html"><![CDATA[Predicting multiple heterogeneous biological and medical targets is a
challenge for traditional deep learning models. In contrast to single-task
learning, in which a separate model is trained for each target, multi-task
learning (MTL) optimizes a single model to predict multiple related targets
simultaneously. To address this challenge, we propose the Multi-gate
Mixture-of-Experts with Exclusivity (MMoEEx). Our work aims to tackle the
heterogeneous MTL setting, in which the same model optimizes multiple tasks
with different characteristics. Such a scenario can overwhelm current MTL
approaches due to the challenges in balancing shared and task-specific
representations and the need to optimize tasks with competing optimization
paths. Our method makes two key contributions: first, we introduce an approach
to induce more diversity among experts, thus creating representations more
suitable for highly imbalanced and heterogenous MTL learning; second, we adopt
a two-step optimization [6, 11] approach to balancing the tasks at the gradient
level. We validate our method on three MTL benchmark datasets, including
Medical Information Mart for Intensive Care (MIMIC-III) and PubChem BioAssay
(PCBA).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aoki_R/0/1/0/all/0/1"&gt;Raquel Aoki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_F/0/1/0/all/0/1"&gt;Frederick Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_G/0/1/0/all/0/1"&gt;Gabriel L. Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEM: A Stochastic Two-Sided Momentum Algorithm Achieving Near-Optimal Sample and Communication Complexities for Federated Learning. (arXiv:2106.10435v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10435</id>
        <link href="http://arxiv.org/abs/2106.10435"/>
        <updated>2021-06-22T01:57:11.445Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) refers to the paradigm where multiple worker nodes
(WNs) build a joint model by using local data. Despite extensive research, for
a generic non-convex FL problem, it is not clear, how to choose the WNs' and
the server's update directions, the minibatch sizes, and the local update
frequency, so that the WNs use the minimum number of samples and communication
rounds to achieve the desired solution. This work addresses the above question
and considers a class of stochastic algorithms where the WNs perform a few
local updates before communication. We show that when both the WN's and the
server's directions are chosen based on a stochastic momentum estimator, the
algorithm requires $\tilde{\mathcal{O}}(\epsilon^{-3/2})$ samples and
$\tilde{\mathcal{O}}(\epsilon^{-1})$ communication rounds to compute an
$\epsilon$-stationary solution. To the best of our knowledge, this is the first
FL algorithm that achieves such {\it near-optimal} sample and communication
complexities simultaneously. Further, we show that there is a trade-off curve
between local update frequencies and local minibatch sizes, on which the above
sample and communication complexities can be maintained. Finally, we show that
for the classical FedAvg (a.k.a. Local SGD, which is a momentum-less special
case of the STEM), a similar trade-off curve exists, albeit with worse sample
and communication complexities. Our insights on this trade-off provides
guidelines for choosing the four important design elements for FL algorithms,
the update frequency, directions, and minibatch sizes to achieve the best
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1"&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Pranay Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1"&gt;Mingyi Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajawat_K/0/1/0/all/0/1"&gt;Ketan Rajawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varshney_P/0/1/0/all/0/1"&gt;Pramod K. Varshney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10479</id>
        <link href="http://arxiv.org/abs/2106.10479"/>
        <updated>2021-06-22T01:57:11.440Z</updated>
        <summary type="html"><![CDATA[Transferability estimation is an essential problem in transfer learning to
predict how good the performance is when transfer a source model (source task)
to a target task. Recent analytical transferability metrics have been widely
used for source model selection and multi-task learning. Earlier metrics does
not work sufficiently well under the challenging cross-domain cross-task
transfer settings, but recent OTCE score achieves a noteworthy performance
using auxiliary tasks. A simplified version named OT-based NCE score sacrifices
accuracy to be more efficient, but it can be further improved. Consequently, we
propose a practical transferability metric called JC-NCE score to further
improve the cross-domain cross-task transferability estimation performance,
which is more efficient than the OTCE score and more accurate than the OT-based
NCE score. Specifically, we build the joint correspondences between source and
target data via solving an optimal transport problem with considering both the
sample distance and label distance, and then compute the transferability score
as the negative conditional entropy. Extensive validations under the
intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE
score outperforms the OT-based NCE score with about 7% and 12% gains,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shao-Lun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Preferences of Uncertain Humans with Inverse Decision Theory. (arXiv:2106.10394v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10394</id>
        <link href="http://arxiv.org/abs/2106.10394"/>
        <updated>2021-06-22T01:57:11.435Z</updated>
        <summary type="html"><![CDATA[Existing observational approaches for learning human preferences, such as
inverse reinforcement learning, usually make strong assumptions about the
observability of the human's environment. However, in reality, people make many
important decisions under uncertainty. To better understand preference learning
in these cases, we study the setting of inverse decision theory (IDT), a
previously proposed framework where a human is observed making non-sequential
binary decisions under uncertainty. In IDT, the human's preferences are
conveyed through their loss function, which expresses a tradeoff between
different types of mistakes. We give the first statistical analysis of IDT,
providing conditions necessary to identify these preferences and characterizing
the sample complexity -- the number of decisions that must be observed to learn
the tradeoff the human is making to a desired precision. Interestingly, we show
that it is actually easier to identify preferences when the decision problem is
more uncertain. Furthermore, uncertain decision problems allow us to relax the
unrealistic assumption that the human is an optimal decision maker but still
identify their exact preferences; we give sample complexities in this
suboptimal case as well. Our analysis contradicts the intuition that partial
observability should make preference learning more difficult. It also provides
a first step towards understanding and improving preference learning methods
for uncertain and suboptimal humans.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Laidlaw_C/0/1/0/all/0/1"&gt;Cassidy Laidlaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Russell_S/0/1/0/all/0/1"&gt;Stuart Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10642</id>
        <link href="http://arxiv.org/abs/2106.10642"/>
        <updated>2021-06-22T01:57:11.430Z</updated>
        <summary type="html"><![CDATA[Meta-learning (ML) has emerged as a promising direction in learning models
under constrained resource settings like few-shot learning. The popular
approaches for ML either learn a generalizable initial model or a generic
parametric optimizer through episodic training. The former approaches leverage
the knowledge from a batch of tasks to learn an optimal prior. In this work, we
study the importance of a batch for ML. Specifically, we first incorporate a
batch episodic training regimen to improve the learning of the generic
parametric optimizer. We also hypothesize that the common assumption in batch
episodic training that each task in a batch has an equal contribution to
learning an optimal meta-model need not be true. We propose to weight the tasks
in a batch according to their "importance" in improving the meta-model's
learning. To this end, we introduce a training curriculum motivated by
selective focus in humans, called task attended meta-training, to weight the
tasks in a batch. Task attention is a standalone module that can be integrated
with any batch episodic training regimen. The comparisons of the models with
their non-task-attended counterparts on complex datasets like miniImageNet and
tieredImageNet validate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1"&gt;Aroof Aimen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1"&gt;Sahil Sidheekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1"&gt;Narayanan C. Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Score-Based Explanations in Data Management and Machine Learning: An Answer-Set Programming Approach to Counterfactual Analysis. (arXiv:2106.10562v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.10562</id>
        <link href="http://arxiv.org/abs/2106.10562"/>
        <updated>2021-06-22T01:57:11.426Z</updated>
        <summary type="html"><![CDATA[We describe some recent approaches to score-based explanations for query
answers in databases and outcomes from classification models in machine
learning. The focus is on work done by the author and collaborators. Special
emphasis is placed on declarative approaches based on answer-set programming to
the use of counterfactual reasoning for score specification and computation.
Several examples that illustrate the flexibility of these methods are shown.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1"&gt;Leopoldo Bertossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Critical Nodes in Temporal Networks by Dynamic Graph Convolutional Networks. (arXiv:2106.10419v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.10419</id>
        <link href="http://arxiv.org/abs/2106.10419"/>
        <updated>2021-06-22T01:57:11.410Z</updated>
        <summary type="html"><![CDATA[Many real-world systems can be expressed in temporal networks with nodes
playing far different roles in structure and function and edges representing
the relationships between nodes. Identifying critical nodes can help us control
the spread of public opinions or epidemics, predict leading figures in
academia, conduct advertisements for various commodities, and so on. However,
it is rather difficult to identify critical nodes because the network structure
changes over time in temporal networks. In this paper, considering the sequence
topological information of temporal networks, a novel and effective learning
framework based on the combination of special GCNs and RNNs is proposed to
identify nodes with the best spreading ability. The effectiveness of the
approach is evaluated by weighted Susceptible-Infected-Recovered model.
Experimental results on four real-world temporal networks demonstrate that the
proposed method outperforms both traditional and deep learning benchmark
methods in terms of the Kendall $\tau$ coefficient and top $k$ hit rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1"&gt;En-Yu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jun-Lin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hong-Liang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Duan-Bing Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teacher's pet: understanding and mitigating biases in distillation. (arXiv:2106.10494v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10494</id>
        <link href="http://arxiv.org/abs/2106.10494"/>
        <updated>2021-06-22T01:57:11.405Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation is widely used as a means of improving the performance
of a relatively simple student model using the predictions from a complex
teacher model. Several works have shown that distillation significantly boosts
the student's overall performance; however, are these gains uniform across all
data subgroups? In this paper, we show that distillation can harm performance
on certain subgroups, e.g., classes with few associated samples. We trace this
behaviour to errors made by the teacher distribution being transferred to and
amplified by the student model. To mitigate this problem, we present techniques
which soften the teacher influence for subgroups where it is less reliable.
Experiments on several image classification benchmarks show that these
modifications of distillation maintain boost in overall accuracy, while
additionally ensuring improvement in subgroup performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1"&gt;Michal Lukasik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1"&gt;Srinadh Bhojanapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1"&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sanjiv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stability of Graph Convolutional Neural Networks to Stochastic Perturbations. (arXiv:2106.10526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10526</id>
        <link href="http://arxiv.org/abs/2106.10526"/>
        <updated>2021-06-22T01:57:11.396Z</updated>
        <summary type="html"><![CDATA[Graph convolutional neural networks (GCNNs) are nonlinear processing tools to
learn representations from network data. A key property of GCNNs is their
stability to graph perturbations. Current analysis considers deterministic
perturbations but fails to provide relevant insights when topological changes
are random. This paper investigates the stability of GCNNs to stochastic graph
perturbations induced by link losses. In particular, it proves the expected
output difference between the GCNN over random perturbed graphs and the GCNN
over the nominal graph is upper bounded by a factor that is linear in the link
loss probability. We perform the stability analysis in the graph spectral
domain such that the result holds uniformly for any graph. This result also
shows the role of the nonlinearity and the architecture width and depth, and
allows identifying handle to improve the GCNN robustness. Numerical simulations
on source localization and robot swarm control corroborate our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1"&gt;Elvin Isufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Perils of Learning Before Optimizing. (arXiv:2106.10349v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10349</id>
        <link href="http://arxiv.org/abs/2106.10349"/>
        <updated>2021-06-22T01:57:11.391Z</updated>
        <summary type="html"><![CDATA[Formulating real-world optimization problems often begins with making
predictions from historical data (e.g., an optimizer that aims to recommend
fast routes relies upon travel-time predictions). Typically, learning the
prediction model used to generate the optimization problem and solving that
problem are performed in two separate stages. Recent work has showed how such
prediction models can be learned end-to-end by differentiating through the
optimization task. Such methods often yield empirical improvements, which are
typically attributed to end-to-end making better error tradeoffs than the
standard loss function used in a two-stage solution. We refine this explanation
and more precisely characterize when end-to-end can improve performance. When
prediction targets are stochastic, a two-stage solution must make an a priori
choice about which statistics of the target distribution to model -- we
consider expectations over prediction targets -- while an end-to-end solution
can make this choice adaptively. We show that the performance gap between a
two-stage and end-to-end approach is closely related to the \emph{price of
correlation} concept in stochastic optimization and show the implications of
some existing POC results for our predict-then-optimize problem. We then
consider a novel and particularly practical setting, where coefficients in the
objective function depend on multiple prediction targets. We give explicit
constructions where (1) two-stage performs unboundedly worse than end-to-end;
and (2) two-stage is optimal. We identify a large set of real-world
applications whose objective functions rely on multiple prediction targets but
which nevertheless deploy two-stage solutions. We also use simulations to
experimentally quantify performance gaps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cameron_C/0/1/0/all/0/1"&gt;Chris Cameron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartford_J/0/1/0/all/0/1"&gt;Jason Hartford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundy_T/0/1/0/all/0/1"&gt;Taylor Lundy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1"&gt;Kevin Leyton-Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Space Partitions for Path Planning. (arXiv:2106.10544v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.10544</id>
        <link href="http://arxiv.org/abs/2106.10544"/>
        <updated>2021-06-22T01:57:11.385Z</updated>
        <summary type="html"><![CDATA[Path planning, the problem of efficiently discovering high-reward
trajectories, often requires optimizing a high-dimensional and multimodal
reward function. Popular approaches like CEM and CMA-ES greedily focus on
promising regions of the search space and may get trapped in local maxima. DOO
and VOOT balance exploration and exploitation, but use space partitioning
strategies independent of the reward function to be optimized. Recently, LaMCTS
empirically learns to partition the search space in a reward-sensitive manner
for black-box optimization. In this paper, we develop a novel formal regret
analysis for when and why such an adaptive region partitioning scheme works. We
also propose a new path planning method PlaLaM which improves the function
value estimation within each sub-region, and uses a latent representation of
the search space. Empirically, PlaLaM outperforms existing path planning
methods in 2D navigation tasks, especially in the presence of
difficult-to-escape local optima, and shows benefits when plugged into
model-based RL with planning components such as PETS. These gains transfer to
highly multimodal real-world tasks, where we outperform strong baselines in
compiler phase ordering by up to 245% and in molecular design by up to 0.4 on
properties on a 0-1 scale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kevin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1"&gt;Chris Cummins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_B/0/1/0/all/0/1"&gt;Benoit Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Neural Networks for Learning Real-Time Prices in Electricity Market. (arXiv:2106.10529v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10529</id>
        <link href="http://arxiv.org/abs/2106.10529"/>
        <updated>2021-06-22T01:57:11.371Z</updated>
        <summary type="html"><![CDATA[Solving the optimal power flow (OPF) problem in real-time electricity market
improves the efficiency and reliability in the integration of low-carbon energy
resources into the power grids. To address the scalability and adaptivity
issues of existing end-to-end OPF learning solutions, we propose a new graph
neural network (GNN) framework for predicting the electricity market prices
from solving OPFs. The proposed GNN-for-OPF framework innovatively exploits the
locality property of prices and introduces physics-aware regularization, while
attaining reduced model complexity and fast adaptivity to varying grid
topology. Numerical tests have validated the learning efficiency and adaptivity
improvements of our proposed method over existing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shaohui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chengyang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of the facial growth direction with Machine Learning methods. (arXiv:2106.10464v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10464</id>
        <link href="http://arxiv.org/abs/2106.10464"/>
        <updated>2021-06-22T01:57:11.365Z</updated>
        <summary type="html"><![CDATA[First attempts of prediction of the facial growth (FG) direction were made
over half of a century ago. Despite numerous attempts and elapsed time, a
satisfactory method has not been established yet and the problem still poses a
challenge for medical experts. To our knowledge, this paper is the first
Machine Learning approach to the prediction of FG direction. Conducted data
analysis reveals the inherent complexity of the problem and explains the
reasons of difficulty in FG direction prediction based on 2D X-ray images. To
perform growth forecasting, we employ a wide range of algorithms, from logistic
regression, through tree ensembles to neural networks and consider three,
slightly different, problem formulations. The resulting classification accuracy
varies between 71% and 75%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazmierczak_S/0/1/0/all/0/1"&gt;Stanis&amp;#x142;aw Ka&amp;#x17a;mierczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juszka_Z/0/1/0/all/0/1"&gt;Zofia Juszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fudalej_P/0/1/0/all/0/1"&gt;Piotr Fudalej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1"&gt;Jacek Ma&amp;#x144;dziuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Informative Class Activation Maps. (arXiv:2106.10472v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10472</id>
        <link href="http://arxiv.org/abs/2106.10472"/>
        <updated>2021-06-22T01:57:11.338Z</updated>
        <summary type="html"><![CDATA[We study how to evaluate the quantitative information content of a region
within an image for a particular label. To this end, we bridge class activation
maps with information theory. We develop an informative class activation map
(infoCAM). Given a classification task, infoCAM depict how to accumulate
information of partial regions to that of the entire image toward a label.
Thus, we can utilise infoCAM to locate the most informative features for a
label. When applied to an image classification task, infoCAM performs better
than the traditional classification map in the weakly supervised object
localisation task. We achieve state-of-the-art results on Tiny-ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Timestamp-Level Representations for Time Series with Hierarchical Contrastive Loss. (arXiv:2106.10466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10466</id>
        <link href="http://arxiv.org/abs/2106.10466"/>
        <updated>2021-06-22T01:57:11.292Z</updated>
        <summary type="html"><![CDATA[This paper presents TS2Vec, a universal framework for learning
timestamp-level representations of time series. Unlike existing methods, TS2Vec
performs timestamp-wise discrimination, which learns a contextual
representation vector directly for each timestamp. We find that the learned
representations have superior predictive ability. A linear regression trained
on top of the learned representations outperforms previous SOTAs for supervised
time series forecasting. Also, the instance-level representations can be simply
obtained by applying a max pooling layer on top of learned representations of
all timestamps. We conduct extensive experiments on time series classification
tasks to evaluate the quality of instance-level representations. As a result,
TS2Vec achieves significant improvement compared with existing SOTAs of
unsupervised time series representation on 125 UCR datasets and 29 UEA
datasets. The source code is publicly available at
https://github.com/yuezhihan/ts2vec.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zhihan Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Juanyong Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianmeng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Congrui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bixiong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiffLoop: Tuning PID controllers by differentiating through the feedback loop. (arXiv:2106.10516v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.10516</id>
        <link href="http://arxiv.org/abs/2106.10516"/>
        <updated>2021-06-22T01:57:11.287Z</updated>
        <summary type="html"><![CDATA[Since most industrial control applications use PID controllers, PID tuning
and anti-windup measures are significant problems. This paper investigates
tuning the feedback gains of a PID controller via back-calculation and
automatic differentiation tools. In particular, we episodically use a cost
function to generate gradients and perform gradient descent to improve
controller performance. We provide a theoretical framework for analyzing this
non-convex optimization and establish a relationship between back-calculation
and disturbance feedback policies. We include numerical experiments on linear
systems with actuator saturation to show the efficacy of this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1"&gt;Athindran Ramesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSN: Efficient Online Mask Selection Network for Video Instance Segmentation. (arXiv:2106.10452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10452</id>
        <link href="http://arxiv.org/abs/2106.10452"/>
        <updated>2021-06-22T01:57:11.281Z</updated>
        <summary type="html"><![CDATA[In this work we present a novel solution for Video Instance
Segmentation(VIS), that is automatically generating instance level segmentation
masks along with object class and tracking them in a video. Our method improves
the masks from segmentation and propagation branches in an online manner using
the Mask Selection Network (MSN) hence limiting the noise accumulation during
mask tracking. We propose an effective design of MSN by using patch-based
convolutional neural network. The network is able to distinguish between very
subtle differences between the masks and choose the better masks out of the
associated masks accurately. Further, we make use of temporal consistency and
process the video sequences in both forward and reverse manner as a post
processing step to recover lost objects. The proposed method can be used to
adapt any video object segmentation method for the task of VIS. Our method
achieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third
place among more than 30 global teams. Our code will be available at
https://github.com/SHI-Labs/Mask-Selection-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1"&gt;Vidit Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Shubhika Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1"&gt;Harsh Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Compositional Generalization in Classification Tasks via Structure Annotations. (arXiv:2106.10434v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10434</id>
        <link href="http://arxiv.org/abs/2106.10434"/>
        <updated>2021-06-22T01:57:11.266Z</updated>
        <summary type="html"><![CDATA[Compositional generalization is the ability to generalize systematically to a
new data distribution by combining known components. Although humans seem to
have a great ability to generalize compositionally, state-of-the-art neural
models struggle to do so. In this work, we study compositional generalization
in classification tasks and present two main contributions. First, we study
ways to convert a natural language sequence-to-sequence dataset to a
classification dataset that also requires compositional generalization. Second,
we show that providing structural hints (specifically, providing parse trees
and entity links as attention masks for a Transformer model) helps
compositional generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juyong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel frequency function-deep neural network for efficient complex broadband signal approximation. (arXiv:2106.10401v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.10401</id>
        <link href="http://arxiv.org/abs/2106.10401"/>
        <updated>2021-06-22T01:57:11.257Z</updated>
        <summary type="html"><![CDATA[A neural network is essentially a high-dimensional complex mapping model by
adjusting network weights for feature fitting. However, the spectral bias in
network training leads to unbearable training epochs for fitting the
high-frequency components in broadband signals. To improve the fitting
efficiency of high-frequency components, the PhaseDNN was proposed recently by
combining complex frequency band extraction and frequency shift techniques [Cai
et al. SIAM J. SCI. COMPUT. 42, A3285 (2020)]. Our paper is devoted to an
alternative candidate for fitting complex signals with high-frequency
components. Here, a parallel frequency function-deep neural network (PFF-DNN)
is proposed to suppress computational overhead while ensuring fitting accuracy
by utilizing fast Fourier analysis of broadband signals and the spectral bias
nature of neural networks. The effectiveness and efficiency of the proposed
PFF-DNN method are verified based on detailed numerical experiments for six
typical broadband signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1"&gt;Zhi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_P/0/1/0/all/0/1"&gt;Pengpeng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_F/0/1/0/all/0/1"&gt;Fulei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1"&gt;Peihan Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Optimal Transport with Self-paced Ensemble for Cross-hospital Sepsis Early Detection. (arXiv:2106.10352v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10352</id>
        <link href="http://arxiv.org/abs/2106.10352"/>
        <updated>2021-06-22T01:57:11.252Z</updated>
        <summary type="html"><![CDATA[The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Ruiqing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jie Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiqiang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1"&gt;He Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanlin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Leye Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Man Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unconstrained Facial Action Unit Detection via Latent Feature Domain. (arXiv:1903.10143v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.10143</id>
        <link href="http://arxiv.org/abs/1903.10143"/>
        <updated>2021-06-22T01:57:11.214Z</updated>
        <summary type="html"><![CDATA[Facial action unit (AU) detection in the wild is a challenging problem, due
to the unconstrained variability in facial appearances and the lack of accurate
annotations. Most existing methods depend on either impractical labor-intensive
labeling or inaccurate pseudo labels. In this paper, we propose an end-to-end
unconstrained facial AU detection framework based on domain adaptation, which
transfers accurate AU labels from a constrained source domain to an
unconstrained target domain by exploiting labels of AU-related facial
landmarks. Specifically, we map a source image with label and a target image
without label into a latent feature domain by combining source landmark-related
feature with target landmark-free feature. Due to the combination of source
AU-related information and target AU-free information, the latent feature
domain with transferred source label can be learned by maximizing the
target-domain AU detection performance. Moreover, we introduce a novel landmark
adversarial loss to disentangle the landmark-free feature from the
landmark-related feature by treating the adversarial learning as a multi-player
minimax game. Our framework can also be naturally extended for use with
target-domain pseudo AU labels. Extensive experiments show that our method
soundly outperforms lower-bounds and upper-bounds of the basic model, as well
as state-of-the-art approaches on the challenging in-the-wild benchmarks. The
code is available at https://github.com/ZhiwenShao/ADLD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhiwen Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1"&gt;Tat-Jen Cham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xuequan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Trainable Class Prototypes for Few-Shot Learning. (arXiv:2106.10846v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10846</id>
        <link href="http://arxiv.org/abs/2106.10846"/>
        <updated>2021-06-22T01:57:11.209Z</updated>
        <summary type="html"><![CDATA[Metric learning is a widely used method for few shot learning in which the
quality of prototypes plays a key role in the algorithm. In this paper we
propose the trainable prototypes for distance measure instead of the artificial
ones within the meta-training and task-training framework. Also to avoid the
disadvantages that the episodic meta-training brought, we adopt non-episodic
meta-training based on self-supervised learning. Overall we solve the few-shot
tasks in two phases: meta-training a transferable feature extractor via
self-supervised learning and training the prototypes for metric classification.
In addition, the simple attention mechanism is used in both meta-training and
task-training. Our method achieves state-of-the-art performance in a variety of
established few-shot tasks on the standard few-shot visual classification
dataset, with about 20% increase compared to the available unsupervised
few-shot learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04884</id>
        <link href="http://arxiv.org/abs/2101.04884"/>
        <updated>2021-06-22T01:57:11.204Z</updated>
        <summary type="html"><![CDATA[Can a computer determine a piano player's skill level? Is it preferable to
base this assessment on visual analysis of the player's performance or should
we trust our ears over our eyes? Since current CNNs have difficulty processing
long video videos, how can shorter clips be sampled to best reflect the players
skill level? In this work, we collect and release a first-of-its-kind dataset
for multimodal skill assessment focusing on assessing piano player's skill
level, answer the asked questions, initiate work in automated evaluation of
piano playing skills and provide baselines for future work. Dataset is
available from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1"&gt;Paritosh Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1"&gt;Jaiden Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1"&gt;Brendan Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning for User Engagement and Adoption in Live Video Streaming Events. (arXiv:2106.10305v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.10305</id>
        <link href="http://arxiv.org/abs/2106.10305"/>
        <updated>2021-06-22T01:57:11.199Z</updated>
        <summary type="html"><![CDATA[Nowadays, live video streaming events have become a mainstay in viewer's
communication in large international enterprises. Provided that viewers are
distributed worldwide, the main challenge resides on how to schedule the
optimal event's time so as to improve both the viewer's engagement and
adoption. In this paper we present a multi-task deep reinforcement learning
model to select the time of a live video streaming event, aiming to optimize
the viewer's engagement and adoption at the same time. We consider the
engagement and adoption of the viewers as independent tasks and formulate a
unified loss function to learn a common policy. In addition, we account for the
fact that each task might have different contribution to the training strategy
of the agent. Therefore, to determine the contribution of each task to the
agent's training, we design a Transformer's architecture for the state-action
transitions of each task. We evaluate our proposed model on four real-world
datasets, generated by the live video streaming events of four large
enterprises spanning from January 2019 until March 2021. Our experiments
demonstrate the effectiveness of the proposed model when compared with several
state-of-the-art strategies. For reproduction purposes, our evaluation datasets
and implementation are publicly available at
https://github.com/stefanosantaris/merlin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1"&gt;Stefanos Antaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1"&gt;Dimitrios Rafailidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arriaza_R/0/1/0/all/0/1"&gt;Romina Arriaza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixed-Privacy Forgetting in Deep Networks. (arXiv:2012.13431v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13431</id>
        <link href="http://arxiv.org/abs/2012.13431"/>
        <updated>2021-06-22T01:57:11.195Z</updated>
        <summary type="html"><![CDATA[We show that the influence of a subset of the training samples can be removed
-- or "forgotten" -- from the weights of a network trained on large-scale image
classification tasks, and we provide strong computable bounds on the amount of
remaining information after forgetting. Inspired by real-world applications of
forgetting techniques, we introduce a novel notion of forgetting in
mixed-privacy setting, where we know that a "core" subset of the training
samples does not need to be forgotten. While this variation of the problem is
conceptually simple, we show that working in this setting significantly
improves the accuracy and guarantees of forgetting methods applied to vision
classification tasks. Moreover, our method allows efficient removal of all
information contained in non-core data by simply setting to zero a subset of
the weights with minimal loss in performance. We achieve these results by
replacing a standard deep network with a suitable linear approximation. With
opportune changes to the network architecture and training procedure, we show
that such linear approximation achieves comparable performance to the original
network and that the forgetting problem becomes quadratic and can be solved
efficiently even for large models. Unlike previous forgetting methods on deep
networks, ours can achieve close to the state-of-the-art accuracy on large
scale vision tasks. In particular, we show that our method allows forgetting
without having to trade off the model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1"&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1"&gt;Alessandro Achille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polito_M/0/1/0/all/0/1"&gt;Marzia Polito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Supervised Method without supervision for foreground segmentation. (arXiv:2011.07954v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07954</id>
        <link href="http://arxiv.org/abs/2011.07954"/>
        <updated>2021-06-22T01:57:11.190Z</updated>
        <summary type="html"><![CDATA[Neural networks are a powerful framework for foreground segmentation in video
acquired by static cameras, segmenting moving objects from the background in a
robust way in various challenging scenarios. The premier methods are those
based on supervision requiring a final training stage on a database of tens to
hundreds of manually segmented images from the specific static camera. In this
work, we propose a method to automatically create an "artificial" database that
is sufficient for training the supervised methods so that it performs better
than current unsupervised methods. It is based on combining a weak foreground
segmenter, compared to the supervised method, to extract suitable objects from
the training images and randomly inserting these objects back into a background
image. Test results are shown on the test sequences in CDnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassel_L/0/1/0/all/0/1"&gt;Levi Kassel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1"&gt;Michael Werman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nuclei Grading of Clear Cell Renal Cell Carcinoma in Histopathological Image by Composite High-Resolution Network. (arXiv:2106.10641v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10641</id>
        <link href="http://arxiv.org/abs/2106.10641"/>
        <updated>2021-06-22T01:57:11.185Z</updated>
        <summary type="html"><![CDATA[The grade of clear cell renal cell carcinoma (ccRCC) is a critical prognostic
factor, making ccRCC nuclei grading a crucial task in RCC pathology analysis.
Computer-aided nuclei grading aims to improve pathologists' work efficiency
while reducing their misdiagnosis rate by automatically identifying the grades
of tumor nuclei within histopathological images. Such a task requires precisely
segment and accurately classify the nuclei. However, most of the existing
nuclei segmentation and classification methods can not handle the inter-class
similarity property of nuclei grading, thus can not be directly applied to the
ccRCC grading task. In this paper, we propose a Composite High-Resolution
Network for ccRCC nuclei grading. Specifically, we propose a segmentation
network called W-Net that can separate the clustered nuclei. Then, we recast
the fine-grained classification of nuclei to two cross-category classification
tasks, based on two high-resolution feature extractors (HRFEs) which are
proposed for learning these two tasks. The two HRFEs share the same backbone
encoder with W-Net by a composite connection so that meaningful features for
the segmentation task can be inherited for the classification task. Last, a
head-fusion block is applied to generate the predicted label of each nucleus.
Furthermore, we introduce a dataset for ccRCC nuclei grading, containing 1000
image patches with 70945 annotated nuclei. We demonstrate that our proposed
method achieves state-of-the-art performance compared to existing methods on
this large ccRCC grading dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zeyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiangbo Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jialun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunbao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attack to Fool and Explain Deep Networks. (arXiv:2106.10606v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10606</id>
        <link href="http://arxiv.org/abs/2106.10606"/>
        <updated>2021-06-22T01:57:11.171Z</updated>
        <summary type="html"><![CDATA[Deep visual models are susceptible to adversarial perturbations to inputs.
Although these signals are carefully crafted, they still appear noise-like
patterns to humans. This observation has led to the argument that deep visual
representation is misaligned with human perception. We counter-argue by
providing evidence of human-meaningful patterns in adversarial perturbations.
We first propose an attack that fools a network to confuse a whole category of
objects (source class) with a target label. Our attack also limits the
unintended fooling by samples from non-sources classes, thereby circumscribing
human-defined semantic notions for network fooling. We show that the proposed
attack not only leads to the emergence of regular geometric patterns in the
perturbations, but also reveals insightful information about the decision
boundaries of deep models. Exploring this phenomenon further, we alter the
`adversarial' objective of our attack to use it as a tool to `explain' deep
visual representation. We show that by careful channeling and projection of the
perturbations computed by our method, we can visualize a model's understanding
of human-defined semantic notions. Finally, we exploit the explanability
properties of our perturbations to perform image generation, inpainting and
interactive image manipulation by attacking adversarialy robust
`classifiers'.In all, our major contribution is a novel pragmatic adversarial
attack that is subsequently transformed into a tool to interpret the visual
models. The article also makes secondary contributions in terms of establishing
the utility of our attack beyond the adversarial objective with multiple
interesting applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1"&gt;Muhammad A. A. K. Jalwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07719</id>
        <link href="http://arxiv.org/abs/2104.07719"/>
        <updated>2021-06-22T01:57:11.162Z</updated>
        <summary type="html"><![CDATA[Few-shot object detection (FSOD) aims to detect objects using only few
examples. It's critically needed for many practical applications but so far
remains challenging. We propose a meta-learning based few-shot object detection
method by transferring meta-knowledge learned from data-abundant base classes
to data-scarce novel classes. Our method incorporates a coarse-to-fine approach
into the proposal based object detection framework and integrates prototype
based classifiers into both the proposal generation and classification stages.
To improve proposal generation for few-shot novel classes, we propose to learn
a lightweight matching network to measure the similarity between each spatial
position in the query image feature map and spatially-pooled class features,
instead of the traditional object/nonobject classifier, thus generating
category-specific proposals and improving proposal recall for novel classes. To
address the spatial misalignment between generated proposals and few-shot class
examples, we propose a novel attentive feature alignment method, thus improving
the performance of few-shot object detection. Meanwhile we jointly learn a
Faster R-CNN detection head for base classes. Extensive experiments conducted
on multiple FSOD benchmarks show our proposed approach achieves state of the
art results under (incremental) few-shot learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1"&gt;Guangxing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shiyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiawei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yicheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shih-Fu Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge. (arXiv:2012.11696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11696</id>
        <link href="http://arxiv.org/abs/2012.11696"/>
        <updated>2021-06-22T01:57:11.142Z</updated>
        <summary type="html"><![CDATA[Image captioning has recently demonstrated impressive progress largely owing
to the introduction of neural network algorithms trained on curated dataset
like MS-COCO. Often work in this field is motivated by the promise of
deployment of captioning systems in practical applications. However, the
scarcity of data and contexts in many competition datasets renders the utility
of systems trained on these datasets limited as an assistive technology in
real-world settings, such as helping visually impaired people navigate and
accomplish everyday tasks. This gap motivated the introduction of the novel
VizWiz dataset, which consists of images taken by the visually impaired and
captions that have useful, task-oriented information. In an attempt to help the
machine learning computer vision field realize its promise of producing
technologies that have positive social impact, the curators of the VizWiz
dataset host several competitions, including one for image captioning. This
work details the theory and engineering from our winning submission to the 2020
captioning competition. Our work provides a step towards improved assistive
image captioning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dognin_P/0/1/0/all/0/1"&gt;Pierre Dognin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melnyk_I/0/1/0/all/0/1"&gt;Igor Melnyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padhi_I/0/1/0/all/0/1"&gt;Inkit Padhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1"&gt;Mattia Rigotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_J/0/1/0/all/0/1"&gt;Jarret Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1"&gt;Richard A. Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belgodere_B/0/1/0/all/0/1"&gt;Brian Belgodere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Input Gradients Highlight Discriminative Features?. (arXiv:2102.12781v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12781</id>
        <link href="http://arxiv.org/abs/2102.12781"/>
        <updated>2021-06-22T01:57:11.118Z</updated>
        <summary type="html"><![CDATA[Post-hoc gradient-based interpretability methods [Simonyan et al., 2013,
Smilkov et al., 2017] that provide instance-specific explanations of model
predictions are often based on assumption (A): magnitude of input gradients --
gradients of logits with respect to input -- noisily highlight discriminative
task-relevant features. In this work, we test the validity of assumption (A)
using a three-pronged approach. First, we develop an evaluation framework,
DiffROAR, to test assumption (A) on four image classification benchmarks. Our
results suggest that (i) input gradients of standard models (i.e., trained on
original data) may grossly violate (A), whereas (ii) input gradients of
adversarially robust models satisfy (A). Second, we then introduce BlockMNIST,
an MNIST-based semi-real dataset, that by design encodes a priori knowledge of
discriminative features. Our analysis on BlockMNIST leverages this information
to validate as well as characterize differences between input gradient
attributions of standard and robust models. Finally, we theoretically prove
that our empirical findings hold on a simplified version of the BlockMNIST
dataset. Specifically, we prove that input gradients of standard
one-hidden-layer MLPs trained on this dataset do not highlight
instance-specific signal coordinates, thus grossly violating assumption (A).
Our findings motivate the need to formalize and test common assumptions in
interpretability in a falsifiable manner [Leavitt and Morcos, 2020].
Additionally, we believe that the DiffROAR evaluation framework and
BlockMNIST-based datasets can serve as sanity checks to audit instance-specific
interpretability methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_H/0/1/0/all/0/1"&gt;Harshay Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Contrastive Learning for Few-Shot Classification. (arXiv:2012.13831v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13831</id>
        <link href="http://arxiv.org/abs/2012.13831"/>
        <updated>2021-06-22T01:57:11.113Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore contrastive learning for few-shot classification,
in which we propose to use it as an additional auxiliary training objective
acting as a data-dependent regularizer to promote more general and transferable
features. In particular, we present a novel attention-based spatial contrastive
objective to learn locally discriminative and class-agnostic features. As a
result, our approach overcomes some of the limitations of the cross-entropy
loss, such as its excessive discrimination towards seen classes, which reduces
the transferability of features to unseen classes. With extensive experiments,
we show that the proposed method outperforms state-of-the-art approaches,
confirming the importance of learning good and transferable embeddings for
few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouali_Y/0/1/0/all/0/1"&gt;Yassine Ouali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FloorPP-Net: Reconstructing Floor Plans using Point Pillars for Scan-to-BIM. (arXiv:2106.10635v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10635</id>
        <link href="http://arxiv.org/abs/2106.10635"/>
        <updated>2021-06-22T01:57:11.108Z</updated>
        <summary type="html"><![CDATA[This paper presents a deep learning-based point cloud processing method named
FloorPP-Net for the task of Scan-to-BIM (building information model).
FloorPP-Net first converts the input point cloud of a building story into point
pillars (PP), then predicts the corners and edges to output the floor plan.
Altogether, FloorPP-Net establishes an end-to-end supervised learning framework
for the Scan-to-Floor-Plan (Scan2FP) task. In the 1st International Scan-to-BIM
Challenge held in conjunction with CVPR 2021, FloorPP-Net was ranked the second
runner-up in the floor plan reconstruction track. Future work includes general
edge proposals, 2D plan regularization, and 3D BIM reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yijie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fan Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Evaluation Metric: Learning to Evaluate Simulated Radar Point Clouds for Virtual Testing of Autonomous Driving. (arXiv:2104.06772v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06772</id>
        <link href="http://arxiv.org/abs/2104.06772"/>
        <updated>2021-06-22T01:57:11.103Z</updated>
        <summary type="html"><![CDATA[The usage of environment sensor models for virtual testing is a promising
approach to reduce the testing effort of autonomous driving. However, in order
to deduce any statements regarding the performance of an autonomous driving
function based on simulation, the sensor model has to be validated to determine
the discrepancy between the synthetic and real sensor data. Since a certain
degree of divergence can be assumed to exist, the sufficient level of fidelity
must be determined, which poses a major challenge. In particular, a method for
quantifying the fidelity of a sensor model does not exist and the problem of
defining an appropriate metric remains. In this work, we train a neural network
to distinguish real and simulated radar sensor data with the purpose of
learning the latent features of real radar point clouds. Furthermore, we
propose the classifier's confidence score for the `real radar point cloud'
class as a metric to determine the degree of fidelity of synthetically
generated radar data. The presented approach is evaluated and it can be
demonstrated that the proposed deep evaluation metric outperforms conventional
metrics in terms of its capability to identify characteristic differences
between real and simulated radar data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1"&gt;Anthony Ngo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1"&gt;Max Paul Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Resch_M/0/1/0/all/0/1"&gt;Michael Resch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neighborhood Contrastive Learning for Novel Class Discovery. (arXiv:2106.10731v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10731</id>
        <link href="http://arxiv.org/abs/2106.10731"/>
        <updated>2021-06-22T01:57:11.086Z</updated>
        <summary type="html"><![CDATA[In this paper, we address Novel Class Discovery (NCD), the task of unveiling
new classes in a set of unlabeled samples given a labeled dataset with known
classes. We exploit the peculiarities of NCD to build a new framework, named
Neighborhood Contrastive Learning (NCL), to learn discriminative
representations that are important to clustering performance. Our contribution
is twofold. First, we find that a feature extractor trained on the labeled set
generates representations in which a generic query sample and its neighbors are
likely to share the same class. We exploit this observation to retrieve and
aggregate pseudo-positive pairs with contrastive learning, thus encouraging the
model to learn more discriminative representations. Second, we notice that most
of the instances are easily discriminated by the network, contributing less to
the contrastive loss. To overcome this issue, we propose to generate hard
negatives by mixing labeled and unlabeled samples in the feature space. We
experimentally demonstrate that these two ingredients significantly contribute
to clustering performance and lead our model to outperform state-of-the-art
methods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%
on ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Subhankar Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhiming Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object Tracking. (arXiv:2003.07584v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07584</id>
        <link href="http://arxiv.org/abs/2003.07584"/>
        <updated>2021-06-22T01:57:11.081Z</updated>
        <summary type="html"><![CDATA[Recently spiking neural networks (SNNs), the third-generation of neural
networks has shown remarkable capabilities of energy-efficient computing, which
is a promising alternative for deep neural networks (DNNs) with high energy
consumption. SNNs have reached competitive results compared to DNNs in
relatively simple tasks and small datasets such as image classification and
MNIST/CIFAR, while few studies on more challenging vision tasks on complex
datasets. In this paper, we focus on extending deep SNNs to object tracking, a
more advanced vision task with embedded applications and energy-saving
requirements, and present a spike-based Siamese network called SiamSNN.
Specifically, we propose an optimized hybrid similarity estimation method to
exploit temporal information in the SNNs, and introduce a novel two-status
coding scheme to optimize the temporal distribution of output spike trains for
further improvements. SiamSNN is the first deep SNN tracker that achieves short
latency and low precision loss on the visual object tracking benchmarks
OTB2013/2015, VOT2016/2018, and GOT-10k. Moreover, SiamSNN achieves notably low
energy consumption and real-time on Neuromorphic chip TrueNorth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yihao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Min Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1"&gt;Caihong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liangqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Libraries: A Deep Learning Framework Designed from Engineers' Perspectives. (arXiv:2102.06725v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06725</id>
        <link href="http://arxiv.org/abs/2102.06725"/>
        <updated>2021-06-22T01:57:11.076Z</updated>
        <summary type="html"><![CDATA[While there exist a plethora of deep learning tools and frameworks, the
fast-growing complexity of the field brings new demands and challenges, such as
more flexible network design, speedy computation on distributed setting, and
compatibility between different tools. In this paper, we introduce Neural
Network Libraries (https://nnabla.org), a deep learning framework designed from
engineer's perspective, with emphasis on usability and compatibility as its
core design principles. We elaborate on each of our design principles and its
merits, and validate our attempts via experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1"&gt;Takuya Narihira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alonsogarcia_J/0/1/0/all/0/1"&gt;Javier Alonsogarcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardinaux_F/0/1/0/all/0/1"&gt;Fabien Cardinaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayakawa_A/0/1/0/all/0/1"&gt;Akio Hayakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1"&gt;Masato Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwaki_K/0/1/0/all/0/1"&gt;Kazunori Iwaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1"&gt;Thomas Kemp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_Y/0/1/0/all/0/1"&gt;Yoshiyuki Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mauch_L/0/1/0/all/0/1"&gt;Lukas Mauch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakamura_A/0/1/0/all/0/1"&gt;Akira Nakamura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Obuchi_Y/0/1/0/all/0/1"&gt;Yukio Obuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_A/0/1/0/all/0/1"&gt;Andrew Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1"&gt;Kenji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiedmann_S/0/1/0/all/0/1"&gt;Stephen Tiedmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhlich_S/0/1/0/all/0/1"&gt;Stefan Uhlich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yashima_T/0/1/0/all/0/1"&gt;Takuya Yashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshiyama_K/0/1/0/all/0/1"&gt;Kazuki Yoshiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10698</id>
        <link href="http://arxiv.org/abs/2106.10698"/>
        <updated>2021-06-22T01:57:11.071Z</updated>
        <summary type="html"><![CDATA[One of the important and tedious task in agricultural practices is the
detection of the disease on crops. It requires huge time as well as skilled
labor. This paper proposes a smart and efficient technique for detection of
crop disease which uses computer vision and machine learning techniques. The
proposed system is able to detect 20 different diseases of 5 common plants with
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1"&gt;Pranesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1"&gt;Atharva Karwande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1"&gt;Tejas Kolhe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1"&gt;Soham Kamble&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Akshay Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1"&gt;Medha Wyawahare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IsMo-GAN: Adversarial Learning for Monocular Non-Rigid 3D Reconstruction. (arXiv:1904.12144v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.12144</id>
        <link href="http://arxiv.org/abs/1904.12144"/>
        <updated>2021-06-22T01:57:11.064Z</updated>
        <summary type="html"><![CDATA[The majority of the existing methods for non-rigid 3D surface regression from
monocular 2D images require an object template or point tracks over multiple
frames as an input, and are still far from real-time processing rates. In this
work, we present the Isometry-Aware Monocular Generative Adversarial Network
(IsMo-GAN) - an approach for direct 3D reconstruction from a single image,
trained for the deformation model in an adversarial manner on a light-weight
synthetic dataset. IsMo-GAN reconstructs surfaces from real images under
varying illumination, camera poses, textures and shading at over 250 Hz. In
multiple experiments, it consistently outperforms several approaches in the
reconstruction accuracy, runtime, generalisation to unknown surfaces and
robustness to occlusions. In comparison to the state-of-the-art, we reduce the
reconstruction error by 10-30% including the textureless case and our surfaces
evince fewer artefacts qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1"&gt;Soshi Shimada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1"&gt;Vladislav Golyanik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1"&gt;Didier Stricker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A non-alternating graph hashing algorithm for large scale image search. (arXiv:2012.13138v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13138</id>
        <link href="http://arxiv.org/abs/2012.13138"/>
        <updated>2021-06-22T01:57:11.049Z</updated>
        <summary type="html"><![CDATA[In the era of big data, methods for improving memory and computational
efficiency have become crucial for successful deployment of technologies.
Hashing is one of the most effective approaches to deal with computational
limitations that come with big data. One natural way for formulating this
problem is spectral hashing that directly incorporates affinity to learn binary
codes. However, due to binary constraints, the optimization becomes
intractable. To mitigate this challenge, different relaxation approaches have
been proposed to reduce the computational load of obtaining binary codes and
still attain a good solution. The problem with all existing relaxation methods
is resorting to one or more additional auxiliary variables to attain high
quality binary codes while relaxing the problem. The existence of auxiliary
variables leads to coordinate descent approach which increases the
computational complexity. We argue that introducing these variables is
unnecessary. To this end, we propose a novel relaxed formulation for spectral
hashing that adds no additional variables to the problem. Furthermore, instead
of solving the problem in original space where number of variables is equal to
the data points, we solve the problem in a much smaller space and retrieve the
binary codes from this solution. This trick reduces both the memory and
computational complexity at the same time. We apply two optimization
techniques, namely projected gradient and optimization on manifold, to obtain
the solution. Using comprehensive experiments on four public datasets, we show
that the proposed efficient spectral hashing (ESH) algorithm achieves highly
competitive retrieval performance compared with state of the art at low
complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1"&gt;Sobhan Hemati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdizavareh_M/0/1/0/all/0/1"&gt;Mohammad Hadi Mehdizavareh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenouri_S/0/1/0/all/0/1"&gt;Shojaeddin Chenouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;Hamid R Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge, Ridge, and Blob Detection with Symmetric Molecules. (arXiv:1901.09723v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.09723</id>
        <link href="http://arxiv.org/abs/1901.09723"/>
        <updated>2021-06-22T01:57:11.033Z</updated>
        <summary type="html"><![CDATA[We present a novel approach to the detection and characterization of edges,
ridges, and blobs in two-dimensional images which exploits the symmetry
properties of directionally sensitive analyzing functions in multiscale systems
that are constructed in the framework of alpha-molecules. The proposed feature
detectors are inspired by the notion of phase congruency, stable in the
presence of noise, and by definition invariant to changes in contrast. We also
show how the behavior of coefficients corresponding to differently scaled and
oriented analyzing functions can be used to obtain a comprehensive
characterization of the geometry of features in terms of local tangent
directions, widths, and heights. The accuracy and robustness of the proposed
measures are validated and compared to various state-of-the-art algorithms in
extensive numerical experiments in which we consider sets of clean and
distorted synthetic images that are associated with reliable ground truths. To
further demonstrate the applicability, we show how the proposed ridge measure
can be used to detect and characterize blood vessels in digital retinal images
and how the proposed blob measure can be applied to automatically count the
number of cell colonies in a Petri dish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reisenhofer_R/0/1/0/all/0/1"&gt;Rafael Reisenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_E/0/1/0/all/0/1"&gt;Emily J. King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal learning for Audio-Visual Video Parsing. (arXiv:2104.04598v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04598</id>
        <link href="http://arxiv.org/abs/2104.04598"/>
        <updated>2021-06-22T01:57:11.022Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel approach to the audio-visual video parsing
(AVVP) task that demarcates events from a video separately for audio and visual
modalities. The proposed parsing approach simultaneously detects the temporal
boundaries in terms of start and end times of such events. We show how AVVP can
benefit from the following techniques geared towards effective cross-modal
learning: (i) adversarial training and skip connections (ii) global context
aware attention and, (iii) self-supervised pretraining using an audio-video
grounding objective to obtain cross-modal audio-video representations. We
present extensive experimental evaluations on the Look, Listen, and Parse (LLP)
dataset and show that we outperform the state-of-the-art Hybrid Attention
Network (HAN) on all five metrics proposed for AVVP. We also present several
ablations to validate the effect of pretraining, global attention and
adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamba_J/0/1/0/all/0/1"&gt;Jatin Lamba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek/0/1/0/all/0/1"&gt;Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akula_J/0/1/0/all/0/1"&gt;Jayaprakash Akula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabral_R/0/1/0/all/0/1"&gt;Rishabh Dabral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10829</id>
        <link href="http://arxiv.org/abs/2106.10829"/>
        <updated>2021-06-22T01:57:11.016Z</updated>
        <summary type="html"><![CDATA[This technical report presents our solution to the HACS Temporal Action
Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of
weakly-supervised temporal action localization is to temporally locate and
classify action of interest in untrimmed videos given only video-level labels.
We adopt the two-stream consensus network (TSCN) as the main framework in this
challenge. The TSCN consists of a two-stream base model training procedure and
a pseudo ground truth learning procedure. The base model training encourages
the model to predict reliable predictions based on single modality (i.e., RGB
or optical flow), based on the fusion of which a pseudo ground truth is
generated and in turn used as supervision to train the base models. On the HACS
v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our
method achieves 22.20% on the validation set and 21.68% on the testing set in
terms of average mAP. Our solution ranked the 2rd in this challenge, and we
hope our method can serve as a baseline for future academic research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1"&gt;Yuanhao Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1"&gt;David Doermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Modeling of Probabilities for Coding the Octree Representation of Point Clouds. (arXiv:2106.06482v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06482</id>
        <link href="http://arxiv.org/abs/2106.06482"/>
        <updated>2021-06-22T01:57:11.010Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel lossless point cloud compression algorithm that
uses a neural network for estimating the coding probabilities for the occupancy
status of voxels, depending on wide three dimensional contexts around the voxel
to be encoded. The point cloud is represented as an octree, with each
resolution layer being sequentially encoded and decoded using arithmetic
coding, starting from the lowest resolution, until the final resolution is
reached. The occupancy probability of each voxel of the splitting pattern at
each node of the octree is modeled by a neural network, having at its input the
already encoded occupancy status of several octree nodes (belonging to the past
and current resolutions), corresponding to a 3D context surrounding the node to
be encoded. The algorithm has a fast and a slow version, the fast version
selecting differently several voxels of the context, which allows an increased
parallelization by sending larger batches of templates to be estimated by the
neural network, at both encoder and decoder. The proposed algorithms yield
state-of-the-art results on benchmark datasets. The implementation will be made
available at https://github.com/marmus12/nnctx]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1"&gt;Emre Can Kaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1"&gt;Ioan Tabus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image Forensic Technique Based on JPEG Ghosts. (arXiv:2106.06439v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06439</id>
        <link href="http://arxiv.org/abs/2106.06439"/>
        <updated>2021-06-22T01:57:10.994Z</updated>
        <summary type="html"><![CDATA[The unprecedented growth in the easy availability of photo-editing tools has
endangered the power of digital images.An image was supposed to be worth more
than a thousand words,but now this can be said only if it can be authenticated
orthe integrity of the image can be proved to be intact. In thispaper, we
propose a digital image forensic technique for JPEG images. It can detect any
forgery in the image if the forged portion called a ghost image is having a
compression quality different from that of the cover image. It is based on
resaving the JPEG image at different JPEG qualities, and the detection of the
forged portion is maximum when it is saved at the same JPEG quality as the
cover image. Also, we can precisely predictthe JPEG quality of the cover image
by analyzing the similarity using Structural Similarity Index Measure (SSIM) or
the energyof the images. The first maxima in SSIM or the first minima inenergy
correspond to the cover image JPEG quality. We created adataset for varying
JPEG compression qualities of the ghost and the cover images and validated the
scalability of the experimental results.We also, experimented with varied
attack scenarios, e.g. high-quality ghost image embedded in low quality of
cover image,low-quality ghost image embedded in high-quality of cover image,and
ghost image and cover image both at the same quality.The proposed method is
able to localize the tampered portions accurately even for forgeries as small
as 10x10 sized pixel blocks.Our technique is also robust against other attack
scenarios like copy-move forgery, inserting text into image, rescaling
(zoom-out/zoom-in) ghost image and then pasting on cover image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1"&gt;Divakar Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OLIVAW: Mastering Othello with neither Humans nor a Penny. (arXiv:2103.17228v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17228</id>
        <link href="http://arxiv.org/abs/2103.17228"/>
        <updated>2021-06-22T01:57:10.873Z</updated>
        <summary type="html"><![CDATA[We introduce OLIVAW, an AI Othello player adopting the design principles of
the famous AlphaGo series. The main motivation behind OLIVAW was to attain
exceptional competence in a non-trivial board game at a tiny fraction of the
cost of its illustrious predecessors. In this paper, we show how the AlphaGo
Zero's paradigm can be successfully applied to the popular game of Othello
using only commodity hardware and free cloud services. While being simpler than
Chess or Go, Othello maintains a considerable search space and difficulty in
evaluating board positions. To achieve this result, OLIVAW implements some
improvements inspired by recent works to accelerate the standard AlphaGo Zero
learning process. The main modification implies doubling the positions
collected per game during the training phase, by including also positions not
played but largely explored by the agent. We tested the strength of OLIVAW in
three different ways: by pitting it against Edax, the strongest open-source
Othello engine, by playing anonymous games on the web platform OthelloQuest,
and finally in two in-person matches against top-notch human players: a
national champion and a former world champion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1"&gt;Antonio Norelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1"&gt;Alessandro Panconesi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias. (arXiv:2101.07296v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07296</id>
        <link href="http://arxiv.org/abs/2101.07296"/>
        <updated>2021-06-22T01:57:10.867Z</updated>
        <summary type="html"><![CDATA[It is widely accepted that reasoning about object shape is important for
object recognition. However, the most powerful object recognition methods today
do not explicitly make use of object shape during learning. In this work,
motivated by recent developments in low-shot learning, findings in
developmental psychology, and the increased use of synthetic data in computer
vision research, we investigate how reasoning about 3D shape can be used to
improve low-shot learning methods' generalization performance. We propose a new
way to improve existing low-shot learning approaches by learning a
discriminative embedding space using 3D object shape, and using this embedding
by learning how to map images into it. Our new approach improves the
performance of image-only low-shot learning approaches on multiple datasets. We
also introduce Toys4K, a 3D object dataset with the largest number of object
categories currently available, which supports low-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1"&gt;Stefan Stojanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1"&gt;Anh Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Dialog Systems for Negotiation with Personality Modeling. (arXiv:2010.09954v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09954</id>
        <link href="http://arxiv.org/abs/2010.09954"/>
        <updated>2021-06-22T01:57:10.861Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the ability to model and infer personality types of
opponents, predict their responses, and use this information to adapt a dialog
agent's high-level strategy in negotiation tasks. Inspired by the idea of
incorporating a theory of mind (ToM) into machines, we introduce a
probabilistic formulation to encapsulate the opponent's personality type during
both learning and inference. We test our approach on the CraigslistBargain
dataset and show that our method using ToM inference achieves a 20% higher
dialog agreement rate compared to baselines on a mixed population of opponents.
We also find that our model displays diverse negotiation behavior with
different types of opponents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Runzhe Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pilot Study on Visually Stimulated Cognitive Tasks for EEG-Based Dementia Recognition. (arXiv:2103.03854v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03854</id>
        <link href="http://arxiv.org/abs/2103.03854"/>
        <updated>2021-06-22T01:57:10.855Z</updated>
        <summary type="html"><![CDATA[In the status quo, dementia is yet to be cured. Precise diagnosis prior to
the onset of the symptoms can prevent the rapid progression of the emerging
cognitive impairment. Recent progress has shown that Electroencephalography
(EEG) is the promising and cost-effective test to facilitate the detection of
neurocognitive disorders. However, most of the existing works have been using
only resting-state EEG. The efficiencies of EEG signals from various cognitive
tasks, for dementia classification, have yet to be thoroughly investigated. In
this study, we designed four cognitive tasks that engage different cognitive
performances: attention, working memory, and executive function. We
investigated these tasks by using statistical analysis on both time and
frequency domains of EEG signals from three classes of human subjects: Dementia
(DEM), Mild Cognitive Impairment (MCI), and Normal Control (NC). We also
further evaluated the classification performances of two features extraction
methods: Principal Component Analysis (PCA) and Filter Bank Common Spatial
Pattern (FBCSP). We found that the working memory related tasks yielded good
performances for dementia recognition in both cases using PCA and FBCSP.
Moreover, FBCSP with features combination from four tasks revealed the best
sensitivity of 0.87 and the specificity of 0.80. To our best knowledge, this is
the first work that concurrently investigated several cognitive tasks for
dementia recognition using both statistical analysis and classification scores.
Our results give essential information to design and aid in conducting further
experimental tasks to early diagnose dementia patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kongwudhikunakorn_S/0/1/0/all/0/1"&gt;Supavit Kongwudhikunakorn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiatthaveephong_S/0/1/0/all/0/1"&gt;Suktipol Kiatthaveephong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thanontip_K/0/1/0/all/0/1"&gt;Kamonwan Thanontip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leelaarporn_P/0/1/0/all/0/1"&gt;Pitshaporn Leelaarporn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1"&gt;Maytus Piriyajitakonkij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charoenpattarawut_T/0/1/0/all/0/1"&gt;Thananya Charoenpattarawut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Autthasan_P/0/1/0/all/0/1"&gt;Phairot Autthasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaisaen_R/0/1/0/all/0/1"&gt;Rattanaphon Chaisaen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dujada_P/0/1/0/all/0/1"&gt;Pathitta Dujada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudhawiyangkul_T/0/1/0/all/0/1"&gt;Thapanun Sudhawiyangkul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1"&gt;Cuntai Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Senanarong_V/0/1/0/all/0/1"&gt;Vorapun Senanarong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual World: A Robotic Benchmark For Continual Reinforcement Learning. (arXiv:2105.10919v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10919</id>
        <link href="http://arxiv.org/abs/2105.10919"/>
        <updated>2021-06-22T01:57:10.838Z</updated>
        <summary type="html"><![CDATA[Continual learning (CL) -- the ability to continuously learn, building on
previously acquired knowledge -- is a natural requirement for long-lived
autonomous reinforcement learning (RL) agents. While building such agents, one
needs to balance opposing desiderata, such as constraints on capacity and
compute, the ability to not catastrophically forget, and to exhibit positive
transfer on new tasks. Understanding the right trade-off is conceptually and
computationally challenging, which we argue has led the community to overly
focus on catastrophic forgetting. In response to these issues, we advocate for
the need to prioritize forward transfer and propose Continual World, a
benchmark consisting of realistic and meaningfully diverse robotic tasks built
on top of Meta-World as a testbed. Following an in-depth empirical evaluation
of existing CL methods, we pinpoint their limitations and highlight unique
algorithmic challenges in the RL setting. Our benchmark aims to provide a
meaningful and computationally inexpensive challenge for the community and thus
help better understand the performance of existing and future solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolczyk_M/0/1/0/all/0/1"&gt;Maciej Wo&amp;#x142;czyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zajac_M/0/1/0/all/0/1"&gt;Micha&amp;#x142; Zaj&amp;#x105;c&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kucinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kuci&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milos_P/0/1/0/all/0/1"&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised on Graphs: Contrastive, Generative,or Predictive. (arXiv:2105.07342v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07342</id>
        <link href="http://arxiv.org/abs/2105.07342"/>
        <updated>2021-06-22T01:57:10.832Z</updated>
        <summary type="html"><![CDATA[Deep learning on graphs has recently achieved remarkable success on a variety
of tasks while such success relies heavily on the massive and carefully labeled
data. However, precise annotations are generally very expensive and
time-consuming. To address this problem, self-supervised learning (SSL) is
emerging as a new paradigm for extracting informative knowledge through
well-designed pretext tasks without relying on manual labels. In this survey,
we extend the concept of SSL, which first emerged in the fields of computer
vision and natural language processing, to present a timely and comprehensive
review of the existing SSL techniques for graph data. Specifically, we divide
existing graph SSL methods into three categories: contrastive, generative, and
predictive. More importantly, unlike many other surveys that only provide a
high-level description of published research, we present an additional
mathematical summary of the existing works in a unified framework. Furthermore,
to facilitate methodological development and empirical comparisons, we also
summarize the commonly used datasets, evaluation metrics, downstream tasks, and
open-source implementations of various algorithms. Finally, we discuss the
technical challenges and potential future directions for improving graph
self-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Haitao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Cheng Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Stan.Z.Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Amazon SageMaker Automatic Model Tuning: Scalable Gradient-Free Optimization. (arXiv:2012.08489v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08489</id>
        <link href="http://arxiv.org/abs/2012.08489"/>
        <updated>2021-06-22T01:57:10.827Z</updated>
        <summary type="html"><![CDATA[Tuning complex machine learning systems is challenging. Machine learning
typically requires to set hyperparameters, be it regularization, architecture,
or optimization parameters, whose tuning is critical to achieve good predictive
performance. To democratize access to machine learning systems, it is essential
to automate the tuning. This paper presents Amazon SageMaker Automatic Model
Tuning (AMT), a fully managed system for gradient-free optimization at scale.
AMT finds the best version of a trained machine learning model by repeatedly
evaluating it with different hyperparameter configurations. It leverages either
random search or Bayesian optimization to choose the hyperparameter values
resulting in the best model, as measured by the metric chosen by the user. AMT
can be used with built-in algorithms, custom algorithms, and Amazon SageMaker
pre-built containers for machine learning frameworks. We discuss the core
functionality, system architecture, our design principles, and lessons learned.
We also describe more advanced features of AMT, such as automated early
stopping and warm-starting, showing in experiments their benefits to users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_V/0/1/0/all/0/1"&gt;Valerio Perrone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Huibin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zolic_A/0/1/0/all/0/1"&gt;Aida Zolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shcherbatyi_I/0/1/0/all/0/1"&gt;Iaroslav Shcherbatyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Amr Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_T/0/1/0/all/0/1"&gt;Tanya Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1"&gt;Michele Donini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winkelmolen_F/0/1/0/all/0/1"&gt;Fela Winkelmolen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faddoul_J/0/1/0/all/0/1"&gt;Jean Baptiste Faddoul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pogorzelska_B/0/1/0/all/0/1"&gt;Barbara Pogorzelska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miladinovic_M/0/1/0/all/0/1"&gt;Miroslav Miladinovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1"&gt;Krishnaram Kenthapadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeger_M/0/1/0/all/0/1"&gt;Matthias Seeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric Archambeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A cappella: Audio-visual Singing Voice Separation. (arXiv:2104.09946v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09946</id>
        <link href="http://arxiv.org/abs/2104.09946"/>
        <updated>2021-06-22T01:57:10.821Z</updated>
        <summary type="html"><![CDATA[Music source separation can be interpreted as the estimation of the
constituent music sources that a music clip is composed of. In this work, we
explore the single-channel singing voice separation problem from a multimodal
perspective, by jointly learning from audio and visual modalities. To do so, we
present Acappella, a dataset spanning around 46 hours of a cappella solo
singing videos sourced from YouTube. We propose Y-Net, an audio-visual
convolutional neural network which achieves state-of-the-art singing voice
separation results on the Acappella dataset and compare it against its
audio-only counterpart, U-Net, and a state-of-the-art audio-visual speech
separation model. Singing voice separation can be particularly challenging when
the audio mixture also comprises of other accompaniment voices and background
sounds along with the target voice of interest. We demonstrate that our model
can outperform the baseline models in the singing voice separation task in such
challenging scenarios. The code, the pre-trained models and the dataset will be
publicly available at https://ipcv.github.io/Acappella/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1"&gt;Juan F. Montesinos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1"&gt;Venkatesh S. Kadandale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1"&gt;Gloria Haro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks. (arXiv:2103.08143v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08143</id>
        <link href="http://arxiv.org/abs/2103.08143"/>
        <updated>2021-06-22T01:57:10.804Z</updated>
        <summary type="html"><![CDATA[Biological neurons have adaptive nature and perform complex computations
involving the filtering of redundant information. However, most common neural
cell models, including biologically plausible, such as Hodgkin-Huxley or
Izhikevich, do not possess predictive dynamics on a single-cell level.
Moreover, the modern rules of synaptic plasticity or interconnections weights
adaptation also do not provide grounding for the ability of neurons to adapt to
the ever-changing input signal intensity. While natural neuron synaptic growth
is precisely controlled and restricted by protein supply and recycling, weight
correction rules such as widely used STDP are efficiently unlimited in change
rate and scale. The present article introduces new mechanics of interconnection
between neuron firing rate homeostasis and weight change through STDP growth
bounded by abstract protein reserve, controlled by the intracellular
optimization algorithm. We show how these cellular dynamics help neurons filter
out the intense noise signals to help neurons keep a stable firing rate. We
also examine that such filtering does not affect the ability of neurons to
recognize the correlated inputs in unsupervised mode. Such an approach might be
used in the machine learning domain to improve the robustness of AI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1"&gt;Oleg Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1"&gt;Olga Lukyanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1"&gt;Alex Kunin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Efficient Social Navigation Using Inverse Reinforcement Learning. (arXiv:2106.10318v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.10318</id>
        <link href="http://arxiv.org/abs/2106.10318"/>
        <updated>2021-06-22T01:57:10.798Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an algorithm to efficiently learn
socially-compliant navigation policies from observations of human trajectories.
As mobile robots come to inhabit and traffic social spaces, they must account
for social cues and behave in a socially compliant manner. We focus on learning
such cues from examples. We describe an inverse reinforcement learning based
algorithm which learns from human trajectory observations without knowing their
specific actions. We increase the sample-efficiency of our approach over
alternative methods by leveraging the notion of a replay buffer (found in many
off-policy reinforcement learning methods) to eliminate the additional sample
complexity associated with inverse reinforcement learning. We evaluate our
method by training agents using publicly available pedestrian motion data sets
and compare it to related methods. We show that our approach yields better
performance while also decreasing training time and sample complexity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baghi_B/0/1/0/all/0/1"&gt;Bobak H. Baghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1"&gt;Gregory Dudek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-Structured Adversarial Training. (arXiv:2106.10324v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10324</id>
        <link href="http://arxiv.org/abs/2106.10324"/>
        <updated>2021-06-22T01:57:10.792Z</updated>
        <summary type="html"><![CDATA[Robust training methods against perturbations to the input data have received
great attention in the machine learning literature. A standard approach in this
direction is adversarial training which learns a model using
adversarially-perturbed training samples. However, adversarial training
performs suboptimally against perturbations structured across samples such as
universal and group-sparse shifts that are commonly present in biological data
such as gene expression levels of different tissues. In this work, we seek to
close this optimality gap and introduce Group-Structured Adversarial Training
(GSAT) which learns a model robust to perturbations structured across samples.
We formulate GSAT as a non-convex concave minimax optimization problem which
minimizes a group-structured optimal transport cost. Specifically, we focus on
the applications of GSAT for group-sparse and rank-constrained perturbations
modeled using group and nuclear norm penalties. In order to solve GSAT's
non-smooth optimization problem in those cases, we propose a new minimax
optimization algorithm called GDADMM by combining Gradient Descent Ascent (GDA)
and Alternating Direction Method of Multipliers (ADMM). We present several
applications of the GSAT framework to gain robustness against structured
perturbations for image recognition and computational biology datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farnia_F/0/1/0/all/0/1"&gt;Farzan Farnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghazadeh_A/0/1/0/all/0/1"&gt;Amirali Aghazadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1"&gt;David Tse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Universal Template for Few-shot Dataset Generalization. (arXiv:2105.07029v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07029</id>
        <link href="http://arxiv.org/abs/2105.07029"/>
        <updated>2021-06-22T01:57:10.787Z</updated>
        <summary type="html"><![CDATA[Few-shot dataset generalization is a challenging variant of the well-studied
few-shot classification problem where a diverse training set of several
datasets is given, for the purpose of training an adaptable model that can then
learn classes from new datasets using only a few examples. To this end, we
propose to utilize the diverse training set to construct a universal template:
a partial model that can define a wide array of dataset-specialized models, by
plugging in appropriate components. For each new few-shot classification
problem, our approach therefore only requires inferring a small number of
parameters to insert into the universal template. We design a separate network
that produces an initialization of those parameters for each given task, and we
then fine-tune its proposed initialization via a few steps of gradient descent.
Our approach is more parameter-efficient, scalable and adaptable compared to
previous methods, and achieves the state-of-the-art on the challenging
Meta-Dataset benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1"&gt;Eleni Triantafillou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1"&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Distortion for Learned Video Compression. (arXiv:2004.09508v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09508</id>
        <link href="http://arxiv.org/abs/2004.09508"/>
        <updated>2021-06-22T01:57:10.781Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel adversarial lossy video compression model.
At extremely low bit-rates, standard video coding schemes suffer from
unpleasant reconstruction artifacts such as blocking, ringing etc. Existing
learned neural approaches to video compression have achieved reasonable success
on reducing the bit-rate for efficient transmission and reduce the impact of
artifacts to an extent. However, they still tend to produce blurred results
under extreme compression. In this paper, we present a deep adversarial learned
video compression model that minimizes an auxiliary adversarial distortion
objective. We find this adversarial objective to correlate better with human
perceptual quality judgement relative to traditional quality metrics such as
MS-SSIM and PSNR. Our experiments using a state-of-the-art learned video
compression system demonstrate a reduction of perceptual artifacts and
reconstruction of detail lost especially under extremely high compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Veerabadran_V/0/1/0/all/0/1"&gt;Vijay Veerabadran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habibian_A/0/1/0/all/0/1"&gt;Amirhossein Habibian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing. (arXiv:2103.01496v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01496</id>
        <link href="http://arxiv.org/abs/2103.01496"/>
        <updated>2021-06-22T01:57:10.775Z</updated>
        <summary type="html"><![CDATA[Deep learning techniques have achieved remarkable performance in wide-ranging
tasks. However, when trained on privacy-sensitive datasets, the model
parameters may expose private information in training data. Prior attempts for
differentially private training, although offering rigorous privacy guarantees,
lead to much lower model performance than the non-private ones. Besides,
different runs of the same training algorithm produce models with large
performance variance. To address these issues, we propose DPlis--Differentially
Private Learning wIth Smoothing. The core idea of DPlis is to construct a
smooth loss function that favors noise-resilient models lying in large flat
regions of the loss landscape. We provide theoretical justification for the
utility improvements of DPlis. Extensive experiments also demonstrate that
DPlis can effectively boost model quality and training stability under a given
privacy budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lun Wang&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_N/0/1/0/all/0/1"&gt;Nanqing Luo&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1"&gt;Dawn Song&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt; (5) ((1) Tsinghua University, (2) Harvard University, (3) University of California, Berkeley, (4) Huazhong University of Science and Technology, (5) Virginia Tech)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Instabilities of Accelerated Gradient Descent. (arXiv:2102.02167v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02167</id>
        <link href="http://arxiv.org/abs/2102.02167"/>
        <updated>2021-06-22T01:57:10.757Z</updated>
        <summary type="html"><![CDATA[We study the algorithmic stability of Nesterov's accelerated gradient method.
For convex quadratic objectives, Chen et al. (2018) proved that the uniform
stability of the method grows quadratically with the number of optimization
steps, and conjectured that the same is true for the general convex and smooth
case. We disprove this conjecture and show, for two notions of algorithmic
stability (including uniform stability), that the stability of Nesterov's
accelerated method in fact deteriorates exponentially fast with the number of
gradient steps. This stands in sharp contrast to the bounds in the quadratic
case, but also to known results for non-accelerated gradient methods where
stability typically grows linearly with the number of steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attia_A/0/1/0/all/0/1"&gt;Amit Attia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-agnostic Feature Importance and Effects with Dependent Features -- A Conditional Subgroup Approach. (arXiv:2006.04628v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04628</id>
        <link href="http://arxiv.org/abs/2006.04628"/>
        <updated>2021-06-22T01:57:10.752Z</updated>
        <summary type="html"><![CDATA[The interpretation of feature importance in machine learning models is
challenging when features are dependent. Permutation feature importance (PFI)
ignores such dependencies, which can cause misleading interpretations due to
extrapolation. A possible remedy is more advanced conditional PFI approaches
that enable the assessment of feature importance conditional on all other
features. Due to this shift in perspective and in order to enable correct
interpretations, it is therefore important that the conditioning is transparent
and humanly comprehensible. In this paper, we propose a new sampling mechanism
for the conditional distribution based on permutations in conditional
subgroups. As these subgroups are constructed using decision trees
(transformation trees), the conditioning becomes inherently interpretable. This
not only provides a simple and effective estimator of conditional PFI, but also
local PFI estimates within the subgroups. In addition, we apply the conditional
subgroups approach to partial dependence plots (PDP), a popular method for
describing feature effects that can also suffer from extrapolation when
features are dependent and interactions are present in the model. We show that
PFI and PDP based on conditional subgroups often outperform methods such as
conditional PFI based on knockoffs, or accumulated local effect plots.
Furthermore, our approach allows for a more fine-grained interpretation of
feature effects and importance within the conditional subgroups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Molnar_C/0/1/0/all/0/1"&gt;Christoph Molnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Konig_G/0/1/0/all/0/1"&gt;Gunnar K&amp;#xf6;nig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1"&gt;Bernd Bischl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Casalicchio_G/0/1/0/all/0/1"&gt;Giuseppe Casalicchio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDRUNet: Single Image HDR Reconstruction with Denoising and Dequantization. (arXiv:2105.13084v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13084</id>
        <link href="http://arxiv.org/abs/2105.13084"/>
        <updated>2021-06-22T01:57:10.746Z</updated>
        <summary type="html"><![CDATA[Most consumer-grade digital cameras can only capture a limited range of
luminance in real-world scenes due to sensor constraints. Besides, noise and
quantization errors are often introduced in the imaging process. In order to
obtain high dynamic range (HDR) images with excellent visual quality, the most
common solution is to combine multiple images with different exposures.
However, it is not always feasible to obtain multiple images of the same scene
and most HDR reconstruction methods ignore the noise and quantization loss. In
this work, we propose a novel learning-based approach using a spatially dynamic
encoder-decoder network, HDRUNet, to learn an end-to-end mapping for single
image HDR reconstruction with denoising and dequantization. The network
consists of a UNet-style base network to make full use of the hierarchical
multi-scale information, a condition network to perform pattern-specific
modulation and a weighting network for selectively retaining information.
Moreover, we propose a Tanh_L1 loss function to balance the impact of
over-exposed values and well-exposed values on the network learning. Our method
achieves the state-of-the-art performance in quantitative comparisons and
visual quality. The proposed HDRUNet model won the second place in the single
frame track of NITRE2021 High Dynamic Range Challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10404</id>
        <link href="http://arxiv.org/abs/2106.10404"/>
        <updated>2021-06-22T01:57:10.741Z</updated>
        <summary type="html"><![CDATA[Works on lottery ticket hypothesis (LTH) and single-shot network pruning
(SNIP) have raised a lot of attention currently on post-training pruning
(iterative magnitude pruning), and before-training pruning (pruning at
initialization). The former method suffers from an extremely large computation
cost and the latter category of methods usually struggles with insufficient
performance. In comparison, during-training pruning, a class of pruning methods
that simultaneously enjoys the training/inference efficiency and the comparable
performance, temporarily, has been less explored. To better understand
during-training pruning, we quantitatively study the effect of pruning
throughout training from the perspective of pruning plasticity (the ability of
the pruned networks to recover the original performance). Pruning plasticity
can help explain several other empirical observations about neural network
pruning in literature. We further find that pruning plasticity can be
substantially improved by injecting a brain-inspired mechanism called
neuroregeneration, i.e., to regenerate the same number of connections as
pruned. Based on the insights from pruning plasticity, we design a novel
gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost
neuroregeneration (GraNet), and its dynamic sparse training (DST) variant
(GraNet-ST). Both of them advance state of the art. Perhaps most impressively,
the latter for the first time boosts the sparse-to-sparse training performance
over various dense-to-sparse methods by a large margin with ResNet-50 on
ImageNet. We will release all codes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1"&gt;Zahra Atashgahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1"&gt;Huanyu Kou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-22T01:57:10.735Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exoskeleton-Based Multimodal Action and Movement Recognition: Identifying and Developing the Optimal Boosted Learning Approach. (arXiv:2106.10331v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.10331</id>
        <link href="http://arxiv.org/abs/2106.10331"/>
        <updated>2021-06-22T01:57:10.720Z</updated>
        <summary type="html"><![CDATA[This paper makes two scientific contributions to the field of
exoskeleton-based action and movement recognition. First, it presents a novel
machine learning and pattern recognition-based framework that can detect a wide
range of actions and movements - walking, walking upstairs, walking downstairs,
sitting, standing, lying, stand to sit, sit to stand, sit to lie, lie to sit,
stand to lie, and lie to stand, with an overall accuracy of 82.63%. Second, it
presents a comprehensive comparative study of different learning approaches -
Random Forest, Artificial Neural Network, Decision Tree, Multiway Decision
Tree, Support Vector Machine, k-NN, Gradient Boosted Trees, Decision Stump,
Auto MLP, Linear Regression, Vector Linear Regression, Random Tree, Na\"ive
Bayes, Na\"ive Bayes (Kernel), Linear Discriminant Analysis, Quadratic
Discriminant Analysis, and Deep Learning applied to this framework. The
performance of each of these learning approaches was boosted by using the
AdaBoost algorithm, and the Cross Validation approach was used for training and
testing. The results show that in boosted form, the k- NN classifier
outperforms all the other boosted learning approaches and is, therefore, the
optimal learning method for this purpose. The results presented and discussed
uphold the importance of this work to contribute towards augmenting the
abilities of exoskeleton-based assisted and independent living of the elderly
in the future of Internet of Things-based living environments, such as Smart
Homes. As a specific use case, we also discuss how the findings of our work are
relevant for augmenting the capabilities of the Hybrid Assistive Limb
exoskeleton, a highly functional lower limb exoskeleton.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1"&gt;Nirmalya Thakur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chia Y. Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Whole MILC: generalizing learned dynamics across tasks, datasets, and populations. (arXiv:2007.16041v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.16041</id>
        <link href="http://arxiv.org/abs/2007.16041"/>
        <updated>2021-06-22T01:57:10.715Z</updated>
        <summary type="html"><![CDATA[Behavioral changes are the earliest signs of a mental disorder, but arguably,
the dynamics of brain function gets affected even earlier. Subsequently,
spatio-temporal structure of disorder-specific dynamics is crucial for early
diagnosis and understanding the disorder mechanism. A common way of learning
discriminatory features relies on training a classifier and evaluating feature
importance. Classical classifiers, based on handcrafted features are quite
powerful, but suffer the curse of dimensionality when applied to large input
dimensions of spatio-temporal data. Deep learning algorithms could handle the
problem and a model introspection could highlight discriminatory
spatio-temporal regions but need way more samples to train. In this paper we
present a novel self supervised training schema which reinforces whole sequence
mutual information local to context (whole MILC). We pre-train the whole MILC
model on unlabeled and unrelated healthy control data. We test our model on
three different disorders (i) Schizophrenia (ii) Autism and (iii) Alzheimers
and four different studies. Our algorithm outperforms existing self-supervised
pre-training methods and provides competitive classification results to
classical machine learning algorithms. Importantly, whole MILC enables
attribution of subject diagnosis to specific spatio-temporal regions in the
fMRI signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_U/0/1/0/all/0/1"&gt;Usman Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mahfuzur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fedorov_A/0/1/0/all/0/1"&gt;Alex Fedorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_N/0/1/0/all/0/1"&gt;Noah Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1"&gt;Zening Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1"&gt;Vince D. Calhoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1"&gt;Sergey M. Plis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time series forecasting with Gaussian Processes needs priors. (arXiv:2009.08102v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08102</id>
        <link href="http://arxiv.org/abs/2009.08102"/>
        <updated>2021-06-22T01:57:10.710Z</updated>
        <summary type="html"><![CDATA[Automatic forecasting is the task of receiving a time series and returning a
forecast for the next time steps without any human intervention. Gaussian
Processes (GPs) are a powerful tool for modeling time series, but so far there
are no competitive approaches for automatic forecasting based on GPs. We
propose practical solutions to two problems: automatic selection of the optimal
kernel and reliable estimation of the hyperparameters. We propose a fixed
composition of kernels, which contains the components needed to model most time
series: linear trend, periodic patterns, and other flexible kernel for modeling
the non-linear trend. Not all components are necessary to model each time
series; during training the unnecessary components are automatically made
irrelevant via automatic relevance determination (ARD). We moreover assign
priors to the hyperparameters, in order to keep the inference within a
plausible range; we design such priors through an empirical Bayes approach. We
present results on many time series of different types; our GP model is more
accurate than state-of-the-art time series models. Thanks to the priors, a
single restart is enough the estimate the hyperparameters; hence the model is
also fast to train.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1"&gt;Giorgio Corani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Benavoli_A/0/1/0/all/0/1"&gt;Alessio Benavoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zaffalon_M/0/1/0/all/0/1"&gt;Marco Zaffalon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Exploration for Robotic Reinforcement Learning. (arXiv:2005.05719v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.05719</id>
        <link href="http://arxiv.org/abs/2005.05719"/>
        <updated>2021-06-22T01:57:10.704Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) enables robots to learn skills from interactions
with the real world. In practice, the unstructured step-based exploration used
in Deep RL -- often very successful in simulation -- leads to jerky motion
patterns on real robots. Consequences of the resulting shaky behavior are poor
exploration, or even damage to the robot. We address these issues by adapting
state-dependent exploration (SDE) to current Deep RL algorithms. To enable this
adaptation, we propose two extensions to the original SDE, using more general
features and re-sampling the noise periodically, which leads to a new
exploration method generalized state-dependent exploration (gSDE). We evaluate
gSDE both in simulation, on PyBullet continuous control tasks, and directly on
three different real robots: a tendon-driven elastic robot, a quadruped and an
RC car. The noise sampling interval of gSDE permits to have a compromise
between performance and smoothness, which allows training directly on the real
robots without loss of performance. The code is available at
https://github.com/DLR-RM/stable-baselines3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raffin_A/0/1/0/all/0/1"&gt;Antonin Raffin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kober_J/0/1/0/all/0/1"&gt;Jens Kober&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stulp_F/0/1/0/all/0/1"&gt;Freek Stulp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring breathing induced oesophageal motion and its dosimetric impact. (arXiv:2010.09391v3 [physics.med-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09391</id>
        <link href="http://arxiv.org/abs/2010.09391"/>
        <updated>2021-06-22T01:57:10.692Z</updated>
        <summary type="html"><![CDATA[Stereotactic body radiation therapy allows for a precise and accurate dose
delivery. Organ motion during treatment bears the risk of undetected high dose
healthy tissue exposure. An organ very susceptible to high dose is the
oesophagus. Its low contrast on CT and the oblong shape renders motion
estimation difficult. We tackle this issue by modern algorithms to measure the
oesophageal motion voxel-wise and to estimate motion related dosimetric impact.
Oesophageal motion was measured using deformable image registration and 4DCT of
11 internal and 5 public datasets. Current clinical practice of contouring the
organ on 3DCT was compared to timely resolved 4DCT contours. The dosimetric
impact of the motion was estimated by analysing the trajectory of each voxel in
the 4D dose distribution. Finally an organ motion model was built, allowing for
easier patient-wise comparisons. Motion analysis showed mean absolute maximal
motion amplitudes of 4.55 +/- 1.81 mm left-right, 5.29 +/- 2.67 mm
anterior-posterior and 10.78 +/- 5.30 mm superior-inferior. Motion between the
cohorts differed significantly. In around 50 % of the cases the dosimetric
passing criteria was violated. Contours created on 3DCT did not cover 14 % of
the organ for 50 % of the respiratory cycle and the 3D contour is around 38 %
smaller than the union of all 4D contours. The motion model revealed that the
maximal motion is not limited to the lower part of the organ. Our results
showed motion amplitudes higher than most reported values in the literature and
that motion is very heterogeneous across patients. Therefore, individual motion
information should be considered in contouring and planning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Fechter_T/0/1/0/all/0/1"&gt;Tobias Fechter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Adebahr_S/0/1/0/all/0/1"&gt;Sonja Adebahr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Grosu_A/0/1/0/all/0/1"&gt;Anca-Ligia Grosu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Baltas_D/0/1/0/all/0/1"&gt;Dimos Baltas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scenic4RL: Programmatic Modeling and Generation of Reinforcement Learning Environments. (arXiv:2106.10365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10365</id>
        <link href="http://arxiv.org/abs/2106.10365"/>
        <updated>2021-06-22T01:57:10.677Z</updated>
        <summary type="html"><![CDATA[The capability of reinforcement learning (RL) agent directly depends on the
diversity of learning scenarios the environment generates and how closely it
captures real-world situations. However, existing environments/simulators lack
the support to systematically model distributions over initial states and
transition dynamics. Furthermore, in complex domains such as soccer, the space
of possible scenarios is infinite, which makes it impossible for one research
group to provide a comprehensive set of scenarios to train, test, and benchmark
RL algorithms. To address this issue, for the first time, we adopt an existing
formal scenario specification language, SCENIC, to intuitively model and
generate interactive scenarios. We interfaced SCENIC to Google Research Soccer
environment to create a platform called SCENIC4RL. Using this platform, we
provide a dataset consisting of 36 scenario programs encoded in SCENIC and
demonstration data generated from a subset of them. We share our experimental
results to show the effectiveness of our dataset and the platform to train,
test, and benchmark RL algorithms. More importantly, we open-source our
platform to enable RL community to collectively contribute to constructing a
comprehensive set of scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azad_A/0/1/0/all/0/1"&gt;Abdus Salam Azad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1"&gt;Edward Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qiancheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seshia_S/0/1/0/all/0/1"&gt;Sanjit A. Seshia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised learning for crop/weed classification based on color and texture features. (arXiv:2106.10581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10581</id>
        <link href="http://arxiv.org/abs/2106.10581"/>
        <updated>2021-06-22T01:57:10.659Z</updated>
        <summary type="html"><![CDATA[Computer vision techniques have attracted a great interest in precision
agriculture, recently. The common goal of all computer vision-based precision
agriculture tasks is to detect the objects of interest (e.g., crop, weed) and
discriminating them from the background. The Weeds are unwanted plants growing
among crops competing for nutrients, water, and sunlight, causing losses to
crop yields. Weed detection and mapping is critical for site-specific weed
management to reduce the cost of labor and impact of herbicides. This paper
investigates the use of color and texture features for discrimination of
Soybean crops and weeds. Feature extraction methods including two color spaces
(RGB, HSV), gray level Co-occurrence matrix (GLCM), and Local Binary Pattern
(LBP) are used to train the Support Vector Machine (SVM) classifier. The
experiment was carried out on image dataset of soybean crop, obtained from an
unmanned aerial vehicle (UAV), which is publicly available. The results from
the experiment showed that the highest accuracy (above 96%) was obtained from
the combination of color and LBP features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mekhalfa_F/0/1/0/all/0/1"&gt;Faiza Mekhalfa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yacef_F/0/1/0/all/0/1"&gt;Fouad Yacef&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards a Query-Optimal and Time-Efficient Algorithm for Clustering with a Faulty Oracle. (arXiv:2106.10374v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10374</id>
        <link href="http://arxiv.org/abs/2106.10374"/>
        <updated>2021-06-22T01:57:10.651Z</updated>
        <summary type="html"><![CDATA[Motivated by applications in crowdsourced entity resolution in database,
signed edge prediction in social networks and correlation clustering, Mazumdar
and Saha [NIPS 2017] proposed an elegant theoretical model for studying
clustering with a faulty oracle. In this model, given a set of $n$ items which
belong to $k$ unknown groups (or clusters), our goal is to recover the clusters
by asking pairwise queries to an oracle. This oracle can answer the query that
``do items $u$ and $v$ belong to the same cluster?''. However, the answer to
each pairwise query errs with probability $\varepsilon$, for some
$\varepsilon\in(0,\frac12)$. Mazumdar and Saha provided two algorithms under
this model: one algorithm is query-optimal while time-inefficient (i.e.,
running in quasi-polynomial time), the other is time efficient (i.e., in
polynomial time) while query-suboptimal. Larsen, Mitzenmacher and Tsourakakis
[WWW 2020] then gave a new time-efficient algorithm for the special case of $2$
clusters, which is query-optimal if the bias $\delta:=1-2\varepsilon$ of the
model is large. It was left as an open question whether one can obtain a
query-optimal, time-efficient algorithm for the general case of $k$ clusters
and other regimes of $\delta$.

In this paper, we make progress on the above question and provide a
time-efficient algorithm with nearly-optimal query complexity (up to a factor
of $O(\log^2 n)$) for all constant $k$ and any $\delta$ in the regime when
information-theoretic recovery is possible. Our algorithm is built on a
connection to the stochastic block model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1"&gt;Pan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiapeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10811</id>
        <link href="http://arxiv.org/abs/2106.10811"/>
        <updated>2021-06-22T01:57:10.642Z</updated>
        <summary type="html"><![CDATA[Neural shape representations have recently shown to be effective in shape
analysis and reconstruction tasks. Existing neural network methods require
point coordinates and corresponding normal vectors to learn the implicit level
sets of the shape. Normal vectors are often not provided as raw data,
therefore, approximation and reorientation are required as pre-processing
stages, both of which can introduce noise. In this paper, we propose a
divergence guided shape representation learning approach that does not require
normal vectors as input. We show that incorporating a soft constraint on the
divergence of the distance function favours smooth solutions that reliably
orients gradients to match the unknown normal at each point, in some cases even
better than approaches that use ground truth normal vectors directly.
Additionally, we introduce a novel geometric initialization method for
sinusoidal shape representation networks that further improves convergence to
the desired solution. We evaluate the effectiveness of our approach on the task
of surface reconstruction and show state-of-the-art performance compared to
other unoriented methods and on-par performance compared to oriented methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1"&gt;Yizhak Ben-Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koneputugodage_C/0/1/0/all/0/1"&gt;Chamin Hewa Koneputugodage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Liquid Sensing Using WiFi Signals. (arXiv:2106.10356v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.10356</id>
        <link href="http://arxiv.org/abs/2106.10356"/>
        <updated>2021-06-22T01:57:10.626Z</updated>
        <summary type="html"><![CDATA[The popularity of Internet-of-Things (IoT) has provided us with unprecedented
opportunities to enable a variety of emerging services in a smart home
environment. Among those services, sensing the liquid level in a container is
critical to building many smart home and mobile healthcare applications that
improve the quality of life. This paper presents LiquidSense, a liquid-level
sensing system that is low-cost, high accuracy, widely applicable to different
daily liquids and containers, and can be easily integrated with existing smart
home networks. LiquidSense uses an existing home WiFi network and a low-cost
transducer that attached to the container to sense the resonance of the
container for liquid level detection. In particular, our system mounts a
low-cost transducer on the surface of the container and emits a well-designed
chirp signal to make the container resonant, which introduces subtle changes to
the home WiFi signals. By analyzing the subtle phase changes of the WiFi
signals, LiquidSense extracts the resonance frequency as a feature for liquid
level detection. Our system constructs prediction models for both continuous
and discrete predictions using curve fitting and SVM respectively. We evaluate
LiquidSense in home environments with containers of three different materials
and six types of liquids. Results show that LiquidSense achieves an overall
accuracy of 97% for continuous prediction and an overall F-score of 0.968 for
discrete prediction. Results also show that our system has a large coverage in
a home environment and works well under non-line-of-sight (NLOS) scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yili Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction-Free, Real-Time Flexible Control of Tidal Lagoons through Proximal Policy Optimisation: A Case Study for the Swansea Lagoon. (arXiv:2106.10360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10360</id>
        <link href="http://arxiv.org/abs/2106.10360"/>
        <updated>2021-06-22T01:57:10.620Z</updated>
        <summary type="html"><![CDATA[Tidal range structures have been considered for large scale electricity
generation for their potential ability to produce reasonable predictable energy
without the emission of greenhouse gases. Once the main forcing components for
driving the tides have deterministic dynamics, the available energy in a given
tidal power plant has been estimated, through analytical and numerical
optimisation routines, as a mostly predictable event. This constraint imposes
state-of-art flexible operation methods to rely on tidal predictions
(concurrent with measured data and up to a multiple of half-tidal cycles into
the future) to infer best operational strategies for tidal lagoons, with the
additional cost of requiring to run optimisation routines for every new tide.
In this paper, we propose a novel optimised operation of tidal lagoons with
proximal policy optimisation through Unity ML-Agents. We compare this technique
with 6 different operation optimisation approaches (baselines) devised from the
literature, utilising the Swansea Bay Tidal Lagoon as a case study. We show
that our approach is successful in maximising energy generation through an
optimised operational policy of turbines and sluices, yielding competitive
results with state-of-the-art methods of optimisation, regardless of test data
used, requiring training once and performing real-time flexible control with
measured ocean data only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1"&gt;T&amp;#xfa;lio Marcondes Moreira&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Faria_J/0/1/0/all/0/1"&gt;Jackson Geraldo de Faria Jr&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Melo_P/0/1/0/all/0/1"&gt;Pedro O.S. Vaz de Melo&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chaimowicz_L/0/1/0/all/0/1"&gt;Luiz Chaimowicz&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Medeiros_Ribeiro_G/0/1/0/all/0/1"&gt;Gilberto Medeiros-Ribeiro&lt;/a&gt; (1) ((1) Universidade Federal de Minas Gerais, Belo Horizonte, Brazil)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-parametric Differentially Private Confidence Intervals for the Median. (arXiv:2106.10333v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.10333</id>
        <link href="http://arxiv.org/abs/2106.10333"/>
        <updated>2021-06-22T01:57:10.614Z</updated>
        <summary type="html"><![CDATA[Differential privacy is a restriction on data processing algorithms that
provides strong confidentiality guarantees for individual records in the data.
However, research on proper statistical inference, that is, research on
properly quantifying the uncertainty of the (noisy) sample estimate regarding
the true value in the population, is currently still limited. This paper
proposes and evaluates several strategies to compute valid differentially
private confidence intervals for the median. Instead of computing a
differentially private point estimate and deriving its uncertainty, we directly
estimate the interval bounds and discuss why this approach is superior if
ensuring privacy is important. We also illustrate that addressing both sources
of uncertainty--the error from sampling and the error from protecting the
output--simultaneously should be preferred over simpler approaches that
incorporate the uncertainty in a sequential fashion. We evaluate the
performance of the different algorithms under various parameter settings in
extensive simulation studies and demonstrate how the findings could be applied
in practical settings using data from the 1940 Decennial Census.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Drechsler_J/0/1/0/all/0/1"&gt;Joerg Drechsler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globus_Harris_I/0/1/0/all/0/1"&gt;Ira Globus-Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1"&gt;Audra McMillan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarathy_J/0/1/0/all/0/1"&gt;Jayshree Sarathy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_A/0/1/0/all/0/1"&gt;Adam Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10836</id>
        <link href="http://arxiv.org/abs/2106.10836"/>
        <updated>2021-06-22T01:57:10.608Z</updated>
        <summary type="html"><![CDATA[When dealing with deep neural network (DNN) applications on edge devices,
continuously updating the model is important. Although updating a model with
real incoming data is ideal, using all of them is not always feasible due to
limits, such as labeling and communication costs. Thus, it is necessary to
filter and select the data to use for training (i.e., active learning) on the
device. In this paper, we formalize a practical active learning problem for
DNNs on edge devices and propose a general task-agnostic framework to tackle
this problem, which reduces it to a stream submodular maximization. This
framework is light enough to be run with low computational resources, yet
provides solutions whose quality is theoretically guaranteed thanks to the
submodular property. Through this framework, we can configure data selection
criteria flexibly, including using methods proposed in previous active learning
studies. We evaluate our approach on both classification and object detection
tasks in a practical setting to simulate a real-life scenario. The results of
our study show that the proposed framework outperforms all other methods in
both tasks, while running at a practical speed on real devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Senzaki_Y/0/1/0/all/0/1"&gt;Yuya Senzaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamelain_C/0/1/0/all/0/1"&gt;Christian Hamelain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Object Classification on Partial Point Clouds: A Practical Perspective. (arXiv:2012.10042v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10042</id>
        <link href="http://arxiv.org/abs/2012.10042"/>
        <updated>2021-06-22T01:57:10.603Z</updated>
        <summary type="html"><![CDATA[As a 3D counterpart of object classification in images, object point cloud
classification is fundamental to 3D scene understanding, and has drawn great
research attention since the release of benchmarking datasets, such as the
ModelNet and the ShapeNet. These benchmarks assume point clouds covering
complete surfaces of object instances, for which plenty of high-performing
methods have been developed. However, their settings deviate from those often
met in practice, where, due to (self-)occlusion, a point cloud covering partial
surface of an object is captured from an arbitrary view. We show in this paper
that performance of existing point cloud classification methods drops
drastically under the considered practical single-view, partial setting; the
phenomenon is consistent with the observation that semantic category of a
partial object surface is less ambiguous only when its distribution on the
whole surface is clearly specified. To this end, we argue for a single-view,
partial setting where supervised learning of object pose estimation should be
accompanied with classification. Technically, we propose a baseline method of
Pose-Accompanied Point cloud classification Network (PAPNet); built upon
SE(3)-equivariant convolutions, the PAPNet learns intermediate pose
transformations for equivariant features defined on vector fields, which makes
the subsequent classification easier (ideally) in the category-level, canonical
pose. We adapt existing ModelNet40 and ScanNet datasets on point set
classification to the introduced single-view, partial setting to verify our
hypothesis. Thorough experiments confirm the necessity of object pose
estimation; our PAPNet also outperforms existing methods greatly on the new
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zelin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Changxing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaowei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPLA-12: An Acoustic Signal Dataset of Gas Pipeline Leakage. (arXiv:2106.10277v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10277</id>
        <link href="http://arxiv.org/abs/2106.10277"/>
        <updated>2021-06-22T01:57:10.587Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a new acoustic leakage dataset of gas pipelines,
called as GPLA-12, which has 12 categories over 684 training/testing acoustic
signals. Unlike massive image and voice datasets, there have relatively few
acoustic signal datasets, especially for engineering fault detection. In order
to enhance the development of fault diagnosis, we collect acoustic leakage
signals on the basis of an intact gas pipe system with external artificial
leakages, and then preprocess the collected data with structured tailoring
which are turned into GPLA-12. GPLA-12 dedicates to serve as a feature learning
dataset for time-series tasks and classifications. To further understand the
dataset, we train both shadow and deep learning algorithms to observe the
performance. The dataset as well as the pretrained models have been released at
both www.daip.club and github.com/Deep-AI-Application-DAIP]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lizhong Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal Video Grounding. (arXiv:2106.10634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10634</id>
        <link href="http://arxiv.org/abs/2106.10634"/>
        <updated>2021-06-22T01:57:10.580Z</updated>
        <summary type="html"><![CDATA[We propose an effective two-stage approach to tackle the problem of
language-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In
the first stage, we propose an Augmented 2D Temporal Adjacent Network
(Augmented 2D-TAN) to temporally ground the target moment corresponding to the
given description. Primarily, we improve the original 2D-TAN from two aspects:
First, a temporal context-aware Bi-LSTM Aggregation Module is developed to
aggregate clip-level representations, replacing the original max-pooling.
Second, we propose to employ Random Concatenation Augmentation (RCA) mechanism
during the training phase. In the second stage, we use pretrained MDETR model
to generate per-frame bounding boxes via language query, and design a set of
hand-crafted rules to select the best matching bounding box outputted by MDETR
for each frame within the grounded moment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1"&gt;Chaolei Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zihang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian-Fang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wei-Shi Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Remote Sensing Images Semantic Segmentation with General Remote Sensing Vision Model via a Self-Supervised Contrastive Learning Method. (arXiv:2106.10605v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10605</id>
        <link href="http://arxiv.org/abs/2106.10605"/>
        <updated>2021-06-22T01:57:10.573Z</updated>
        <summary type="html"><![CDATA[A new learning paradigm, self-supervised learning (SSL), can be used to solve
such problems by pre-training a general model with large unlabeled images and
then fine-tuning on a downstream task with very few labeled samples.
Contrastive learning is a typical method of SSL, which can learn general
invariant features. However, most of the existing contrastive learning is
designed for classification tasks to obtain an image-level representation,
which may be sub-optimal for semantic segmentation tasks requiring pixel-level
discrimination. Therefore, we propose Global style and Local matching
Contrastive Learning Network (GLCNet) for remote sensing semantic segmentation.
Specifically, the global style contrastive module is used to learn an
image-level representation better, as we consider the style features can better
represent the overall image features; The local features matching contrastive
module is designed to learn representations of local regions which is
beneficial for semantic segmentation. We evaluate four remote sensing semantic
segmentation datasets, and the experimental results show that our method mostly
outperforms state-of-the-art self-supervised methods and ImageNet pre-training.
Specifically, with 1\% annotation from the original dataset, our approach
improves Kappa by 6\% on the ISPRS Potsdam dataset and 3\% on Deep Globe Land
Cover Classification dataset relative to the existing baseline. Moreover, our
method outperforms supervised learning when there are some differences between
the datasets of upstream tasks and downstream tasks. Our study promotes the
development of self-supervised learning in the field of remote sensing semantic
segmentation. The source code is available at
https://github.com/GeoX-Lab/G-RSIM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruoyun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haozhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chao Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Underwater Image Restoration via Contrastive Learning and a Real-world Dataset. (arXiv:2106.10718v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10718</id>
        <link href="http://arxiv.org/abs/2106.10718"/>
        <updated>2021-06-22T01:57:10.565Z</updated>
        <summary type="html"><![CDATA[Underwater image restoration is of significant importance in unveiling the
underwater world. Numerous techniques and algorithms have been developed in the
past decades. However, due to fundamental difficulties associated with
imaging/sensing, lighting, and refractive geometric distortions, in capturing
clear underwater images, no comprehensive evaluations have been conducted of
underwater image restoration. To address this gap, we have constructed a
large-scale real underwater image dataset, dubbed `HICRD' (Heron Island Coral
Reef Dataset), for the purpose of benchmarking existing methods and supporting
the development of new deep-learning based methods. We employ accurate water
parameter (diffuse attenuation coefficient) in generating reference images.
There are 2000 reference restored images and 6003 original underwater images in
the unpaired training set. Further, we present a novel method for underwater
image restoration based on unsupervised image-to-image translation framework.
Our proposed method leveraged contrastive learning and generative adversarial
networks to maximize the mutual information between raw and restored images.
Extensive experiments with comparisons to recent approaches further demonstrate
the superiority of our proposed method. Our code and dataset are publicly
available at GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1"&gt;Junlin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shoeiby_M/0/1/0/all/0/1"&gt;Mehrdad Shoeiby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Malthus_T/0/1/0/all/0/1"&gt;Tim Malthus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Botha_E/0/1/0/all/0/1"&gt;Elizabeth Botha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anstee_J/0/1/0/all/0/1"&gt;Janet Anstee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1"&gt;Saeed Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_R/0/1/0/all/0/1"&gt;Ran Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Armin_M/0/1/0/all/0/1"&gt;Mohammad Ali Armin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongdong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petersson_L/0/1/0/all/0/1"&gt;Lars Petersson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Functional Data Analysis with Adaptive Basis Layers. (arXiv:2106.10414v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10414</id>
        <link href="http://arxiv.org/abs/2106.10414"/>
        <updated>2021-06-22T01:57:10.550Z</updated>
        <summary type="html"><![CDATA[Despite their widespread success, the application of deep neural networks to
functional data remains scarce today. The infinite dimensionality of functional
data means standard learning algorithms can be applied only after appropriate
dimension reduction, typically achieved via basis expansions. Currently, these
bases are chosen a priori without the information for the task at hand and thus
may not be effective for the designated task. We instead propose to adaptively
learn these bases in an end-to-end fashion. We introduce neural networks that
employ a new Basis Layer whose hidden units are each basis functions themselves
implemented as a micro neural network. Our architecture learns to apply
parsimonious dimension reduction to functional inputs that focuses only on
information relevant to the target rather than irrelevant variation in the
input function. Across numerous classification/regression tasks with functional
data, our method empirically outperforms other types of neural networks, and we
prove that our approach is statistically consistent with low generalization
error. Code is available at: \url{https://github.com/jwyyy/AdaFNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yao_J/0/1/0/all/0/1"&gt;Junwen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Jonas Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jane-Ling Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Power Multi-Camera Object Re-Identification using Hierarchical Neural Networks. (arXiv:2106.10588v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10588</id>
        <link href="http://arxiv.org/abs/2106.10588"/>
        <updated>2021-06-22T01:57:10.545Z</updated>
        <summary type="html"><![CDATA[Low-power computer vision on embedded devices has many applications. This
paper describes a low-power technique for the object re-identification (reID)
problem: matching a query image against a gallery of previously seen images.
State-of-the-art techniques rely on large, computationally-intensive Deep
Neural Networks (DNNs). We propose a novel hierarchical DNN architecture that
uses attribute labels in the training dataset to perform efficient object reID.
At each node in the hierarchy, a small DNN identifies a different attribute of
the query image. The small DNN at each leaf node is specialized to re-identify
a subset of the gallery: only the images with the attributes identified along
the path from the root to a leaf. Thus, a query image is re-identified
accurately after processing with a few small DNNs. We compare our method with
state-of-the-art object reID techniques. With a 4% loss in accuracy, our
approach realizes significant resource savings: 74% less memory, 72% fewer
operations, and 67% lower query latency, yielding 65% less energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1"&gt;Abhinav Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1"&gt;Caleb Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James C. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1"&gt;George K. Thiruvathukal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yung-Hsiang Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN Inversion: A Survey. (arXiv:2101.05278v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05278</id>
        <link href="http://arxiv.org/abs/2101.05278"/>
        <updated>2021-06-22T01:57:10.539Z</updated>
        <summary type="html"><![CDATA[GAN inversion aims to invert a given image back into the latent space of a
pretrained GAN model, for the image to be faithfully reconstructed from the
inverted code by the generator. As an emerging technique to bridge the real and
fake image domains, GAN inversion plays an essential role in enabling the
pretrained GAN models such as StyleGAN and BigGAN to be used for real image
editing applications. Meanwhile, GAN inversion also provides insights on the
interpretation of GAN's latent space and how the realistic images can be
generated. In this paper, we provide an overview of GAN inversion with a focus
on its recent algorithms and applications. We cover important techniques of GAN
inversion and their applications to image restoration and image manipulation.
We further elaborate on some trends and challenges for future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Weihao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yulun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yujiu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-level Features for Resource Economy and Fast Learning in Skill Transfer. (arXiv:2106.10354v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.10354</id>
        <link href="http://arxiv.org/abs/2106.10354"/>
        <updated>2021-06-22T01:57:10.533Z</updated>
        <summary type="html"><![CDATA[Abstraction is an important aspect of intelligence which enables agents to
construct robust representations for effective decision making. In the last
decade, deep networks are proven to be effective due to their ability to form
increasingly complex abstractions. However, these abstractions are distributed
over many neurons, making the re-use of a learned skill costly. Previous work
either enforced formation of abstractions creating a designer bias, or used a
large number of neural units without investigating how to obtain high-level
features that may more effectively capture the source task. For avoiding
designer bias and unsparing resource use, we propose to exploit neural response
dynamics to form compact representations to use in skill transfer. For this, we
consider two competing methods based on (1) maximum information compression
principle and (2) the notion that abstract events tend to generate slowly
changing signals, and apply them to the neural signals generated during task
execution. To be concrete, in our simulation experiments, we either apply
principal component analysis (PCA) or slow feature analysis (SFA) on the
signals collected from the last hidden layer of a deep network while it
performs a source task, and use these features for skill transfer in a new
target task. We compare the generalization performance of these alternatives
with the baselines of skill transfer with full layer output and no-transfer
settings. Our results show that SFA units are the most successful for skill
transfer. SFA as well as PCA, incur less resources compared to usual skill
transfer, whereby many units formed show a localized response reflecting
end-effector-obstacle-goal relations. Finally, SFA units with lowest
eigenvalues resembles symbolic representations that highly correlate with
high-level features such as joint angles which might be thought of precursors
for fully symbolic systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmetoglu_A/0/1/0/all/0/1"&gt;Alper Ahmetoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ugur_E/0/1/0/all/0/1"&gt;Emre Ugur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asada_M/0/1/0/all/0/1"&gt;Minoru Asada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oztop_E/0/1/0/all/0/1"&gt;Erhan Oztop&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Track Object Position through Occlusion. (arXiv:2106.10766v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10766</id>
        <link href="http://arxiv.org/abs/2106.10766"/>
        <updated>2021-06-22T01:57:10.527Z</updated>
        <summary type="html"><![CDATA[Occlusion is one of the most significant challenges encountered by object
detectors and trackers. While both object detection and tracking has received a
lot of attention in the past, most existing methods in this domain do not
target detecting or tracking objects when they are occluded. However, being
able to detect or track an object of interest through occlusion has been a long
standing challenge for different autonomous tasks. Traditional methods that
employ visual object trackers with explicit occlusion modeling experience drift
and make several fundamental assumptions about the data. We propose to address
this with a `tracking-by-detection` approach that builds upon the success of
region based video object detectors. Our video level object detector uses a
novel recurrent computational unit at its core that enables long term
propagation of object features even under occlusion. Finally, we compare our
approach with existing state-of-the-art video object detectors and show that
our approach achieves superior results on a dataset of furniture assembly
videos collected from the internet, where small objects like screws, nuts, and
bolts often get occluded from the camera viewpoint.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1"&gt;Satyaki Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1"&gt;Martial Hebert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12391</id>
        <link href="http://arxiv.org/abs/2007.12391"/>
        <updated>2021-06-22T01:57:10.522Z</updated>
        <summary type="html"><![CDATA[This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1"&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1"&gt;David Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Model's Uncertainty and Confidences for Adversarial Example Detection. (arXiv:2103.05354v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05354</id>
        <link href="http://arxiv.org/abs/2103.05354"/>
        <updated>2021-06-22T01:57:10.505Z</updated>
        <summary type="html"><![CDATA[Security-sensitive applications that rely on Deep Neural Networks (DNNs) are
vulnerable to small perturbations that are crafted to generate Adversarial
Examples(AEs). The AEs are imperceptible to humans and cause DNN to misclassify
them. Many defense and detection techniques have been proposed. Model's
confidences and Dropout, as a popular way to estimate the model's uncertainty,
have been used for AE detection but they showed limited success against black-
and gray-box attacks. Moreover, the state-of-the-art detection techniques have
been designed for specific attacks or broken by others, need knowledge about
the attacks, are not consistent, increase model parameters overhead, are
time-consuming, or have latency in inference time. To trade off these factors,
we revisit the model's uncertainty and confidences and propose a novel
unsupervised ensemble AE detection mechanism that 1) uses the uncertainty
method called SelectiveNet, 2) processes model layers outputs, i.e.feature
maps, to generate new confidence probabilities. The detection method is called
Selective and Feature based Adversarial Detection (SFAD). Experimental results
show that the proposed approach achieves better performance against black- and
gray-box attacks than the state-of-the-art methods and achieves comparable
performance against white-box attacks. Moreover, results show that SFAD is
fully robust against High Confidence Attacks (HCAs) for MNIST and partially
robust for CIFAR10 datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldahdooh_A/0/1/0/all/0/1"&gt;Ahmed Aldahdooh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier D&amp;#xe9;forges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Generative Learning via Schr\"{o}dinger Bridge. (arXiv:2106.10410v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10410</id>
        <link href="http://arxiv.org/abs/2106.10410"/>
        <updated>2021-06-22T01:57:10.499Z</updated>
        <summary type="html"><![CDATA[We propose to learn a generative model via entropy interpolation with a
Schr\"{o}dinger Bridge. The generative learning task can be formulated as
interpolating between a reference distribution and a target distribution based
on the Kullback-Leibler divergence. At the population level, this entropy
interpolation is characterized via an SDE on $[0,1]$ with a time-varying drift
term. At the sample level, we derive our Schr\"{o}dinger Bridge algorithm by
plugging the drift term estimated by a deep score estimator and a deep density
ratio estimator into the Euler-Maruyama method. Under some mild smoothness
assumptions of the target distribution, we prove the consistency of both the
score estimator and the density ratio estimator, and then establish the
consistency of the proposed Schr\"{o}dinger Bridge approach. Our theoretical
results guarantee that the distribution learned by our approach converges to
the target distribution. Experimental results on multimodal synthetic data and
benchmark data support our theoretical findings and indicate that the
generative model via Schr\"{o}dinger Bridge is comparable with state-of-the-art
GANs, suggesting a new formulation of generative learning. We demonstrate its
usefulness in image interpolation and image inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gefei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yuling Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qian Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Can Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Deepfake Detection. (arXiv:2106.10705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10705</id>
        <link href="http://arxiv.org/abs/2106.10705"/>
        <updated>2021-06-22T01:57:10.493Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose to utilize Automated Machine Learning to
automatically search architecture for deepfake detection. Unlike previous
works, our method benefits from the superior capability of deep learning while
relieving us from the high labor cost in the manual network design process. It
is experimentally proved that our proposed method not only outperforms previous
non-deep learning methods but achieves comparable or even better prediction
accuracy compared to previous deep learning methods. To improve the generality
of our method, especially when training data and testing data are manipulated
by different methods, we propose a multi-task strategy in our network learning
process, making it estimate potential manipulation regions in given samples as
well as predict whether the samples are real. Comparing to previous works using
similar strategies, our method depends much less on prior knowledge, such as no
need to know which manipulation method is utilized and whether it is utilized
already. Extensive experimental results on two benchmark datasets demonstrate
the effectiveness of our proposed method on deepfake detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Attended Meta-Learning for Few-Shot Learning. (arXiv:2106.10642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10642</id>
        <link href="http://arxiv.org/abs/2106.10642"/>
        <updated>2021-06-22T01:57:10.488Z</updated>
        <summary type="html"><![CDATA[Meta-learning (ML) has emerged as a promising direction in learning models
under constrained resource settings like few-shot learning. The popular
approaches for ML either learn a generalizable initial model or a generic
parametric optimizer through episodic training. The former approaches leverage
the knowledge from a batch of tasks to learn an optimal prior. In this work, we
study the importance of a batch for ML. Specifically, we first incorporate a
batch episodic training regimen to improve the learning of the generic
parametric optimizer. We also hypothesize that the common assumption in batch
episodic training that each task in a batch has an equal contribution to
learning an optimal meta-model need not be true. We propose to weight the tasks
in a batch according to their "importance" in improving the meta-model's
learning. To this end, we introduce a training curriculum motivated by
selective focus in humans, called task attended meta-training, to weight the
tasks in a batch. Task attention is a standalone module that can be integrated
with any batch episodic training regimen. The comparisons of the models with
their non-task-attended counterparts on complex datasets like miniImageNet and
tieredImageNet validate its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aimen_A/0/1/0/all/0/1"&gt;Aroof Aimen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidheekh_S/0/1/0/all/0/1"&gt;Sahil Sidheekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1"&gt;Narayanan C. Krishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10587</id>
        <link href="http://arxiv.org/abs/2106.10587"/>
        <updated>2021-06-22T01:57:10.474Z</updated>
        <summary type="html"><![CDATA[Existing computer vision research in categorization struggles with
fine-grained attributes recognition due to the inherently high intra-class
variances and low inter-class variances. SOTA methods tackle this challenge by
locating the most informative image regions and rely on them to classify the
complete image. The most recent work, Vision Transformer (ViT), shows its
strong performance in both traditional and fine-grained classification tasks.
In this work, we propose a multi-stage ViT framework for fine-grained image
classification tasks, which localizes the informative image regions without
requiring architectural changes using the inherent multi-head self-attention
mechanism. We also introduce attention-guided augmentations for improving the
model's capabilities. We demonstrate the value of our approach by experimenting
with four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,
Stanford Dogs, and FGVC7 Plant Pathology. We also prove our model's
interpretability via qualitative results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1"&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1"&gt;Kerem Turgutlu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-06-22T01:57:10.469Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10696</id>
        <link href="http://arxiv.org/abs/2101.10696"/>
        <updated>2021-06-22T01:57:10.463Z</updated>
        <summary type="html"><![CDATA[Recently, some approaches are proposed to harness deep convolutional networks
to facilitate superpixel segmentation. The common practice is to first evenly
divide the image into a pre-defined number of grids and then learn to associate
each pixel with its surrounding grids. However, simply applying a series of
convolution operations with limited receptive fields can only implicitly
perceive the relations between the pixel and its surrounding grids.
Consequently, existing methods often fail to provide an effective context when
inferring the association map. To remedy this issue, we propose a novel
\textbf{A}ssociation \textbf{I}mplantation (AI) module to enable the network to
explicitly capture the relations between the pixel and its surrounding grids.
The proposed AI module directly implants the features of grid cells to the
surrounding of its corresponding central pixel, and conducts convolution on the
padded window to adaptively transfer knowledge between them. With such an
implantation operation, the network could explicitly harvest the pixel-grid
level context, which is more in line with the target of superpixel segmentation
comparing to the pixel-wise relation. Furthermore, to pursue better boundary
precision, we design a boundary-perceiving loss to help the network
discriminate the pixels around boundaries in hidden feature level, which could
benefit the subsequent inferring modules to accurately identify more boundary
pixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our
method could not only achieve state-of-the-art performance but maintain
satisfactory inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xueming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Li Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Semantic Relationships for Unpaired Image Captioning. (arXiv:2106.10658v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10658</id>
        <link href="http://arxiv.org/abs/2106.10658"/>
        <updated>2021-06-22T01:57:10.458Z</updated>
        <summary type="html"><![CDATA[Recently, image captioning has aroused great interest in both academic and
industrial worlds. Most existing systems are built upon large-scale datasets
consisting of image-sentence pairs, which, however, are time-consuming to
construct. In addition, even for the most advanced image captioning systems, it
is still difficult to realize deep image understanding. In this work, we
achieve unpaired image captioning by bridging the vision and the language
domains with high-level semantic information. The motivation stems from the
fact that the semantic concepts with the same modality can be extracted from
both images and descriptions. To further improve the quality of captions
generated by the model, we propose the Semantic Relationship Explorer, which
explores the relationships between semantic concepts for better understanding
of the image. Extensive experiments on MSCOCO dataset show that we can generate
desirable captions without paired datasets. Furthermore, the proposed approach
boosts five strong baselines under the paired setting, where the most
significant improvement in CIDEr score reaches 8%, demonstrating that it is
effective and generalizes well to a wide range of models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Meng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias. (arXiv:2101.07296v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07296</id>
        <link href="http://arxiv.org/abs/2101.07296"/>
        <updated>2021-06-22T01:57:10.453Z</updated>
        <summary type="html"><![CDATA[It is widely accepted that reasoning about object shape is important for
object recognition. However, the most powerful object recognition methods today
do not explicitly make use of object shape during learning. In this work,
motivated by recent developments in low-shot learning, findings in
developmental psychology, and the increased use of synthetic data in computer
vision research, we investigate how reasoning about 3D shape can be used to
improve low-shot learning methods' generalization performance. We propose a new
way to improve existing low-shot learning approaches by learning a
discriminative embedding space using 3D object shape, and using this embedding
by learning how to map images into it. Our new approach improves the
performance of image-only low-shot learning approaches on multiple datasets. We
also introduce Toys4K, a 3D object dataset with the largest number of object
categories currently available, which supports low-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojanov_S/0/1/0/all/0/1"&gt;Stefan Stojanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thai_A/0/1/0/all/0/1"&gt;Anh Thai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CompConv: A Compact Convolution Module for Efficient Feature Learning. (arXiv:2106.10486v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10486</id>
        <link href="http://arxiv.org/abs/2106.10486"/>
        <updated>2021-06-22T01:57:10.435Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have achieved remarkable success in
various computer vision tasks but rely on tremendous computational cost. To
solve this problem, existing approaches either compress well-trained
large-scale models or learn lightweight models with carefully designed network
structures. In this work, we make a close study of the convolution operator,
which is the basic unit used in CNNs, to reduce its computing load. In
particular, we propose a compact convolution module, called CompConv, to
facilitate efficient feature learning. With the divide-and-conquer strategy,
CompConv is able to save a great many computations as well as parameters to
produce a certain dimensional feature map. Furthermore, CompConv discreetly
integrates the input features into the outputs to efficiently inherit the input
information. More importantly, the novel CompConv is a plug-and-play module
that can be directly applied to modern CNN structures to replace the vanilla
convolution layers without further effort. Extensive experimental results
suggest that CompConv can adequately compress the benchmark CNN structures yet
barely sacrifice the performance, surpassing other competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yujun Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Humble Teachers Teach Better Students for Semi-Supervised Object Detection. (arXiv:2106.10456v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10456</id>
        <link href="http://arxiv.org/abs/2106.10456"/>
        <updated>2021-06-22T01:57:10.429Z</updated>
        <summary type="html"><![CDATA[We propose a semi-supervised approach for contemporary object detectors
following the teacher-student dual model framework. Our method is featured with
1) the exponential moving averaging strategy to update the teacher from the
student online, 2) using plenty of region proposals and soft pseudo-labels as
the student's training targets, and 3) a light-weighted detection-specific data
ensemble for the teacher to generate more reliable pseudo-labels. Compared to
the recent state-of-the-art -- STAC, which uses hard labels on sparsely
selected hard pseudo samples, the teacher in our model exposes richer
information to the student with soft-labels on many proposals. Our model
achieves COCO-style AP of 53.04% on VOC07 val set, 8.4% better than STAC, when
using VOC12 as unlabeled data. On MS-COCO, it outperforms prior work when only
a small percentage of data is taken as labeled. It also reaches 53.8% AP on
MS-COCO test-dev with 3.1% gain over the fully supervised ResNet-152 Cascaded
R-CNN, by tapping into unlabeled data of a similar size to the labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yihe Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weifeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yijun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Place recognition survey: An update on deep learning approaches. (arXiv:2106.10458v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10458</id>
        <link href="http://arxiv.org/abs/2106.10458"/>
        <updated>2021-06-22T01:57:10.423Z</updated>
        <summary type="html"><![CDATA[Autonomous Vehicles (AV) are becoming more capable of navigating in complex
environments with dynamic and changing conditions. A key component that enables
these intelligent vehicles to overcome such conditions and become more
autonomous is the sophistication of the perception and localization systems. As
part of the localization system, place recognition has benefited from recent
developments in other perception tasks such as place categorization or object
recognition, namely with the emergence of deep learning (DL) frameworks. This
paper surveys recent approaches and methods used in place recognition,
particularly those based on deep learning. The contributions of this work are
twofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in
place recognition; and categorizing the various DL-based place recognition
works into supervised, unsupervised, semi-supervised, parallel, and
hierarchical categories. First, this survey introduces key place recognition
concepts to contextualize the reader. Then, sensor characteristics are
addressed. This survey proceeds by elaborating on the various DL-based works,
presenting summaries for each framework. Some lessons learned from this survey
include: the importance of NetVLAD for supervised end-to-end learning; the
advantages of unsupervised approaches in place recognition, namely for
cross-domain applications; or the increasing tendency of recent works to seek,
not only for higher performance but also for higher efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1"&gt;Tiago Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_R/0/1/0/all/0/1"&gt;Ricardo Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrote_L/0/1/0/all/0/1"&gt;Lu&amp;#xed;s Garrote&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1"&gt;Cristiano Premebida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1"&gt;Urbano J. Nunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10479</id>
        <link href="http://arxiv.org/abs/2106.10479"/>
        <updated>2021-06-22T01:57:10.411Z</updated>
        <summary type="html"><![CDATA[Transferability estimation is an essential problem in transfer learning to
predict how good the performance is when transfer a source model (source task)
to a target task. Recent analytical transferability metrics have been widely
used for source model selection and multi-task learning. Earlier metrics does
not work sufficiently well under the challenging cross-domain cross-task
transfer settings, but recent OTCE score achieves a noteworthy performance
using auxiliary tasks. A simplified version named OT-based NCE score sacrifices
accuracy to be more efficient, but it can be further improved. Consequently, we
propose a practical transferability metric called JC-NCE score to further
improve the cross-domain cross-task transferability estimation performance,
which is more efficient than the OTCE score and more accurate than the OT-based
NCE score. Specifically, we build the joint correspondences between source and
target data via solving an optimal transport problem with considering both the
sample distance and label distance, and then compute the transferability score
as the negative conditional entropy. Extensive validations under the
intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE
score outperforms the OT-based NCE score with about 7% and 12% gains,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1"&gt;Yang Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shao-Lun Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel Pruning Guided by Spatial and Channel Attention for DNNs in Intelligent Edge Computing. (arXiv:2011.03891v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03891</id>
        <link href="http://arxiv.org/abs/2011.03891"/>
        <updated>2021-06-22T01:57:10.397Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) have achieved remarkable success in many computer
vision tasks recently, but the huge number of parameters and the high
computation overhead hinder their deployments on resource-constrained edge
devices. It is worth noting that channel pruning is an effective approach for
compressing DNN models. A critical challenge is to determine which channels are
to be removed, so that the model accuracy will not be negatively affected. In
this paper, we first propose Spatial and Channel Attention (SCA), a new
attention module combining both spatial and channel attention that respectively
focuses on "where" and "what" are the most informative parts. Guided by the
scale values generated by SCA for measuring channel importance, we further
propose a new channel pruning approach called Channel Pruning guided by Spatial
and Channel Attention (CPSCA). Experimental results indicate that SCA achieves
the best inference accuracy, while incurring negligibly extra resource
consumption, compared to other state-of-the-art attention modules. Our
evaluation on two benchmark datasets shows that, with the guidance of SCA, our
CPSCA approach achieves higher inference accuracy than other state-of-the-art
pruning methods under the same pruning ratios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1"&gt;Weiwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaodong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1"&gt;Naixue Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yi Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymmetrical Bi-RNN for pedestrian trajectory encoding. (arXiv:2106.04419v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04419</id>
        <link href="http://arxiv.org/abs/2106.04419"/>
        <updated>2021-06-22T01:57:10.324Z</updated>
        <summary type="html"><![CDATA[Pedestrian motion behavior involves a combination of individual goals and
social interactions with other agents. In this article, we present an
asymmetrical bidirectional recurrent neural network architecture called U-RNN
to encode pedestrian trajectories and evaluate its relevance to replace LSTMs
for various forecasting models. Experimental results on the Trajnet++ benchmark
show that the U-LSTM variant yields better results regarding every available
metrics (ADE, FDE, Collision rate) than common trajectory encoders for a
variety of approaches and interaction modules, suggesting that the proposed
approach is a viable alternative to the de facto sequence encoding RNNs.

Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is
available at:
github.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozenberg_R/0/1/0/all/0/1"&gt;Rapha&amp;#xeb;l Rozenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1"&gt;Joseph Gesnouin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1"&gt;Fabien Moutarde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamical Deep Generative Latent Modeling of 3D Skeletal Motion. (arXiv:2106.10393v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10393</id>
        <link href="http://arxiv.org/abs/2106.10393"/>
        <updated>2021-06-22T01:57:10.319Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a Bayesian switching dynamical model for
segmentation of 3D pose data over time that uncovers interpretable patterns in
the data and is generative. Our model decomposes highly correlated skeleton
data into a set of few spatial basis of switching temporal processes in a
low-dimensional latent framework. We parameterize these temporal processes with
regard to a switching deep vector autoregressive prior in order to accommodate
both multimodal and higher-order nonlinear inter-dependencies. This results in
a dynamical deep generative latent model that parses the meaningful intrinsic
states in the dynamics of 3D pose data using approximate variational inference,
and enables a realistic low-level dynamical generation and segmentation of
complex skeleton movements. Our experiments on four biological motion data
containing bat flight, salsa dance, walking, and golf datasets substantiate
superior performance of our model in comparison with the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farnoosh_A/0/1/0/all/0/1"&gt;Amirreza Farnoosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1"&gt;Sarah Ostadabbas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation. (arXiv:2101.08106v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08106</id>
        <link href="http://arxiv.org/abs/2101.08106"/>
        <updated>2021-06-22T01:57:10.313Z</updated>
        <summary type="html"><![CDATA[Despite pre-trained language models such as BERT have achieved appealing
performance in a wide range of natural language processing tasks, they are
computationally expensive to be deployed in real-time applications. A typical
method is to adopt knowledge distillation to compress these large pre-trained
models (teacher models) to small student models. However, for a target domain
with scarce training data, the teacher can hardly pass useful knowledge to the
student, which yields performance degradation for the student models. To tackle
this problem, we propose a method to learn to augment for data-scarce domain
BERT knowledge distillation, by learning a cross-domain manipulation scheme
that automatically augments the target with the help of resource-rich source
domains. Specifically, the proposed method generates samples acquired from a
stationary distribution near the target data and adopts a reinforced selector
to automatically refine the augmentation strategy according to the performance
of the student. Extensive experiments demonstrate that the proposed method
significantly outperforms state-of-the-art baselines on four different tasks,
and for the data-scarce domains, the compressed student models even perform
better than the original large teacher model, with much fewer parameters (only
${\sim}13.3\%$) when only a few labeled examples available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1"&gt;Lingyun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Ying Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks. (arXiv:2105.02968v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02968</id>
        <link href="http://arxiv.org/abs/2105.02968"/>
        <updated>2021-06-22T01:57:10.285Z</updated>
        <summary type="html"><![CDATA[Deep neural networks that yield human interpretable decisions by
architectural design have lately become an increasingly popular alternative to
post hoc interpretation of traditional black-box models. Among these networks,
the arguably most widespread approach is so-called prototype learning, where
similarities to learned latent prototypes serve as the basis of classifying an
unseen data point. In this work, we point to an important shortcoming of such
approaches. Namely, there is a semantic gap between similarity in latent space
and similarity in input space, which can corrupt interpretability. We design
two experiments that exemplify this issue on the so-called ProtoPNet.
Specifically, we find that this network's interpretability mechanism can be led
astray by intentionally crafted or even JPEG compression artefacts, which can
produce incomprehensible decisions. We argue that practitioners ought to have
this shortcoming in mind when deploying prototype-based models in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_A/0/1/0/all/0/1"&gt;Adrian Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fanconi_C/0/1/0/all/0/1"&gt;Claudio Fanconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1"&gt;Rahul Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1"&gt;Jonas Kohler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Universal Template for Few-shot Dataset Generalization. (arXiv:2105.07029v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07029</id>
        <link href="http://arxiv.org/abs/2105.07029"/>
        <updated>2021-06-22T01:57:10.279Z</updated>
        <summary type="html"><![CDATA[Few-shot dataset generalization is a challenging variant of the well-studied
few-shot classification problem where a diverse training set of several
datasets is given, for the purpose of training an adaptable model that can then
learn classes from new datasets using only a few examples. To this end, we
propose to utilize the diverse training set to construct a universal template:
a partial model that can define a wide array of dataset-specialized models, by
plugging in appropriate components. For each new few-shot classification
problem, our approach therefore only requires inferring a small number of
parameters to insert into the universal template. We design a separate network
that produces an initialization of those parameters for each given task, and we
then fine-tune its proposed initialization via a few steps of gradient descent.
Our approach is more parameter-efficient, scalable and adaptable compared to
previous methods, and achieves the state-of-the-art on the challenging
Meta-Dataset benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triantafillou_E/0/1/0/all/0/1"&gt;Eleni Triantafillou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1"&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1"&gt;Richard Zemel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1"&gt;Vincent Dumoulin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Object Segmentation with Dynamic Click Transform. (arXiv:2106.10465v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10465</id>
        <link href="http://arxiv.org/abs/2106.10465"/>
        <updated>2021-06-22T01:57:10.264Z</updated>
        <summary type="html"><![CDATA[In the interactive segmentation, users initially click on the target object
to segment the main body and then provide corrections on mislabeled regions to
iteratively refine the segmentation masks. Most existing methods transform
these user-provided clicks into interaction maps and concatenate them with
image as the input tensor. Typically, the interaction maps are determined by
measuring the distance of each pixel to the clicked points, ignoring the
relation between clicks and mislabeled regions. We propose a Dynamic Click
Transform Network~(DCT-Net), consisting of Spatial-DCT and Feature-DCT, to
better represent user interactions. Spatial-DCT transforms each user-provided
click with individual diffusion distance according to the target scale, and
Feature-DCT normalizes the extracted feature map to a specific distribution
predicted from the clicked points. We demonstrate the effectiveness of our
proposed method and achieve favorable performance compared to the
state-of-the-art on three standard benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chun-Tse Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1"&gt;Wei-Chih Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chih-Ting Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1"&gt;Shao-Yi Chien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00666</id>
        <link href="http://arxiv.org/abs/2106.00666"/>
        <updated>2021-06-22T01:57:10.259Z</updated>
        <summary type="html"><![CDATA[Can Transformer perform $2\mathrm{D}$ object-level recognition from a pure
sequence-to-sequence perspective with minimal knowledge about the $2\mathrm{D}$
spatial structure? To answer this question, we present You Only Look at One
Sequence (YOLOS), a series of object detection models based on the na\"ive
Vision Transformer with the fewest possible modifications as well as inductive
biases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset
only can already achieve competitive object detection performance on COCO,
\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$
box AP. We also discuss the impacts as well as limitations of current pre-train
schemes and model scaling strategies for Transformer in vision through object
detection. Code and model weights are available at
\url{https://github.com/hustvl/YOLOS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Bencheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jiyang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1"&gt;Jianwei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregating Nested Transformers. (arXiv:2105.12723v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12723</id>
        <link href="http://arxiv.org/abs/2105.12723"/>
        <updated>2021-06-22T01:57:10.254Z</updated>
        <summary type="html"><![CDATA[Although hierarchical structures are popular in recent vision transformers,
they require sophisticated designs and massive datasets to work well. In this
work, we explore the idea of nesting basic local transformers on
non-overlapping image blocks and aggregating them in a hierarchical manner. We
find that the block aggregation function plays a critical role in enabling
cross-block non-local information communication. This observation leads us to
design a simplified architecture with minor code changes upon the original
vision transformer and obtains improved performance compared to existing
methods. Our empirical results show that the proposed method NesT converges
faster and requires much less training data to achieve good generalization. For
example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs
achieves $82.3\%/83.8\%$ accuracy evaluated on $224\times 224$ image size,
outperforming previous methods with up to $57\%$ parameter reduction. Training
a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy
using a single GPU, setting a new state of the art for vision transformers.
Beyond image classification, we extend the key idea to image generation and
show NesT leads to a strong decoder that is 8$\times$ faster than previous
transformer based generators. Furthermore, we also propose a novel method for
visually interpreting the learned model. Source code is available
https://github.com/google-research/nested-transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zizhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Han Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLIB: Towards Automated Test Oracle for Graphically-Rich Applications. (arXiv:2106.10507v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.10507</id>
        <link href="http://arxiv.org/abs/2106.10507"/>
        <updated>2021-06-22T01:57:10.249Z</updated>
        <summary type="html"><![CDATA[Graphically-rich applications such as games are ubiquitous with attractive
visual effects of Graphical User Interface (GUI) that offers a bridge between
software applications and end-users. However, various types of graphical
glitches may arise from such GUI complexity and have become one of the main
component of software compatibility issues. Our study on bug reports from game
development teams in NetEase Inc. indicates that graphical glitches frequently
occur during the GUI rendering and severely degrade the quality of
graphically-rich applications such as video games. Existing automated testing
techniques for such applications focus mainly on generating various GUI test
sequences and check whether the test sequences can cause crashes. These
techniques require constant human attention to captures non-crashing bugs such
as bugs causing graphical glitches. In this paper, we present the first step in
automating the test oracle for detecting non-crashing bugs in graphically-rich
applications. Specifically, we propose \texttt{GLIB} based on a code-based data
augmentation technique to detect game GUI glitches. We perform an evaluation of
\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the
result shows that \texttt{GLIB} can achieve 100\% precision and 99.5\% recall
in detecting non-crashing bugs such as game GUI glitches. Practical application
of \texttt{GLIB} on another 14 real-world games (without bug reports) further
demonstrates that \texttt{GLIB} can effectively uncover GUI glitches, with 48
of 53 bugs reported by \texttt{GLIB} having been confirmed and fixed so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Ke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yufei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Changjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering. (arXiv:2106.10446v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10446</id>
        <link href="http://arxiv.org/abs/2106.10446"/>
        <updated>2021-06-22T01:57:10.236Z</updated>
        <summary type="html"><![CDATA[Video Question Answering is a task which requires an AI agent to answer
questions grounded in video. This task entails three key challenges: (1)
understand the intention of various questions, (2) capturing various elements
of the input video (e.g., object, action, causality), and (3) cross-modal
grounding between language and vision information. We propose Motion-Appearance
Synergistic Networks (MASN), which embed two cross-modal features grounded on
motion and appearance information and selectively utilize them depending on the
question's intentions. MASN consists of a motion module, an appearance module,
and a motion-appearance fusion module. The motion module computes the
action-oriented cross-modal joint representations, while the appearance module
focuses on the appearance aspect of the input video. Finally, the
motion-appearance fusion module takes each output of the motion module and the
appearance module as input, and performs question-guided fusion. As a result,
MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA
datasets. We also conduct qualitative analysis by visualizing the inference
results of MASN. The code is available at
https://github.com/ahjeongseo/MASN-pytorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_A/0/1/0/all/0/1"&gt;Ahjeong Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1"&gt;Gi-Cheon Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Joonhan Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Byoung-Tak Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direct Reconstruction of Linear Parametric Images from Dynamic PET Using Nonlocal Deep Image Prior. (arXiv:2106.10359v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10359</id>
        <link href="http://arxiv.org/abs/2106.10359"/>
        <updated>2021-06-22T01:57:10.231Z</updated>
        <summary type="html"><![CDATA[Direct reconstruction methods have been developed to estimate parametric
images directly from the measured PET sinograms by combining the PET imaging
model and tracer kinetics in an integrated framework. Due to limited counts
received, signal-to-noise-ratio (SNR) and resolution of parametric images
produced by direct reconstruction frameworks are still limited. Recently
supervised deep learning methods have been successfully applied to medical
imaging denoising/reconstruction when large number of high-quality training
labels are available. For static PET imaging, high-quality training labels can
be acquired by extending the scanning time. However, this is not feasible for
dynamic PET imaging, where the scanning time is already long enough. In this
work, we proposed an unsupervised deep learning framework for direct parametric
reconstruction from dynamic PET, which was tested on the Patlak model and the
relative equilibrium Logan model. The patient's anatomical prior image, which
is readily available from PET/CT or PET/MR scans, was supplied as the network
input to provide a manifold constraint, and also utilized to construct a kernel
layer to perform non-local feature denoising. The linear kinetic model was
embedded in the network structure as a 1x1 convolution layer. The training
objective function was based on the PET statistical model. Evaluations based on
dynamic datasets of 18F-FDG and 11C-PiB tracers show that the proposed
framework can outperform the traditional and the kernel method-based direct
reconstruction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gong_K/0/1/0/all/0/1"&gt;Kuang Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Catana_C/0/1/0/all/0/1"&gt;Ciprian Catana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jinyi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Quanzheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specular reflections removal in colposcopic images based on neural networks: Supervised training with no ground truth previous knowledge. (arXiv:2106.02221v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02221</id>
        <link href="http://arxiv.org/abs/2106.02221"/>
        <updated>2021-06-22T01:57:10.226Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is a malignant tumor that seriously threatens women's health,
and is one of the most common that affects women worldwide. For its early
detection, colposcopic images of the cervix are used for searching for possible
injuries or abnormalities. An inherent characteristic of these images is the
presence of specular reflections (brightness) that make it difficult to observe
some regions, which might imply misdiagnosis. In this paper, a new strategy
based on neural networks is introduced for eliminating specular reflections and
estimating the unobserved anatomical cervix portion under the bright zones. For
overcoming the fact that the ground truth corresponding to the specular
reflection regions is always unknown, the new strategy proposes the supervised
training of a neural network to learn how to restore any hidden regions of
colposcopic images. Once the specular reflections are identified, they are
removed from the image, and the previously trained network is used to fulfill
these deleted areas. The quality of the processed images was evaluated
quantitatively and qualitatively. In 21 of the 22 evaluated images, the
detected specular reflections were eliminated, whereas, in the remaining one,
these reflections were almost completely eliminated. The distribution of the
colors and the content of the restored images are similar to those of the
originals. The evaluation carried out by a specialist in Cervix Pathology
concluded that, after eliminating the specular reflections, the anatomical and
physiological elements of the cervix are observable in the restored images,
which facilitates the medical diagnosis of cervical pathologies. Our method has
the potential to improve the early detection of cervical cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_Martin_L/0/1/0/all/0/1"&gt;Lauren Jimenez-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perez_D/0/1/0/all/0/1"&gt;Daniel A. Vald&amp;#xe9;s P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asteasuainzarra_A/0/1/0/all/0/1"&gt;Ana M. Solares Asteasuainzarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonard_L/0/1/0/all/0/1"&gt;Ludwig Leonard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_Romanach_M/0/1/0/all/0/1"&gt;Marta L. Baguer D&amp;#xed;az-Roma&amp;#xf1;ach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-22T01:57:10.220Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group-CAM: Group Score-Weighted Visual Explanations for Deep Convolutional Networks. (arXiv:2103.13859v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13859</id>
        <link href="http://arxiv.org/abs/2103.13859"/>
        <updated>2021-06-22T01:57:10.215Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an efficient saliency map generation method, called
Group score-weighted Class Activation Mapping (Group-CAM), which adopts the
"split-transform-merge" strategy to generate saliency maps. Specifically, for
an input image, the class activations are firstly split into groups. In each
group, the sub-activations are summed and de-noised as an initial mask. After
that, the initial masks are transformed with meaningful perturbations and then
applied to preserve sub-pixels of the input (i.e., masked inputs), which are
then fed into the network to calculate the confidence scores. Finally, the
initial masks are weighted summed to form the final saliency map, where the
weights are confidence scores produced by the masked inputs. Group-CAM is
efficient yet effective, which only requires dozens of queries to the network
while producing target-related saliency maps. As a result, Group-CAM can be
served as an effective data augment trick for fine-tuning the networks. We
comprehensively evaluate the performance of Group-CAM on common-used
benchmarks, including deletion and insertion tests on ImageNet-1k, and pointing
game tests on COCO2017. Extensive experimental results demonstrate that
Group-CAM achieves better visual performance than the current state-of-the-art
explanation approaches. The code is available at
https://github.com/wofmanaf/Group-CAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_L/0/1/0/all/0/1"&gt;Lu Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention to Warp: Deep Metric Learning for Multivariate Time Series. (arXiv:2103.15074v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15074</id>
        <link href="http://arxiv.org/abs/2103.15074"/>
        <updated>2021-06-22T01:57:10.208Z</updated>
        <summary type="html"><![CDATA[Deep time series metric learning is challenging due to the difficult
trade-off between temporal invariance to nonlinear distortion and
discriminative power in identifying non-matching sequences. This paper proposes
a novel neural network-based approach for robust yet discriminative time series
classification and verification. This approach adapts a parameterized attention
model to time warping for greater and more adaptive temporal invariance. It is
robust against not only local but also large global distortions, so that even
matching pairs that do not satisfy the monotonicity, continuity, and boundary
conditions can still be successfully identified. Learning of this model is
further guided by dynamic time warping to impose temporal constraints for
stabilized training and higher discriminative power. It can learn to augment
the inter-class variation through warping, so that similar but different
classes can be effectively distinguished. We experimentally demonstrate the
superiority of the proposed approach over previous non-parametric and deep
models by combining it with a deep online signature verification framework,
after confirming its promising behavior in single-letter handwriting
classification on the Unipen dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matsuo_S/0/1/0/all/0/1"&gt;Shinnosuke Matsuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaomeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atarsaikhan_G/0/1/0/all/0/1"&gt;Gantugs Atarsaikhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1"&gt;Akisato Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashino_K/0/1/0/all/0/1"&gt;Kunio Kashino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1"&gt;Brian Kenji Iwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1"&gt;Seiichi Uchida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis Towards Classification of Infection and Ischaemia of Diabetic Foot Ulcers. (arXiv:2104.03068v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03068</id>
        <link href="http://arxiv.org/abs/2104.03068"/>
        <updated>2021-06-22T01:57:10.202Z</updated>
        <summary type="html"><![CDATA[This paper introduces the Diabetic Foot Ulcers dataset (DFUC2021) for
analysis of pathology, focusing on infection and ischaemia. We describe the
data preparation of DFUC2021 for ground truth annotation, data curation and
data analysis. The final release of DFUC2021 consists of 15,683 DFU patches,
with 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The
ground truth labels are four classes, i.e. control, infection, ischaemia and
both conditions. We curate the dataset using image hashing techniques and
analyse the separability using UMAP projection. We benchmark the performance of
five key backbones of deep learning, i.e. VGG16, ResNet101, InceptionV3,
DenseNet121 and EfficientNet on DFUC2021. We report the optimised results of
these key backbones with different strategies. Based on our observations, we
conclude that EfficientNetB0 with data augmentation and transfer learning
provided the best results for multi-class (4-class) classification with
macro-average Precision, Recall and F1-score of 0.57, 0.62 and 0.55,
respectively. In ischaemia and infection recognition, when trained on
one-versus-all, EfficientNetB0 achieved comparable results with the state of
the art. Finally, we interpret the results with statistical analysis and
Grad-CAM visualisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1"&gt;Moi Hoon Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassidy_B/0/1/0/all/0/1"&gt;Bill Cassidy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pappachan_J/0/1/0/all/0/1"&gt;Joseph M. Pappachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OShea_C/0/1/0/all/0/1"&gt;Claire O&amp;#x27;Shea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gillespie_D/0/1/0/all/0/1"&gt;David Gillespie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reeves_N/0/1/0/all/0/1"&gt;Neil Reeves&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning Inverts the Data Generating Process. (arXiv:2102.08850v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08850</id>
        <link href="http://arxiv.org/abs/2102.08850"/>
        <updated>2021-06-22T01:57:10.187Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has recently seen tremendous success in self-supervised
learning. So far, however, it is largely unclear why the learned
representations generalize so effectively to a large variety of downstream
tasks. We here prove that feedforward models trained with objectives belonging
to the commonly used InfoNCE family learn to implicitly invert the underlying
generative model of the observed data. While the proofs make certain
statistical assumptions about the generative model, we observe empirically that
our findings hold even if these assumptions are severely violated. Our theory
highlights a fundamental connection between contrastive learning, generative
modeling, and nonlinear independent component analysis, thereby furthering our
understanding of the learned representations as well as providing a theoretical
foundation to derive more effective contrastive losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1"&gt;Yash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Steffen Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis. (arXiv:2103.15348v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15348</id>
        <link href="http://arxiv.org/abs/2103.15348"/>
        <updated>2021-06-22T01:57:10.177Z</updated>
        <summary type="html"><![CDATA[Recent advances in document image analysis (DIA) have been primarily driven
by the application of neural networks. Ideally, research outcomes could be
easily deployed in production and extended for further investigation. However,
various factors like loosely organized codebases and sophisticated model
configurations complicate the easy reuse of important innovations by a wide
audience. Though there have been on-going efforts to improve reusability and
simplify deep learning (DL) model development in disciplines like natural
language processing and computer vision, none of them are optimized for
challenges in the domain of DIA. This represents a major gap in the existing
toolkit, as DIA is central to academic research across a wide range of
disciplines in the social sciences and humanities. This paper introduces
layoutparser, an open-source library for streamlining the usage of DL in DIA
research and applications. The core layoutparser library comes with a set of
simple and intuitive interfaces for applying and customizing DL models for
layout detection, character recognition, and many other document processing
tasks. To promote extensibility, layoutparser also incorporates a community
platform for sharing both pre-trained models and full document digitization
pipelines. We demonstrate that layoutparser is helpful for both lightweight and
large-scale digitization pipelines in real-word use cases. The library is
publicly available at https://layout-parser.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zejiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruochen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dell_M/0/1/0/all/0/1"&gt;Melissa Dell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Benjamin Charles Germain Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlson_J/0/1/0/all/0/1"&gt;Jacob Carlson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weining Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDDA: Explanation-driven Data Augmentation to Improve Model and Explanation Alignment. (arXiv:2105.14162v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14162</id>
        <link href="http://arxiv.org/abs/2105.14162"/>
        <updated>2021-06-22T01:57:10.167Z</updated>
        <summary type="html"><![CDATA[Recent years have seen the introduction of a range of methods for post-hoc
explainability of image classifier predictions. However, these post-hoc
explanations may not always align perfectly with classifier predictions, which
poses a significant challenge when attempting to debug models based on such
explanations. To this end, we seek a methodology that can improve alignment
between model predictions and explanation method that is both agnostic to the
model and explanation classes and which does not require ground truth
explanations. We achieve this through a novel explanation-driven data
augmentation (EDDA) method that augments the training data with occlusions of
existing data stemming from model-explanations; this is based on the simple
motivating principle that occluding salient regions for the model prediction
should decrease the model confidence in the prediction, while occluding
non-salient regions should not change the prediction -- if the model and
explainer are aligned. To verify that this augmentation method improves model
and explainer alignment, we evaluate the methodology on a variety of datasets,
image classification models, and explanation methods. We verify in all cases
that our explanation-driven data augmentation method improves alignment of the
model and explanation in comparison to no data augmentation and non-explanation
driven data augmentation methods. In conclusion, this approach provides a novel
model- and explainer-agnostic methodology for improving alignment between model
predictions and explanations, which we see as a critical step forward for
practical deployment and debugging of image classification models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruiwen Li&lt;/a&gt; (co-first author), &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhibo Zhang&lt;/a&gt; (co-first author), &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiani Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1"&gt;Scott Sanner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1"&gt;Jongseong Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yeonjeong Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1"&gt;Dongsub Shim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning for Gastritis Detection with Gastric X-ray Images. (arXiv:2104.02864v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02864</id>
        <link href="http://arxiv.org/abs/2104.02864"/>
        <updated>2021-06-22T01:57:10.159Z</updated>
        <summary type="html"><![CDATA[Background and Objective: Manually annotating gastric X-ray images for
gastritis detection is time-consuming and expensive because it typically
requires expert knowledge. This paper proposes a self-supervised learning
method to solve this problem. This study aims to verify the effectiveness of
the proposed self-supervised learning method in gastritis detection using a few
annotated gastric X-ray images. Methods: In this paper, we propose a novel
self-supervised learning method that can perform explicit self-supervised
learning and learn discriminative representations from gastric X-ray images.
Models trained with the proposed method were fine-tuned on datasets with a few
annotated gastric X-ray images. For comparison, several state-of-the-art
self-supervised learning methods, i.e., containing SimSiam, BYOL, PIRL-jigsaw,
PIRL-rotation, and SimCLR, were compared with the proposed method. Furthermore,
two baseline methods, one pretrained on ImageNet and the other trained from
scratch, were compared with the proposed method. Results: The proposed method's
harmonic mean score of sensitivity and specificity after fine-tuning with the
annotated data of 10, 20, 30, and 40 patients were 0.875, 0.911, 0.915, and
0.931, respectively. The proposed method outperformed all comparative methods,
including the five state-of-the-art self-supervised learning and two baseline
methods. Experimental results showed the effectiveness of the proposed method
in gastritis detection with a few annotated gastric X-ray images. Conclusions:
The proposed self-supervised learning method shows potential for clinical use
in gastritis detection using a few annotated gastric X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togo_R/0/1/0/all/0/1"&gt;Ren Togo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogawa_T/0/1/0/all/0/1"&gt;Takahiro Ogawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haseyama_M/0/1/0/all/0/1"&gt;Miki Haseyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Animal ID Problem: Continual Curation. (arXiv:2106.10377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10377</id>
        <link href="http://arxiv.org/abs/2106.10377"/>
        <updated>2021-06-22T01:57:10.127Z</updated>
        <summary type="html"><![CDATA[Hoping to stimulate new research in individual animal identification from
images, we propose to formulate the problem as the human-machine Continual
Curation of images and animal identities. This is an open world recognition
problem, where most new animals enter the system after its algorithms are
initially trained and deployed. Continual Curation, as defined here, requires
(1) an improvement in the effectiveness of current recognition methods, (2) a
pairwise verification algorithm that allows the possibility of no decision, and
(3) an algorithmic decision mechanism that seeks human input to guide the
curation process. Error metrics must evaluate the ability of recognition
algorithms to identify not only animals that have been seen just once or twice
but also recognize new animals not in the database. An important measure of
overall system performance is accuracy as a function of the amount of human
input required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_C/0/1/0/all/0/1"&gt;Charles V. Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parham_J/0/1/0/all/0/1"&gt;Jason R. Parham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holmberg_J/0/1/0/all/0/1"&gt;Jason Holmberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_Wolf_T/0/1/0/all/0/1"&gt;Tanya Y. Berger-Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction. (arXiv:2103.04174v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04174</id>
        <link href="http://arxiv.org/abs/2103.04174"/>
        <updated>2021-06-22T01:57:10.114Z</updated>
        <summary type="html"><![CDATA[A video prediction model that generalizes to diverse scenes would enable
intelligent agents such as robots to perform a variety of tasks via planning
with the model. However, while existing video prediction models have produced
promising results on small datasets, they suffer from severe underfitting when
trained on large and diverse datasets. To address this underfitting challenge,
we first observe that the ability to train larger video prediction models is
often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep
hierarchical latent variable models can produce higher quality predictions by
capturing the multi-level stochasticity of future observations, but end-to-end
optimization of such models is notably difficult. Our key insight is that
greedy and modular optimization of hierarchical autoencoders can simultaneously
address both the memory constraints and the optimization challenges of
large-scale video prediction. We introduce Greedy Hierarchical Variational
Autoencoders (GHVAEs), a method that learns high-fidelity video predictions by
greedily training each level of a hierarchical autoencoder. In comparison to
state-of-the-art models, GHVAEs provide 17-55% gains in prediction performance
on four video datasets, a 35-40% higher success rate on real robot tasks, and
can improve performance monotonically by simply adding more modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bohan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1"&gt;Roberto Martin-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Large Margin Learning. (arXiv:2103.13598v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13598</id>
        <link href="http://arxiv.org/abs/2103.13598"/>
        <updated>2021-06-22T01:57:10.109Z</updated>
        <summary type="html"><![CDATA[This paper serves as a survey of recent advances in large margin training and
its theoretical foundations, mostly for (nonlinear) deep neural networks (DNNs)
that are probably the most prominent machine learning models for large-scale
data in the community over the past decade. We generalize the formulation of
classification margins from classical research to latest DNNs, summarize
theoretical connections between the margin, network generalization, and
robustness, and introduce recent efforts in enlarging the margins for DNNs
comprehensively. Since the viewpoint of different methods is discrepant, we
categorize them into groups for ease of comparison and discussion in the paper.
Hopefully, our discussions and overview inspire new research work in the
community that aim to improve the performance of DNNs, and we also point to
directions where the large margin principle can be verified to provide
theoretical evidence why certain regularizations for DNNs function well in
practice. We managed to shorten the paper such that the crucial spirit of large
margin learning and related methods are better emphasized.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yiwen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetiNerveNet: Using Recursive Deep Learning to Estimate Pointwise 24-2 Visual Field Data based on Retinal Structure. (arXiv:2010.07488v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07488</id>
        <link href="http://arxiv.org/abs/2010.07488"/>
        <updated>2021-06-22T01:57:10.103Z</updated>
        <summary type="html"><![CDATA[Glaucoma is the leading cause of irreversible blindness in the world,
affecting over 70 million people. The cumbersome Standard Automated Perimetry
(SAP) test is most frequently used to detect visual loss due to glaucoma. Due
to the SAP test's innate difficulty and its high test-retest variability, we
propose the RetiNerveNet, a deep convolutional recursive neural network for
obtaining estimates of the SAP visual field. RetiNerveNet uses information from
the more objective Spectral-Domain Optical Coherence Tomography (SDOCT).
RetiNerveNet attempts to trace-back the arcuate convergence of the retinal
nerve fibers, starting from the Retinal Nerve Fiber Layer (RNFL) thickness
around the optic disc, to estimate individual age-corrected 24-2 SAP values.
Recursive passes through the proposed network sequentially yield estimates of
the visual locations progressively farther from the optic disc. While all the
methods used for our experiments exhibit lower performance for the advanced
disease group, the proposed network is observed to be more accurate than all
the baselines for estimating the individual visual field values. We further
augment RetiNerveNet to additionally predict the SAP Mean Deviation values and
also create an ensemble of RetiNerveNets that further improves the performance,
by increasingly weighting-up underrepresented parts of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1"&gt;Shounak Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mariottoni_E/0/1/0/all/0/1"&gt;Eduardo B. Mariottoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dov_D/0/1/0/all/0/1"&gt;David Dov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jammal_A/0/1/0/all/0/1"&gt;Alessandro A. Jammal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medeiros_F/0/1/0/all/0/1"&gt;Felipe A. Medeiros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration. (arXiv:2102.08946v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08946</id>
        <link href="http://arxiv.org/abs/2102.08946"/>
        <updated>2021-06-22T01:57:10.097Z</updated>
        <summary type="html"><![CDATA[Previous studies dominantly target at self-supervised learning on real-valued
networks and have achieved many promising results. However, on the more
challenging binary neural networks (BNNs), this task has not yet been fully
explored in the community. In this paper, we focus on this more difficult
scenario: learning networks where both weights and activations are binary,
meanwhile, without any human annotated labels. We observe that the commonly
used contrastive objective is not satisfying on BNNs for competitive accuracy,
since the backbone network contains relatively limited capacity and
representation ability. Hence instead of directly applying existing
self-supervised methods, which cause a severe decline in performance, we
present a novel guided learning paradigm from real-valued to distill binary
networks on the final prediction distribution, to minimize the loss and obtain
desirable accuracy. Our proposed method can boost the simple contrastive
learning baseline by an absolute gain of 5.5~15% on BNNs. We further reveal
that it is difficult for BNNs to recover the similar predictive distributions
as real-valued models when training without labels. Thus, how to calibrate them
is key to address the degradation in performance. Extensive experiments are
conducted on the large-scale ImageNet and downstream datasets. Our method
achieves substantial improvement over the simple contrastive learning baseline,
and is even comparable to many mainstream supervised BNN methods. Code is
available at https://github.com/szq0214/S2-BNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zechun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1"&gt;Marios Savvides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02803</id>
        <link href="http://arxiv.org/abs/2011.02803"/>
        <updated>2021-06-22T01:57:10.081Z</updated>
        <summary type="html"><![CDATA[Contrastive loss and its variants have become very popular recently for
learning visual representations without supervision. In this work, we study
three intriguing properties of contrastive learning. We first generalize the
standard contrastive loss to a broader family of losses, and we find that
various instantiations of the generalized loss perform similarly under the
presence of a multi-layer non-linear projection head. We then study if
instance-based contrastive learning (such as in SimCLR, MoCo, BYOL, and so on,
which are based on global image representation) can learn well on images with
multiple objects present. We find that meaningful hierarchical local features
can be learned despite the fact that these objectives operate on global
instance-level features.

Finally, we study an intriguing phenomenon of feature suppression among
competing features shared across augmented views, such as "color distribution"
vs "object class". We construct datasets with explicit and controllable
competing features, and show that, for contrastive learning, a few bits of
easy-to-learn shared features can suppress, and even fully prevent, the
learning of other sets of competing features. In scenarios where there are
multiple objects in an image, the dominant object would suppress the learning
of smaller objects. Existing contrastive learning methods critically rely on
data augmentation to favor certain sets of features over others, and face
potential limitation for scenarios where existing augmentations cannot fully
address the feature suppression. This poses open challenges to existing
contrastive learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Calvin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lala Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Stitching Algorithm for Automated Surface Inspection of Rotationally Symmetric Components. (arXiv:2012.00308v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00308</id>
        <link href="http://arxiv.org/abs/2012.00308"/>
        <updated>2021-06-22T01:57:10.075Z</updated>
        <summary type="html"><![CDATA[This paper provides a novel approach to stitching surface images of
rotationally symmetric parts. It presents a process pipeline that uses a
feature-based stitching approach to create a distortion-free and true-to-life
image from a video file. The developed process thus enables, for example,
condition monitoring without having to view many individual images. For
validation purposes, this will be demonstrated in the paper using the concrete
example of a worn ball screw drive spindle. The developed algorithm aims at
reproducing the functional principle of a line scan camera system, whereby the
physical measuring systems are replaced by a feature-based approach. For
evaluation of the stitching algorithms, metrics are used, some of which have
only been developed in this work or have been supplemented by test procedures
already in use. The applicability of the developed algorithm is not only
limited to machine tool spindles. Instead, the developed method allows a
general approach to the surface inspection of various rotationally symmetric
components and can therefore be used in a variety of industrial applications.
Deep-learning-based detection Algorithms can easily be implemented to generate
a complete pipeline for failure detection and condition monitoring on
rotationally symmetric parts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1"&gt;Tobias Schlagenhauf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brander_T/0/1/0/all/0/1"&gt;Tim Brander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1"&gt;Juergen Fleischer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LEGAN: Disentangled Manipulation of Directional Lighting and Facial Expressions by Leveraging Human Perceptual Judgements. (arXiv:2010.01464v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01464</id>
        <link href="http://arxiv.org/abs/2010.01464"/>
        <updated>2021-06-22T01:57:10.070Z</updated>
        <summary type="html"><![CDATA[Building facial analysis systems that generalize to extreme variations in
lighting and facial expressions is a challenging problem that can potentially
be alleviated using natural-looking synthetic data. Towards that, we propose
LEGAN, a novel synthesis framework that leverages perceptual quality judgments
for jointly manipulating lighting and expressions in face images, without
requiring paired training data. LEGAN disentangles the lighting and expression
subspaces and performs transformations in the feature space before upscaling to
the desired output image. The fidelity of the synthetic image is further
refined by integrating a perceptual quality estimation model, trained with face
images rendered using multiple synthesis methods and their crowd-sourced
naturalness ratings, into the LEGAN framework as an auxiliary discriminator.
Using objective metrics like FID and LPIPS, LEGAN is shown to generate higher
quality face images when compared with popular GAN models like StarGAN and
StarGAN-v2 for lighting and expression synthesis. We also conduct a perceptual
study using images synthesized by LEGAN and other GAN models and show the
correlation between our quality estimation and visual fidelity. Finally, we
demonstrate the effectiveness of LEGAN as training data augmenter for
expression recognition and face verification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1"&gt;Sandipan Banerjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Ajjen Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_P/0/1/0/all/0/1"&gt;Prashant Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Sneha Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kyal_S/0/1/0/all/0/1"&gt;Survi Kyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_T/0/1/0/all/0/1"&gt;Taniya Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Object Detection with Pointformer. (arXiv:2012.11409v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11409</id>
        <link href="http://arxiv.org/abs/2012.11409"/>
        <updated>2021-06-22T01:57:10.064Z</updated>
        <summary type="html"><![CDATA[Feature learning for 3D object detection from point clouds is very
challenging due to the irregularity of 3D point cloud data. In this paper, we
propose Pointformer, a Transformer backbone designed for 3D point clouds to
learn features effectively. Specifically, a Local Transformer module is
employed to model interactions among points in a local region, which learns
context-dependent region features at an object level. A Global Transformer is
designed to learn context-aware representations at the scene level. To further
capture the dependencies among multi-scale representations, we propose
Local-Global Transformer to integrate local features with global features from
higher resolution. In addition, we introduce an efficient coordinate refinement
module to shift down-sampled points closer to object centroids, which improves
object proposal generation. We use Pointformer as the backbone for
state-of-the-art object detection models and demonstrate significant
improvements over original models on both indoor and outdoor datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1"&gt;Zhuofan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Li Erran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlocking Pixels for Reinforcement Learning via Implicit Attention. (arXiv:2102.04353v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04353</id>
        <link href="http://arxiv.org/abs/2102.04353"/>
        <updated>2021-06-22T01:57:10.058Z</updated>
        <summary type="html"><![CDATA[There has recently been significant interest in training reinforcement
learning (RL) agents in vision-based environments. This poses many challenges,
such as high dimensionality and potential for observational overfitting through
spurious correlations. A promising approach to solve both of these problems is
a self-attention bottleneck, which provides a simple and effective framework
for learning high performing policies, even in the presence of distractions.
However, due to poor scalability of attention architectures, these methods do
not scale beyond low resolution visual inputs, using large patches (thus small
attention matrices). In this paper we make use of new efficient attention
algorithms, recently shown to be highly effective for Transformers, and
demonstrate that these new techniques can be applied in the RL setting. This
allows our attention-based controllers to scale to larger visual inputs, and
facilitate the use of smaller patches, even individual pixels, improving
generalization. In addition, we propose a new efficient algorithm approximating
softmax attention with what we call hybrid random features, leveraging the
theory of angular kernels. We show theoretically and empirically that hybrid
random features is a promising approach when using attention for vision-based
RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santara_A/0/1/0/all/0/1"&gt;Anirban Santara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Representation Learning with Feedback for Single Image Deraining. (arXiv:2101.12463v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.12463</id>
        <link href="http://arxiv.org/abs/2101.12463"/>
        <updated>2021-06-22T01:57:10.043Z</updated>
        <summary type="html"><![CDATA[A deraining network can be interpreted as a conditional generator that aims
at removing rain streaks from image. Most existing image deraining methods
ignore model errors caused by uncertainty that reduces embedding quality.
Unlike existing image deraining methods that embed low-quality features into
the model directly, we replace low-quality features by latent high-quality
features. The spirit of closed-loop feedback in the automatic control field is
borrowed to obtain latent high-quality features. A new method for error
detection and feature compensation is proposed to address model errors.
Extensive experiments on benchmark datasets as well as specific real datasets
demonstrate that the proposed method outperforms recent state-of-the-art
methods. Code is available at: \\ https://github.com/LI-Hao-SJTU/DerainRLNet]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chenghao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Space-time Neural Irradiance Fields for Free-Viewpoint Video. (arXiv:2011.12950v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12950</id>
        <link href="http://arxiv.org/abs/2011.12950"/>
        <updated>2021-06-22T01:57:10.038Z</updated>
        <summary type="html"><![CDATA[We present a method that learns a spatiotemporal neural irradiance field for
dynamic scenes from a single video. Our learned representation enables
free-viewpoint rendering of the input video. Our method builds upon recent
advances in implicit representations. Learning a spatiotemporal irradiance
field from a single video poses significant challenges because the video
contains only one observation of the scene at any point in time. The 3D
geometry of a scene can be legitimately represented in numerous ways since
varying geometry (motion) can be explained with varying appearance and vice
versa. We address this ambiguity by constraining the time-varying geometry of
our dynamic scene representation using the scene depth estimated from video
depth estimation methods, aggregating contents from individual frames into a
single global representation. We provide an extensive quantitative evaluation
and demonstrate compelling free-viewpoint rendering results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xian_W/0/1/0/all/0/1"&gt;Wenqi Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jia-Bin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_J/0/1/0/all/0/1"&gt;Johannes Kopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Changil Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Distortion for Learned Video Compression. (arXiv:2004.09508v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09508</id>
        <link href="http://arxiv.org/abs/2004.09508"/>
        <updated>2021-06-22T01:57:10.032Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a novel adversarial lossy video compression model.
At extremely low bit-rates, standard video coding schemes suffer from
unpleasant reconstruction artifacts such as blocking, ringing etc. Existing
learned neural approaches to video compression have achieved reasonable success
on reducing the bit-rate for efficient transmission and reduce the impact of
artifacts to an extent. However, they still tend to produce blurred results
under extreme compression. In this paper, we present a deep adversarial learned
video compression model that minimizes an auxiliary adversarial distortion
objective. We find this adversarial objective to correlate better with human
perceptual quality judgement relative to traditional quality metrics such as
MS-SSIM and PSNR. Our experiments using a state-of-the-art learned video
compression system demonstrate a reduction of perceptual artifacts and
reconstruction of detail lost especially under extremely high compression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Veerabadran_V/0/1/0/all/0/1"&gt;Vijay Veerabadran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habibian_A/0/1/0/all/0/1"&gt;Amirhossein Habibian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Semantic Description of Objects based on Prototype Theory. (arXiv:1906.03365v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.03365</id>
        <link href="http://arxiv.org/abs/1906.03365"/>
        <updated>2021-06-22T01:57:10.026Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a novel semantic description approach inspired on
Prototype Theory foundations. We propose a Computational Prototype Model (CPM)
that encodes and stores the central semantic meaning of objects category: the
semantic prototype. Also, we introduce a Prototype-based Description Model that
encodes the semantic meaning of an object while describing its features using
our CPM model. Our description method uses semantic prototypes computed by
CNN-classifications models to create discriminative signatures that describe an
object highlighting its most distinctive features within the category. Our
experiments show that: i) our CPM model (semantic prototype + distance metric)
is able to describe the internal semantic structure of objects categories; ii)
our semantic distance metric can be understood as the object visual typicality
score within a category; iii) our descriptor encoding is semantically
interpretable and significantly outperforms other image global encodings in
clustering and classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pino_O/0/1/0/all/0/1"&gt;Omar Vidal Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1"&gt;Erickson Rangel Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Campos_M/0/1/0/all/0/1"&gt;Mario Fernando Montenegro Campos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Invariant Adversarial Learning. (arXiv:2104.00322v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00322</id>
        <link href="http://arxiv.org/abs/2104.00322"/>
        <updated>2021-06-22T01:57:10.021Z</updated>
        <summary type="html"><![CDATA[The phenomenon of adversarial examples illustrates one of the most basic
vulnerabilities of deep neural networks. Among the variety of techniques
introduced to surmount this inherent weakness, adversarial training has emerged
as the most common and efficient strategy to achieve robustness. Typically,
this is achieved by balancing robust and natural objectives. In this work, we
aim to achieve better trade-off between robust and natural performances by
enforcing a domain-invariant feature representation. We present a new
adversarial training method, Domain Invariant Adversarial Learning (DIAL),
which learns a feature representation which is both robust and domain
invariant. DIAL uses a variant of Domain Adversarial Neural Network (DANN) on
the natural domain and its corresponding adversarial domain. In a case where
the source domain consists of natural examples and the target domain is the
adversarially perturbed examples, our method learns a feature representation
constrained not to discriminate between the natural and adversarial examples,
and can therefore achieve a more robust representation. Our experiments
indicate that our method improves both robustness and natural accuracy, when
compared to current state-of-the-art adversarial training methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levi_M/0/1/0/all/0/1"&gt;Matan Levi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Attias_I/0/1/0/all/0/1"&gt;Idan Attias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontorovich_A/0/1/0/all/0/1"&gt;Aryeh Kontorovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Localize in New Environments from Synthetic Training Data. (arXiv:2011.04539v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04539</id>
        <link href="http://arxiv.org/abs/2011.04539"/>
        <updated>2021-06-22T01:57:10.005Z</updated>
        <summary type="html"><![CDATA[Most existing approaches for visual localization either need a detailed 3D
model of the environment or, in the case of learning-based methods, must be
retrained for each new scene. This can either be very expensive or simply
impossible for large, unknown environments, for example in search-and-rescue
scenarios. Although there are learning-based approaches that operate
scene-agnostically, the generalization capability of these methods is still
outperformed by classical approaches. In this paper, we present an approach
that can generalize to new scenes by applying specific changes to the model
architecture, including an extended regression part, the use of hierarchical
correlation layers, and the exploitation of scale and uncertainty information.
Our approach outperforms the 5-point algorithm using SIFT features on equally
big images and additionally surpasses all previous learning-based approaches
that were trained on different data. It is also superior to most of the
approaches that were specifically trained on the respective scenes. We also
evaluate our approach in a scenario where only very few reference images are
available, showing that under such more realistic conditions our learning-based
approach considerably exceeds both existing learning-based and classical
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Winkelbauer_D/0/1/0/all/0/1"&gt;Dominik Winkelbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denninger_M/0/1/0/all/0/1"&gt;Maximilian Denninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1"&gt;Rudolph Triebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction. (arXiv:2106.01609v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01609</id>
        <link href="http://arxiv.org/abs/2106.01609"/>
        <updated>2021-06-22T01:57:09.997Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of Chinese Grammatical Error Correction (CGEC) and
present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive
sequence prediction to address the deep issues hidden in CGEC. Considering that
most tokens are correct and can be conveyed directly from source to target, and
the error positions can be estimated and corrected based on the bidirectional
context information, thus we employ a BERT-initialized Transformer Encoder as
the backbone model to conduct information modeling and conveying. Considering
that only relying on the same position substitution cannot handle the
variable-length correction cases, various operations such substitution,
deletion, insertion, and local paraphrasing are required jointly. Therefore, a
Conditional Random Fields (CRF) layer is stacked on the up tail to conduct
non-autoregressive sequence prediction by modeling the token dependencies.
Since most tokens are correct and easily to be predicted/conveyed to the
target, then the models may suffer from a severe class imbalance issue. To
alleviate this problem, focal loss penalty strategies are integrated into the
loss functions. Moreover, besides the typical fix-length error correction
datasets, we also construct a variable-length corpus to conduct experiments.
Experimental results on standard datasets, especially on the variable-length
datasets, demonstrate the effectiveness of TtT in terms of sentence-level
Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and
Correction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Object Detection for Autonomous Driving: A Survey. (arXiv:2106.10823v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10823</id>
        <link href="http://arxiv.org/abs/2106.10823"/>
        <updated>2021-06-22T01:57:09.991Z</updated>
        <summary type="html"><![CDATA[Autonomous driving is regarded as one of the most promising remedies to
shield human beings from severe crashes. To this end, 3D object detection
serves as the core basis of such perception system especially for the sake of
path planning, motion prediction, collision avoidance, etc. Generally, stereo
or monocular images with corresponding 3D point clouds are already standard
layout for 3D object detection, out of which point clouds are increasingly
prevalent with accurate depth information being provided. Despite existing
efforts, 3D object detection on point clouds is still in its infancy due to
high sparseness and irregularity of point clouds by nature, misalignment view
between camera view and LiDAR bird's eye of view for modality synergies,
occlusions and scale variations at long distances, etc. Recently, profound
progress has been made in 3D object detection, with a large body of literature
being investigated to address this vision task. As such, we present a
comprehensive review of the latest progress in this field covering all the main
topics including sensors, fundamentals, and the recent state-of-the-art
detection methods with their pros and cons. Furthermore, we introduce metrics
and provide quantitative comparisons on popular public datasets. The avenues
for future work are going to be judiciously identified after an in-deep
analysis of the surveyed works. Finally, we conclude this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1"&gt;Xin Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xirong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sequence-to-Set Network for Nested Named Entity Recognition. (arXiv:2105.08901v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08901</id>
        <link href="http://arxiv.org/abs/2105.08901"/>
        <updated>2021-06-22T01:57:09.986Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) is a widely studied task in natural language
processing. Recently, a growing number of studies have focused on the nested
NER. The span-based methods, considering the entity recognition as a span
classification task, can deal with nested entities naturally. But they suffer
from the huge search space and the lack of interactions between entities. To
address these issues, we propose a novel sequence-to-set neural network for
nested NER. Instead of specifying candidate spans in advance, we provide a
fixed set of learnable vectors to learn the patterns of the valuable spans. We
utilize a non-autoregressive decoder to predict the final set of entities in
one pass, in which we are able to capture dependencies between entities.
Compared with the sequence-to-sequence method, our model is more suitable for
such unordered recognition task as it is insensitive to the label order. In
addition, we utilize the loss function based on bipartite matching to compute
the overall training loss. Experimental results show that our proposed model
achieves state-of-the-art on three nested NER corpora: ACE 2004, ACE 2005 and
KBP 2017. The code is available at
https://github.com/zqtan1024/sequence-to-set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zeqi Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yongliang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Weiming Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Survey of Regularization and Normalization in GANs. (arXiv:2008.08930v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08930</id>
        <link href="http://arxiv.org/abs/2008.08930"/>
        <updated>2021-06-22T01:57:09.981Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been widely applied in different
scenarios thanks to the development of deep neural networks. The original GAN
was proposed based on the non-parametric assumption of the infinite capacity of
networks. However, it is still unknown whether GANs can generate realistic
samples without any prior information. Due to the overconfident assumption,
many issues remain unaddressed in GANs' training, such as non-convergence, mode
collapses, gradient vanishing. Regularization and normalization are common
methods of introducing prior information to stabilize training and improve
discrimination. Although a handful number of regularization and normalization
methods have been proposed for GANs, to the best of our knowledge, there exists
no comprehensive survey which primarily focuses on objectives and development
of these methods, apart from some in-comprehensive and limited scope studies.
In this work, we conduct a comprehensive survey on the regularization and
normalization techniques from different perspectives of GANs training. First,
we systematically describe different perspectives of GANs training and thus
obtain the different objectives of regularization and normalization. Based on
these objectives, we propose a new taxonomy. Furthermore, we compare the
performance of the mainstream methods on different datasets and investigate the
regularization and normalization techniques that have been frequently employed
in SOTA GANs. Finally, we highlight potential future directions of research in
this domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xintian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1"&gt;Rentuo Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1"&gt;Pengfei Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Noise for Multi-Attack Robustness. (arXiv:2006.12135v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12135</id>
        <link href="http://arxiv.org/abs/2006.12135"/>
        <updated>2021-06-22T01:57:09.965Z</updated>
        <summary type="html"><![CDATA[Adversarial learning has emerged as one of the successful techniques to
circumvent the susceptibility of existing methods against adversarial
perturbations. However, the majority of existing defense methods are tailored
to defend against a single category of adversarial perturbation (e.g.
$\ell_\infty$-attack). In safety-critical applications, this makes these
methods extraneous as the attacker can adopt diverse adversaries to deceive the
system. Moreover, training on multiple perturbations simultaneously
significantly increases the computational overhead during training. To address
these challenges, we propose a novel meta-learning framework that explicitly
learns to generate noise to improve the model's robustness against multiple
types of attacks. Its key component is Meta Noise Generator (MNG) that outputs
optimal noise to stochastically perturb a given sample, such that it helps
lower the error on diverse adversarial perturbations. By utilizing samples
generated by MNG, we train a model by enforcing the label consistency across
multiple perturbations. We validate the robustness of models trained by our
scheme on various datasets and against a wide variety of perturbations,
demonstrating that it significantly outperforms the baselines across multiple
perturbations with a marginal computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Face Manipulation Detection via Feature Whitening. (arXiv:2106.10834v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10834</id>
        <link href="http://arxiv.org/abs/2106.10834"/>
        <updated>2021-06-22T01:57:09.942Z</updated>
        <summary type="html"><![CDATA[Why should we trust the detections of deep neural networks for manipulated
faces? Understanding the reasons is important for users in improving the
fairness, reliability, privacy and trust of the detection models. In this work,
we propose an interpretable face manipulation detection approach to achieve the
trustworthy and accurate inference. The approach could make the face
manipulation detection process transparent by embedding the feature whitening
module. This module aims to whiten the internal working mechanism of deep
networks through feature decorrelation and feature constraint. The experimental
results demonstrate that our proposed approach can strike a balance between the
detection accuracy and the model interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yingying Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Daichi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengju Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shiming Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure-Grounded Pretraining for Text-to-SQL. (arXiv:2010.12773v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12773</id>
        <link href="http://arxiv.org/abs/2010.12773"/>
        <updated>2021-06-22T01:57:09.935Z</updated>
        <summary type="html"><![CDATA[Learning to capture text-table alignment is essential for tasks like
text-to-SQL. A model needs to correctly recognize natural language references
to columns and values and to ground them in the given database schema. In this
paper, we present a novel weakly supervised Structure-Grounded pretraining
framework (StruG) for text-to-SQL that can effectively learn to capture
text-table alignment based on a parallel text-table corpus. We identify a set
of novel prediction tasks: column grounding, value grounding and column-value
mapping, and leverage them to pretrain a text-table encoder. Additionally, to
evaluate different methods under more realistic text-table alignment settings,
we create a new evaluation set Spider-Realistic based on Spider dev set with
explicit mentions of column names removed, and adopt eight existing text-to-SQL
datasets for cross-database evaluation. STRUG brings significant improvement
over BERT-LARGE in all settings. Compared with existing pretraining methods
such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms
all baselines on more realistic sets. All the code and data used in this work
is public available at https://aka.ms/strug.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meek_C/0/1/0/all/0/1"&gt;Christopher Meek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1"&gt;Oleksandr Polozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Huan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1"&gt;Matthew Richardson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-term Pedestrian Trajectory Prediction using Mutable Intention Filter and Warp LSTM. (arXiv:2007.00113v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00113</id>
        <link href="http://arxiv.org/abs/2007.00113"/>
        <updated>2021-06-22T01:57:09.919Z</updated>
        <summary type="html"><![CDATA[Trajectory prediction is one of the key capabilities for robots to safely
navigate and interact with pedestrians. Critical insights from human intention
and behavioral patterns need to be integrated to effectively forecast long-term
pedestrian behavior. Thus, we propose a framework incorporating a Mutable
Intention Filter and a Warp LSTM (MIF-WLSTM) to simultaneously estimate human
intention and perform trajectory prediction. The Mutable Intention Filter is
inspired by particle filtering and genetic algorithms, where particles
represent intention hypotheses that can be mutated throughout the pedestrian
motion. Instead of predicting sequential displacement over time, our Warp LSTM
learns to generate offsets on a full trajectory predicted by a nominal
intention-aware linear model, which considers the intention hypotheses during
filtering process. Through experiments on a publicly available dataset, we show
that our method outperforms baseline approaches and demonstrate the robust
performance of our method under abnormal intention-changing scenarios. Code is
available at https://github.com/tedhuang96/mifwlstm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1"&gt;Aamir Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kazuki Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruohua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1"&gt;Katherine Driggs-Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solution for Large-scale Long-tailed Recognition with Noisy Labels. (arXiv:2106.10683v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10683</id>
        <link href="http://arxiv.org/abs/2106.10683"/>
        <updated>2021-06-22T01:57:09.913Z</updated>
        <summary type="html"><![CDATA[This is a technical report for CVPR 2021 AliProducts Challenge. AliProducts
Challenge is a competition proposed for studying the large-scale and
fine-grained commodity image recognition problem encountered by worldleading
ecommerce companies. The large-scale product recognition simultaneously meets
the challenge of noisy annotations, imbalanced (long-tailed) data distribution
and fine-grained classification. In our solution, we adopt stateof-the-art
model architectures of both CNNs and Transformer, including ResNeSt,
EfficientNetV2, and DeiT. We found that iterative data cleaning, classifier
weight normalization, high-resolution finetuning, and test time augmentation
are key components to improve the performance of training with the noisy and
imbalanced dataset. Finally, we obtain 6.4365% mean class error rate in the
leaderboard with our ensemble model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xian_Y/0/1/0/all/0/1"&gt;Yuqiao Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1"&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fufu Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Assessment of Generalization Performance Robustness for Deep Networks via Contrastive Examples. (arXiv:2106.10653v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10653</id>
        <link href="http://arxiv.org/abs/2106.10653"/>
        <updated>2021-06-22T01:57:09.898Z</updated>
        <summary type="html"><![CDATA[Training images with data transformations have been suggested as contrastive
examples to complement the testing set for generalization performance
evaluation of deep neural networks (DNNs). In this work, we propose a practical
framework ContRE (The word "contre" means "against" or "versus" in French.)
that uses Contrastive examples for DNN geneRalization performance Estimation.
Specifically, ContRE follows the assumption in contrastive learning that robust
DNN models with good generalization performance are capable of extracting a
consistent set of features and making consistent predictions from the same
image under varying data transformations. Incorporating with a set of
randomized strategies for well-designed data transformations over the training
set, ContRE adopts classification errors and Fisher ratios on the generated
contrastive examples to assess and analyze the generalization performance of
deep models in complement with a testing set. To show the effectiveness and the
efficiency of ContRE, extensive experiments have been done using various DNN
models on three open source benchmark datasets with thorough ablation studies
and applicability analyses. Our experiment results confirm that (1) behaviors
of deep models on contrastive examples are strongly correlated to what on the
testing set, and (2) ContRE is a robust measure of generalization performance
complementing to the testing set in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xuanyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation. (arXiv:2106.10812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10812</id>
        <link href="http://arxiv.org/abs/2106.10812"/>
        <updated>2021-06-22T01:57:09.892Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptive classification intends to improve
theclassification performance on unlabeled target domain. To alleviate the
adverse effect of domain shift, many approaches align the source and target
domains in the feature space. However, a feature is usually taken as a whole
for alignment without explicitly making domain alignment proactively serve the
classification task, leading to sub-optimal solution. What sub-feature should
be aligned for better adaptation is under-explored. In this paper, we propose
an effective Task-oriented Alignment (ToAlign) for unsupervised domain
adaptation (UDA). We study what features should be aligned across domains and
propose to make the domain alignment proactively serve classification by
performing feature decomposition and alignment under the guidance of the prior
knowledge induced from the classification taskitself. Particularly, we
explicitly decompose a feature in the source domain intoa
task-related/discriminative feature that should be aligned, and a
task-irrelevant feature that should be avoided/ignored, based on the
classification meta-knowledge. Extensive experimental results on various
benchmarks (e.g., Office-Home, Visda-2017, and DomainNet) under different
domain adaptation settings demonstrate theeffectiveness of ToAlign which helps
achieve the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1"&gt;Guoqiang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. (arXiv:2106.10686v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10686</id>
        <link href="http://arxiv.org/abs/2106.10686"/>
        <updated>2021-06-22T01:57:09.886Z</updated>
        <summary type="html"><![CDATA[Despite recent progress of automatic medical image segmentation techniques,
fully automatic results usually fail to meet the clinical use and typically
require further refinement. In this work, we propose a quality-aware memory
network for interactive segmentation of 3D medical images. Provided by user
guidance on an arbitrary slice, an interaction network is firstly employed to
obtain an initial 2D segmentation. The quality-aware memory network
subsequently propagates the initial segmentation estimation bidirectionally
over the entire volume. Subsequent refinement based on additional user guidance
on other slices can be incorporated in the same manner. To further facilitate
interactive segmentation, a quality assessment module is introduced to suggest
the next slice to segment based on the current segmentation quality of each
slice. The proposed network has two appealing characteristics: 1) The
memory-augmented network offers the ability to quickly encode past segmentation
information, which will be retrieved for the segmentation of other slices; 2)
The quality assessment module enables the model to directly estimate the
qualities of segmentation predictions, which allows an active learning paradigm
where users preferentially label the lowest-quality slice for multi-round
refinement. The proposed network leads to a robust interactive segmentation
engine, which can generalize well to various types of user annotations (e.g.,
scribbles, boxes). Experimental results on various medical datasets demonstrate
the superiority of our approach in comparison with existing techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianfei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liulei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1"&gt;Gustav Bredell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianwu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1"&gt;Ender Konukoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mobile Sensing for Multipurpose Applications in Transportation. (arXiv:2106.10733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10733</id>
        <link href="http://arxiv.org/abs/2106.10733"/>
        <updated>2021-06-22T01:57:09.871Z</updated>
        <summary type="html"><![CDATA[Routine and consistent data collection is required to address contemporary
transportation issues.The cost of data collection increases significantly when
sophisticated machines are used to collect data. Due to this constraint, State
Departments of Transportation struggles to collect consistent data for
analyzing and resolving transportation problems in a timely manner. Recent
advancements in the sensors integrated into smartphones have resulted in a more
affordable method of data collection.The primary objective of this study is to
develop and implement a smartphone application for data collection.The
currently designed app consists of three major modules: a frontend graphical
user interface (GUI), a sensor module, and a backend module. While the frontend
user interface enables interaction with the app, the sensor modules collect
relevant data such as video and accelerometer readings while the app is in use.
The backend, on the other hand, is made up of firebase storage, which is used
to store the gathered data.In comparison to other developed apps for collecting
pavement information, this current app is not overly reliant on the internet
enabling the app to be used in areas of restricted internet access.The
developed application was evaluated by collecting data on the i70W highway
connecting Columbia, Missouri, and Kansas City, Missouri.The data was analyzed
for a variety of purposes, including calculating the International Roughness
Index (IRI), identifying pavement distresses, and understanding driver's
behaviour and environment .The results of the application indicate that the
data collected by the app is of high quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aboah_A/0/1/0/all/0/1"&gt;Armstrong Aboah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boeding_M/0/1/0/all/0/1"&gt;Michael Boeding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_Y/0/1/0/all/0/1"&gt;Yaw Adu-Gyamfi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Sparse R-CNN for Direct Scene Graph Generation. (arXiv:2106.10815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10815</id>
        <link href="http://arxiv.org/abs/2106.10815"/>
        <updated>2021-06-22T01:57:09.853Z</updated>
        <summary type="html"><![CDATA[Scene graph generation (SGG) is to detect entity pairs with their relations
in an image. Existing SGG approaches often use multi-stage pipelines to
decompose this task into object detection, relation graph construction, and
dense or dense-to-sparse relation prediction. Instead, from a perspective on
SGG as a direct set prediction, this paper presents a simple, sparse, and
unified framework for relation detection, termed as Structured Sparse R-CNN.
The key to our method is a set of learnable triplet queries and structured
triplet detectors which could be jointly optimized from the training set in an
end-to-end manner. Specifically, the triplet queries encode the general prior
for entity pair locations, categories, and their relations, and provide an
initial guess of relation detection for subsequent refinement. The triplet
detector presents a cascaded dynamic head design to progressively refine the
results of relation detection. In addition, to relieve the training difficulty
of Structured Sparse R-CNN, we propose a relaxed and enhanced training strategy
based on knowledge distillation from a Siamese Sparse R-CNN. We also propose
adaptive focusing parameter and average logit approach for imbalance data
distribution. We perform experiments on two benchmarks: Visual Genome and Open
Images, and the results demonstrate that our method achieves the
state-of-the-art performance. Meanwhile, we perform in-depth ablation studies
to provide insights on our structured modeling in triplet detector design and
training strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1"&gt;Yao Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. (arXiv:2106.10689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10689</id>
        <link href="http://arxiv.org/abs/2106.10689"/>
        <updated>2021-06-22T01:57:09.848Z</updated>
        <summary type="html"><![CDATA[We present a novel neural surface reconstruction method, called NeuS, for
reconstructing objects and scenes with high fidelity from 2D image inputs.
Existing neural surface reconstruction approaches, such as DVR and IDR, require
foreground mask as supervision, easily get trapped in local minima, and
therefore struggle with the reconstruction of objects with severe
self-occlusion or thin structures. Meanwhile, recent neural methods for novel
view synthesis, such as NeRF and its variants, use volume rendering to produce
a neural scene representation with robustness of optimization, even for highly
complex objects. However, extracting high-quality surfaces from this learned
implicit representation is difficult because there are not sufficient surface
constraints in the representation. In NeuS, we propose to represent a surface
as the zero-level set of a signed distance function (SDF) and develop a new
volume rendering method to train a neural SDF representation. We observe that
the conventional volume rendering method causes inherent geometric errors (i.e.
bias) for surface reconstruction, and therefore propose a new formulation that
is free of bias in the first order of approximation, thus leading to more
accurate surface reconstruction even without the mask supervision. Experiments
on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the
state-of-the-arts in high-quality surface reconstruction, especially for
objects and scenes with complex structures and self-occlusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1"&gt;Taku Komura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10681</id>
        <link href="http://arxiv.org/abs/2106.10681"/>
        <updated>2021-06-22T01:57:09.842Z</updated>
        <summary type="html"><![CDATA[Visual information extraction (VIE) has attracted increasing attention in
recent years. The existing methods usually first organized optical character
recognition (OCR) results into plain texts and then utilized token-level entity
annotations as supervision to train a sequence tagging model. However, it
expends great annotation costs and may be exposed to label confusion, and the
OCR errors will also significantly affect the final performance. In this paper,
we propose a unified weakly-supervised learning framework called TCPN (Tag,
Copy or Predict Network), which introduces 1) an efficient encoder to
simultaneously model the semantic and layout information in 2D OCR results; 2)
a weakly-supervised training strategy that utilizes only key information
sequences as supervision; and 3) a flexible and switchable decoder which
contains two inference modes: one (Copy or Predict Mode) is to output key
information sequences of different categories by copying a token from the input
or predicting one in each time step, and the other (Tag Mode) is to directly
tag the input sequence in a single forward pass. Our method shows new
state-of-the-art performance on several public benchmarks, which fully proves
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weihong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kai Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yichao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implementing a Detection System for COVID-19 based on Lung Ultrasound Imaging and Deep Learning. (arXiv:2106.10651v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10651</id>
        <link href="http://arxiv.org/abs/2106.10651"/>
        <updated>2021-06-22T01:57:09.836Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic started in China in December 2019 and quickly spread to
several countries. The consequences of this pandemic are incalculable, causing
the death of millions of people and damaging the global economy. To achieve
large-scale control of this pandemic, fast tools for detection and treatment of
patients are needed. Thus, the demand for alternative tools for the diagnosis
of COVID-19 has increased dramatically since accurated and automated tools are
not available. In this paper we present the ongoing work on a system for
COVID-19 detection using ultrasound imaging and using Deep Learning techniques.
Furthermore, such a system is implemented on a Raspberry Pi to make it portable
and easy to use in remote regions without an Internet connection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rojas_Azabache_C/0/1/0/all/0/1"&gt;Carlos Rojas-Azabache&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vilca_Janampa_K/0/1/0/all/0/1"&gt;Karen Vilca-Janampa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guerrero_Huayta_R/0/1/0/all/0/1"&gt;Renzo Guerrero-Huayta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nunez_Fernandez_D/0/1/0/all/0/1"&gt;Dennis N&amp;#xfa;&amp;#xf1;ez-Fern&amp;#xe1;ndez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-scale image segmentation based on distributed clustering algorithms. (arXiv:2106.10795v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10795</id>
        <link href="http://arxiv.org/abs/2106.10795"/>
        <updated>2021-06-22T01:57:09.829Z</updated>
        <summary type="html"><![CDATA[Many approaches to 3D image segmentation are based on hierarchical clustering
of supervoxels into image regions. Here we describe a distributed algorithm
capable of handling a tremendous number of supervoxels. The algorithm works
recursively, the regions are divided into chunks that are processed
independently in parallel by multiple workers. At each round of the recursive
procedure, the chunk size in all dimensions are doubled until a single chunk
encompasses the entire image. The final result is provably independent of the
chunking scheme, and the same as if the entire image were processed without
division into chunks. This is nontrivial because a pair of adjacent regions is
scored by some statistical property (e.g. mean or median) of the affinities at
the interface, and the interface may extend over arbitrarily many chunks. The
trick is to delay merge decisions for regions that touch chunk boundaries, and
only complete them in a later round after the regions are fully contained
within a chunk. We demonstrate the algorithm by clustering an affinity graph
with over 1.5 trillion edges between 135 billion supervoxels derived from a 3D
electron microscopic brain image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Ran Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zlateski_A/0/1/0/all/0/1"&gt;Aleksandar Zlateski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1"&gt;H. Sebastian Seung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10777</id>
        <link href="http://arxiv.org/abs/2106.10777"/>
        <updated>2021-06-22T01:57:09.811Z</updated>
        <summary type="html"><![CDATA[We propose a manifold matching approach to generative models which includes a
distribution generator (or data generator) and a metric generator. In our
framework, we view the real data set as some manifold embedded in a
high-dimensional Euclidean space. The distribution generator aims at generating
samples that follow some distribution condensed around the real data manifold.
It is achieved by matching two sets of points using their geometric shape
descriptors, such as centroid and $p$-diameter, with learned distance metric;
the metric generator utilizes both real data and generated samples to learn a
distance metric which is close to some intrinsic geodesic distance on the real
data manifold. The produced distance metric is further used for manifold
matching. The two networks are learned simultaneously during the training
process. We apply the approach on both unsupervised and supervised learning
tasks: in unconditional image generation task, the proposed method obtains
competitive results compared with existing generative models; in
super-resolution task, we incorporate the framework in perception-based models
and improve visual qualities by producing samples with more natural textures.
Both theoretical analysis and real data experiments guarantee the feasibility
and effectiveness of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1"&gt;Mengyu Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1"&gt;Haibin Hang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency. (arXiv:2106.10649v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10649</id>
        <link href="http://arxiv.org/abs/2106.10649"/>
        <updated>2021-06-22T01:57:09.805Z</updated>
        <summary type="html"><![CDATA[Backpropagation image saliency aims at explaining model predictions by
estimating model-centric importance of individual pixels in the input. However,
class-insensitivity of the earlier layers in a network only allows saliency
computation with low resolution activation maps of the deeper layers, resulting
in compromised image saliency. Remedifying this can lead to sanity failures. We
propose CAMERAS, a technique to compute high-fidelity backpropagation saliency
maps without requiring any external priors and preserving the map sanity. Our
method systematically performs multi-scale accumulation and fusion of the
activation maps and backpropagated gradients to compute precise saliency maps.
From accurate image saliency to articulation of relative importance of input
features for different models, and precise discrimination between model
perception of visually similar objects, our high-resolution mapping offers
multiple novel insights into the black-box deep visual models, which are
presented in the paper. We also demonstrate the utility of our saliency maps in
adversarial setup by drastically reducing the norm of attack signals by
focusing them on the precise regions identified by our maps. Our method also
inspires new evaluation metrics and a sanity check for this developing research
direction. Code is available here https://github.com/VisMIL/CAMERAS]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalwana_M/0/1/0/all/0/1"&gt;Mohammad A. A. K. Jalwana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbalanced Feature Transport for Exemplar-based Image Translation. (arXiv:2106.10482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10482</id>
        <link href="http://arxiv.org/abs/2106.10482"/>
        <updated>2021-06-22T01:57:09.799Z</updated>
        <summary type="html"><![CDATA[Despite the great success of GANs in images translation with different
conditioned inputs such as semantic segmentation and edge maps, generating
high-fidelity realistic images with reference styles remains a grand challenge
in conditional image-to-image translation. This paper presents a general image
translation framework that incorporates optimal transport for feature alignment
between conditional inputs and style exemplars in image translation. The
introduction of optimal transport mitigates the constraint of many-to-one
feature matching significantly while building up accurate semantic
correspondences between conditional inputs and exemplars. We design a novel
unbalanced optimal transport to address the transport between features with
deviational distributions which exists widely between conditional inputs and
exemplars. In addition, we design a semantic-activation normalization scheme
that injects style features of exemplars into the image translation process
successfully. Extensive experiments over multiple image translation tasks show
that our method achieves superior image translation qualitatively and
quantitatively as compared with the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yingchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1"&gt;Kaiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gongjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jianxiong Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Feiying Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03824</id>
        <link href="http://arxiv.org/abs/2105.03824"/>
        <updated>2021-06-22T01:57:09.793Z</updated>
        <summary type="html"><![CDATA[We show that Transformer encoder architectures can be massively sped up, with
limited accuracy costs, by replacing the self-attention sublayers with simple
linear transformations that "mix" input tokens. These linear transformations,
along with standard nonlinearities in feed-forward layers, prove competent at
modeling semantic relationships in several text classification tasks. Most
surprisingly, we find that replacing the self-attention sublayer in a
Transformer encoder with a standard, unparameterized Fourier Transform achieves
92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains
nearly seven times faster on GPUs and twice as fast on TPUs. The resulting
model, FNet, also scales very efficiently to long inputs. Specifically, when
compared to the "efficient" Transformers on the Long Range Arena benchmark,
FNet matches the accuracy of the most accurate models, but is faster than the
fastest models across all sequence lengths on GPUs (and across relatively
shorter lengths on TPUs). Finally, FNet has a light memory footprint and is
particularly efficient at smaller model sizes: for a fixed speed and accuracy
budget, small FNet models outperform Transformer counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1"&gt;James Lee-Thorp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1"&gt;Ilya Eckstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Ontanon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-to-many Approach for Improving Super-Resolution. (arXiv:2106.10437v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10437</id>
        <link href="http://arxiv.org/abs/2106.10437"/>
        <updated>2021-06-22T01:57:09.787Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) is a one-to-many task with multiple possible solutions.
However, previous works were not concerned about this characteristic. For a
one-to-many pipeline, the generator should be able to generate multiple
estimates of the reconstruction, and not be penalized for generating similar
and equally realistic images. To achieve this, we propose adding weighted
pixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable
the generator to generate various images. We modify the strict content loss to
not penalize the stochastic variation in reconstructed images as long as it has
consistent content. Additionally, we observe that there are out-of-focus
regions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We
filter blurry regions in the training data using the method of [10]. Finally,
we modify the discriminator to receive the low-resolution image as a reference
image along with the target image to provide better feedback to the generator.
Using our proposed methods, we were able to improve the performance of ESRGAN
in x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16
perceptual extreme SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sieun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1"&gt;Eunho Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of the facial growth direction with Machine Learning methods. (arXiv:2106.10464v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10464</id>
        <link href="http://arxiv.org/abs/2106.10464"/>
        <updated>2021-06-22T01:57:09.780Z</updated>
        <summary type="html"><![CDATA[First attempts of prediction of the facial growth (FG) direction were made
over half of a century ago. Despite numerous attempts and elapsed time, a
satisfactory method has not been established yet and the problem still poses a
challenge for medical experts. To our knowledge, this paper is the first
Machine Learning approach to the prediction of FG direction. Conducted data
analysis reveals the inherent complexity of the problem and explains the
reasons of difficulty in FG direction prediction based on 2D X-ray images. To
perform growth forecasting, we employ a wide range of algorithms, from logistic
regression, through tree ensembles to neural networks and consider three,
slightly different, problem formulations. The resulting classification accuracy
varies between 71% and 75%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazmierczak_S/0/1/0/all/0/1"&gt;Stanis&amp;#x142;aw Ka&amp;#x17a;mierczak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Juszka_Z/0/1/0/all/0/1"&gt;Zofia Juszka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fudalej_P/0/1/0/all/0/1"&gt;Piotr Fudalej&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1"&gt;Jacek Ma&amp;#x144;dziuk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis. (arXiv:2106.10548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10548</id>
        <link href="http://arxiv.org/abs/2106.10548"/>
        <updated>2021-06-22T01:57:09.764Z</updated>
        <summary type="html"><![CDATA[Visual Question Answering system integrated with Unmanned Aerial Vehicle
(UAV) has a lot of potentials to advance the post-disaster damage assessment
purpose. Providing assistance to affected areas is highly dependent on
real-time data assessment and analysis. Scope of the Visual Question Answering
is to understand the scene and provide query related answer which certainly
faster the recovery process after any disaster. In this work, we address the
importance of \textit{visual question answering (VQA)} task for post-disaster
damage assessment by presenting our recently developed VQA dataset called
\textit{HurMic-VQA} collected during hurricane Michael, and comparing the
performances of baseline VQA models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1"&gt;Argho Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnemoonfar_M/0/1/0/all/0/1"&gt;Maryam Rahnemoonfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Informative Class Activation Maps. (arXiv:2106.10472v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10472</id>
        <link href="http://arxiv.org/abs/2106.10472"/>
        <updated>2021-06-22T01:57:09.759Z</updated>
        <summary type="html"><![CDATA[We study how to evaluate the quantitative information content of a region
within an image for a particular label. To this end, we bridge class activation
maps with information theory. We develop an informative class activation map
(infoCAM). Given a classification task, infoCAM depict how to accumulate
information of partial regions to that of the entire image toward a label.
Thus, we can utilise infoCAM to locate the most informative features for a
label. When applied to an image classification task, infoCAM performs better
than the traditional classification map in the weakly supervised object
localisation task. We achieve state-of-the-art results on Tiny-ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhenyue Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1"&gt;Tom Gedeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robustness of Text-to-SQL Models against Synonym Substitution. (arXiv:2106.01065v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01065</id>
        <link href="http://arxiv.org/abs/2106.01065"/>
        <updated>2021-06-22T01:57:09.754Z</updated>
        <summary type="html"><![CDATA[Recently, there has been significant progress in studying neural networks to
translate text descriptions into SQL queries. Despite achieving good
performance on some public benchmarks, existing text-to-SQL models typically
rely on the lexical matching between words in natural language (NL) questions
and tokens in table schemas, which may render the models vulnerable to attacks
that break the schema linking mechanism. In this work, we investigate the
robustness of text-to-SQL models to synonym substitution. In particular, we
introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for
text-to-SQL translation. NL questions in Spider-Syn are modified from Spider,
by replacing their schema-related words with manually selected synonyms that
reflect real-world question paraphrases. We observe that the accuracy
dramatically drops by eliminating such explicit correspondence between NL
questions and table schemas, even if the synonyms are not adversarially
selected to conduct worst-case adversarial attacks. Finally, we present two
categories of approaches to improve the model robustness. The first category of
approaches utilizes additional synonym annotations for table schemas by
modifying the model input, while the second category is based on adversarial
training. We demonstrate that both categories of approaches significantly
outperform their counterparts without the defense, and the first category of
approaches are more effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1"&gt;Yujian Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiuping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1"&gt;Matthew Purver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1"&gt;John R. Woodward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jinxia Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Pengsheng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10598</id>
        <link href="http://arxiv.org/abs/2106.10598"/>
        <updated>2021-06-22T01:57:09.748Z</updated>
        <summary type="html"><![CDATA[A table arranging data in rows and columns is a very effective data
structure, which has been widely used in business and scientific research.
Considering large-scale tabular data in online and offline documents, automatic
table recognition has attracted increasing attention from the document analysis
community. Though human can easily understand the structure of tables, it
remains a challenge for machines to understand that, especially due to a
variety of different table layouts and styles. Existing methods usually model a
table as either the markup sequence or the adjacency matrix between different
table cells, failing to address the importance of the logical location of table
cells, e.g., a cell is located in the first row and the second column of the
table. In this paper, we reformulate the problem of table structure recognition
as the table graph reconstruction, and propose an end-to-end trainable table
graph reconstruction network (TGRNet) for table structure recognition.
Specifically, the proposed method has two main branches, a cell detection
branch and a cell logical location branch, to jointly predict the spatial
location and the logical location of different cells. Experimental results on
three popular table recognition datasets and a new dataset with table graph
annotations (TableGraph-350K) demonstrate the effectiveness of the proposed
TGRNet for table structure recognition. Code and annotations will be made
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wenyuan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Baosheng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingyong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Summarization through Reinforcement Learning with a 3D Spatio-Temporal U-Net. (arXiv:2106.10528v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10528</id>
        <link href="http://arxiv.org/abs/2106.10528"/>
        <updated>2021-06-22T01:57:09.743Z</updated>
        <summary type="html"><![CDATA[Intelligent video summarization algorithms allow to quickly convey the most
relevant information in videos through the identification of the most essential
and explanatory content while removing redundant video frames. In this paper,
we introduce the 3DST-UNet-RL framework for video summarization. A 3D
spatio-temporal U-Net is used to efficiently encode spatio-temporal information
of the input videos for downstream reinforcement learning (RL). An RL agent
learns from spatio-temporal latent scores and predicts actions for keeping or
rejecting a video frame in a video summary. We investigate if real/inflated 3D
spatio-temporal CNN features are better suited to learn representations from
videos than commonly used 2D image features. Our framework can operate in both,
a fully unsupervised mode and a supervised training mode. We analyse the impact
of prescribed summary lengths and show experimental evidence for the
effectiveness of 3DST-UNet-RL on two commonly used general video summarization
benchmarks. We also applied our method on a medical video summarization task.
The proposed video summarization method has the potential to save storage costs
of ultrasound screening videos as well as to increase efficiency when browsing
patient video data during retrospective analysis or audit without loosing
essential information]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianrui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qingjie Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jun-Jie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1"&gt;Athanasios Vlontzos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10601</id>
        <link href="http://arxiv.org/abs/2106.10601"/>
        <updated>2021-06-22T01:57:09.728Z</updated>
        <summary type="html"><![CDATA[We aim to tackle the challenging yet practical scenery image outpainting task
in this work. Recently, generative adversarial learning has significantly
advanced the image outpainting by producing semantic consistent content for the
given image. However, the existing methods always suffer from the blurry
texture and the artifacts of the generative part, making the overall
outpainting results lack authenticity. To overcome the weakness, this work
investigates a principle way to synthesize texture-rich results by borrowing
pixels from its neighbors (\ie, reference images), named
\textbf{Re}ference-\textbf{G}uided \textbf{O}utpainting (ReGO). Particularly,
the ReGO designs an Adaptive Content Selection (ACS) module to transfer the
pixel of reference images for texture compensating of the target one. To
prevent the style of the generated part from being affected by the reference
images, a style ranking loss is further proposed to augment the ReGO to
synthesize style-consistent results. Extensive experiments on two popular
benchmarks, NS6K~\cite{yangzx} and NS8K~\cite{wang}, well demonstrate the
effectiveness of our ReGO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xueming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Li Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Visual Context for Weakly Supervised Person Search. (arXiv:2106.10506v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10506</id>
        <link href="http://arxiv.org/abs/2106.10506"/>
        <updated>2021-06-22T01:57:09.722Z</updated>
        <summary type="html"><![CDATA[Person search has recently emerged as a challenging task that jointly
addresses pedestrian detection and person re-identification. Existing
approaches follow a fully supervised setting where both bounding box and
identity annotations are available. However, annotating identities is
labor-intensive, limiting the practicability and scalability of current
frameworks. This paper inventively considers weakly supervised person search
with only bounding box annotations. We proposed the first framework to address
this novel task, namely Context-Guided Person Search (CGPS), by investigating
three levels of context clues (i.e., detection, memory and scene) in
unconstrained natural images. The first two are employed to promote local and
global discriminative capabilities, while the latter enhances clustering
accuracy. Despite its simple design, our CGPS boosts the baseline model by 8.3%
in mAP on CUHK-SYSU. Surprisingly, it even achieves comparable performance to
two-step person search models, while displaying higher efficiency. Our code is
available at https://github.com/ljpadam/CGPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yichao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shengcai Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More than Encoder: Introducing Transformer Decoder to Upsample. (arXiv:2106.10637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10637</id>
        <link href="http://arxiv.org/abs/2106.10637"/>
        <updated>2021-06-22T01:57:09.717Z</updated>
        <summary type="html"><![CDATA[General segmentation models downsample images and then upsample to restore
resolution for pixel level prediction. In such schema, upsample technique is
vital in maintaining information for better performance. In this paper, we
present a new upsample approach, Attention Upsample (AU), that could serve as
general upsample method and be incorporated into any segmentation model that
possesses lateral connections. AU leverages pixel-level attention to model long
range dependency and global information for better reconstruction. It consists
of Attention Decoder (AD) and bilinear upsample as residual connection to
complement the upsampled features. AD adopts the idea of decoder from
transformer which upsamples features conditioned on local and detailed
information from contracting path. Moreover, considering the extensive memory
and computation cost of pixel-level attention, we further propose to use window
attention scheme to restrict attention computation in local windows instead of
global range. Incorporating window attention, we denote our decoder as Window
Attention Decoder (WAD) and our upsample method as Window Attention Upsample
(WAU). We test our method on classic U-Net structure with lateral connection to
deliver information from contracting path and achieve state-of-the-arts
performance on Synapse (80.30 DSC and 23.12 HD) and MSD Brain (74.75 DSC)
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yijiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Wentian Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Ying Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiping Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v5 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07464</id>
        <link href="http://arxiv.org/abs/2105.07464"/>
        <updated>2021-06-22T01:57:09.712Z</updated>
        <summary type="html"><![CDATA[Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Guangwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yulin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengjun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CenterAtt: Fast 2-stage Center Attention Network. (arXiv:2106.10493v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10493</id>
        <link href="http://arxiv.org/abs/2106.10493"/>
        <updated>2021-06-22T01:57:09.706Z</updated>
        <summary type="html"><![CDATA[In this technical report, we introduce the methods of HIKVISION_LiDAR_Det in
the challenge of waymo open dataset real-time 3D detection. Our solution for
the competition are built upon Centerpoint 3D detection framework. Several
variants of CenterPoint are explored, including center attention head and
feature pyramid network neck. In order to achieve real time detection, methods
like batchnorm merge, half-precision floating point network and GPU-accelerated
voxelization process are adopted. By using these methods, our team ranks 6th
among all the methods on real-time 3D detection challenge in the waymo open
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jianyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1"&gt;Jian Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xu Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yushi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reversible Colour Density Compression of Images using cGANs. (arXiv:2106.10542v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10542</id>
        <link href="http://arxiv.org/abs/2106.10542"/>
        <updated>2021-06-22T01:57:09.689Z</updated>
        <summary type="html"><![CDATA[Image compression using colour densities is historically impractical to
decompress losslessly. We examine the use of conditional generative adversarial
networks in making this transformation more feasible, through learning a
mapping between the images and a loss function to train on. We show that this
method is effective at producing visually lossless generations, indicating that
efficient colour compression is viable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jose_A/0/1/0/all/0/1"&gt;Arun Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Francis_A/0/1/0/all/0/1"&gt;Abraham Francis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10309</id>
        <link href="http://arxiv.org/abs/2106.10309"/>
        <updated>2021-06-22T01:57:09.684Z</updated>
        <summary type="html"><![CDATA[The costly process of obtaining semantic segmentation labels has driven
research towards weakly supervised semantic segmentation (WSSS) methods, using
only image-level, point, or box labels. The lack of dense scene representation
requires methods to increase complexity to obtain additional semantic
information about the scene, often done through multiple stages of training and
refinement. Current state-of-the-art (SOTA) models leverage image-level labels
to produce class activation maps (CAMs) which go through multiple stages of
refinement before they are thresholded to make pseudo-masks for supervision.
The multi-stage approach is computationally expensive, and dependency on
image-level labels for CAMs generation lacks generalizability to more complex
scenes. In contrary, our method offers a single-stage approach generalizable to
arbitrary dataset, that is trainable from scratch, without any dependency on
pre-trained backbones, classification, or separate refinement tasks. We utilize
point annotations to generate reliable, on-the-fly pseudo-masks through refined
and filtered features. While our method requires point annotations that are
only slightly more expensive than image-level annotations, we are to
demonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as
significantly outperform other SOTA WSSS methods on recent real-world datasets
(CRAID, CityPersons, IAD).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1"&gt;Peri Akiva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1"&gt;Kristin Dana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single View Physical Distance Estimation using Human Pose. (arXiv:2106.10335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10335</id>
        <link href="http://arxiv.org/abs/2106.10335"/>
        <updated>2021-06-22T01:57:09.679Z</updated>
        <summary type="html"><![CDATA[We propose a fully automated system that simultaneously estimates the camera
intrinsics, the ground plane, and physical distances between people from a
single RGB image or video captured by a camera viewing a 3-D scene from a fixed
vantage point. To automate camera calibration and distance estimation, we
leverage priors about human pose and develop a novel direct formulation for
pose-based auto-calibration and distance estimation, which shows
state-of-the-art performance on publicly available datasets. The proposed
approach enables existing camera systems to measure physical distances without
needing a dedicated calibration process or range sensors, and is applicable to
a broad range of use cases such as social distancing and workplace safety.
Furthermore, to enable evaluation and drive research in this area, we
contribute to the publicly available MEVA dataset with additional distance
annotations, resulting in MEVADA -- the first evaluation benchmark in the world
for the pose-based auto-calibration and distance estimation problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1"&gt;Xiaohan Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Henry Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xiangyu Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheong_L/0/1/0/all/0/1"&gt;Lin Lee Cheong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Disentangled Speech Content and Style Representation. (arXiv:2010.12973v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12973</id>
        <link href="http://arxiv.org/abs/2010.12973"/>
        <updated>2021-06-22T01:57:09.674Z</updated>
        <summary type="html"><![CDATA[We present an approach for unsupervised learning of speech representation
disentangling contents and styles. Our model consists of: (1) a local encoder
that captures per-frame information; (2) a global encoder that captures
per-utterance information; and (3) a conditional decoder that reconstructs
speech given local and global latent variables. Our experiments show that (1)
the local latent variables encode speech contents, as reconstructed speech can
be recognized by ASR with low word error rates (WER), even with a different
global encoding; (2) the global latent variables encode speaker style, as
reconstructed speech shares speaker identity with the source utterance of the
global encoding. Additionally, we demonstrate an useful application from our
pre-trained model, where we can train a speaker recognition model from the
global latent variables and achieve high accuracy by fine-tuning with as few
data as one label per speaker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Ruoming Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karita_S/0/1/0/all/0/1"&gt;Shigeki Karita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Contextual Design of Convolutional Neural Network for Steganalysis. (arXiv:2106.10430v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.10430</id>
        <link href="http://arxiv.org/abs/2106.10430"/>
        <updated>2021-06-22T01:57:09.668Z</updated>
        <summary type="html"><![CDATA[In recent times, deep learning-based steganalysis classifiers became popular
due to their state-of-the-art performance. Most deep steganalysis classifiers
usually extract noise residuals using high-pass filters as preprocessing steps
and feed them to their deep model for classification. It is observed that
recent steganographic embedding does not always restrict their embedding in the
high-frequency zone; instead, they distribute it as per embedding policy.
Therefore, besides noise residual, learning the embedding zone is another
challenging task. In this work, unlike the conventional approaches, the
proposed model first extracts the noise residual using learned denoising
kernels to boost the signal-to-noise ratio. After preprocessing, the sparse
noise residuals are fed to a novel Multi-Contextual Convolutional Neural
Network (M-CNET) that uses heterogeneous context size to learn the sparse and
low-amplitude representation of noise residuals. The model performance is
further improved by incorporating the Self-Attention module to focus on the
areas prone to steganalytic embedding. A set of comprehensive experiments is
performed to show the proposed scheme's efficacy over the prior arts. Besides,
an ablation study is given to justify the contribution of various modules of
the proposed architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1"&gt;Brijesh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1"&gt;Arijit Sur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1"&gt;Pinaki Mitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out of Context: A New Clue for Context Modeling of Aspect-based Sentiment Analysis. (arXiv:2106.10816v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10816</id>
        <link href="http://arxiv.org/abs/2106.10816"/>
        <updated>2021-06-22T01:57:09.653Z</updated>
        <summary type="html"><![CDATA[Aspect-based sentiment analysis (ABSA) aims to predict the sentiment
expressed in a review with respect to a given aspect. The core of ABSA is to
model the interaction between the context and given aspect to extract the
aspect-related information. In prior work, attention mechanisms and dependency
graph networks are commonly adopted to capture the relations between the
context and given aspect. And the weighted sum of context hidden states is used
as the final representation fed to the classifier. However, the information
related to the given aspect may be already discarded and adverse information
may be retained in the context modeling processes of existing models. This
problem cannot be solved by subsequent modules and there are two reasons:
first, their operations are conducted on the encoder-generated context hidden
states, whose value cannot change after the encoder; second, existing encoders
only consider the context while not the given aspect. To address this problem,
we argue the given aspect should be considered as a new clue out of context in
the context modeling process. As for solutions, we design several aspect-aware
context encoders based on different backbones: an aspect-aware LSTM and three
aspect-aware BERTs. They are dedicated to generate aspect-aware hidden states
which are tailored for ABSA task. In these aspect-aware context encoders, the
semantics of the given aspect is used to regulate the information flow.
Consequently, the aspect-related information can be retained and
aspect-irrelevant information can be excluded in the generated hidden states.
We conduct extensive experiments on several benchmark datasets with empirical
analysis, demonstrating the efficacies and advantages of our proposed
aspect-aware context encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1"&gt;Bowen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W. Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSN: Efficient Online Mask Selection Network for Video Instance Segmentation. (arXiv:2106.10452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10452</id>
        <link href="http://arxiv.org/abs/2106.10452"/>
        <updated>2021-06-22T01:57:09.648Z</updated>
        <summary type="html"><![CDATA[In this work we present a novel solution for Video Instance
Segmentation(VIS), that is automatically generating instance level segmentation
masks along with object class and tracking them in a video. Our method improves
the masks from segmentation and propagation branches in an online manner using
the Mask Selection Network (MSN) hence limiting the noise accumulation during
mask tracking. We propose an effective design of MSN by using patch-based
convolutional neural network. The network is able to distinguish between very
subtle differences between the masks and choose the better masks out of the
associated masks accurately. Further, we make use of temporal consistency and
process the video sequences in both forward and reverse manner as a post
processing step to recover lost objects. The proposed method can be used to
adapt any video object segmentation method for the task of VIS. Our method
achieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third
place among more than 30 global teams. Our code will be available at
https://github.com/SHI-Labs/Mask-Selection-Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goel_V/0/1/0/all/0/1"&gt;Vidit Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1"&gt;Shubhika Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_H/0/1/0/all/0/1"&gt;Harsh Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Humphrey Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews. (arXiv:2010.11384v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11384</id>
        <link href="http://arxiv.org/abs/2010.11384"/>
        <updated>2021-06-22T01:57:09.643Z</updated>
        <summary type="html"><![CDATA[The flexibility of the inference process in Variational Autoencoders (VAEs)
has recently led to revising traditional probabilistic topic models giving rise
to Neural Topic Models (NTMs). Although these approaches have achieved
significant results, surprisingly very little work has been done on how to
disentangle the latent topics. Existing topic models when applied to reviews
may extract topics associated with writers' subjective opinions mixed with
those related to factual descriptions such as plot summaries in movie and book
reviews. It is thus desirable to automatically separate opinion topics from
plot/neutral ones enabling a better interpretability. In this paper, we propose
a neural topic model combined with adversarial training to disentangle opinion
topics from plot and neutral ones. We conduct an extensive experimental
assessment introducing a new collection of movie and book reviews paired with
their plots, namely MOBO dataset, showing an improved coherence and variety of
topics, a consistent disentanglement rate, and sentiment classification
performance superior to other supervised topic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1"&gt;Gabriele Pergola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1"&gt;Lin Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaZoom: Adaptive Zoom Network for Multi-Scale Object Detection in Large Scenes. (arXiv:2106.10409v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10409</id>
        <link href="http://arxiv.org/abs/2106.10409"/>
        <updated>2021-06-22T01:57:09.637Z</updated>
        <summary type="html"><![CDATA[Detection in large-scale scenes is a challenging problem due to small objects
and extreme scale variation. It is essential to focus on the image regions of
small objects. In this paper, we propose a novel Adaptive Zoom (AdaZoom)
network as a selective magnifier with flexible shape and focal length to
adaptively zoom the focus regions for object detection. Based on policy
gradient, we construct a reinforcement learning framework for focus region
generation, with the reward formulated by object distributions. The scales and
aspect ratios of the generated regions are adaptive to the scales and
distribution of objects inside. We apply variable magnification according to
the scale of the region for adaptive multi-scale detection. We further propose
collaborative training to complementarily promote the performance of AdaZoom
and the detection network. To validate the effectiveness, we conduct extensive
experiments on VisDrone2019, UAVDT, and DOTA datasets. The experiments show
AdaZoom brings a consistent and significant improvement over different
detection networks, achieving state-of-the-art performance on these datasets,
especially outperforming the existing methods by AP of 4.64% on Vis-Drone2019.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jingtao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yali Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengjin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10404</id>
        <link href="http://arxiv.org/abs/2106.10404"/>
        <updated>2021-06-22T01:57:09.632Z</updated>
        <summary type="html"><![CDATA[Works on lottery ticket hypothesis (LTH) and single-shot network pruning
(SNIP) have raised a lot of attention currently on post-training pruning
(iterative magnitude pruning), and before-training pruning (pruning at
initialization). The former method suffers from an extremely large computation
cost and the latter category of methods usually struggles with insufficient
performance. In comparison, during-training pruning, a class of pruning methods
that simultaneously enjoys the training/inference efficiency and the comparable
performance, temporarily, has been less explored. To better understand
during-training pruning, we quantitatively study the effect of pruning
throughout training from the perspective of pruning plasticity (the ability of
the pruned networks to recover the original performance). Pruning plasticity
can help explain several other empirical observations about neural network
pruning in literature. We further find that pruning plasticity can be
substantially improved by injecting a brain-inspired mechanism called
neuroregeneration, i.e., to regenerate the same number of connections as
pruned. Based on the insights from pruning plasticity, we design a novel
gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost
neuroregeneration (GraNet), and its dynamic sparse training (DST) variant
(GraNet-ST). Both of them advance state of the art. Perhaps most impressively,
the latter for the first time boosts the sparse-to-sparse training performance
over various dense-to-sparse methods by a large margin with ResNet-50 on
ImageNet. We will release all codes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaohan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1"&gt;Zahra Atashgahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1"&gt;Lu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1"&gt;Huanyu Kou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1"&gt;Decebal Constantin Mocanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Abstractive Unsupervised Summarization of Online News Discussions. (arXiv:2106.03953v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03953</id>
        <link href="http://arxiv.org/abs/2106.03953"/>
        <updated>2021-06-22T01:57:09.617Z</updated>
        <summary type="html"><![CDATA[Summarization has usually relied on gold standard summaries to train
extractive or abstractive models. Social media brings a hurdle to summarization
techniques since it requires addressing a multi-document multi-author approach.
We address this challenging task by introducing a novel method that generates
abstractive summaries of online news discussions. Our method extends a
BERT-based architecture, including an attention encoding that fed comments'
likes during the training stage. To train our model, we define a task which
consists of reconstructing high impact comments based on popularity (likes).
Accordingly, our model learns to summarize online discussions based on their
most relevant comments. Our novel approach provides a summary that represents
the most relevant aspects of a news item that users comment on, incorporating
the social context as a source of information to summarize texts in online
social networks. Our model is evaluated using ROUGE scores between the
generated summary and each comment on the thread. Our model, including the
social attention encoding, significantly outperforms both extractive and
abstractive summarization methods based on such evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palma_I/0/1/0/all/0/1"&gt;Ignacio Tampe Palma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1"&gt;Marcelo Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1"&gt;Evangelos Milios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A system of vision sensor based deep neural networks for complex driving scene analysis in support of crash risk assessment and prevention. (arXiv:2106.10319v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10319</id>
        <link href="http://arxiv.org/abs/2106.10319"/>
        <updated>2021-06-22T01:57:09.592Z</updated>
        <summary type="html"><![CDATA[To assist human drivers and autonomous vehicles in assessing crash risks,
driving scene analysis using dash cameras on vehicles and deep learning
algorithms is of paramount importance. Although these technologies are
increasingly available, driving scene analysis for this purpose still remains a
challenge. This is mainly due to the lack of annotated large image datasets for
analyzing crash risk indicators and crash likelihood, and the lack of an
effective method to extract lots of required information from complex driving
scenes. To fill the gap, this paper develops a scene analysis system. The
Multi-Net of the system includes two multi-task neural networks that perform
scene classification to provide four labels for each scene. The DeepLab v3 and
YOLO v3 are combined by the system to detect and locate risky pedestrians and
the nearest vehicles. All identified information can provide the situational
awareness to autonomous vehicles or human drivers for identifying crash risks
from the surrounding traffic. To address the scarcity of annotated image
datasets for studying traffic crashes, two completely new datasets have been
developed by this paper and made available to the public, which were proved to
be effective in training the proposed deep neural networks. The paper further
evaluates the performance of the Multi-Net and the efficiency of the developed
system. Comprehensive scene analysis is further illustrated with representative
examples. Results demonstrate the effectiveness of the developed system and
datasets for driving scene analysis, and their supportiveness for crash risk
assessment and crash prevention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Muhammad Monjurul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruwen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaozheng Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Facial Authentication for Public Electric Vehicle Charging Station. (arXiv:2106.10432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10432</id>
        <link href="http://arxiv.org/abs/2106.10432"/>
        <updated>2021-06-22T01:57:09.584Z</updated>
        <summary type="html"><![CDATA[This study is to investigate and compare the facial recognition accuracy
performance of Dlib ResNet against a K-Nearest Neighbour (KNN) classifier.
Particularly when used against a dataset from an Asian ethnicity as Dlib ResNet
was reported to have an accuracy deficiency when it comes to Asian faces. The
comparisons are both implemented on the facial vectors extracted using the
Histogram of Oriented Gradients (HOG) method and use the same dataset for a
fair comparison. Authentication of a user by facial recognition in an electric
vehicle (EV) charging station demonstrates a practical use case for such an
authentication system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haris_M/0/1/0/all/0/1"&gt;Muhamad Amin Husni Abdul Haris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Sin Liang Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings. (arXiv:2012.15484v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15484</id>
        <link href="http://arxiv.org/abs/2012.15484"/>
        <updated>2021-06-22T01:57:09.568Z</updated>
        <summary type="html"><![CDATA[Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,
requires a QA-system to include facts from a diverse knowledge graph (KG) in
its reasoning process to produce an answer. Large KGs, especially common-sense
KGs, are known to be incomplete, i.e., not all non-existent facts are always
incorrect. Therefore, being able to reason over incomplete KGs for QA is a
critical requirement in real-world applications that has not been addressed
extensively in the literature. We develop a novel QA architecture that allows
us to reason over incomplete KGs, something current FVQA state-of-the-art
(SOTA) approaches lack due to their critical reliance on fact retrieval. We use
KG Embeddings, a technique widely used for KG completion, for the downstream
task of FVQA. We also employ a new image representation technique we call
'Image-as-Knowledge' to enable this capability, alongside a simple one-step
CoAttention mechanism to attend to text and image during QA. Our FVQA
architecture is faster during inference time, being O(m), as opposed to
existing FVQA SOTA methods which are O(N log N), where m = number of vertices,
N = number of edges = O(m^2). KG embeddings are shown to hold complementary
information to word embeddings: a combination of both metrics permits
performance comparable to SOTA methods in the standard answer retrieval task,
and significantly better (26% absolute) in the proposed missing-edge reasoning
task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramnath_K/0/1/0/all/0/1"&gt;Kiran Ramnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1"&gt;Mark Hasegawa-Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Speaker Diarization: Recent Advances with Deep Learning. (arXiv:2101.09624v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09624</id>
        <link href="http://arxiv.org/abs/2101.09624"/>
        <updated>2021-06-22T01:57:09.562Z</updated>
        <summary type="html"><![CDATA[Speaker diarization is a task to label audio or video recordings with classes
that correspond to speaker identity, or in short, a task to identify "who spoke
when". In the early years, speaker diarization algorithms were developed for
speech recognition on multispeaker audio recordings to enable speaker adaptive
processing. These algorithms also gained their own value as a standalone
application over time to provide speaker-specific metainformation for
downstream tasks such as audio retrieval. More recently, with the emergence of
deep learning technology, which has driven revolutionary changes in research
and practices across speech application domains, rapid advancements have been
made for speaker diarization. In this paper, we review not only the historical
development of speaker diarization technology but also the recent advancements
in neural speaker diarization approaches. Furthermore, we discuss how speaker
diarization systems have been integrated with speech recognition applications
and how the recent surge of deep learning is leading the way of jointly
modeling these two components to be complementary to each other. By considering
such exciting technical trends, we believe that this paper is a valuable
contribution to the community to provide a survey work by consolidating the
recent developments with neural methods and thus facilitating further progress
toward a more efficient speaker diarization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Park_T/0/1/0/all/0/1"&gt;Tae Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dimitriadis_D/0/1/0/all/0/1"&gt;Dimitrios Dimitriadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1"&gt;Kyu J. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shrikanth Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training. (arXiv:2106.10835v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10835</id>
        <link href="http://arxiv.org/abs/2106.10835"/>
        <updated>2021-06-22T01:57:09.547Z</updated>
        <summary type="html"><![CDATA[With recent advances in distantly supervised (DS) relation extraction (RE),
considerable attention is attracted to leverage multi-instance learning (MIL)
to distill high-quality supervision from the noisy DS. Here, we go beyond label
noise and identify the key bottleneck of DS-MIL to be its low data utilization:
as high-quality supervision being refined by MIL, MIL abandons a large amount
of training instances, which leads to a low data utilization and hinders model
training from having abundant supervision. In this paper, we propose
collaborative adversarial training to improve the data utilization, which
coordinates virtual adversarial training (VAT) and adversarial training (AT) at
different levels. Specifically, since VAT is label-free, we employ the
instance-level VAT to recycle instances abandoned by MIL. Besides, we deploy AT
at the bag-level to unleash the full potential of the high-quality supervision
got by MIL. Our proposed method brings consistent improvements (~ 5 absolute
AUC score) to the previous state of the art, which verifies the importance of
the data utilization issue and the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haochen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1"&gt;Jian Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhigang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1"&gt;Yueting Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Fact Verification with Kernel Graph Attention Network. (arXiv:1910.09796v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.09796</id>
        <link href="http://arxiv.org/abs/1910.09796"/>
        <updated>2021-06-22T01:57:09.534Z</updated>
        <summary type="html"><![CDATA[Fact Verification requires fine-grained natural language inference capability
that finds subtle clues to identify the syntactical and semantically correct
but not well-supported claims. This paper presents Kernel Graph Attention
Network (KGAT), which conducts more fine-grained fact verification with
kernel-based attentions. Given a claim and a set of potential evidence
sentences that form an evidence graph, KGAT introduces node kernels, which
better measure the importance of the evidence node, and edge kernels, which
conduct fine-grained evidence propagation in the graph, into Graph Attention
Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER
score and significantly outperforms existing fact verification models on FEVER,
a large-scale benchmark for fact verification. Our analyses illustrate that,
compared to dot-product attentions, the kernel-based attention concentrates
more on relevant evidence sentences and meaningful clues in the evidence graph,
which is the main source of KGAT's effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Order in the Court: Explainable AI Methods Prone to Disagreement. (arXiv:2105.03287v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03287</id>
        <link href="http://arxiv.org/abs/2105.03287"/>
        <updated>2021-06-22T01:57:09.524Z</updated>
        <summary type="html"><![CDATA[By computing the rank correlation between attention weights and
feature-additive explanation methods, previous analyses either invalidate or
support the role of attention-based explanations as a faithful and plausible
measure of salience. To investigate whether this approach is appropriate, we
compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and
attention-based explanations, applied to two neural architectures trained on
single- and pair-sequence language tasks. In most cases, we find that none of
our chosen methods agree. Based on our empirical observations and theoretical
objections, we conclude that rank correlation does not measure the quality of
feature-additive methods. Practitioners should instead use the numerous and
rigorous diagnostic methods proposed by the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neely_M/0/1/0/all/0/1"&gt;Michael Neely&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schouten_S/0/1/0/all/0/1"&gt;Stefan F. Schouten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bleeker_M/0/1/0/all/0/1"&gt;Maurits J. R. Bleeker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1"&gt;Ana Lucic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains. (arXiv:2106.02792v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02792</id>
        <link href="http://arxiv.org/abs/2106.02792"/>
        <updated>2021-06-22T01:57:09.518Z</updated>
        <summary type="html"><![CDATA[Social media has become a valuable resource for the study of suicidal
ideation and the assessment of suicide risk. Among social media platforms,
Reddit has emerged as the most promising one due to its anonymity and its focus
on topic-based communities (subreddits) that can be indicative of someone's
state of mind or interest regarding mental health disorders such as
r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on
suicide risk assessment has been the small amount of labeled data. We propose
an empirical investigation into several classes of weakly-supervised
approaches, and show that using pseudo-labeling based on related issues around
mental health (e.g., anxiety, depression) helps improve model performance for
suicide risk assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yudong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Institutional Grammar 2.0 Codebook. (arXiv:2008.08937v3 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08937</id>
        <link href="http://arxiv.org/abs/2008.08937"/>
        <updated>2021-06-22T01:57:09.503Z</updated>
        <summary type="html"><![CDATA[The Grammar of Institutions, or Institutional Grammar, is an established
approach to encode policy information in terms of institutional statements
based on a set of pre-defined syntactic components. This codebook provides
coding guidelines for a revised version of the Institutional Grammar, the
Institutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at
facilitating the encoding of policy to meet varying analytical objectives. To
this end, it revises the grammar with respect to comprehensiveness,
flexibility, and specificity by offering multiple levels of expressiveness (IG
Core, IG Extended, IG Logico). In addition to the encoding of regulative
statements, it further introduces the encoding of constitutive institutional
statements, as well as statements that exhibit both constitutive and regulative
characteristics. Introducing those aspects, the codebook initially covers
fundamental concepts of IG 2.0, before providing an overview of pre-coding
steps relevant for document preparation. Detailed coding guidelines are
provided for both regulative and constitutive statements across all levels of
expressiveness, along with the encoding guidelines for statements of mixed form
-- hybrid and polymorphic institutional statements. The document further
provides an overview of taxonomies used in the encoding process and referred to
throughout the codebook. The codebook concludes with a summary and discussion
of relevant considerations to facilitate the coding process. An initial
Reader's Guide helps the reader tailor the content to her interest.

Note that this codebook specifically focuses on operational aspects of IG 2.0
in the context of policy coding. Links to additional resources such as the
underlying scientific literature (that offers a comprehensive treatment of the
underlying theoretical concepts) are referred to in the concluding section of
the codebook.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frantz_C/0/1/0/all/0/1"&gt;Christopher K. Frantz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddiki_S/0/1/0/all/0/1"&gt;Saba N. Siddiki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inducing Language-Agnostic Multilingual Representations. (arXiv:2008.09112v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09112</id>
        <link href="http://arxiv.org/abs/2008.09112"/>
        <updated>2021-06-22T01:57:09.486Z</updated>
        <summary type="html"><![CDATA[Cross-lingual representations have the potential to make NLP techniques
available to the vast majority of languages in the world. However, they
currently require large pretraining corpora or access to typologically similar
languages. In this work, we address these obstacles by removing language
identity signals from multilingual embeddings. We examine three approaches for
this: (i) re-aligning the vector spaces of target languages (all together) to a
pivot source language; (ii) removing language-specific means and variances,
which yields better discriminativeness of embeddings as a by-product; and (iii)
increasing input similarity across languages by removing morphological
contractions and sentence reordering. We evaluate on XNLI and reference-free MT
across 19 typologically diverse languages. Our findings expose the limitations
of these approaches -- unlike vector normalization, vector space re-alignment
and text normalization do not achieve consistent gains across encoders and
languages. Due to the approaches' additive effects, their combination decreases
the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R)
on average across all tasks and languages, however. Our code and models are
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bjerva_J/0/1/0/all/0/1"&gt;Johannes Bjerva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00676</id>
        <link href="http://arxiv.org/abs/2106.00676"/>
        <updated>2021-06-22T01:57:09.481Z</updated>
        <summary type="html"><![CDATA[Classifying the core textual components of a scientific paper-title, author,
body text, etc.-is a critical first step in automated scientific document
understanding. Previous work has shown how using elementary layout information,
i.e., each token's 2D position on the page, leads to more accurate
classification. We introduce new methods for incorporating VIsual LAyout (VILA)
structures, e.g., the grouping of page texts into text lines or text blocks,
into language models to further improve performance. We show that the I-VILA
approach, which simply adds special tokens denoting the boundaries of layout
structures into model inputs, can lead to 1.9% Macro F1 improvements for token
classification. Moreover, we design a hierarchical model, H-VILA, that encodes
the text based on layout structures and record an up-to 47% inference time
reduction with less than 1.5% Macro F1 loss for the text classification models.
Experiments are conducted on a newly curated evaluation suite, S2-VLUE, with a
novel metric measuring classification uniformity within visual groups and a new
dataset of gold annotations covering papers from 19 scientific disciplines.
Pre-trained weights, benchmark datasets, and source code will be available at
https://github.com/allenai/VILA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zejiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lucy Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1"&gt;Doug Downey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-06-22T01:57:09.472Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Brief Study on the Effects of Training Generative Dialogue Models with a Semantic loss. (arXiv:2106.10619v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10619</id>
        <link href="http://arxiv.org/abs/2106.10619"/>
        <updated>2021-06-22T01:57:09.465Z</updated>
        <summary type="html"><![CDATA[Neural models trained for next utterance generation in dialogue task learn to
mimic the n-gram sequences in the training set with training objectives like
negative log-likelihood (NLL) or cross-entropy. Such commonly used training
objectives do not foster generating alternate responses to a context. But, the
effects of minimizing an alternate training objective that fosters a model to
generate alternate response and score it on semantic similarity has not been
well studied. We hypothesize that a language generation model can improve on
its diversity by learning to generate alternate text during training and
minimizing a semantic loss as an auxiliary objective. We explore this idea on
two different sized data sets on the task of next utterance generation in goal
oriented dialogues. We make two observations (1) minimizing a semantic
objective improved diversity in responses in the smaller data set (Frames) but
only as-good-as minimizing the NLL in the larger data set (MultiWoZ) (2) large
language model embeddings can be more useful as a semantic loss objective than
as initialization for token embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1"&gt;Mohamed Abdelsalam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01051</id>
        <link href="http://arxiv.org/abs/2105.01051"/>
        <updated>2021-06-22T01:57:09.459Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning (SSL) has proven vital for advancing research in
natural language processing (NLP) and computer vision (CV). The paradigm
pretrains a shared model on large volumes of unlabeled data and achieves
state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the
speech processing community lacks a similar setup to systematically explore the
paradigm. To bridge this gap, we introduce Speech processing Universal
PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the
performance of a shared model across a wide range of speech processing tasks
with minimal architecture changes and labeled data. Among multiple usages of
the shared model, we especially focus on extracting the representation learned
from SSL due to its preferable re-usability. We present a simple framework to
solve SUPERB tasks by learning task-specialized lightweight prediction heads on
top of the frozen shared model. Our results demonstrate that the framework is
promising as SSL representations show competitive generalizability and
accessibility across SUPERB tasks. We release SUPERB as a challenge with a
leaderboard and a benchmark toolkit to fuel the research in representation
learning and general speech processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shu-wen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1"&gt;Po-Han Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1"&gt;Yung-Sung Chuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1"&gt;Cheng-I Jeff Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1"&gt;Kushal Lakhotia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yist Y. Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jiatong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1"&gt;Xuankai Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guan-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tzu-Hsien Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1"&gt;Wei-Cheng Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Ko-tik Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Da-Rong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zili Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1"&gt;Shuyan Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdelrahman Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Urdu Caption Generation using Attention based LSTM. (arXiv:2008.01663v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01663</id>
        <link href="http://arxiv.org/abs/2008.01663"/>
        <updated>2021-06-22T01:57:09.443Z</updated>
        <summary type="html"><![CDATA[Recent advancements in deep learning have created many opportunities to solve
real-world problems that remained unsolved for more than a decade. Automatic
caption generation is a major research field, and the research community has
done a lot of work on it in most common languages like English. Urdu is the
national language of Pakistan and also much spoken and understood in the
sub-continent region of Pakistan-India, and yet no work has been done for Urdu
language caption generation. Our research aims to fill this gap by developing
an attention-based deep learning model using techniques of sequence modeling
specialized for the Urdu language. We have prepared a dataset in the Urdu
language by translating a subset of the "Flickr8k" dataset containing 700 'man'
images. We evaluate our proposed technique on this dataset and show that it can
achieve a BLEU score of 0.83 in the Urdu language. We improve on the previous
state-of-the-art by using better CNN architectures and optimization techniques.
Furthermore, we provide a discussion on how the generated captions can be made
correct grammar-wise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilahi_I/0/1/0/all/0/1"&gt;Inaam Ilahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zia_H/0/1/0/all/0/1"&gt;Hafiz Muhammad Abdullah Zia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1"&gt;Muhammad Ahtazaz Ahsan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabassam_R/0/1/0/all/0/1"&gt;Rauf Tabassam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Armaghan Ahmed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech2Phone: A Novel and Efficient Method for Training Speaker Recognition Models. (arXiv:2002.11213v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11213</id>
        <link href="http://arxiv.org/abs/2002.11213"/>
        <updated>2021-06-22T01:57:09.436Z</updated>
        <summary type="html"><![CDATA[In this paper we present an efficient method for training models for speaker
recognition using small or under-resourced datasets. This method requires less
data than other SOTA (State-Of-The-Art) methods, e.g. the Angular Prototypical
and GE2E loss functions, while achieving similar results to those methods. This
is done using the knowledge of the reconstruction of a phoneme in the speaker's
voice. For this purpose, a new dataset was built, composed of 40 male speakers,
who read sentences in Portuguese, totaling approximately 3h. We compare the
three best architectures trained using our method to select the best one, which
is the one with a shallow architecture. Then, we compared this model with the
SOTA method for the speaker recognition task: the Fast ResNet-34 trained with
approximately 2,000 hours, using the loss functions Angular Prototypical and
GE2E. Three experiments were carried out with datasets in different languages.
Among these three experiments, our model achieved the second best result in two
experiments and the best result in one of them. This highlights the importance
of our method, which proved to be a great competitor to SOTA speaker
recognition models, with 500x less data and a simpler approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casanova_E/0/1/0/all/0/1"&gt;Edresson Casanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_A/0/1/0/all/0/1"&gt;Arnaldo Candido Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shulby_C/0/1/0/all/0/1"&gt;Christopher Shulby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1"&gt;Frederico Santos de Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gris_L/0/1/0/all/0/1"&gt;Lucas Rafael Stefanel Gris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1"&gt;Hamilton Pereira da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aluisio_S/0/1/0/all/0/1"&gt;Sandra Maria Aluisio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1"&gt;Moacir Antonelli Ponti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v3 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-06-22T01:57:09.429Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Robustness Improve Fairness? Approaching Fairness with Word Substitution Robustness Methods for Text Classification. (arXiv:2106.10826v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10826</id>
        <link href="http://arxiv.org/abs/2106.10826"/>
        <updated>2021-06-22T01:57:09.398Z</updated>
        <summary type="html"><![CDATA[Existing bias mitigation methods to reduce disparities in model outcomes
across cohorts have focused on data augmentation, debiasing model embeddings,
or adding fairness-based optimization objectives during training. Separately,
certified word substitution robustness methods have been developed to decrease
the impact of spurious features and synonym substitutions on model predictions.
While their end goals are different, they both aim to encourage models to make
the same prediction for certain changes in the input. In this paper, we
investigate the utility of certified word substitution robustness methods to
improve equality of odds and equality of opportunity on multiple text
classification tasks. We observe that certified robustness methods improve
fairness, and using both robustness and bias mitigation methods in training
results in an improvement in both fronts]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhamala_J/0/1/0/all/0/1"&gt;Jwala Dhamala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rahul Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Dialog Systems for Negotiation with Personality Modeling. (arXiv:2010.09954v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09954</id>
        <link href="http://arxiv.org/abs/2010.09954"/>
        <updated>2021-06-22T01:57:09.393Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the ability to model and infer personality types of
opponents, predict their responses, and use this information to adapt a dialog
agent's high-level strategy in negotiation tasks. Inspired by the idea of
incorporating a theory of mind (ToM) into machines, we introduce a
probabilistic formulation to encapsulate the opponent's personality type during
both learning and inference. We test our approach on the CraigslistBargain
dataset and show that our method using ToM inference achieves a 20% higher
dialog agreement rate compared to baselines on a mixed population of opponents.
We also find that our model displays diverse negotiation behavior with
different types of opponents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Runzhe Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingxiao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks. (arXiv:2009.05092v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.05092</id>
        <link href="http://arxiv.org/abs/2009.05092"/>
        <updated>2021-06-22T01:57:09.387Z</updated>
        <summary type="html"><![CDATA[Dialogue relation extraction (DRE) aims to detect the relation between two
entities mentioned in a multi-party dialogue. It plays an important role in
constructing knowledge graphs from conversational data increasingly abundant on
the internet and facilitating intelligent dialogue system development. The
prior methods of DRE do not meaningfully leverage speaker information-they just
prepend the utterances with the respective speaker names. Thus, they fail to
model the crucial inter-speaker relations that may give additional context to
relevant argument entities through pronouns and triggers. We, however, present
a graph attention network-based method for DRE where a graph, that contains
meaningfully connected speaker, entity, entity-type, and utterance nodes, is
constructed. This graph is fed to a graph attention network for context
propagation among relevant nodes, which effectively captures the dialogue
context. We empirically show that this graph-based approach quite effectively
captures the relations between different entity pairs in a dialogue as it
outperforms the state-of-the-art approaches by a significant margin on the
benchmark dataset DialogRE. Our code is released at:
https://github.com/declare-lab/dialog-HGAT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hui Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-22T01:57:09.382Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO PAL: Diachronic Spanish Sonnet Corpus with Psychological and Affective Labels. (arXiv:2007.04626v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04626</id>
        <link href="http://arxiv.org/abs/2007.04626"/>
        <updated>2021-06-22T01:57:09.342Z</updated>
        <summary type="html"><![CDATA[Nowadays, there are many applications of text mining over corpora from
different languages. However, most of them are based on texts in prose, lacking
applications that work with poetry texts. An example of an application of text
mining in poetry is the usage of features derived from their individual words
in order to capture the lexical, sublexical and interlexical meaning, and infer
the General Affective Meaning (GAM) of the text. However, even though this
proposal has been proved as useful for poetry in some languages, there is a
lack of studies for both Spanish poetry and for highly-structured poetic
compositions such as sonnets. This article presents a study over an annotated
corpus of Spanish sonnets, in order to analyse if it is possible to build
features from their individual words for predicting their GAM. The purpose of
this is to model sonnets at an affective level. The article also analyses the
relationship between the GAM of the sonnets and the content itself. For this,
we consider the content from a psychological perspective, identifying with tags
when a sonnet is related to a specific term. Then, we study how GAM changes
according to each of those psychological terms.

The corpus used contains 274 Spanish sonnets from authors of different
centuries, from 15th to 19th. This corpus was annotated by different domain
experts. The experts annotated the poems with affective and lexico-semantic
features, as well as with domain concepts that belong to psychology. Thanks to
this, the corpus of sonnets can be used in different applications, such as
poetry recommender systems, personality text mining studies of the authors, or
the usage of poetry for therapeutic purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1"&gt;Alberto Barbado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fresno_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Fresno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riesco_A/0/1/0/all/0/1"&gt;&amp;#xc1;ngeles Manjarr&amp;#xe9;s Riesco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ros_S/0/1/0/all/0/1"&gt;Salvador Ros&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoreGen: Contextualized Code Representation Learning for Commit Message Generation. (arXiv:2007.06934v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06934</id>
        <link href="http://arxiv.org/abs/2007.06934"/>
        <updated>2021-06-22T01:57:09.313Z</updated>
        <summary type="html"><![CDATA[Automatic generation of high-quality commit messages for code commits can
substantially facilitate software developers' works and coordination. However,
the semantic gap between source code and natural language poses a major
challenge for the task. Several studies have been proposed to alleviate the
challenge but none explicitly involves code contextual information during
commit message generation. Specifically, existing research adopts static
embedding for code tokens, which maps a token to the same vector regardless of
its context. In this paper, we propose a novel Contextualized code
representation learning strategy for commit message Generation (CoreGen).
CoreGen first learns contextualized code representations which exploit the
contextual information behind code commit sequences. The learned
representations of code commits built upon Transformer are then fine-tuned for
downstream commit message generation. Experiments on the benchmark dataset
demonstrate the superior effectiveness of our model over the baseline models
with at least 28.18% improvement in terms of BLEU-4 score. Furthermore, we also
highlight the future opportunities in training contextualized code
representations on larger code corpus as a solution to low-resource tasks and
adapting the contextualized code representation framework to other code-to-text
generation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1"&gt;Lun Yiu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Cuiyun Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zhicong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1"&gt;Wai Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers for Headline Selection for Russian News Clusters. (arXiv:2106.10487v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10487</id>
        <link href="http://arxiv.org/abs/2106.10487"/>
        <updated>2021-06-22T01:57:09.306Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore various multilingual and Russian pre-trained
transformer-based models for the Dialogue Evaluation 2021 shared task on
headline selection. Our experiments show that the combined approach is superior
to individual multilingual and monolingual models. We present an analysis of a
number of ways to obtain sentence embeddings and learn a ranking model on top
of them. We achieve the result of 87.28% and 86.60% accuracy for the public and
private test sets respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Voropaev_P/0/1/0/all/0/1"&gt;Pavel Voropaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sopilnyak_O/0/1/0/all/0/1"&gt;Olga Sopilnyak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calliar: An Online Handwritten Dataset for Arabic Calligraphy. (arXiv:2106.10745v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10745</id>
        <link href="http://arxiv.org/abs/2106.10745"/>
        <updated>2021-06-22T01:57:09.218Z</updated>
        <summary type="html"><![CDATA[Calligraphy is an essential part of the Arabic heritage and culture. It has
been used in the past for the decoration of houses and mosques. Usually, such
calligraphy is designed manually by experts with aesthetic insights. In the
past few years, there has been a considerable effort to digitize such type of
art by either taking a photo of decorated buildings or drawing them using
digital devices. The latter is considered an online form where the drawing is
tracked by recording the apparatus movement, an electronic pen for instance, on
a screen. In the literature, there are many offline datasets collected with a
diversity of Arabic styles for calligraphy. However, there is no available
online dataset for Arabic calligraphy. In this paper, we illustrate our
approach for the collection and annotation of an online dataset for Arabic
calligraphy called Calliar that consists of 2,500 sentences. Calliar is
annotated for stroke, character, word and sentence level prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1"&gt;Zaid Alyafeai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1"&gt;Maged S. Al-shaibani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1"&gt;Mustafa Ghaleb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Wajih_Y/0/1/0/all/0/1"&gt;Yousif Ahmed Al-Wajih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction. (arXiv:2106.10786v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10786</id>
        <link href="http://arxiv.org/abs/2106.10786"/>
        <updated>2021-06-22T01:57:09.197Z</updated>
        <summary type="html"><![CDATA[Natural reading orders of words are crucial for information extraction from
form-like documents. Despite recent advances in Graph Convolutional Networks
(GCNs) on modeling spatial layout patterns of documents, they have limited
ability to capture reading orders of given word-level node representations in a
graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new
positional encoding technique designed to apprehend the sequential presentation
of words in documents. ROPE generates unique reading order codes for
neighboring words relative to the target word given a word-level graph
connectivity. We study two fundamental document entity extraction tasks
including word labeling and word grouping on the public FUNSD dataset and a
large-scale payment dataset. We show that ROPE consistently improves existing
GCNs with a margin up to 8.4% F1-score.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Renshen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1"&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1"&gt;Siyang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popat_A/0/1/0/all/0/1"&gt;Ashok Popat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1"&gt;Tomas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Legal Citation Recommendation using Deep Learning. (arXiv:2106.10776v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10776</id>
        <link href="http://arxiv.org/abs/2106.10776"/>
        <updated>2021-06-22T01:57:09.190Z</updated>
        <summary type="html"><![CDATA[Lawyers and judges spend a large amount of time researching the proper legal
authority to cite while drafting decisions. In this paper, we develop a
citation recommendation tool that can help improve efficiency in the process of
opinion drafting. We train four types of machine learning models, including a
citation-list based method (collaborative filtering) and three context-based
methods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show
that leveraging local textual context improves recommendation, and that deep
neural models achieve decent performance. We show that non-deep text-based
methods benefit from access to structured case metadata, but deep models only
benefit from such access when predicting from context of insufficient length.
We also find that, even after extensive training, RoBERTa does not outperform a
recurrent neural model, despite its benefits of pretraining. Our behavior
analysis of the RoBERTa model further shows that predictive performance is
stable across time and citation classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1"&gt;Charles Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1"&gt;Mengqiu Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1"&gt;Mark S. Krass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1"&gt;Matthias Grabmair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07719</id>
        <link href="http://arxiv.org/abs/2104.07719"/>
        <updated>2021-06-22T01:57:09.175Z</updated>
        <summary type="html"><![CDATA[Few-shot object detection (FSOD) aims to detect objects using only few
examples. It's critically needed for many practical applications but so far
remains challenging. We propose a meta-learning based few-shot object detection
method by transferring meta-knowledge learned from data-abundant base classes
to data-scarce novel classes. Our method incorporates a coarse-to-fine approach
into the proposal based object detection framework and integrates prototype
based classifiers into both the proposal generation and classification stages.
To improve proposal generation for few-shot novel classes, we propose to learn
a lightweight matching network to measure the similarity between each spatial
position in the query image feature map and spatially-pooled class features,
instead of the traditional object/nonobject classifier, thus generating
category-specific proposals and improving proposal recall for novel classes. To
address the spatial misalignment between generated proposals and few-shot class
examples, we propose a novel attentive feature alignment method, thus improving
the performance of few-shot object detection. Meanwhile we jointly learn a
Faster R-CNN detection head for base classes. Extensive experiments conducted
on multiple FSOD benchmarks show our proposed approach achieves state of the
art results under (incremental) few-shot learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1"&gt;Guangxing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shiyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiawei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yicheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shih-Fu Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CPM-2: Large-scale Cost-effective Pre-trained Language Models. (arXiv:2106.10715v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10715</id>
        <link href="http://arxiv.org/abs/2106.10715"/>
        <updated>2021-06-22T01:57:09.158Z</updated>
        <summary type="html"><![CDATA[In recent years, the size of pre-trained language models (PLMs) has grown by
leaps and bounds. However, efficiency issues of these large-scale PLMs limit
their utilization in real-world scenarios. We present a suite of cost-effective
techniques for the use of PLMs to deal with the efficiency issues of
pre-training, fine-tuning, and inference. (1) We introduce knowledge
inheritance to accelerate the pre-training process by exploiting existing PLMs
instead of training models from scratch. (2) We explore the best practice of
prompt tuning with large-scale PLMs. Compared with conventional fine-tuning,
prompt tuning significantly reduces the number of task-specific parameters. (3)
We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs
with limited computational resources. Based on our cost-effective pipeline, we
pre-train two models: an encoder-decoder bilingual model with 11 billion
parameters (CPM-2) and its corresponding MoE version with 198 billion
parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks.
Experimental results show that CPM-2 has excellent general language
intelligence. Moreover, we validate the efficiency of InfMoE when conducting
inference of large-scale models having tens of billions of parameters on a
single GPU. All source code and model parameters are available at
https://github.com/TsinghuaAI/CPM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuxian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chaojun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenbo Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jian Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1"&gt;Pei Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yanzheng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1"&gt;Guoyang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1"&gt;Zhixing Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wentao Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in Translation of Emotions in Multilingual User-Generated Content: Twitter as a Case Study. (arXiv:2106.10719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10719</id>
        <link href="http://arxiv.org/abs/2106.10719"/>
        <updated>2021-06-22T01:57:09.150Z</updated>
        <summary type="html"><![CDATA[Although emotions are universal concepts, transferring the different shades
of emotion from one language to another may not always be straightforward for
human translators, let alone for machine translation systems. Moreover, the
cognitive states are established by verbal explanations of experience which is
shaped by both the verbal and cultural contexts. There are a number of verbal
contexts where expression of emotions constitutes the pivotal component of the
message. This is particularly true for User-Generated Content (UGC) which can
be in the form of a review of a product or a service, a tweet, or a social
media post. Recently, it has become common practice for multilingual websites
such as Twitter to provide an automatic translation of UGC to reach out to
their linguistically diverse users. In such scenarios, the process of
translating the user's emotion is entirely automatic with no human
intervention, neither for post-editing nor for accuracy checking. In this
research, we assess whether automatic translation tools can be a successful
real-life utility in transferring emotion in user-generated multilingual data
such as tweets. We show that there are linguistic phenomena specific of Twitter
data that pose a challenge in translation of emotions in different languages.
We summarise these challenges in a list of linguistic features and show how
frequent these features are in different language pairs. We also assess the
capacity of commonly used methods for evaluating the performance of an MT
system with respect to the preservation of emotion in the source text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1"&gt;Hadeel Saadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1"&gt;Constantin Orasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quintana_R/0/1/0/all/0/1"&gt;Rocio Caro Quintana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carmo_F/0/1/0/all/0/1"&gt;Felix do Carmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1"&gt;Leonardo Zilio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Legal Citation Recommendation using Deep Learning. (arXiv:2106.10776v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10776</id>
        <link href="http://arxiv.org/abs/2106.10776"/>
        <updated>2021-06-22T01:57:09.143Z</updated>
        <summary type="html"><![CDATA[Lawyers and judges spend a large amount of time researching the proper legal
authority to cite while drafting decisions. In this paper, we develop a
citation recommendation tool that can help improve efficiency in the process of
opinion drafting. We train four types of machine learning models, including a
citation-list based method (collaborative filtering) and three context-based
methods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show
that leveraging local textual context improves recommendation, and that deep
neural models achieve decent performance. We show that non-deep text-based
methods benefit from access to structured case metadata, but deep models only
benefit from such access when predicting from context of insufficient length.
We also find that, even after extensive training, RoBERTa does not outperform a
recurrent neural model, despite its benefits of pretraining. Our behavior
analysis of the RoBERTa model further shows that predictive performance is
stable across time and citation classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zihan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1"&gt;Charles Low&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_M/0/1/0/all/0/1"&gt;Mengqiu Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1"&gt;Mark S. Krass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grabmair_M/0/1/0/all/0/1"&gt;Matthias Grabmair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid approach to detecting symptoms of depression in social media entries. (arXiv:2106.10485v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10485</id>
        <link href="http://arxiv.org/abs/2106.10485"/>
        <updated>2021-06-22T01:57:09.134Z</updated>
        <summary type="html"><![CDATA[Sentiment and lexical analyses are widely used to detect depression or
anxiety disorders. It has been documented that there are significant
differences in the language used by a person with emotional disorders in
comparison to a healthy individual. Still, the effectiveness of these lexical
approaches could be improved further because the current analysis focuses on
what the social media entries are about, and not how they are written. In this
study, we focus on aspects in which these short texts are similar to each
other, and how they were created. We present an innovative approach to the
depression screening problem by applying Collgram analysis, which is a known
effective method of obtaining linguistic information from texts. We compare
these results with sentiment analysis based on the BERT architecture. Finally,
we create a hybrid model achieving a diagnostic accuracy of 71%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolk_A/0/1/0/all/0/1"&gt;Agnieszka Wo&amp;#x142;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chlasta_K/0/1/0/all/0/1"&gt;Karol Chlasta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holas_P/0/1/0/all/0/1"&gt;Pawe&amp;#x142; Holas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Service Addition in Multilevel Index Model for Edge Computing. (arXiv:2106.04494v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04494</id>
        <link href="http://arxiv.org/abs/2106.04494"/>
        <updated>2021-06-22T01:57:09.125Z</updated>
        <summary type="html"><![CDATA[With the development of Edge Computing and Artificial Intelligence (AI)
technologies, edge devices are witnessed to generate data at unprecedented
volume. The Edge Intelligence (EI) has led to the emergence of edge devices in
various application domains. The EI can provide efficient services to
delay-sensitive applications, where the edge devices are deployed as edge nodes
to host the majority of execution, which can effectively manage services and
improve service discovery efficiency. The multilevel index model is a
well-known model used for indexing service, such a model is being introduced
and optimized in the edge environments to efficiently services discovery whilst
managing large volumes of data. However, effectively updating the multilevel
index model by adding new services timely and precisely in the dynamic Edge
Computing environments is still a challenge. Addressing this issue, this paper
proposes a designated key selection method to improve the efficiency of adding
services in the multilevel index models. Our experimental results show that in
the partial index and the full index of multilevel index model, our method
reduces the service addition time by around 84% and 76%, respectively when
compared with the original key selection method and by around 78% and 66%,
respectively when compared with the random selection method. Our proposed
method significantly improves the service addition efficiency in the multilevel
index model, when compared with existing state-of-the-art key selection
methods, without compromising the service retrieval stability to any notable
level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiayan Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1"&gt;Ashiq Anjum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panneerselvam_J/0/1/0/all/0/1"&gt;John Panneerselvam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1"&gt;Bo Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Condense-then-Select Strategy for Text Summarization. (arXiv:2106.10468v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10468</id>
        <link href="http://arxiv.org/abs/2106.10468"/>
        <updated>2021-06-22T01:57:09.103Z</updated>
        <summary type="html"><![CDATA[Select-then-compress is a popular hybrid, framework for text summarization
due to its high efficiency. This framework first selects salient sentences and
then independently condenses each of the selected sentences into a concise
version. However, compressing sentences separately ignores the context
information of the document, and is therefore prone to delete salient
information. To address this limitation, we propose a novel
condense-then-select framework for text summarization. Our framework first
concurrently condenses each document sentence. Original document sentences and
their compressed versions then become the candidates for extraction. Finally,
an extractor utilizes the context information of the document to select
candidates and assembles them into a summary. If salient information is deleted
during condensing, the extractor can select an original sentence to retain the
information. Thus, our framework helps to avoid the loss of salient
information, while preserving the high efficiency of sentence-level
compression. Experiment results on the CNN/DailyMail, DUC-2002, and Pubmed
datasets demonstrate that our framework outperforms the select-then-compress
framework and other strong baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1"&gt;Hou Pong Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs. (arXiv:2106.10502v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10502</id>
        <link href="http://arxiv.org/abs/2106.10502"/>
        <updated>2021-06-22T01:57:09.095Z</updated>
        <summary type="html"><![CDATA[Existing pre-trained models for knowledge-graph-to-text (KG-to-text)
generation simply fine-tune text-to-text pre-trained models such as BART or T5
on KG-to-text datasets, which largely ignore the graph structure during
encoding and lack elaborate pre-training tasks to explicitly model graph-text
alignments. To tackle these problems, we propose a graph-text joint
representation learning model called JointGT. During encoding, we devise a
structure-aware semantic aggregation module which is plugged into each
Transformer layer to preserve the graph structure. Furthermore, we propose
three new pre-training tasks to explicitly enhance the graph-text alignment
including respective text / graph reconstruction, and graph-text alignment in
the embedding space via Optimal Transport. Experiments show that JointGT
obtains new state-of-the-art performance on various KG-to-text datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1"&gt;Pei Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Haozhe Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_Y/0/1/0/all/0/1"&gt;Yu Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Linfeng Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TweeNLP: A Twitter Exploration Portal for Natural Language Processing. (arXiv:2106.10512v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10512</id>
        <link href="http://arxiv.org/abs/2106.10512"/>
        <updated>2021-06-22T01:57:09.083Z</updated>
        <summary type="html"><![CDATA[We present TweeNLP, a one-stop portal that organizes Twitter's natural
language processing (NLP) data and builds a visualization and exploration
platform. It curates 19,395 tweets (as of April 2021) from various NLP
conferences and general NLP discussions. It supports multiple features such as
TweetExplorer to explore tweets by topics, visualize insights from Twitter
activity throughout the organization cycle of conferences, discover popular
research papers and researchers. It also builds a timeline of conference and
workshop submission deadlines. We envision TweeNLP to function as a collective
memory unit for the NLP community by integrating the tweets pertaining to
research papers with the NLPExplorer scientific literature search engine. The
current system is hosted at this http URL .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1"&gt;Viraj Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Shruti Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Graph-based Search for Lessons-Learned Documents in the Semiconductor Industry. (arXiv:2105.08442v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08442</id>
        <link href="http://arxiv.org/abs/2105.08442"/>
        <updated>2021-06-22T01:57:09.070Z</updated>
        <summary type="html"><![CDATA[Industrial processes produce a considerable volume of data and thus
information. Whether it is structured sensory data or semi- to unstructured
textual data, the knowledge that can be derived from it is critical to the
sustainable development of the industrial process. A key challenge of this
sustainability is the intelligent management of the generated data, as well as
the knowledge extracted from it, in order to utilize this knowledge for
improving future procedures. This challenge is a result of the tailored
documentation methods and domain-specific requirements, which include the need
for quick visibility of the documented knowledge. In this paper, we utilize the
expert knowledge documented in chip-design failure reports in supporting user
access to information that is relevant to a current chip design. Unstructured,
free, textual data in previous failure documentations provides a valuable
source of lessons-learned, which expert design-engineers have experienced,
solved and documented. To achieve a sustainable utilization of knowledge within
the company, not only the inherent knowledge has to be mined from unstructured
textual data, but also the relations between the lessons-learned, uncovering
potentially unknown links. In this research, a knowledge graph is constructed,
in order to represent and use the interconnections between reported design
failures. A search engine is developed and applied onto the graph to answer
queries. In contrast to mere keyword-based searching, the searchability of the
knowledge graph offers enhanced search results beyond direct matches and acts
as a mean for generating explainable results and result recommendations.
Results are provided to the design engineer through an interactive search
interface, in which, the feedback from the user is used to further optimize
relations for future iterations of the knowledge graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abu_Rasheed_H/0/1/0/all/0/1"&gt;Hasan Abu-Rasheed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Christian Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenkert_J/0/1/0/all/0/1"&gt;Johannes Zenkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krumm_R/0/1/0/all/0/1"&gt;Roland Krumm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathi_M/0/1/0/all/0/1"&gt;Madjid Fathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Compositional Generalization in Classification Tasks via Structure Annotations. (arXiv:2106.10434v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10434</id>
        <link href="http://arxiv.org/abs/2106.10434"/>
        <updated>2021-06-22T01:57:09.061Z</updated>
        <summary type="html"><![CDATA[Compositional generalization is the ability to generalize systematically to a
new data distribution by combining known components. Although humans seem to
have a great ability to generalize compositionally, state-of-the-art neural
models struggle to do so. In this work, we study compositional generalization
in classification tasks and present two main contributions. First, we study
ways to convert a natural language sequence-to-sequence dataset to a
classification dataset that also requires compositional generalization. Second,
we show that providing structural hints (specifically, providing parse trees
and entity links as attention masks for a Transformer model) helps
compositional generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juyong Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1"&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1"&gt;Joshua Ainslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1"&gt;Santiago Onta&amp;#xf1;&amp;#xf3;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Key Technologies for Networked Virtual Environments. (arXiv:2102.09847v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09847</id>
        <link href="http://arxiv.org/abs/2102.09847"/>
        <updated>2021-06-22T01:57:09.033Z</updated>
        <summary type="html"><![CDATA[Thanks to the improvements experienced in technology in the last few years,
most especially in virtual reality systems, the number and potential of
networked virtual environments or NVEs and their users are increasing. NVEs aim
to give distributed users a feeling of immersion in a virtual world and the
possibility of interacting with other users or with virtual objects inside it,
like when they interact in the real world. Being able to provide that feeling
and natural interactions when the users are geographically separated is one of
the goals of these systems. Nevertheless, this goal is especially sensitive to
different issues, such as different connections with heterogeneous throughput
or different network latencies, which can lead to consistency and
synchronization problems and, thus, to a worsening of the users' quality of
experience or QoE. With the purpose of solving these issues, researchers have
proposed and evaluated numerous technical solutions, in fields like network
architectures, data distribution and filtering, resource balancing, computing
models, predictive modeling and synchronization in NVEs. This paper gathers and
classifies them, summarizing their advantages and disadvantages, using a new
way of classification. With the current increase of the number of NVEs and the
multiple solutions proposed so far, this work aims to become a useful tool and
a starting point not only for future researchers in this field but also for
those who are new in NVEs development, in which guaranteeing a good users' QoE
is essential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Juan Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boronat_F/0/1/0/all/0/1"&gt;Fernando Boronat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapena_A/0/1/0/all/0/1"&gt;Almanzor Sapena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pastor_J/0/1/0/all/0/1"&gt;Javier Pastor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Question Generation with Commonsense Knowledge. (arXiv:2106.10454v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10454</id>
        <link href="http://arxiv.org/abs/2106.10454"/>
        <updated>2021-06-22T01:57:09.019Z</updated>
        <summary type="html"><![CDATA[Question generation (QG) is to generate natural and grammatical questions
that can be answered by a specific answer for a given context. Previous
sequence-to-sequence models suffer from a problem that asking high-quality
questions requires commonsense knowledge as backgrounds, which in most cases
can not be learned directly from training data, resulting in unsatisfactory
questions deprived of knowledge. In this paper, we propose a multi-task
learning framework to introduce commonsense knowledge into question generation
process. We first retrieve relevant commonsense knowledge triples from mature
databases and select triples with the conversion information from source
context to question. Based on these informative knowledge triples, we design
two auxiliary tasks to incorporate commonsense knowledge into the main QG
model, where one task is Concept Relation Classification and the other is Tail
Concept Generation. Experimental results on SQuAD show that our proposed
methods are able to noticeably improve the QG performance on both automatic and
human evaluation metrics, demonstrating that incorporating external commonsense
knowledge with multi-task learning can help the model generate human-like and
high-quality questions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xin Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1"&gt;Dawei Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfang Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets. (arXiv:2106.10328v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10328</id>
        <link href="http://arxiv.org/abs/2106.10328"/>
        <updated>2021-06-22T01:57:09.009Z</updated>
        <summary type="html"><![CDATA[Language models can generate harmful and biased outputs and exhibit
undesirable behavior. We propose a Process for Adapting Language Models to
Society (PALMS) with Values-Targeted Datasets, an iterative process to
significantly change model behavior by crafting and fine-tuning on a dataset
that reflects a predetermined set of target values. We evaluate our process
using three metrics: quantitative metrics with human evaluations that score
output adherence to a target value, and toxicity scoring on outputs; and
qualitative metrics analyzing the most common word associated with a given
social category. Through each iteration, we add additional training dataset
examples based on observed shortcomings from evaluations. PALMS performs
significantly better on all metrics compared to baseline and control models for
a broad range of GPT-3 language model sizes without compromising capability
integrity. We find that the effectiveness of PALMS increases with model size.
We show that significantly adjusting language model behavior is feasible with a
small, hand-curated dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Solaiman_I/0/1/0/all/0/1"&gt;Irene Solaiman&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Dennison_C/0/1/0/all/0/1"&gt;Christy Dennison&lt;/a&gt; (1) ((1) OpenAI)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Pair Text Style Transfer on Unbalanced Data. (arXiv:2106.10608v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10608</id>
        <link href="http://arxiv.org/abs/2106.10608"/>
        <updated>2021-06-22T01:57:08.999Z</updated>
        <summary type="html"><![CDATA[Text-style transfer aims to convert text given in one domain into another by
paraphrasing the sentence or substituting the keywords without altering the
content. By necessity, state-of-the-art methods have evolved to accommodate
nonparallel training data, as it is frequently the case there are multiple data
sources of unequal size, with a mixture of labeled and unlabeled sentences.
Moreover, the inherent style defined within each source might be distinct. A
generic bidirectional (e.g., formal $\Leftrightarrow$ informal) style transfer
regardless of different groups may not generalize well to different
applications. In this work, we developed a task adaptive meta-learning
framework that can simultaneously perform a multi-pair text-style transfer
using a single model. The proposed method can adaptively balance the difference
of meta-knowledge across multiple tasks. Results show that our method leads to
better quantitative performance as well as coherent style variations. Common
challenges of unbalanced data and mismatched domains are handled well by this
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xing Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lundin_J/0/1/0/all/0/1"&gt;Jessica Lundin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling. (arXiv:2106.10840v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10840</id>
        <link href="http://arxiv.org/abs/2106.10840"/>
        <updated>2021-06-22T01:57:08.985Z</updated>
        <summary type="html"><![CDATA[Multi-head attention has each of the attention heads collect salient
information from different parts of an input sequence, making it a powerful
mechanism for sequence modeling. Multilingual and multi-domain learning are
common scenarios for sequence modeling, where the key challenge is to maximize
positive transfer and mitigate negative transfer across languages and domains.
In this paper, we find that non-selective attention sharing is sub-optimal for
achieving good generalization across all languages and domains. We further
propose attention sharing strategies to facilitate parameter sharing and
specialization in multilingual and multi-domain sequence modeling. Our approach
automatically learns shared and specialized attention heads for different
languages and domains to mitigate their interference. Evaluated in various
tasks including speech recognition, text-to-text and speech-to-text
translation, the proposed attention sharing strategies consistently bring gains
to sequence models built upon multi-head attention. For speech-to-text
translation, our approach yields an average of $+2.0$ BLEU over $13$ language
directions in multilingual setting and $+2.0$ BLEU over $3$ domains in
multi-domain setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1"&gt;Hongyu Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yun Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Faced Humans on Twitter and Facebook: Harvesting Social Multimedia for Human Personality Profiling. (arXiv:2106.10673v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.10673</id>
        <link href="http://arxiv.org/abs/2106.10673"/>
        <updated>2021-06-22T01:57:08.794Z</updated>
        <summary type="html"><![CDATA[Human personality traits are the key drivers behind our decision-making,
influencing our life path on a daily basis. Inference of personality traits,
such as Myers-Briggs Personality Type, as well as an understanding of
dependencies between personality traits and users' behavior on various social
media platforms is of crucial importance to modern research and industry
applications. The emergence of diverse and cross-purpose social media avenues
makes it possible to perform user personality profiling automatically and
efficiently based on data represented across multiple data modalities. However,
the research efforts on personality profiling from multi-source multi-modal
social media data are relatively sparse, and the level of impact of different
social network data on machine learning performance has yet to be
comprehensively evaluated. Furthermore, there is not such dataset in the
research community to benchmark. This study is one of the first attempts
towards bridging such an important research gap. Specifically, in this work, we
infer the Myers-Briggs Personality Type indicators, by applying a novel
multi-view fusion framework, called "PERS" and comparing the performance
results not just across data modalities but also with respect to different
social network data sources. Our experimental results demonstrate the PERS's
ability to learn from multi-view data for personality profiling by efficiently
leveraging on the significantly different data arriving from diverse social
multimedia sources. We have also found that the selection of a machine learning
approach is of crucial importance when choosing social network data sources and
that people tend to reveal multiple facets of their personality in different
social media avenues. Our released social multimedia dataset facilitates future
research on this direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farseev_A/0/1/0/all/0/1"&gt;Aleksandr Farseev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comprehensive Review on Non-Neural Networks Collaborative Filtering Recommendation Systems. (arXiv:2106.10679v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10679</id>
        <link href="http://arxiv.org/abs/2106.10679"/>
        <updated>2021-06-22T01:57:08.778Z</updated>
        <summary type="html"><![CDATA[Over the past two decades, recommender systems have attracted a lot of
interest due to the explosion in the amount of data in online applications. A
particular attention has been paid to collaborative filtering, which is the
most widely used in applications that involve information recommendations.
Collaborative filtering (CF) uses the known preference of a group of users to
make predictions and recommendations about the unknown preferences of other
users (recommendations are made based on the past behavior of users). First
introduced in the 1990s, a wide variety of increasingly successful models have
been proposed. Due to the success of machine learning techniques in many areas,
there has been a growing emphasis on the application of such algorithms in
recommendation systems. In this article, we present an overview of the CF
approaches for recommender systems, their two main categories, and their
evaluation metrics. We focus on the application of classical Machine Learning
algorithms to CF recommender systems by presenting their evolution from their
first use-cases to advanced Machine Learning models. We attempt to provide a
comprehensive and comparative overview of CF systems (with python
implementations) that can serve as a guideline for research and practice in
this area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wenga_C/0/1/0/all/0/1"&gt;Carmel Wenga&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fansi_M/0/1/0/all/0/1"&gt;Majirus Fansi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Chabrier_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Chabrier&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Mari_J/0/1/0/all/0/1"&gt;Jean-Martial Mari&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gabillon_A/0/1/0/all/0/1"&gt;Alban Gabillon&lt;/a&gt; (1) ((1) University of French Polynesia, (2) NzhinuSoft)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Recommendation in Online Games with Multiple Sequences, Tasks and User Levels. (arXiv:2102.06950v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06950</id>
        <link href="http://arxiv.org/abs/2102.06950"/>
        <updated>2021-06-22T01:57:08.754Z</updated>
        <summary type="html"><![CDATA[Online gaming is growing faster than ever before, with increasing challenges
of providing better user experience. Recommender systems (RS) for online games
face unique challenges since they must fulfill players' distinct desires, at
different user levels, based on their action sequences of various action types.
Although many sequential RS already exist, they are mainly single-sequence,
single-task, and single-user-level. In this paper, we introduce a new
sequential recommendation model for multiple sequences, multiple tasks, and
multiple user levels (abbreviated as M$^3$Rec) in Tencent Games platform, which
can fully utilize complex data in online games. We leverage Graph Neural
Network and multi-task learning to design M$^3$Rec in order to model the
complex information in the heterogeneous sequential recommendation scenario of
Tencent Games. We verify the effectiveness of M$^3$Rec on three online games of
Tencent Games platform, in both offline and online evaluations. The results
show that M$^3$Rec successfully addresses the challenges of recommendation in
online games, and it generates superior recommendations compared with
state-of-the-art sequential recommendation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuqiu Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10681</id>
        <link href="http://arxiv.org/abs/2106.10681"/>
        <updated>2021-06-22T01:57:08.719Z</updated>
        <summary type="html"><![CDATA[Visual information extraction (VIE) has attracted increasing attention in
recent years. The existing methods usually first organized optical character
recognition (OCR) results into plain texts and then utilized token-level entity
annotations as supervision to train a sequence tagging model. However, it
expends great annotation costs and may be exposed to label confusion, and the
OCR errors will also significantly affect the final performance. In this paper,
we propose a unified weakly-supervised learning framework called TCPN (Tag,
Copy or Predict Network), which introduces 1) an efficient encoder to
simultaneously model the semantic and layout information in 2D OCR results; 2)
a weakly-supervised training strategy that utilizes only key information
sequences as supervision; and 3) a flexible and switchable decoder which
contains two inference modes: one (Copy or Predict Mode) is to output key
information sequences of different categories by copying a token from the input
or predicting one in each time step, and the other (Tag Mode) is to directly
tag the input sequence in a single forward pass. Our method shows new
state-of-the-art performance on several public benchmarks, which fully proves
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weihong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kai Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yichao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Multiple Online Sources for Accurate Income Verification. (arXiv:2106.10547v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10547</id>
        <link href="http://arxiv.org/abs/2106.10547"/>
        <updated>2021-06-22T01:57:08.706Z</updated>
        <summary type="html"><![CDATA[Income verification is the problem of validating a person's stated income
given basic identity information such as name, location, job title and
employer. It is widely used in the context of mortgage lending, rental
applications and other financial risk models. However, the current processes
surrounding verification involve significant human effort and document
gathering which can be both time-consuming and expensive. In this paper, we
propose a novel model for verifying an individual's income given very limited
identity information typically available in loan applications. Our model is a
combination of a deep neural network and hand-engineered features. The hand
engineered features are based upon matching the input information against
income records extracted automatically from various publicly available online
sources (e.g. payscale.com, H-1B filings, government employee salaries). We
conduct experiments on two data sets, one simulated from H-1B records and the
other from a real-world data set of peer-to-peer (P2P) loan applications
obtained from one of the world's largest P2P lending platform. Our results show
a significant reduction in error of 3-6% relative to several strong baselines.
We also perform ablation studies to demonstrate that a combined model is indeed
necessary to achieve state-of-the-art performance on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahapatra_C/0/1/0/all/0/1"&gt;Chirag Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellare_K/0/1/0/all/0/1"&gt;Kedar Bellare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Sampling Top-K Recommendation Evaluation. (arXiv:2106.10621v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10621</id>
        <link href="http://arxiv.org/abs/2106.10621"/>
        <updated>2021-06-22T01:57:08.693Z</updated>
        <summary type="html"><![CDATA[Recently, Rendle has warned that the use of sampling-based top-$k$ metrics
might not suffice. This throws a number of recent studies on deep
learning-based recommendation algorithms, and classic non-deep-learning
algorithms using such a metric, into jeopardy. In this work, we thoroughly
investigate the relationship between the sampling and global top-$K$ Hit-Ratio
(HR, or Recall), originally proposed by Koren[2] and extensively used by
others. By formulating the problem of aligning sampling top-$k$ ($SHR@k$) and
global top-$K$ ($HR@K$) Hit-Ratios through a mapping function $f$, so that
$SHR@k\approx HR@f(k)$, we demonstrate both theoretically and experimentally
that the sampling top-$k$ Hit-Ratio provides an accurate approximation of its
global (exact) counterpart, and can consistently predict the correct winners
(the same as indicate by their corresponding global Hit-Ratios).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Ruoming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-06-22T01:57:08.672Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for Visual Information Extraction using Sequences. (arXiv:2106.10681v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10681</id>
        <link href="http://arxiv.org/abs/2106.10681"/>
        <updated>2021-06-22T01:57:08.660Z</updated>
        <summary type="html"><![CDATA[Visual information extraction (VIE) has attracted increasing attention in
recent years. The existing methods usually first organized optical character
recognition (OCR) results into plain texts and then utilized token-level entity
annotations as supervision to train a sequence tagging model. However, it
expends great annotation costs and may be exposed to label confusion, and the
OCR errors will also significantly affect the final performance. In this paper,
we propose a unified weakly-supervised learning framework called TCPN (Tag,
Copy or Predict Network), which introduces 1) an efficient encoder to
simultaneously model the semantic and layout information in 2D OCR results; 2)
a weakly-supervised training strategy that utilizes only key information
sequences as supervision; and 3) a flexible and switchable decoder which
contains two inference modes: one (Copy or Predict Mode) is to output key
information sequences of different categories by copying a token from the input
or predicting one in each time step, and the other (Tag Mode) is to directly
tag the input sequence in a single forward pass. Our method shows new
state-of-the-art performance on several public benchmarks, which fully proves
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guozhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Weihong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kai Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yichao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Subverting the Jewtocracy": Online Antisemitism Detection Using Multimodal Deep Learning. (arXiv:2104.05947v3 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05947</id>
        <link href="http://arxiv.org/abs/2104.05947"/>
        <updated>2021-06-22T01:57:08.648Z</updated>
        <summary type="html"><![CDATA[The exponential rise of online social media has enabled the creation,
distribution, and consumption of information at an unprecedented rate. However,
it has also led to the burgeoning of various forms of online abuse. Increasing
cases of online antisemitism have become one of the major concerns because of
its socio-political consequences. Unlike other major forms of online abuse like
racism, sexism, etc., online antisemitism has not been studied much from a
machine learning perspective. To the best of our knowledge, we present the
first work in the direction of automated multimodal detection of online
antisemitism. The task poses multiple challenges that include extracting
signals across multiple modalities, contextual references, and handling
multiple aspects of antisemitism. Unfortunately, there does not exist any
publicly available benchmark corpus for this critical task. Hence, we collect
and label two datasets with 3,102 and 3,509 social media posts from Twitter and
Gab respectively. Further, we present a multimodal deep learning system that
detects the presence of antisemitic content and its specific antisemitism
category using text and images from posts. We perform an extensive set of
experiments on the two datasets to evaluate the efficacy of the proposed
system. Finally, we also present a qualitative analysis of our study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1"&gt;Mohit Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pailla_D/0/1/0/all/0/1"&gt;Dheeraj Pailla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_H/0/1/0/all/0/1"&gt;Himanshu Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchawala_A/0/1/0/all/0/1"&gt;Aadilmehdi Sanchawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1"&gt;Manish Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1"&gt;Ponnurangam Kumaraguru&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?. (arXiv:2106.10622v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10622</id>
        <link href="http://arxiv.org/abs/2106.10622"/>
        <updated>2021-06-22T01:57:08.609Z</updated>
        <summary type="html"><![CDATA[Predicting the next utterance in dialogue is contingent on encoding of users'
input text to generate appropriate and relevant response in data-driven
approaches. Although the semantic and syntactic quality of the language
generated is evaluated, more often than not, the encoded representation of
input is not evaluated. As the representation of the encoder is essential for
predicting the appropriate response, evaluation of encoder representation is a
challenging yet important problem. In this work, we showcase evaluating the
text generated through human or automatic metrics is not sufficient to
appropriately evaluate soundness of the language understanding of dialogue
models and, to that end, propose a set of probe tasks to evaluate encoder
representation of different language encoders commonly used in dialogue models.
From experiments, we observe that some of the probe tasks are easier and some
are harder for even sophisticated model architectures to learn. And, through
experiments we observe that RNN based architectures have lower performance on
automatic metrics on text generation than transformer model but perform better
than the transformer model on the probe tasks indicating that RNNs might
preserve task information better than the Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1"&gt;Prasanna Parthasarathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1"&gt;Sarath Chandar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piano Skills Assessment. (arXiv:2101.04884v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04884</id>
        <link href="http://arxiv.org/abs/2101.04884"/>
        <updated>2021-06-22T01:57:08.220Z</updated>
        <summary type="html"><![CDATA[Can a computer determine a piano player's skill level? Is it preferable to
base this assessment on visual analysis of the player's performance or should
we trust our ears over our eyes? Since current CNNs have difficulty processing
long video videos, how can shorter clips be sampled to best reflect the players
skill level? In this work, we collect and release a first-of-its-kind dataset
for multimodal skill assessment focusing on assessing piano player's skill
level, answer the asked questions, initiate work in automated evaluation of
piano playing skills and provide baselines for future work. Dataset is
available from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1"&gt;Paritosh Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_J/0/1/0/all/0/1"&gt;Jaiden Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1"&gt;Brendan Morris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Contextual Design of Convolutional Neural Network for Steganalysis. (arXiv:2106.10430v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.10430</id>
        <link href="http://arxiv.org/abs/2106.10430"/>
        <updated>2021-06-22T01:57:08.194Z</updated>
        <summary type="html"><![CDATA[In recent times, deep learning-based steganalysis classifiers became popular
due to their state-of-the-art performance. Most deep steganalysis classifiers
usually extract noise residuals using high-pass filters as preprocessing steps
and feed them to their deep model for classification. It is observed that
recent steganographic embedding does not always restrict their embedding in the
high-frequency zone; instead, they distribute it as per embedding policy.
Therefore, besides noise residual, learning the embedding zone is another
challenging task. In this work, unlike the conventional approaches, the
proposed model first extracts the noise residual using learned denoising
kernels to boost the signal-to-noise ratio. After preprocessing, the sparse
noise residuals are fed to a novel Multi-Contextual Convolutional Neural
Network (M-CNET) that uses heterogeneous context size to learn the sparse and
low-amplitude representation of noise residuals. The model performance is
further improved by incorporating the Self-Attention module to focus on the
areas prone to steganalytic embedding. A set of comprehensive experiments is
performed to show the proposed scheme's efficacy over the prior arts. Besides,
an ablation study is given to justify the contribution of various modules of
the proposed architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1"&gt;Brijesh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1"&gt;Arijit Sur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1"&gt;Pinaki Mitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions. (arXiv:2010.07904v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07904</id>
        <link href="http://arxiv.org/abs/2010.07904"/>
        <updated>2021-06-21T02:07:41.330Z</updated>
        <summary type="html"><![CDATA[We consider a best arm identification (BAI) problem for stochastic bandits
with adversarial corruptions in the fixed-budget setting of T steps. We design
a novel randomized algorithm, Probabilistic Sequential Shrinking($u$)
(PSS($u$)), which is agnostic to the amount of corruptions. When the amount of
corruptions per step (CPS) is below a threshold, PSS($u$) identifies the best
arm or item with probability tending to $1$ as $T\rightarrow \infty$.
Otherwise, the optimality gap of the identified item degrades gracefully with
the CPS.We argue that such a bifurcation is necessary. In PSS($u$), the
parameter $u$ serves to balance between the optimality gap and success
probability. The injection of randomization is shown to be essential to
mitigate the impact of corruptions. To demonstrate this, we design two attack
strategies that are applicable to any algorithm. We apply one of them to a
deterministic analogue of PSS($u$) known as Successive Halving (SH) by Karnin
et al. (2013). The attack strategy results in a high failure probability for
SH, but PSS($u$) remains robust. In the absence of corruptions, PSS($2$)'s
performance guarantee matches SH's. We show that when the CPS is sufficiently
large, no algorithm can achieve a BAI probability tending to $1$ as
$T\rightarrow \infty$. Numerical experiments corroborate our theoretical
findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1"&gt;Zixin Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_W/0/1/0/all/0/1"&gt;Wang Chi Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis. (arXiv:2010.05050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05050</id>
        <link href="http://arxiv.org/abs/2010.05050"/>
        <updated>2021-06-21T02:07:41.323Z</updated>
        <summary type="html"><![CDATA[Probabilistic software analysis aims at quantifying the probability of a
target event occurring during the execution of a program processing uncertain
incoming data or written itself using probabilistic programming constructs.
Recent techniques combine symbolic execution with model counting or solution
space quantification methods to obtain accurate estimates of the occurrence
probability of rare target events, such as failures in a mission-critical
system. However, they face several scalability and applicability limitations
when analyzing software processing with high-dimensional and correlated
multivariate input distributions. In this paper, we present SYMbolic Parallel
Adaptive Importance Sampling (SYMPAIS), a new inference method tailored to
analyze path conditions generated from the symbolic execution of programs with
high-dimensional, correlated input distributions. SYMPAIS combines results from
importance sampling and constraint solving to produce accurate estimates of the
satisfaction probability for a broad class of constraints that cannot be
analyzed by current solution space quantification methods. We demonstrate
SYMPAIS's generality and performance compared with state-of-the-art
alternatives on a set of problems from different application domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yicheng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1"&gt;Antonio Filieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02225</id>
        <link href="http://arxiv.org/abs/2106.02225"/>
        <updated>2021-06-21T02:07:41.307Z</updated>
        <summary type="html"><![CDATA[The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1"&gt;Shufeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01939</id>
        <link href="http://arxiv.org/abs/2106.01939"/>
        <updated>2021-06-21T02:07:41.301Z</updated>
        <summary type="html"><![CDATA[We address the estimation of conditional average treatment effects (CATEs)
when treatments are graph-structured (e.g., molecular graphs of drugs). Given a
weak condition on the effect, we propose a plug-in estimator that decomposes
CATE estimation into separate, simpler optimization problems. Our estimator (a)
isolates the causal estimands (reducing regularization bias), and (b) allows
one to plug in arbitrary models for learning. In experiments with small-world
and molecular graphs, we show that our approach outperforms prior approaches
and is robust to varying selection biases. Our implementation is online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1"&gt;Jean Kaddour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off. (arXiv:2106.05522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05522</id>
        <link href="http://arxiv.org/abs/2106.05522"/>
        <updated>2021-06-21T02:07:41.280Z</updated>
        <summary type="html"><![CDATA[A common assumption in machine learning is that samples are independently and
identically distributed (i.i.d). However, the contributions of different
samples are not identical in training. Some samples are difficult to learn and
some samples are noisy. The unequal contributions of samples has a considerable
effect on training performances. Studies focusing on unequal sample
contributions (e.g., easy, hard, noisy) in learning usually refer to these
contributions as robust machine learning (RML). Weighing and regularization are
two common techniques in RML. Numerous learning algorithms have been proposed
but the strategies for dealing with easy/hard/noisy samples differ or even
contradict with different learning algorithms. For example, some strategies
take the hard samples first, whereas some strategies take easy first.
Conducting a clear comparison for existing RML algorithms in dealing with
different samples is difficult due to lack of a unified theoretical framework
for RML. This study attempts to construct a mathematical foundation for RML
based on the bias-variance trade-off theory. A series of definitions and
properties are presented and proved. Several classical learning algorithms are
also explained and compared. Improvements of existing methods are obtained
based on the comparison. A unified method that combines two classical learning
strategies is proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_O/0/1/0/all/0/1"&gt;Ou Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Weiyao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yingjun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haixiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1"&gt;Qinghu Hou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-21T02:07:41.262Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Kernel Matrix Algebra via Density Estimation. (arXiv:2102.08341v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08341</id>
        <link href="http://arxiv.org/abs/2102.08341"/>
        <updated>2021-06-21T02:07:41.256Z</updated>
        <summary type="html"><![CDATA[We study fast algorithms for computing fundamental properties of a positive
semidefinite kernel matrix $K \in \mathbb{R}^{n \times n}$ corresponding to $n$
points $x_1,\ldots,x_n \in \mathbb{R}^d$. In particular, we consider estimating
the sum of kernel matrix entries, along with its top eigenvalue and
eigenvector.

We show that the sum of matrix entries can be estimated to $1+\epsilon$
relative error in time $sublinear$ in $n$ and linear in $d$ for many popular
kernels, including the Gaussian, exponential, and rational quadratic kernels.
For these kernels, we also show that the top eigenvalue (and an approximate
eigenvector) can be approximated to $1+\epsilon$ relative error in time
$subquadratic$ in $n$ and linear in $d$.

Our algorithms represent significant advances in the best known runtimes for
these problems. They leverage the positive definiteness of the kernel matrix,
along with a recent line of work on efficient kernel density estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1"&gt;Arturs Backurs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indyk_P/0/1/0/all/0/1"&gt;Piotr Indyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1"&gt;Tal Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAN for time series prediction, data assimilation and uncertainty quantification. (arXiv:2105.13859v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13859</id>
        <link href="http://arxiv.org/abs/2105.13859"/>
        <updated>2021-06-21T02:07:41.248Z</updated>
        <summary type="html"><![CDATA[We propose a new method in which a generative adversarial network (GAN) is
used to quantify the uncertainty of forward simulations in the presence of
observed data. Previously, a method has been developed which enables GANs to
make time series predictions and data assimilation by training a GAN with
unconditional simulations of a high-fidelity numerical model. After training,
the GAN can be used to predict the evolution of the spatial distribution of the
simulation states and observed data is assimilated. In this paper, we describe
the process required in order to quantify uncertainty, during which no
additional simulations of the high-fidelity numerical model are required. These
methods take advantage of the adjoint-like capabilities of generative models
and the ability to simulate forwards and backwards in time. Set within a
reduced-order model framework for efficiency, we apply these methods to a
compartmental model in epidemiology to predict the spread of COVID-19 in an
idealised town. The results show that the proposed method can efficiently
quantify uncertainty in the presence of measurements using only unconditional
simulations of the high-fidelity numerical model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Vinicius L. S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1"&gt;Claire E. Heaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1"&gt;Christopher C. Pain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference. (arXiv:2106.00075v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00075</id>
        <link href="http://arxiv.org/abs/2106.00075"/>
        <updated>2021-06-21T02:07:41.236Z</updated>
        <summary type="html"><![CDATA[Bayesian phylogenetic inference is often conducted via local or sequential
search over topologies and branch lengths using algorithms such as random-walk
Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).
However, when MCMC is used for evolutionary parameter learning, convergence
requires long runs with inefficient exploration of the state space. We
introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful
framework that establishes variational sequential search to learn distributions
over intricate combinatorial structures. We then develop nested CSMC, an
efficient proposal distribution for CSMC and prove that nested CSMC is an exact
approximation to the (intractable) locally optimal proposal. We use nested CSMC
to define a second objective, VNCSMC which yields tighter lower bounds than
VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore
higher probability spaces than existing methods on a range of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Moretti_A/0/1/0/all/0/1"&gt;Antonio Khalil Moretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1"&gt;Christian A. Naesseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venner_H/0/1/0/all/0/1"&gt;Hadiah Venner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peer_I/0/1/0/all/0/1"&gt;Itsik Pe&amp;#x27;er&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning. (arXiv:2104.02532v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02532</id>
        <link href="http://arxiv.org/abs/2104.02532"/>
        <updated>2021-06-21T02:07:41.229Z</updated>
        <summary type="html"><![CDATA[Machine Learning models have been deployed across many different aspects of
society, often in situations that affect social welfare. Although these models
offer streamlined solutions to large problems, they may contain biases and
treat groups or individuals unfairly based on protected attributes such as
gender. In this paper, we introduce several examples of machine learning gender
bias in practice followed by formalizations of fairness. We provide a survey of
fairness research by detailing influential pre-processing, in-processing, and
post-processing bias mitigation algorithms. We then propose an
\textup{end-to-end bias mitigation} framework, which employs a fusion of pre-,
in-, and post-processing methods to leverage the strengths of each individual
technique. We test this method, along with the standard techniques we review,
on a deep neural network to analyze bias mitigation in a deep learning setting.
We find that our end-to-end bias mitigation framework outperforms the baselines
with respect to several fairness metrics, suggesting its promise as a method
for improving fairness. As society increasingly relies on artificial
intelligence to help in decision-making, addressing gender biases present in
deep learning models is imperative. To provide readers with the tools to assess
the fairness of machine learning models and mitigate the biases present in
them, we discuss multiple open source packages for fairness in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_T/0/1/0/all/0/1"&gt;Tal Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peake_A/0/1/0/all/0/1"&gt;Ashley Peake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-06-21T02:07:41.212Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09874</id>
        <link href="http://arxiv.org/abs/2106.09874"/>
        <updated>2021-06-21T02:07:41.205Z</updated>
        <summary type="html"><![CDATA[Finding a suitable data representation for a specific task has been shown to
be crucial in many applications. The success of subspace clustering depends on
the assumption that the data can be separated into different subspaces.
However, this simple assumption does not always hold since the raw data might
not be separable into subspaces. To recover the ``clustering-friendly''
representation and facilitate the subsequent clustering, we propose a graph
filtering approach by which a smooth representation is achieved. Specifically,
it injects graph similarity into data features by applying a low-pass filter to
extract useful data representations for clustering. Extensive experiments on
image and document clustering datasets demonstrate that our method improves
upon state-of-the-art subspace clustering techniques. Especially, its
comparable performance with deep learning methods emphasizes the effectiveness
of the simple graph filtering scheme for many real-world applications. An
ablation study shows that graph filtering can remove noise, preserve structure
in the image, and increase the separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guangchun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection. (arXiv:2106.09989v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09989</id>
        <link href="http://arxiv.org/abs/2106.09989"/>
        <updated>2021-06-21T02:07:41.197Z</updated>
        <summary type="html"><![CDATA[Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful
representation abilities of graphs as well as recent advances in graph mining
techniques. These GAD tools, however, expose a new attacking surface,
ironically due to their unique advantage of being able to exploit the relations
among data. That is, attackers now can manipulate those relations (i.e., the
structure of the graph) to allow some target nodes to evade detection. In this
paper, we exploit this vulnerability by designing a new type of targeted
structural poisoning attacks to a representative regression-based GAD system
termed OddBall. Specially, we formulate the attack against OddBall as a
bi-level optimization problem, where the key technical challenge is to
efficiently solve the problem in a discrete domain. We propose a novel attack
method termed BinarizedAttack based on gradient descent. Comparing to prior
arts, BinarizedAttack can better use the gradient information, making it
particularly suitable for solving combinatorial optimization problems.
Furthermore, we investigate the attack transferability of BinarizedAttack by
employing it to attack other representation-learning-based GAD systems. Our
comprehensive experiments demonstrate that BinarizedAttack is very effective in
enabling target nodes to evade graph-based anomaly detection tools with limited
attackers' budget, and in the black-box transfer attack setting,
BinarizedAttack is also tested effective and in particular, can significantly
change the node embeddings learned by the GAD systems. Our research thus opens
the door to studying a new type of attack against security analytic tools that
rely on graph data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yulin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yuni Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1"&gt;Kaifa Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiapu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Mingquan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kai Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising simulations for diphoton production at hadron colliders using amplitude neural networks. (arXiv:2106.09474v1 [hep-ph] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.09474</id>
        <link href="http://arxiv.org/abs/2106.09474"/>
        <updated>2021-06-21T02:07:41.142Z</updated>
        <summary type="html"><![CDATA[Machine learning technology has the potential to dramatically optimise event
generation and simulations. We continue to investigate the use of neural
networks to approximate matrix elements for high-multiplicity scattering
processes. We focus on the case of loop-induced diphoton production through
gluon fusion and develop a realistic simulation method that can be applied to
hadron collider observables. Neural networks are trained using the one-loop
amplitudes implemented in the NJet C++ library and interfaced to the Sherpa
Monte Carlo event generator where we perform a detailed study for $2\to3$ and
$2\to4$ scattering problems. We also consider how the trained networks perform
when varying the kinematic cuts effecting the phase space and the reliability
of the neural network simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Aylett_Bullock_J/0/1/0/all/0/1"&gt;Joseph Aylett-Bullock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Badger_S/0/1/0/all/0/1"&gt;Simon Badger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Moodie_R/0/1/0/all/0/1"&gt;Ryan Moodie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coresets for Classification -- Simplified and Strengthened. (arXiv:2106.04254v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04254</id>
        <link href="http://arxiv.org/abs/2106.04254"/>
        <updated>2021-06-21T02:07:41.136Z</updated>
        <summary type="html"><![CDATA[We give relative error coresets for training linear classifiers with a broad
class of loss functions, including the logistic loss and hinge loss. Our
construction achieves $(1\pm \epsilon)$ relative error with $\tilde O(d \cdot
\mu_y(X)^2/\epsilon^2)$ points, where $\mu_y(X)$ is a natural complexity
measure of the data matrix $X \in \mathbb{R}^{n \times d}$ and label vector $y
\in \{-1,1\}^n$, introduced in by Munteanu et al. 2018. Our result is based on
subsampling data points with probabilities proportional to their $\ell_1$
$Lewis$ $weights$. It significantly improves on existing theoretical bounds and
performs well in practice, outperforming uniform subsampling along with other
importance sampling methods. Our sampling distribution does not depend on the
labels, so can be used for active learning. It also does not depend on the
specific loss function, so a single coreset can be used in multiple training
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_T/0/1/0/all/0/1"&gt;Tung Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1"&gt;Anup B. Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1"&gt;Cameron Musco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Doubly Constrained Batch Reinforcement Learning. (arXiv:2102.09225v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09225</id>
        <link href="http://arxiv.org/abs/2102.09225"/>
        <updated>2021-06-21T02:07:41.077Z</updated>
        <summary type="html"><![CDATA[Reliant on too many experiments to learn good actions, current Reinforcement
Learning (RL) algorithms have limited applicability in real-world settings,
which can be too expensive to allow exploration. We propose an algorithm for
batch RL, where effective policies are learned using only a fixed offline
dataset instead of online interactions with the environment. The limited data
in batch RL produces inherent uncertainty in value estimates of states/actions
that were insufficiently represented in the training data. This leads to
particularly severe extrapolation when our candidate policies diverge from one
that generated the data. We propose to mitigate this issue via two
straightforward penalties: a policy-constraint to reduce this divergence and a
value-constraint that discourages overly optimistic estimates. Over a
comprehensive set of 32 continuous-action batch RL benchmarks, our approach
compares favorably to state-of-the-art methods, regardless of how the offline
data were collected.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1"&gt;Rasool Fakoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1"&gt;Jonas Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadi_K/0/1/0/all/0/1"&gt;Kavosh Asadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1"&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1"&gt;Alexander J. Smola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Implicit Networks via Non-Euclidean Contractions. (arXiv:2106.03194v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03194</id>
        <link href="http://arxiv.org/abs/2106.03194"/>
        <updated>2021-06-21T02:07:41.070Z</updated>
        <summary type="html"><![CDATA[Implicit neural networks, a.k.a., deep equilibrium networks, are a class of
implicit-depth learning models where function evaluation is performed by
solving a fixed point equation. They generalize classic feedforward models and
are equivalent to infinite-depth weight-tied feedforward networks. While
implicit models show improved accuracy and significant reduction in memory
consumption, they can suffer from ill-posedness and convergence instability.

This paper provides a new framework to design well-posed and robust implicit
neural networks based upon contraction theory for the non-Euclidean norm
$\ell_\infty$. Our framework includes (i) a novel condition for well-posedness
based on one-sided Lipschitz constants, (ii) an average iteration for computing
fixed-points, and (iii) explicit estimates on input-output Lipschitz constants.
Additionally, we design a training problem with the well-posedness condition
and the average iteration as constraints and, to achieve robust models, with
the input-output Lipschitz constant as a regularizer. Our $\ell_\infty$
well-posedness condition leads to a larger polytopic training search space than
existing conditions and our average iteration enjoys accelerated convergence.
Finally, we perform several numerical experiments for function estimation and
digit classification through the MNIST data set. Our numerical results
demonstrate improved accuracy and robustness of the implicit models with
smaller input-output Lipschitz bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jafarpour_S/0/1/0/all/0/1"&gt;Saber Jafarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davydov_A/0/1/0/all/0/1"&gt;Alexander Davydov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Proskurnikov_A/0/1/0/all/0/1"&gt;Anton V. Proskurnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1"&gt;Francesco Bullo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline. (arXiv:2106.06054v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06054</id>
        <link href="http://arxiv.org/abs/2106.06054"/>
        <updated>2021-06-21T02:07:41.062Z</updated>
        <summary type="html"><![CDATA[In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1"&gt;Sumon Biswas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1"&gt;Hridesh Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ray-based framework for state identification in quantum dot devices. (arXiv:2102.11784v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11784</id>
        <link href="http://arxiv.org/abs/2102.11784"/>
        <updated>2021-06-21T02:07:41.054Z</updated>
        <summary type="html"><![CDATA[Quantum dots (QDs) defined with electrostatic gates are a leading platform
for a scalable quantum computing implementation. However, with increasing
numbers of qubits, the complexity of the control parameter space also grows.
Traditional measurement techniques, relying on complete or near-complete
exploration via two-parameter scans (images) of the device response, quickly
become impractical with increasing numbers of gates. Here we propose to
circumvent this challenge by introducing a measurement technique relying on
one-dimensional projections of the device response in the multidimensional
parameter space. Dubbed the ``ray-based classification (RBC) framework,'' we
use this machine learning approach to implement a classifier for QD states,
enabling automated recognition of qubit-relevant parameter regimes. We show
that RBC surpasses the 82 % accuracy benchmark from the experimental
implementation of image-based classification techniques from prior work while
reducing the number of measurement points needed by up to 70 %. The reduction
in measurement cost is a significant gain for time-intensive QD measurements
and is a step forward toward the scalability of these devices. We also discuss
how the RBC-based optimizer, which tunes the device to a multiqubit regime,
performs when tuning in the two-dimensional and three-dimensional parameter
spaces defined by plunger and barrier gates that control the QDs.This work
provides experimental validation of both efficient state identification and
optimization with machine learning techniques for non-traditional measurements
in quantum systems with high-dimensional parameter spaces and time-intensive
measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1"&gt;Thomas McJunkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1"&gt;Sandesh S. Kalantre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Neyens_S/0/1/0/all/0/1"&gt;Samuel F. Neyens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+MacQuarrie_E/0/1/0/all/0/1"&gt;E. R. MacQuarrie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1"&gt;Mark A. Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1"&gt;Jacob M. Taylor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Slow Momentum with Fast Reversion: A Trading Strategy Using Deep Learning and Changepoint Detection. (arXiv:2105.13727v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13727</id>
        <link href="http://arxiv.org/abs/2105.13727"/>
        <updated>2021-06-21T02:07:41.037Z</updated>
        <summary type="html"><![CDATA[Momentum strategies are an important part of alternative investments and are
at the heart of commodity trading advisors (CTAs). These strategies have
however been found to have difficulties adjusting to rapid changes in market
conditions, such as during the 2020 market crash. In particular, immediately
after momentum turning points, where a trend reverses from an uptrend
(downtrend) to a downtrend (uptrend), time-series momentum (TSMOM) strategies
are prone to making bad bets. To improve the response to regime change, we
introduce a novel approach, where we insert an online change-point detection
(CPD) module into a Deep Momentum Network (DMN) [1904.04912] pipeline, which
uses an LSTM deep-learning architecture to simultaneously learn both trend
estimation and position sizing. Furthermore, our model is able to optimise the
way in which it balances 1) a slow momentum strategy which exploits persisting
trends, but does not overreact to localised price moves, and 2) a fast
mean-reversion strategy regime by quickly flipping its position, then swapping
it back again to exploit localised price moves. Our CPD module outputs a
changepoint location and severity score, allowing our model to learn to respond
to varying degrees of disequilibrium, or smaller and more localised
changepoints, in a data driven manner. Using a portfolio of 50, liquid,
continuous futures contracts over the period 1990-2020, the addition of the CPD
module leads to an improvement in Sharpe ratio of one-third. Even more notably,
this module is especially beneficial in periods of significant nonstationarity,
and in particular, over the most recent years tested (2015-2020) the
performance boost is approximately two-thirds. This is especially interesting
as traditional momentum strategies have been underperforming in this period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wood_K/0/1/0/all/0/1"&gt;Kieran Wood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Max-Margin is Dead, Long Live Max-Margin!. (arXiv:2105.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15069</id>
        <link href="http://arxiv.org/abs/2105.15069"/>
        <updated>2021-06-21T02:07:41.028Z</updated>
        <summary type="html"><![CDATA[The foundational concept of Max-Margin in machine learning is ill-posed for
output spaces with more than two labels such as in structured prediction. In
this paper, we show that the Max-Margin loss can only be consistent to the
classification task under highly restrictive assumptions on the discrete loss
measuring the error between outputs. These conditions are satisfied by
distances defined in tree graphs, for which we prove consistency, thus being
the first losses shown to be consistent for Max-Margin beyond the binary
setting. We finally address these limitations by correcting the concept of
Max-Margin and introducing the Restricted-Max-Margin, where the maximization of
the loss-augmented scores is maintained, but performed over a subset of the
original domain. The resulting loss is also a generalization of the binary
support vector machine and it is consistent under milder conditions on the
discrete loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nowak_Vila_A/0/1/0/all/0/1"&gt;Alex Nowak-Vila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Pharmacodynamic State Space Modeling. (arXiv:2102.11218v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11218</id>
        <link href="http://arxiv.org/abs/2102.11218"/>
        <updated>2021-06-21T02:07:41.022Z</updated>
        <summary type="html"><![CDATA[Modeling the time-series of high-dimensional, longitudinal data is important
for predicting patient disease progression. However, existing neural network
based approaches that learn representations of patient state, while very
flexible, are susceptible to overfitting. We propose a deep generative model
that makes use of a novel attention-based neural architecture inspired by the
physics of how treatments affect disease state. The result is a scalable and
accurate model of high-dimensional patient biomarkers as they vary over time.
Our proposed model yields significant improvements in generalization and, on
real-world clinical data, provides interpretable insights into the dynamics of
cancer progression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_Z/0/1/0/all/0/1"&gt;Zeshan Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1"&gt;Rahul G. Krishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01886</id>
        <link href="http://arxiv.org/abs/2102.01886"/>
        <updated>2021-06-21T02:07:41.014Z</updated>
        <summary type="html"><![CDATA[In adversarial training (AT), the main focus has been the objective and
optimizer while the model has been less studied, so that the models being used
are still those classic ones in standard training (ST). Classic network
architectures (NAs) are generally worse than searched NAs in ST, which should
be the same in AT. In this paper, we argue that NA and AT cannot be handled
independently, since given a dataset, the optimal NA in ST would be no longer
optimal in AT. That being said, AT is time-consuming itself; if we directly
search NAs in AT over large search spaces, the computation will be practically
infeasible. Thus, we propose a diverse-structured network (DS-Net), to
significantly reduce the size of the search space: instead of low-level
operations, we only consider predefined atomic blocks, where an atomic block is
a time-tested building block like the residual block. There are only a few
atomic blocks and thus we can weight all atomic blocks rather than find the
best one in a searched block of DS-Net, which is an essential trade-off between
exploring diverse structures and exploiting the best structures. Empirical
results demonstrate the advantages of DS-Net, i.e., weighting the atomic
blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08482</id>
        <link href="http://arxiv.org/abs/2101.08482"/>
        <updated>2021-06-21T02:07:41.007Z</updated>
        <summary type="html"><![CDATA[We present a plug-in replacement for batch normalization (BN) called
exponential moving average normalization (EMAN), which improves the performance
of existing student-teacher based self- and semi-supervised learning
techniques. Unlike the standard BN, where the statistics are computed within
each batch, EMAN, used in the teacher, updates its statistics by exponential
moving average from the BN statistics of the student. This design reduces the
intrinsic cross-sample dependency of BN and enhances the generalization of the
teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2
points and semi-supervised learning by about 7/2 points, when 1%/10% supervised
labels are available on ImageNet. These improvements are consistent across
methods, network architectures, training duration, and datasets, demonstrating
the general effectiveness of this technique. The code is available at
https://github.com/amazon-research/exponential-moving-average-normalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhaowei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Assimilation Predictive GAN (DA-PredGAN): applied to determine the spread of COVID-19. (arXiv:2105.07729v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07729</id>
        <link href="http://arxiv.org/abs/2105.07729"/>
        <updated>2021-06-21T02:07:41.000Z</updated>
        <summary type="html"><![CDATA[We propose the novel use of a generative adversarial network (GAN) (i) to
make predictions in time (PredGAN) and (ii) to assimilate measurements
(DA-PredGAN). In the latter case, we take advantage of the natural adjoint-like
properties of generative models and the ability to simulate forwards and
backwards in time. GANs have received much attention recently, after achieving
excellent results for their generation of realistic-looking images. We wish to
explore how this property translates to new applications in computational
modelling and to exploit the adjoint-like properties for efficient data
assimilation. To predict the spread of COVID-19 in an idealised town, we apply
these methods to a compartmental model in epidemiology that is able to model
space and time variations. To do this, the GAN is set within a reduced-order
model (ROM), which uses a low-dimensional space for the spatial distribution of
the simulation states. Then the GAN learns the evolution of the low-dimensional
states over time. The results show that the proposed methods can accurately
predict the evolution of the high-fidelity numerical simulation, and can
efficiently assimilate observed data and determine the corresponding model
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Vinicius L. S. Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heaney_C/0/1/0/all/0/1"&gt;Claire E. Heaney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pain_C/0/1/0/all/0/1"&gt;Christopher C. Pain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quasi-Global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data. (arXiv:2102.04761v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04761</id>
        <link href="http://arxiv.org/abs/2102.04761"/>
        <updated>2021-06-21T02:07:40.983Z</updated>
        <summary type="html"><![CDATA[Decentralized training of deep learning models is a key element for enabling
data privacy and on-device learning over networks. In realistic learning
scenarios, the presence of heterogeneity across different clients' local
datasets poses an optimization challenge and may severely deteriorate the
generalization performance. In this paper, we investigate and identify the
limitation of several decentralized optimization algorithms for different
degrees of data heterogeneity. We propose a novel momentum-based method to
mitigate this decentralized training difficulty. We show in extensive empirical
experiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and
several network topologies (Ring and Social Network) that our method is much
more robust to the heterogeneity of clients' data than other existing methods,
by a significant improvement in test performance ($1\% \!-\! 20\%$). Our code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1"&gt;Sai Praneeth Karimireddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Dropout Variational Inference for Bayesian Neural Networks. (arXiv:2102.07927v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07927</id>
        <link href="http://arxiv.org/abs/2102.07927"/>
        <updated>2021-06-21T02:07:40.975Z</updated>
        <summary type="html"><![CDATA[Approximate inference in deep Bayesian networks exhibits a dilemma of how to
yield high fidelity posterior approximations while maintaining computational
efficiency and scalability. We tackle this challenge by introducing a novel
variational structured approximation inspired by the Bayesian interpretation of
Dropout regularization. Concretely, we focus on the inflexibility of the
factorized structure in Dropout posterior and then propose an improved method
called Variational Structured Dropout (VSD). VSD employs an orthogonal
transformation to learn a structured representation on the variational noise
and consequently induces statistical dependencies in the approximate posterior.
Theoretically, VSD successfully addresses the pathologies of previous
Variational Dropout methods and thus offers a standard Bayesian justification.
We further show that VSD induces an adaptive regularization term with several
desirable properties which contribute to better generalization. Finally, we
conduct extensive experiments on standard benchmarks to demonstrate the
effectiveness of VSD over state-of-the-art variational methods on predictive
accuracy, uncertainty estimation, and out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1"&gt;Son Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khai Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Than_K/0/1/0/all/0/1"&gt;Khoat Than&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_H/0/1/0/all/0/1"&gt;Hung Bui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound. (arXiv:2010.15761v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15761</id>
        <link href="http://arxiv.org/abs/2010.15761"/>
        <updated>2021-06-21T02:07:40.968Z</updated>
        <summary type="html"><![CDATA[Transcranial ultrasound therapy is increasingly used for the non-invasive
treatment of brain disorders. However, conventional numerical wave solvers are
currently too computationally expensive to be used online during treatments to
predict the acoustic field passing through the skull (e.g., to account for
subject-specific dose and targeting variations). As a step towards real-time
predictions, in the current work, a fast iterative solver for the heterogeneous
Helmholtz equation in 2D is developed using a fully-learned optimizer. The
lightweight network architecture is based on a modified UNet that includes a
learned hidden state. The network is trained using a physics-based loss
function and a set of idealized sound speed distributions with fully
unsupervised training (no knowledge of the true solution is required). The
learned optimizer shows excellent performance on the test set, and is capable
of generalization well outside the training examples, including to much larger
computational domains, and more complex source and sound speed distributions,
for example, those derived from x-ray computed tomography images of the skull.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Stanziola_A/0/1/0/all/0/1"&gt;Antonio Stanziola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Arridge_S/0/1/0/all/0/1"&gt;Simon R. Arridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Cox_B/0/1/0/all/0/1"&gt;Ben T. Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Treeby_B/0/1/0/all/0/1"&gt;Bradley E. Treeby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Mesh-Based Simulation with Graph Networks. (arXiv:2010.03409v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.03409</id>
        <link href="http://arxiv.org/abs/2010.03409"/>
        <updated>2021-06-21T02:07:40.961Z</updated>
        <summary type="html"><![CDATA[Mesh-based simulations are central to modeling complex physical systems in
many disciplines across science and engineering. Mesh representations support
powerful numerical integration methods and their resolution can be adapted to
strike favorable trade-offs between accuracy and efficiency. However,
high-dimensional scientific simulations are very expensive to run, and solvers
and parameters must often be tuned individually to each system studied. Here we
introduce MeshGraphNets, a framework for learning mesh-based simulations using
graph neural networks. Our model can be trained to pass messages on a mesh
graph and to adapt the mesh discretization during forward simulation. Our
results show it can accurately predict the dynamics of a wide range of physical
systems, including aerodynamics, structural mechanics, and cloth. The model's
adaptivity supports learning resolution-independent dynamics and can scale to
more complex state spaces at test time. Our method is also highly efficient,
running 1-2 orders of magnitude faster than the simulation on which it is
trained. Our approach broadens the range of problems on which neural network
simulators can operate and promises to improve the efficiency of complex,
scientific modeling tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfaff_T/0/1/0/all/0/1"&gt;Tobias Pfaff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1"&gt;Meire Fortunato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1"&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1"&gt;Peter W. Battaglia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrap an end-to-end ASR system by multilingual training, transfer learning, text-to-text mapping and synthetic audio. (arXiv:2011.12696v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12696</id>
        <link href="http://arxiv.org/abs/2011.12696"/>
        <updated>2021-06-21T02:07:40.944Z</updated>
        <summary type="html"><![CDATA[Bootstrapping speech recognition on limited data resources has been an area
of active research for long. The recent transition to all-neural models and
end-to-end (E2E) training brought along particular challenges as these models
are known to be data hungry, but also came with opportunities around
language-agnostic representations derived from multilingual data as well as
shared word-piece output representations across languages that share script and
roots. We investigate here the effectiveness of different strategies to
bootstrap an RNN-Transducer (RNN-T) based automatic speech recognition (ASR)
system in the low resource regime, while exploiting the abundant resources
available in other languages as well as the synthetic audio from a
text-to-speech (TTS) engine. Our experiments demonstrate that transfer learning
from a multilingual model, using a post-ASR text-to-text mapping and synthetic
audio deliver additive improvements, allowing us to bootstrap a model for a new
language with a fraction of the data that would otherwise be needed. The best
system achieved a 46% relative word error rate (WER) reduction compared to the
monolingual baseline, among which 25% relative WER improvement is attributed to
the post-ASR text-to-text mappings and the TTS synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Giollo_M/0/1/0/all/0/1"&gt;Manuel Giollo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunceler_D/0/1/0/all/0/1"&gt;Deniz Gunceler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yulan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willett_D/0/1/0/all/0/1"&gt;Daniel Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00816</id>
        <link href="http://arxiv.org/abs/2104.00816"/>
        <updated>2021-06-21T02:07:40.937Z</updated>
        <summary type="html"><![CDATA[Despite the success of Generative Adversarial Networks (GANs), their training
suffers from several well-known problems, including mode collapse and
difficulties learning a disconnected set of manifolds. In this paper, we break
down the challenging task of learning complex high dimensional distributions,
supporting diverse data samples, to simpler sub-tasks. Our solution relies on
designing a partitioner that breaks the space into smaller regions, each having
a simpler distribution, and training a different generator for each partition.
This is done in an unsupervised manner without requiring any labels.

We formulate two desired criteria for the space partitioner that aid the
training of our mixture of generators: 1) to produce connected partitions and
2) provide a proxy of distance between partitions and data samples, along with
a direction for reducing that distance. These criteria are developed to avoid
producing samples from places with non-existent data density, and also
facilitate training by providing additional direction to the generators. We
develop theoretical constraints for a space partitioner to satisfy the above
criteria. Guided by our theoretical analysis, we design an effective neural
architecture for the space partitioner that empirically assures these
conditions. Experimental results on various standard benchmarks show that the
proposed unsupervised model outperforms several recent methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1"&gt;Mohammadreza Armandpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1"&gt;Ali Sadeghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue Problems. (arXiv:2007.01420v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01420</id>
        <link href="http://arxiv.org/abs/2007.01420"/>
        <updated>2021-06-21T02:07:40.929Z</updated>
        <summary type="html"><![CDATA[Physics-guided Neural Networks (PGNNs) represent an emerging class of neural
networks that are trained using physics-guided (PG) loss functions (capturing
violations in network outputs with known physics), along with the supervision
contained in data. Existing work in PGNNs have demonstrated the efficacy of
adding single PG loss functions in the neural network objectives, using
constant trade-off parameters, to ensure better generalizability. However, in
the presence of multiple physics loss functions with competing gradient
directions, there is a need to adaptively tune the contribution of competing PG
loss functions during the course of training to arrive at generalizable
solutions. We demonstrate the presence of competing PG losses in the generic
neural network problem of solving for the lowest (or highest) eigenvector of a
physics-based eigenvalue equation, common to many scientific problems. We
present a novel approach to handle competing PG losses and demonstrate its
efficacy in learning generalizable solutions in two motivating applications of
quantum mechanics and electromagnetic propagation. All the code and data used
in this work is available at https://github.com/jayroxis/Cophy-PGNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Elhamod_M/0/1/0/all/0/1"&gt;Mohannad Elhamod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1"&gt;Jie Bu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1"&gt;Christopher Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redell_M/0/1/0/all/0/1"&gt;Matthew Redell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Abantika Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podolskiy_V/0/1/0/all/0/1"&gt;Viktor Podolskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1"&gt;Wei-Cheng Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1"&gt;Anuj Karpatne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03384</id>
        <link href="http://arxiv.org/abs/2011.03384"/>
        <updated>2021-06-21T02:07:40.920Z</updated>
        <summary type="html"><![CDATA[Despite its best performance in image denoising, the supervised deep
denoising methods require paired noise-clean data, which are often unavailable.
To address this challenge, Noise2Noise was designed based on the fact that
paired noise-clean images can be replaced by paired noise-noise images that are
easier to collect. However, in many scenarios the collection of paired
noise-noise images is still impractical. To bypass labeled images, Noise2Void
methods predict masked pixels from their surroundings with single noisy images
only and give improved denoising results that still need improvements. An
observation on classic denoising methods is that non-local mean (NLM) outcomes
are typically superior to locally denoised results. In contrast, Noise2Void and
its variants do not utilize self-similarities in an image as the NLM-based
methods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for
image denoising. Specifically, Noise2Sim leverages the self-similarity of image
pixels to train the denoising network, requiring single noisy images only. Our
theoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise
under mild conditions. To efficiently manage the computational burden for
globally searching similar pixels, we design a two-step procedure to provide
data for Noise2Sim training. Extensive experiments demonstrate the superiority
of Noise2Sim on common benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1"&gt;Fenglei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qing Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10783</id>
        <link href="http://arxiv.org/abs/2010.10783"/>
        <updated>2021-06-21T02:07:40.911Z</updated>
        <summary type="html"><![CDATA[Representation learning on user-item graph for recommendation has evolved
from using single ID or interaction history to exploiting higher-order
neighbors. This leads to the success of graph convolution networks (GCNs) for
recommendation such as PinSage and LightGCN. Despite effectiveness, we argue
that they suffer from two limitations: (1) high-degree nodes exert larger
impact on the representation learning, deteriorating the recommendations of
low-degree (long-tail) items; and (2) representations are vulnerable to noisy
interactions, as the neighborhood aggregation scheme further enlarges the
impact of observed edges.

In this work, we explore self-supervised learning on user-item graph, so as
to improve the accuracy and robustness of GCNs for recommendation. The idea is
to supplement the classical supervised task of recommendation with an auxiliary
self-supervised task, which reinforces node representation learning via
self-discrimination. Specifically, we generate multiple views of a node,
maximizing the agreement between different views of the same node compared to
that of other nodes. We devise three operators to generate the views -- node
dropout, edge dropout, and random walk -- that change the graph structure in
different manners. We term this new learning paradigm as
\textit{Self-supervised Graph Learning} (SGL), implementing it on the
state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL
has the ability of automatically mining hard negatives. Empirical studies on
three benchmark datasets demonstrate the effectiveness of SGL, which improves
the recommendation accuracy, especially on long-tail items, and the robustness
against interaction noises. Our implementations are available at
\url{https://github.com/wujcan/SGL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiancan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jianxun Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RobustSleepNet: Transfer learning for automated sleep staging at scale. (arXiv:2101.02452v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02452</id>
        <link href="http://arxiv.org/abs/2101.02452"/>
        <updated>2021-06-21T02:07:40.904Z</updated>
        <summary type="html"><![CDATA[Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)
records. As a preliminary step of this examination, sleep stages are
systematically determined. In practice, sleep stage classification relies on
the visual inspection of 30-second epochs of polysomnography signals. Numerous
automatic approaches have been developed to replace this tedious and expensive
task. Although these methods demonstrated better performance than human sleep
experts on specific datasets, they remain largely unused in sleep clinics. The
main reason is that each sleep clinic uses a specific PSG montage that most
automatic approaches cannot handle out-of-the-box. Moreover, even when the PSG
montage is compatible, publications have shown that automatic approaches
perform poorly on unseen data with different demographics. To address these
issues, we introduce RobustSleepNet, a deep learning model for automatic sleep
stage classification able to handle arbitrary PSG montages. We trained and
evaluated this model in a leave-one-out-dataset fashion on a large corpus of 8
heterogeneous sleep staging datasets to make it robust to demographic changes.
When evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a
model explicitly trained on this dataset. Hence, RobustSleepNet unlocks the
possibility to perform high-quality out-of-the-box automatic sleep staging with
any clinical setup. We further show that finetuning RobustSleepNet, using a
part of the unseen dataset, increases the F1 by 2% when compared to a model
trained specifically for this dataset. Therefore, finetuning might be used to
reach a state-of-the-art level of performance on a specific population.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Guillot_A/0/1/0/all/0/1"&gt;Antoine Guillot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thorey_V/0/1/0/all/0/1"&gt;Valentin Thorey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overparameterization of deep ResNet: zero loss and mean-field analysis. (arXiv:2105.14417v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14417</id>
        <link href="http://arxiv.org/abs/2105.14417"/>
        <updated>2021-06-21T02:07:40.897Z</updated>
        <summary type="html"><![CDATA[Finding parameters in a deep neural network (NN) that fit training data is a
nonconvex optimization problem, but a basic first-order optimization method
(gradient descent) finds a global solution with perfect fit in many practical
situations. We examine this phenomenon for the case of Residual Neural Networks
(ResNet) with smooth activation functions in a limiting regime in which both
the number of layers (depth) and the number of neurons in each layer (width) go
to infinity. First, we use a mean-field-limit argument to prove that the
gradient descent for parameter training becomes a partial differential equation
(PDE) that characterizes gradient flow for a probability distribution in the
large-NN limit. Next, we show that the solution to the PDE converges in the
training time to a zero-loss solution. Together, these results imply that
training of the ResNet also gives a near-zero loss if the Resnet is large
enough. We give estimates of the depth and width needed to reduce the loss
below a given threshold, with high probability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1"&gt;Zhiyan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wright_S/0/1/0/all/0/1"&gt;Stephen Wright&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Learning Vector Quantization for Classification in Randomized Neural Networks and Hyperdimensional Computing. (arXiv:2106.09821v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09821</id>
        <link href="http://arxiv.org/abs/2106.09821"/>
        <updated>2021-06-21T02:07:40.879Z</updated>
        <summary type="html"><![CDATA[Machine learning algorithms deployed on edge devices must meet certain
resource constraints and efficiency requirements. Random Vector Functional Link
(RVFL) networks are favored for such applications due to their simple design
and training efficiency. We propose a modified RVFL network that avoids
computationally expensive matrix operations during training, thus expanding the
network's range of potential applications. Our modification replaces the
least-squares classifier with the Generalized Learning Vector Quantization
(GLVQ) classifier, which only employs simple vector and distance calculations.
The GLVQ classifier can also be considered an improvement upon certain
classification algorithms popularly used in the area of Hyperdimensional
Computing. The proposed approach achieved state-of-the-art accuracy on a
collection of datasets from the UCI Machine Learning Repository - higher than
previously proposed RVFL networks. We further demonstrate that our approach
still achieves high accuracy while severely limited in training iterations
(using on average only 21% of the least-squares classifier computational
costs).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_C/0/1/0/all/0/1"&gt;Cameron Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabaey_J/0/1/0/all/0/1"&gt;Jan M. Rabaey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1"&gt;Bruno A. Olshausen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedADC: Accelerated Federated Learning with Drift Control. (arXiv:2012.09102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09102</id>
        <link href="http://arxiv.org/abs/2012.09102"/>
        <updated>2021-06-21T02:07:40.850Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) has become de facto framework for collaborative
learning among edge devices with privacy concern. The core of the FL strategy
is the use of stochastic gradient descent (SGD) in a distributed manner. Large
scale implementation of FL brings new challenges, such as the incorporation of
acceleration techniques designed for SGD into the distributed setting, and
mitigation of the drift problem due to non-homogeneous distribution of local
datasets. These two problems have been separately studied in the literature;
whereas, in this paper, we show that it is possible to address both problems
using a single strategy without any major alteration to the FL framework, or
introducing additional computation and communication load. To achieve this
goal, we propose FedADC, which is an accelerated FL algorithm with drift
control. We empirically illustrate the advantages of FedADC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1"&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1"&gt;Kerem Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09835</id>
        <link href="http://arxiv.org/abs/2106.09835"/>
        <updated>2021-06-21T02:07:40.839Z</updated>
        <summary type="html"><![CDATA[This paper proposes two novel knowledge transfer techniques for
class-incremental learning (CIL). First, we propose data-free generative replay
(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples
from a generative model. In the conventional generative replay, the generative
model is pre-trained for old data and shared in extra memory for later
incremental learning. In our proposed DF-GR, we train a generative model from
scratch without using any training data, based on the pre-trained
classification model from the past, so we curtail the cost of sharing
pre-trained generative models. Second, we introduce dual-teacher information
distillation (DT-ID) for knowledge distillation from two teachers to one
student. In CIL, we use DT-ID to learn new classes incrementally based on the
pre-trained model for old classes and another model (pre-)trained on the new
data for new classes. We implemented the proposed schemes on top of one of the
state-of-the-art CIL methods and showed the performance improvement on
CIFAR-100 and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yoojin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1"&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungwon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Architectural Patterns for the Design of Federated Learning Systems. (arXiv:2101.02373v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02373</id>
        <link href="http://arxiv.org/abs/2101.02373"/>
        <updated>2021-06-21T02:07:40.830Z</updated>
        <summary type="html"><![CDATA[Federated learning has received fast-growing interests from academia and
industry to tackle the challenges of data hungriness and privacy in machine
learning. A federated learning system can be viewed as a large-scale
distributed system with different components and stakeholders as numerous
client devices participate in federated learning. Designing a federated
learning system requires software system design thinking apart from machine
learning knowledge. Although much effort has been put into federated learning
from the machine learning technique aspects, the software architecture design
concerns in building federated learning systems have been largely ignored.
Therefore, in this paper, we present a collection of architectural patterns to
deal with the design challenges of federated learning systems. Architectural
patterns present reusable solutions to a commonly occurring problem within a
given context during software architecture design. The presented patterns are
based on the results of a systematic literature review and include three client
management patterns, four model management patterns, three model training
patterns, and four model aggregation patterns. The patterns are associated to
the particular state transitions in a federated learning model lifecycle,
serving as a guidance for effective use of the patterns in the design of
federated learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lo_S/0/1/0/all/0/1"&gt;Sin Kit Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qinghua Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_H/0/1/0/all/0/1"&gt;Hye-young Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiwei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Water Temperature Dynamics of Unmonitored Lakes with Meta Transfer Learning. (arXiv:2011.05369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05369</id>
        <link href="http://arxiv.org/abs/2011.05369"/>
        <updated>2021-06-21T02:07:40.818Z</updated>
        <summary type="html"><![CDATA[Most environmental data come from a minority of well-monitored sites. An
ongoing challenge in the environmental sciences is transferring knowledge from
monitored sites to unmonitored sites. Here, we demonstrate a novel transfer
learning framework that accurately predicts depth-specific temperature in
unmonitored lakes (targets) by borrowing models from well-monitored lakes
(sources). This method, Meta Transfer Learning (MTL), builds a meta-learning
model to predict transfer performance from candidate source models to targets
using lake attributes and candidates' past performance. We constructed source
models at 145 well-monitored lakes using calibrated process-based modeling (PB)
and a recently developed approach called process-guided deep learning (PGDL).
We applied MTL to either PB or PGDL source models (PB-MTL or PGDL-MTL,
respectively) to predict temperatures in 305 target lakes treated as
unmonitored in the Upper Midwestern United States. We show significantly
improved performance relative to the uncalibrated process-based General Lake
Model, where the median RMSE for the target lakes is $2.52^{\circ}C$. PB-MTL
yielded a median RMSE of $2.43^{\circ}C$; PGDL-MTL yielded $2.16^{\circ}C$; and
a PGDL-MTL ensemble of nine sources per target yielded $1.88^{\circ}C$. For
sparsely monitored target lakes, PGDL-MTL often outperformed PGDL models
trained on the target lakes themselves. Differences in maximum depth between
the source and target were consistently the most important predictors. Our
approach readily scales to thousands of lakes in the Midwestern United States,
demonstrating that MTL with meaningful predictor variables and high-quality
source models is a promising approach for many kinds of unmonitored systems and
environmental variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willard_J/0/1/0/all/0/1"&gt;Jared D. Willard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;Jordan S. Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Appling_A/0/1/0/all/0/1"&gt;Alison P. Appling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliver_S/0/1/0/all/0/1"&gt;Samantha K. Oliver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaowei Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vipin Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Mixup Improves the Model Performance. (arXiv:2006.06231v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06231</id>
        <link href="http://arxiv.org/abs/2006.06231"/>
        <updated>2021-06-21T02:07:40.801Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques are used in a wide range of domains. However,
machine learning models often suffer from the problem of over-fitting. Many
data augmentation methods have been proposed to tackle such a problem, and one
of them is called mixup. Mixup is a recently proposed regularization procedure,
which linearly interpolates a random pair of training examples. This
regularization method works very well experimentally, but its theoretical
guarantee is not adequately discussed. In this study, we aim to discover why
mixup works well from the aspect of the statistical learning theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kimura_M/0/1/0/all/0/1"&gt;Masanari Kimura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep State Space Models for Nonlinear System Identification. (arXiv:2003.14162v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.14162</id>
        <link href="http://arxiv.org/abs/2003.14162"/>
        <updated>2021-06-21T02:07:40.774Z</updated>
        <summary type="html"><![CDATA[Deep state space models (SSMs) are an actively researched model class for
temporal models developed in the deep learning community which have a close
connection to classic SSMs. The use of deep SSMs as a black-box identification
model can describe a wide range of dynamics due to the flexibility of deep
neural networks. Additionally, the probabilistic nature of the model class
allows the uncertainty of the system to be modelled. In this work a deep SSM
class and its parameter learning algorithm are explained in an effort to extend
the toolbox of nonlinear identification methods with a deep learning based
method. Six recent deep SSMs are evaluated in a first unified implementation on
nonlinear system identification benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gedon_D/0/1/0/all/0/1"&gt;Daniel Gedon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wahlstrom_N/0/1/0/all/0/1"&gt;Niklas Wahlstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ljung_L/0/1/0/all/0/1"&gt;Lennart Ljung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples. (arXiv:2006.15714v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15714</id>
        <link href="http://arxiv.org/abs/2006.15714"/>
        <updated>2021-06-21T02:07:40.766Z</updated>
        <summary type="html"><![CDATA[Despite the fact that deep reinforcement learning (RL) has surpassed
human-level performances in various tasks, it still has several fundamental
challenges. First, most RL methods require intensive data from the exploration
of the environment to achieve satisfactory performance. Second, the use of
neural networks in RL renders it hard to interpret the internals of the system
in a way that humans can understand. To address these two challenges, we
propose a framework that enables an RL agent to reason over its exploration
process and distill high-level knowledge for effectively guiding its future
explorations. Specifically, we propose a novel RL algorithm that learns
high-level knowledge in the form of a finite reward automaton by using the L*
learning algorithm. We prove that in episodic RL, a finite reward automaton can
express any non-Markovian bounded reward functions with finitely many reward
values and approximate any non-Markovian bounded reward function (with
infinitely many reward values) with arbitrary precision. We also provide a
lower bound for the episode length such that the proposed RL approach almost
surely converges to an optimal policy in the limit. We test this approach on
two RL environments with non-Markovian reward functions, choosing a variety of
tasks with increasing complexity for each environment. We compare our algorithm
with the state-of-the-art RL algorithms for non-Markovian reward functions,
such as Joint Inference of Reward machines and Policies for RL (JIRP), Learning
Reward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show
that our algorithm converges to an optimal policy faster than other baseline
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1"&gt;Aditya Ojha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1"&gt;Daniel Neider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01496</id>
        <link href="http://arxiv.org/abs/2007.01496"/>
        <updated>2021-06-21T02:07:40.759Z</updated>
        <summary type="html"><![CDATA[Despite the great progress made by deep neural networks in the semantic
segmentation task, traditional neural-networkbased methods typically suffer
from a shortage of large amounts of pixel-level annotations. Recent progress in
fewshot semantic segmentation tackles the issue by only a few pixel-level
annotated examples. However, these few-shot approaches cannot easily be applied
to multi-way or weak annotation settings. In this paper, we advance the
few-shot segmentation paradigm towards a scenario where image-level annotations
are available to help the training process of a few pixel-level annotations.
Our key idea is to learn a better prototype representation of the class by
fusing the knowledge from the image-level labeled data. Specifically, we
propose a new framework, called PAIA, to learn the class prototype
representation in a metric space by integrating image-level annotations.
Furthermore, by considering the uncertainty of pseudo-masks, a distilled soft
masked average pooling strategy is designed to handle distractions in
image-level annotations. Extensive empirical results on two datasets show
superior performance of PAIA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuo Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Neural Network : A model of competition between times series. (arXiv:2009.14610v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.14610</id>
        <link href="http://arxiv.org/abs/2009.14610"/>
        <updated>2021-06-21T02:07:40.753Z</updated>
        <summary type="html"><![CDATA[Competition between times series often arises in sales prediction, when
similar products are on sale on a marketplace. This article provides a model of
the presence of cannibalization between times series. This model creates a
"competitiveness" function that depends on external features such as price and
margin. It also provides a theoretical guaranty on the error of the model under
some reasonable conditions, and implement this model using a neural network to
compute this competitiveness function. This implementation outperforms other
traditional time series methods and classical neural networks for market share
prediction on a real-world data set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Garnier_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Garnier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Information Agent Modelling in Partially-Observable Environments. (arXiv:2006.09447v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09447</id>
        <link href="http://arxiv.org/abs/2006.09447"/>
        <updated>2021-06-21T02:07:40.735Z</updated>
        <summary type="html"><![CDATA[Modelling the behaviours of other agents is essential for understanding how
agents interact and making effective decisions. Existing methods for agent
modelling commonly assume knowledge of the local observations and chosen
actions of the modelled agents during execution. To eliminate this assumption,
we extract representations from the local information of the controlled agent
using encoder-decoder architectures. Using the observations and actions of the
modelled agents during training, our models learn to extract representations
about the modelled agents conditioned only on the local observations of the
controlled agent. The representations are used to augment the controlled
agent's decision policy which is trained via deep reinforcement learning; thus,
during execution, the policy does not require access to other agents'
information. We provide a comprehensive evaluation and ablations studies in
cooperative, competitive and mixed multi-agent environments, showing that our
method achieves significantly higher returns than baseline methods which do not
use the learned representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Papoudakis_G/0/1/0/all/0/1"&gt;Georgios Papoudakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christianos_F/0/1/0/all/0/1"&gt;Filippos Christianos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1"&gt;Stefano V. Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction. (arXiv:2106.03135v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03135</id>
        <link href="http://arxiv.org/abs/2106.03135"/>
        <updated>2021-06-21T02:07:40.727Z</updated>
        <summary type="html"><![CDATA[Recently normalizing flows (NFs) have demonstrated state-of-the-art
performance on modeling 3D point clouds while allowing sampling with arbitrary
resolution at inference time. However, these flow-based models still require
long training times and large models for representing complicated geometries.
This work enhances their representational power by applying mixtures of NFs to
point clouds. We show that in this more general framework each component learns
to specialize in a particular subregion of an object in a completely
unsupervised fashion. By instantiating each mixture component with a
comparatively small NF we generate point clouds with improved details compared
to single-flow-based models while using fewer parameters and considerably
reducing the inference runtime. We further demonstrate that by adding data
augmentation, individual mixture components can learn to specialize in a
semantically meaningful manner. We evaluate mixtures of NFs on generation,
autoencoding and single-view reconstruction based on the ShapeNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1"&gt;Janis Postels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengya Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spezialetti_R/0/1/0/all/0/1"&gt;Riccardo Spezialetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIRA Guide to Custom Loss Functions for Neural Networks in Environmental Sciences -- Version 1. (arXiv:2106.09757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09757</id>
        <link href="http://arxiv.org/abs/2106.09757"/>
        <updated>2021-06-21T02:07:40.721Z</updated>
        <summary type="html"><![CDATA[Neural networks are increasingly used in environmental science applications.
Furthermore, neural network models are trained by minimizing a loss function,
and it is crucial to choose the loss function very carefully for environmental
science applications, as it determines what exactly is being optimized.
Standard loss functions do not cover all the needs of the environmental
sciences, which makes it important for scientists to be able to develop their
own custom loss functions so that they can implement many of the classic
performance measures already developed in environmental science, including
measures developed for spatial model verification. However, there are very few
resources available that cover the basics of custom loss function development
comprehensively, and to the best of our knowledge none that focus on the needs
of environmental scientists. This document seeks to fill this gap by providing
a guide on how to write custom loss functions targeted toward environmental
science applications. Topics include the basics of writing custom loss
functions, common pitfalls, functions to use in loss functions, examples such
as fractions skill score as loss function, how to incorporate physical
constraints, discrete and soft discretization, and concepts such as focal,
robust, and adaptive loss. While examples are currently provided in this guide
for Python with Keras and the TensorFlow backend, the basic concepts also apply
to other environments, such as Python with PyTorch. Similarly, while the sample
loss functions provided here are from meteorology, these are just examples of
how to create custom loss functions. Other fields in the environmental sciences
have very similar needs for custom loss functions, e.g., for evaluating spatial
forecasts effectively, and the concepts discussed here can be applied there as
well. All code samples are provided in a GitHub repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebert_Uphoff_I/0/1/0/all/0/1"&gt;Imme Ebert-Uphoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lagerquist_R/0/1/0/all/0/1"&gt;Ryan Lagerquist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilburn_K/0/1/0/all/0/1"&gt;Kyle Hilburn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yoonjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haynes_K/0/1/0/all/0/1"&gt;Katherine Haynes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stock_J/0/1/0/all/0/1"&gt;Jason Stock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumler_C/0/1/0/all/0/1"&gt;Christina Kumler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_J/0/1/0/all/0/1"&gt;Jebb Q. Stewart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARS: Masked Automatic Ranks Selection in Tensor Decompositions. (arXiv:2006.10859v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.10859</id>
        <link href="http://arxiv.org/abs/2006.10859"/>
        <updated>2021-06-21T02:07:40.714Z</updated>
        <summary type="html"><![CDATA[Tensor decomposition methods are known to be efficient for compressing and
accelerating neural networks. However, the problem of optimal decomposition
structure determination is still not well studied while being quite important.
Specifically, decomposition ranks present the crucial parameter controlling the
compression-accuracy trade-off. In this paper, we introduce MARS -- a new
efficient method for the automatic selection of ranks in general tensor
decompositions. During training, the procedure learns binary masks over
decomposition cores that "select" the optimal tensor structure. The learning is
performed via relaxed maximum a posteriori (MAP) estimation in a specific
Bayesian model. The proposed method achieves better results compared to
previous works in various tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1"&gt;Maxim Kodryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kropotov_D/0/1/0/all/0/1"&gt;Dmitry Kropotov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1"&gt;Dmitry Vetrov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization. (arXiv:2008.10847v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10847</id>
        <link href="http://arxiv.org/abs/2008.10847"/>
        <updated>2021-06-21T02:07:40.707Z</updated>
        <summary type="html"><![CDATA[Stochastic compositional optimization generalizes classic (non-compositional)
stochastic optimization to the minimization of compositions of functions. Each
composition may introduce an additional expectation. The series of expectations
may be nested. Stochastic compositional optimization is gaining popularity in
applications such as reinforcement learning and meta learning. This paper
presents a new Stochastically Corrected Stochastic Compositional gradient
method (SCSC). SCSC runs in a single-time scale with a single loop, uses a
fixed batch size, and guarantees to converge at the same rate as the stochastic
gradient descent (SGD) method for non-compositional stochastic optimization.
This is achieved by making a careful improvement to a popular stochastic
compositional gradient method. It is easy to apply SGD-improvement techniques
to accelerate SCSC. This helps SCSC achieve state-of-the-art performance for
stochastic compositional optimization. In particular, we apply Adam to SCSC,
and the exhibited rate of convergence matches that of the original Adam on
non-compositional stochastic optimization. We test SCSC using the portfolio
management and model-agnostic meta-learning tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuejiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Change-Point Detection with Training Sequences in the Large and Moderate Deviations Regimes. (arXiv:2003.06511v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.06511</id>
        <link href="http://arxiv.org/abs/2003.06511"/>
        <updated>2021-06-21T02:07:40.688Z</updated>
        <summary type="html"><![CDATA[This paper investigates a novel offline change-point detection problem from
an information-theoretic perspective. In contrast to most related works, we
assume that the knowledge of the underlying pre- and post-change distributions
are not known and can only be learned from the training sequences which are
available. We further require the probability of the \emph{estimation error} to
decay either exponentially or sub-exponentially fast (corresponding
respectively to the large and moderate deviations regimes in information theory
parlance). Based on the training sequences as well as the test sequence
consisting of a single change-point, we design a change-point estimator and
further show that this estimator is optimal by establishing matching (strong)
converses. This leads to a full characterization of the optimal confidence
width (i.e., half the width of the confidence interval within which the true
change-point is located at with high probability) as a function of the
undetected error, under both the large and moderate deviations regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haiyun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Trimmed Lasso: Sparse Recovery Guarantees and Practical Optimization by the Generalized Soft-Min Penalty. (arXiv:2005.09021v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09021</id>
        <link href="http://arxiv.org/abs/2005.09021"/>
        <updated>2021-06-21T02:07:40.681Z</updated>
        <summary type="html"><![CDATA[We present a new approach to solve the sparse approximation or best subset
selection problem, namely find a $k$-sparse vector ${\bf x}\in\mathbb{R}^d$
that minimizes the $\ell_2$ residual $\lVert A{\bf x}-{\bf y} \rVert_2$. We
consider a regularized approach, whereby this residual is penalized by the
non-convex $\textit{trimmed lasso}$, defined as the $\ell_1$-norm of ${\bf x}$
excluding its $k$ largest-magnitude entries. We prove that the trimmed lasso
has several appealing theoretical properties, and in particular derive sparse
recovery guarantees assuming successful optimization of the penalized
objective. Next, we show empirically that directly optimizing this objective
can be quite challenging. Instead, we propose a surrogate for the trimmed
lasso, called the $\textit{generalized soft-min}$. This penalty smoothly
interpolates between the classical lasso and the trimmed lasso, while taking
into account all possible $k$-sparse patterns. The generalized soft-min penalty
involves summation over $\binom{d}{k}$ terms, yet we derive a polynomial-time
algorithm to compute it. This, in turn, yields a practical method for the
original sparse approximation problem. Via simulations, we demonstrate its
competitive performance compared to current state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amir_T/0/1/0/all/0/1"&gt;Tal Amir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadler_B/0/1/0/all/0/1"&gt;Boaz Nadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leakage of Dataset Properties in Multi-Party Machine Learning. (arXiv:2006.07267v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07267</id>
        <link href="http://arxiv.org/abs/2006.07267"/>
        <updated>2021-06-21T02:07:40.662Z</updated>
        <summary type="html"><![CDATA[Secure multi-party machine learning allows several parties to build a model
on their pooled data to increase utility while not explicitly sharing data with
each other. We show that such multi-party computation can cause leakage of
global dataset properties between the parties even when parties obtain only
black-box access to the final model. In particular, a ``curious'' party can
infer the distribution of sensitive attributes in other parties' data with high
accuracy. This raises concerns regarding the confidentiality of properties
pertaining to the whole dataset as opposed to individual data records. We show
that our attack can leak population-level properties in datasets of different
types, including tabular, text, and graph data. To understand and measure the
source of leakage, we consider several models of correlation between a
sensitive attribute and the rest of the data. Using multiple machine learning
models, we show that leakage occurs even if the sensitive attribute is not
included in the training data and has a low correlation with other attributes
or the target variable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NoiseGrad: enhancing explanations by introducing stochasticity to model weights. (arXiv:2106.10185v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10185</id>
        <link href="http://arxiv.org/abs/2106.10185"/>
        <updated>2021-06-21T02:07:40.252Z</updated>
        <summary type="html"><![CDATA[Attribution methods remain a practical instrument that is used in real-world
applications to explain the decision-making process of complex learning
machines. It has been shown that a simple method called SmoothGrad can
effectively reduce the visual diffusion of gradient-based attribution methods
and has established itself among both researchers and practitioners. What
remains unexplored in research, however, is how explanations can be improved by
introducing stochasticity to the model weights. In the light of this, we
introduce - NoiseGrad - a stochastic, method-agnostic explanation-enhancing
method that adds noise to the weights instead of the input data. We investigate
our proposed method through various experiments including different datasets,
explanation methods and network architectures and conclude that NoiseGrad (and
its extension NoiseGrad++) with multiplicative Gaussian noise offers a clear
advantage compared to SmoothGrad on several evaluation criteria. We connect our
proposed method to Bayesian Learning and provide the user with a heuristic for
choosing hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1"&gt;Kirill Bykov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1"&gt;Anna Hedstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1"&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1"&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10159</id>
        <link href="http://arxiv.org/abs/2106.10159"/>
        <updated>2021-06-21T02:07:40.210Z</updated>
        <summary type="html"><![CDATA[Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yi-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yu-Che Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Resource German ASR with Untranscribed Data Spoken by Non-native Children -- INTERSPEECH 2021 Shared Task SPAPL System. (arXiv:2106.09963v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09963</id>
        <link href="http://arxiv.org/abs/2106.09963"/>
        <updated>2021-06-21T02:07:40.192Z</updated>
        <summary type="html"><![CDATA[This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:
Shared Task on Automatic Speech Recognition for Non-Native Children's Speech in
German. ~ 5 hours of transcribed data and ~ 60 hours of untranscribed data are
provided to develop a German ASR system for children. For the training of the
transcribed data, we propose a non-speech state discriminative loss (NSDL) to
mitigate the influence of long-duration non-speech segments within speech
utterances. In order to explore the use of the untranscribed data, various
approaches are implemented and combined together to incrementally improve the
system performance. First, bidirectional autoregressive predictive coding
(Bi-APC) is used to learn initial parameters for acoustic modelling using the
provided untranscribed data. Second, incremental semi-supervised learning is
further used to iteratively generate pseudo-transcribed data. Third, different
data augmentation schemes are used at different training stages to increase the
variability and size of the training data. Finally, a recurrent neural network
language model (RNNLM) is used for rescoring. Our system achieves a word error
rate (WER) of 39.68% on the evaluation data, an approximately 12% relative
improvement over the official baseline (45.21%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yunzheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_R/0/1/0/all/0/1"&gt;Ruchao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wei Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Alwan_A/0/1/0/all/0/1"&gt;Abeer Alwan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Generate Code Sketches. (arXiv:2106.10158v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10158</id>
        <link href="http://arxiv.org/abs/2106.10158"/>
        <updated>2021-06-21T02:07:40.185Z</updated>
        <summary type="html"><![CDATA[Traditional generative models are limited to predicting sequences of terminal
tokens. However, ambiguities in the generation task may lead to incorrect
outputs. Towards addressing this, we introduce Grammformers, transformer-based
grammar-guided models that learn (without explicit supervision) to generate
sketches -- sequences of tokens with holes. Through reinforcement learning,
Grammformers learn to introduce holes avoiding the generation of incorrect
tokens where there is ambiguity in the target task.

We train Grammformers for statement-level source code completion, i.e., the
generation of code snippets given an ambiguous user intent, such as a partial
code context. We evaluate Grammformers on code completion for C# and Python and
show that it generates 10-50% more accurate sketches compared to traditional
generative models and 37-50% longer sketches compared to sketch-generating
baselines trained with similar techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1"&gt;Daya Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1"&gt;Alexey Svyatkovskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Allamanis_M/0/1/0/all/0/1"&gt;Miltiadis Allamanis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09943</id>
        <link href="http://arxiv.org/abs/2106.09943"/>
        <updated>2021-06-21T02:07:40.158Z</updated>
        <summary type="html"><![CDATA[Noise contrastive learning is a popular technique for unsupervised
representation learning. In this approach, a representation is obtained via
reduction to supervised learning, where given a notion of semantic similarity,
the learner tries to distinguish a similar (positive) example from a collection
of random (negative) examples. The success of modern contrastive learning
pipelines relies on many parameters such as the choice of data augmentation,
the number of negative examples, and the batch size; however, there is limited
understanding as to how these parameters interact and affect downstream
performance. We focus on disambiguating the role of one of these parameters:
the number of negative examples. Theoretically, we show the existence of a
collision-coverage trade-off suggesting that the optimal number of negative
examples should scale with the number of underlying concepts in the data.
Empirically, we scrutinize the role of the number of negatives in both NLP and
vision tasks. In the NLP task, we find that the results broadly agree with our
theory, while our vision experiments are murkier with performance sometimes
even being insensitive to the number of negatives. We discuss plausible
explanations for this behavior and suggest future directions to better align
theory and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10155</id>
        <link href="http://arxiv.org/abs/2106.10155"/>
        <updated>2021-06-21T02:07:40.152Z</updated>
        <summary type="html"><![CDATA[This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1"&gt;Maren Awiszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1"&gt;Frederik Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Contrastive Representations of Stochastic Processes. (arXiv:2106.10052v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10052</id>
        <link href="http://arxiv.org/abs/2106.10052"/>
        <updated>2021-06-21T02:07:40.145Z</updated>
        <summary type="html"><![CDATA[Learning representations of stochastic processes is an emerging problem in
machine learning with applications from meta-learning to physical object models
to time series. Typical methods rely on exact reconstruction of observations,
but this approach breaks down as observations become high-dimensional or noise
distributions become complex. To address this, we propose a unifying framework
for learning contrastive representations of stochastic processes (CRESP) that
does away with exact reconstruction. We dissect potential use cases for
stochastic process representations, and propose methods that accommodate each.
Empirically, we show that our methods are effective for learning
representations of periodic functions, 3D objects and dynamical processes. Our
methods tolerate noisy high-dimensional observations better than traditional
approaches, and the learned representations transfer to a range of downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1"&gt;Emile Mathieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1"&gt;Adam Foster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide stochastic networks: Gaussian limit and PAC-Bayesian training. (arXiv:2106.09798v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.09798</id>
        <link href="http://arxiv.org/abs/2106.09798"/>
        <updated>2021-06-21T02:07:40.031Z</updated>
        <summary type="html"><![CDATA[The limit of infinite width allows for substantial simplifications in the
analytical study of overparameterized neural networks. With a suitable random
initialization, an extremely large network is well approximated by a Gaussian
process, both before and during training. In the present work, we establish a
similar result for a simple stochastic architecture whose parameters are random
variables. The explicit evaluation of the output distribution allows for a
PAC-Bayesian training procedure that directly optimizes the generalization
bound. For a large but finite-width network, we show empirically on MNIST that
this training approach can outperform standard PAC-Bayesian methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Clerico_E/0/1/0/all/0/1"&gt;Eugenio Clerico&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1"&gt;George Deligiannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises. (arXiv:2106.10241v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10241</id>
        <link href="http://arxiv.org/abs/2106.10241"/>
        <updated>2021-06-21T02:07:40.024Z</updated>
        <summary type="html"><![CDATA[Diferentially private (DP) synthetic datasets are a powerful approach for
training machine learning models while respecting the privacy of individual
data providers. The effect of DP on the fairness of the resulting trained
models is not yet well understood. In this contribution, we systematically
study the effects of differentially private synthetic data generation on
classification. We analyze disparities in model utility and bias caused by the
synthetic dataset, measured through algorithmic fairness metrics. Our first set
of results show that although there seems to be a clear negative correlation
between privacy and utility (the more private, the less accurate) across all
data synthesizers we evaluated, more privacy does not necessarily imply more
bias. Additionally, we assess the effects of utilizing synthetic datasets for
model training and model evaluation. We show that results obtained on synthetic
data can misestimate the actual model performance when it is deployed on real
data. We hence advocate on the need for defining proper testing protocols in
scenarios where differentially private synthetic datasets are utilized for
model training and evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pereira_M/0/1/0/all/0/1"&gt;Mayana Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kshirsagar_M/0/1/0/all/0/1"&gt;Meghana Kshirsagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sumit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistency of Extreme Learning Machines and Regression under Non-Stationarity and Dependence for ML-Enhanced Moving Objects. (arXiv:2005.11115v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11115</id>
        <link href="http://arxiv.org/abs/2005.11115"/>
        <updated>2021-06-21T02:07:40.017Z</updated>
        <summary type="html"><![CDATA[Supervised learning by extreme learning machines resp. neural networks with
random weights is studied under a non-stationary spatial-temporal sampling
design which especially addresses settings where an autonomous object moving in
a non-stationary spatial environment collects and analyzes data. The stochastic
model especially allows for spatial heterogeneity and weak dependence. As
efficient and computationally cheap learning methods (unconstrained) least
squares, ridge regression and $\ell_s$-penalized least squares (including the
LASSO) are studied. Consistency and asymptotic normality of the least squares
and ridge regression estimates as well as corresponding consistency results for
the $\ell_s$-penalty are shown under weak conditions. The resuts also cover
bounds for the sample squared predicition error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Steland_A/0/1/0/all/0/1"&gt;Ansgar Steland&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Reduction and Neural Networks for Parametric PDEs. (arXiv:2005.03180v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03180</id>
        <link href="http://arxiv.org/abs/2005.03180"/>
        <updated>2021-06-21T02:07:40.010Z</updated>
        <summary type="html"><![CDATA[We develop a general framework for data-driven approximation of input-output
maps between infinite-dimensional spaces. The proposed approach is motivated by
the recent successes of neural networks and deep learning, in combination with
ideas from model reduction. This combination results in a neural network
approximation which, in principle, is defined on infinite-dimensional spaces
and, in practice, is robust to the dimension of finite-dimensional
approximations of these spaces required for computation. For a class of
input-output maps, and suitably chosen probability measures on the inputs, we
prove convergence of the proposed approximation methodology. We also include
numerical experiments which demonstrate the effectiveness of the method,
showing convergence and robustness of the approximation scheme with respect to
the size of the discretization, and compare it with existing algorithms from
the literature; our examples include the mapping from coefficient to solution
in a divergence form elliptic partial differential equation (PDE) problem, and
the solution operator for viscous Burgers' equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Bhattacharya_K/0/1/0/all/0/1"&gt;Kaushik Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hosseini_B/0/1/0/all/0/1"&gt;Bamdad Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kovachki_N/0/1/0/all/0/1"&gt;Nikola B. Kovachki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10212</id>
        <link href="http://arxiv.org/abs/2106.10212"/>
        <updated>2021-06-21T02:07:40.004Z</updated>
        <summary type="html"><![CDATA[Despite the significant advances in deep learning over the past decade, a
major challenge that limits the wide-spread adoption of deep learning has been
their fragility to adversarial attacks. This sensitivity to making erroneous
predictions in the presence of adversarially perturbed data makes deep neural
networks difficult to adopt for certain real-world, mission-critical
applications. While much of the research focus has revolved around adversarial
example creation and adversarial hardening, the area of performance measures
for assessing adversarial robustness is not well explored. Motivated by this,
this study presents the concept of residual error, a new performance measure
for not only assessing the adversarial robustness of a deep neural network at
the individual sample level, but also can be used to differentiate between
adversarial and non-adversarial examples to facilitate for adversarial example
detection. Furthermore, we introduce a hybrid model for approximating the
residual error in a tractable manner. Experimental results using the case of
image classification demonstrates the effectiveness and efficacy of the
proposed residual error metric for assessing several well-known deep neural
network architectures. These results thus illustrate that the proposed measure
could be a useful tool for not only assessing the robustness of deep neural
networks used in mission-critical scenarios, but also in the design of
adversarially robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1"&gt;Hossein Aboutalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1"&gt;Michelle Karg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1"&gt;Christian Scharfenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09136</id>
        <link href="http://arxiv.org/abs/2001.09136"/>
        <updated>2021-06-21T02:07:39.984Z</updated>
        <summary type="html"><![CDATA[Most capsule network designs rely on traditional matrix multiplication
between capsule layers and computationally expensive routing mechanisms to deal
with the capsule dimensional entanglement that the matrix multiplication
introduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise
multiplication rather than matrix multiplication, the dimensions of the
capsules remain unentangled. In this work, we study HVCs as applied to the
highly structured MNIST dataset in order to produce a direct comparison to the
capsule research direction of Geoffrey Hinton, et al. In our study, we show
that a simple convolutional neural network using HVCs performs as well as the
prior best performing capsule network on MNIST using 5.5x fewer parameters, 4x
fewer training epochs, no reconstruction sub-network, and requiring no routing
mechanism. The addition of multiple classification branches to the network
establishes a new state of the art for the MNIST dataset with an accuracy of
99.87% for an ensemble of these models, as well as establishing a new state of
the art for a single model (99.83% accurate).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1"&gt;Adam Byerly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1"&gt;Tatiana Kalganova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1"&gt;Ian Dear&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Private Graph Neural Networks. (arXiv:2006.05535v8 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05535</id>
        <link href="http://arxiv.org/abs/2006.05535"/>
        <updated>2021-06-21T02:07:39.978Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) have demonstrated superior performance in
learning node representations for various graph inference tasks. However,
learning over graph data can raise privacy concerns when nodes represent people
or human-related variables that involve sensitive or personal information.
While numerous techniques have been proposed for privacy-preserving deep
learning over non-relational data, there is less work addressing the privacy
issues pertained to applying deep learning algorithms on graphs. In this paper,
we study the problem of node data privacy, where graph nodes have potentially
sensitive data that is kept private, but they could be beneficial for a central
server for training a GNN over the graph. To address this problem, we develop a
privacy-preserving, architecture-agnostic GNN learning algorithm with formal
privacy guarantees based on Local Differential Privacy (LDP). Specifically, we
propose an LDP encoder and an unbiased rectifier, by which the server can
communicate with the graph nodes to privately collect their data and
approximate the GNN's first layer. To further reduce the effect of the injected
noise, we propose to prepend a simple graph convolution layer, called KProp,
which is based on the multi-hop aggregation of the nodes' features acting as a
denoising mechanism. Finally, we propose a robust training framework, in which
we benefit from KProp's denoising capability to increase the accuracy of
inference in the presence of noisy labels. Extensive experiments conducted over
real-world datasets demonstrate that our method can maintain a satisfying level
of accuracy with low privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sajadmanesh_S/0/1/0/all/0/1"&gt;Sina Sajadmanesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatica_Perez_D/0/1/0/all/0/1"&gt;Daniel Gatica-Perez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Pseudo-Point and State Space Approximations for Sum-Separable Gaussian Processes. (arXiv:2106.10210v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10210</id>
        <link href="http://arxiv.org/abs/2106.10210"/>
        <updated>2021-06-21T02:07:39.970Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are important probabilistic tools for inference and
learning in spatio-temporal modelling problems such as those in climate science
and epidemiology. However, existing GP approximations do not simultaneously
support large numbers of off-the-grid spatial data-points and long time-series
which is a hallmark of many applications.

Pseudo-point approximations, one of the gold-standard methods for scaling GPs
to large data sets, are well suited for handling off-the-grid spatial data.
However, they cannot handle long temporal observation horizons effectively
reverting to cubic computational scaling in the time dimension. State space GP
approximations are well suited to handling temporal data, if the temporal GP
prior admits a Markov form, leading to linear complexity in the number of
temporal observations, but have a cubic spatial cost and cannot handle
off-the-grid spatial data.

In this work we show that there is a simple and elegant way to combine
pseudo-point methods with the state space GP approximation framework to get the
best of both worlds. The approach hinges on a surprising conditional
independence property which applies to space--time separable GPs. We
demonstrate empirically that the combined approach is more scalable and
applicable to a greater range of spatio-temporal problems than either method on
its own.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tebbutt_W/0/1/0/all/0/1"&gt;Will Tebbutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1"&gt;Arno Solin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Richard E. Turner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonparametric Hamiltonian Monte Carlo. (arXiv:2106.10238v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10238</id>
        <link href="http://arxiv.org/abs/2106.10238"/>
        <updated>2021-06-21T02:07:39.963Z</updated>
        <summary type="html"><![CDATA[Probabilistic programming uses programs to express generative models whose
posterior probability is then computed by built-in inference engines. A
challenging goal is to develop general purpose inference algorithms that work
out-of-the-box for arbitrary programs in a universal probabilistic programming
language (PPL). The densities defined by such programs, which may use
stochastic branching and recursion, are (in general) nonparametric, in the
sense that they correspond to models on an infinite-dimensional parameter
space. However standard inference algorithms, such as the Hamiltonian Monte
Carlo (HMC) algorithm, target distributions with a fixed number of parameters.
This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)
algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a
new class of measurable functions called "tree representable", which serve as a
language-independent representation of the density functions of probabilistic
programs in a universal PPL. We provide a correctness proof of NP-HMC, and
empirically demonstrate significant performance improvements over existing
approaches on several nonparametric examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mak_C/0/1/0/all/0/1"&gt;Carol Mak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaiser_F/0/1/0/all/0/1"&gt;Fabian Zaiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_L/0/1/0/all/0/1"&gt;Luke Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Theoretical Insights into Wasserstein GANs. (arXiv:2006.02682v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.02682</id>
        <link href="http://arxiv.org/abs/2006.02682"/>
        <updated>2021-06-21T02:07:39.956Z</updated>
        <summary type="html"><![CDATA[Generative Adversarial Networks (GANs) have been successful in producing
outstanding results in areas as diverse as image, video, and text generation.
Building on these successes, a large number of empirical studies have validated
the benefits of the cousin approach called Wasserstein GANs (WGANs), which
brings stabilization in the training process. In the present paper, we add a
new stone to the edifice by proposing some theoretical advances in the
properties of WGANs. First, we properly define the architecture of WGANs in the
context of integral probability metrics parameterized by neural networks and
highlight some of their basic mathematical features. We stress in particular
interesting optimization properties arising from the use of a parametric
1-Lipschitz discriminator. Then, in a statistically-driven approach, we study
the convergence of empirical WGANs as the sample size tends to infinity, and
clarify the adversarial effects of the generator and the discriminator by
underlining some trade-off properties. These features are finally illustrated
with experiments using both synthetic and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biau_G/0/1/0/all/0/1"&gt;G&amp;#xe9;rard Biau&lt;/a&gt; (LPSM (UMR\_8001)), &lt;a href="http://arxiv.org/find/cs/1/au:+Sangnier_M/0/1/0/all/0/1"&gt;Maxime Sangnier&lt;/a&gt; (LPSM (UMR\_8001)), &lt;a href="http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1"&gt;Ugo Tanielian&lt;/a&gt; (LPSM (UMR\_8001))</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boltzmann machine learning and regularization methods for inferring evolutionary fields and couplings from a multiple sequence alignment. (arXiv:1909.05006v3 [q-bio.PE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.05006</id>
        <link href="http://arxiv.org/abs/1909.05006"/>
        <updated>2021-06-21T02:07:39.936Z</updated>
        <summary type="html"><![CDATA[The inverse Potts problem to infer a Boltzmann distribution for homologous
protein sequences from their single-site and pairwise amino acid frequencies
recently attracts a great deal of attention in the studies of protein structure
and evolution. We study regularization and learning methods and how to tune
regularization parameters to correctly infer interactions in Boltzmann machine
learning. Using $L_2$ regularization for fields, group $L_1$ for couplings is
shown to be very effective for sparse couplings in comparison with $L_2$ and
$L_1$. Two regularization parameters are tuned to yield equal values for both
the sample and ensemble averages of evolutionary energy. Both averages smoothly
change and converge, but their learning profiles are very different between
learning methods. The Adam method is modified to make stepsize proportional to
the gradient for sparse couplings and to use a soft-thresholding function for
group $L_1$. It is shown by first inferring interactions from protein sequences
and then from Monte Carlo samples that the fields and couplings can be well
recovered, but that recovering the pairwise correlations in the resolution of a
total energy is harder for the natural proteins than for the protein-like
sequences. Selective temperature for folding/structural constrains in protein
evolution is also estimated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Miyazawa_S/0/1/0/all/0/1"&gt;Sanzo Miyazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10259</id>
        <link href="http://arxiv.org/abs/2106.10259"/>
        <updated>2021-06-21T02:07:39.929Z</updated>
        <summary type="html"><![CDATA[While current state-of-the-art Automatic Speech Recognition (ASR) systems
achieve high accuracy on typical speech, they suffer from significant
performance degradation on disordered speech and other atypical speech
patterns. Personalization of ASR models, a commonly applied solution to this
problem, is usually performed in a server-based training environment posing
problems around data privacy, delayed model-update times, and communication
cost for copying data and models between mobile device and server
infrastructure. In this paper, we present an approach to on-device based ASR
personalization with very small amounts of speaker-specific data. We test our
approach on a diverse set of 100 speakers with disordered speech and find
median relative word error rate improvement of 71% with only 50 short
utterances required per speaker. When tested on a voice-controlled home
automation platform, on-device personalized models show a median task success
rate of 81%, compared to only 40% of the unadapted models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1"&gt;Katrin Tomanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;oise Beaufays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1"&gt;Julie Cattiau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1"&gt;Angad Chandorkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1"&gt;Khe Chai Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.01210</id>
        <link href="http://arxiv.org/abs/1910.01210"/>
        <updated>2021-06-21T02:07:39.922Z</updated>
        <summary type="html"><![CDATA[We propose associating language utterances to 3D visual abstractions of the
scene they describe. The 3D visual abstractions are encoded as 3-dimensional
visual feature maps. We infer these 3D visual scene feature maps from RGB
images of the scene via view prediction: when the generated 3D scene feature
map is neurally projected from a camera viewpoint, it should match the
corresponding RGB image. We present generative models that condition on the
dependency tree of an utterance and generate a corresponding visual 3D feature
map as well as reason about its plausibility, and detector models that
condition on both the dependency tree of an utterance and a related image and
localize the object referents in the 3D feature map inferred from the image.
Our model outperforms models of language and vision that associate language
with 2D CNN activations or 2D images by a large margin in a variety of tasks,
such as, classifying plausibility of utterances, detecting referential
expressions, and supplying rewards for trajectory optimization of object
placement policies from language instructions. We perform numerous ablations
and show the improved performance of our detectors is due to its better
generalization across camera viewpoints and lack of object interferences in the
inferred 3D feature space, and the improved performance of our generators is
due to their ability to spatially reason about objects and their configurations
in 3D when mapping from language to scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1"&gt;Mihir Prabhudesai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiao-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1"&gt;Syed Ashar Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1"&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1"&gt;Katerina Fragkiadaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probabilistic Representation of DNNs: Bridging Mutual Information and Generalization. (arXiv:2106.10262v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10262</id>
        <link href="http://arxiv.org/abs/2106.10262"/>
        <updated>2021-06-21T02:07:39.915Z</updated>
        <summary type="html"><![CDATA[Recently, Mutual Information (MI) has attracted attention in bounding the
generalization error of Deep Neural Networks (DNNs). However, it is intractable
to accurately estimate the MI in DNNs, thus most previous works have to relax
the MI bound, which in turn weakens the information theoretic explanation for
generalization. To address the limitation, this paper introduces a
probabilistic representation of DNNs for accurately estimating the MI.
Leveraging the proposed MI estimator, we validate the information theoretic
explanation for generalization, and derive a tighter generalization bound than
the state-of-the-art relaxations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1"&gt;Xinjie Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barner_K/0/1/0/all/0/1"&gt;Kenneth Barner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LoRMIkA: Local rule-based model interpretability with k-optimal associations. (arXiv:1908.03840v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03840</id>
        <link href="http://arxiv.org/abs/1908.03840"/>
        <updated>2021-06-21T02:07:39.909Z</updated>
        <summary type="html"><![CDATA[As we rely more and more on machine learning models for real-life
decision-making, being able to understand and trust the predictions becomes
ever more important. Local explainer models have recently been introduced to
explain the predictions of complex machine learning models at the instance
level. In this paper, we propose Local Rule-based Model Interpretability with
k-optimal Associations (LoRMIkA), a novel model-agnostic approach that obtains
k-optimal association rules from a neighbourhood of the instance to be
explained. Compared with other rule-based approaches in the literature, we
argue that the most predictive rules are not necessarily the rules that provide
the best explanations. Consequently, the LoRMIkA framework provides a flexible
way to obtain predictive and interesting rules. It uses an efficient search
algorithm guaranteed to find the k-optimal rules with respect to objectives
such as confidence, lift, leverage, coverage, and support. It also provides
multiple rules which explain the decision and counterfactual rules, which give
indications for potential changes to obtain different outputs for given
instances. We compare our approach to other state-of-the-art approaches in
local model interpretability on three different datasets and achieve
competitive results in terms of local accuracy and interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajapaksha_D/0/1/0/all/0/1"&gt;Dilini Rajapaksha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1"&gt;Christoph Bergmeir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1"&gt;Wray Buntine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Investigation into Deep and Shallow Rule Learning. (arXiv:2106.10254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10254</id>
        <link href="http://arxiv.org/abs/2106.10254"/>
        <updated>2021-06-21T02:07:39.890Z</updated>
        <summary type="html"><![CDATA[Inductive rule learning is arguably among the most traditional paradigms in
machine learning. Although we have seen considerable progress over the years in
learning rule-based theories, all state-of-the-art learners still learn
descriptions that directly relate the input features to the target concept. In
the simplest case, concept learning, this is a disjunctive normal form (DNF)
description of the positive class. While it is clear that this is sufficient
from a logical point of view because every logical expression can be reduced to
an equivalent DNF expression, it could nevertheless be the case that more
structured representations, which form deep theories by forming intermediate
concepts, could be easier to learn, in very much the same way as deep neural
networks are able to outperform shallow networks, even though the latter are
also universal function approximators. In this paper, we empirically compare
deep and shallow rule learning with a uniform general algorithm, which relies
on greedy mini-batch based optimization. Our experiments on both artificial and
real-world benchmark data indicate that deep rule networks outperform shallow
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1"&gt;Florian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Riemannian Convex Potential Maps. (arXiv:2106.10272v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10272</id>
        <link href="http://arxiv.org/abs/2106.10272"/>
        <updated>2021-06-21T02:07:39.884Z</updated>
        <summary type="html"><![CDATA[Modeling distributions on Riemannian manifolds is a crucial component in
understanding non-Euclidean data that arises, e.g., in physics and geology. The
budding approaches in this space are limited by representational and
computational tradeoffs. We propose and study a class of flows that uses convex
potentials from Riemannian optimal transport. These are universal and can model
distributions on any compact Riemannian manifold without requiring domain
knowledge of the manifold to be integrated into the architecture. We
demonstrate that these flows can model standard distributions on spheres, and
tori, on synthetic and geological data. Our source code is freely available
online at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Samuel Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1"&gt;Brandon Amos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1"&gt;Yaron Lipman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-view Molecule Pre-training. (arXiv:2106.10234v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.10234</id>
        <link href="http://arxiv.org/abs/2106.10234"/>
        <updated>2021-06-21T02:07:39.878Z</updated>
        <summary type="html"><![CDATA[Inspired by its success in natural language processing and computer vision,
pre-training has attracted substantial attention in cheminformatics and
bioinformatics, especially for molecule based tasks. A molecule can be
represented by either a graph (where atoms are connected by bonds) or a SMILES
sequence (where depth-first-search is applied to the molecular graph with
specific rules). Existing works on molecule pre-training use either graph
representations only or SMILES representations only. In this work, we propose
to leverage both the representations and design a new pre-training algorithm,
dual-view molecule pre-training (briefly, DMP), that can effectively combine
the strengths of both types of molecule representations. The model of DMP
consists of two branches: a Transformer branch that takes the SMILES sequence
of a molecule as input, and a GNN branch that takes a molecular graph as input.
The training of DMP contains three tasks: (1) predicting masked tokens in a
SMILES sequence by the Transformer branch, (2) predicting masked atoms in a
molecular graph by the GNN branch, and (3) maximizing the consistency between
the two high-level representations output by the Transformer and GNN branches
separately. After pre-training, we can use either the Transformer branch (this
one is recommended according to empirical results), the GNN branch, or both for
downstream tasks. DMP is tested on nine molecular property prediction tasks and
achieves state-of-the-art performances on seven of them. Furthermore, we test
DMP on three retrosynthesis tasks and achieve state-of-the-result on the
USPTO-full dataset. Our code will be released soon.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jinhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingce Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic Gibbs Sampling via Ordinary Differential Equations. (arXiv:2106.10188v1 [stat.CO])]]></title>
        <id>http://arxiv.org/abs/2106.10188</id>
        <link href="http://arxiv.org/abs/2106.10188"/>
        <updated>2021-06-21T02:07:39.870Z</updated>
        <summary type="html"><![CDATA[Deterministic dynamics is an essential part of many MCMC algorithms, e.g.
Hybrid Monte Carlo or samplers utilizing normalizing flows. This paper presents
a general construction of deterministic measure-preserving dynamics using
autonomous ODEs and tools from differential geometry. We show how Hybrid Monte
Carlo and other deterministic samplers follow as special cases of our theory.
We then demonstrate the utility of our approach by constructing a continuous
non-sequential version of Gibbs sampling in terms of an ODE flow and extending
it to discrete state spaces. We find that our deterministic samplers are more
sample efficient than stochastic counterparts, even if the latter generate
independent samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bondesan_R/0/1/0/all/0/1"&gt;Roberto Bondesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Deep Learning in Open Collaborations. (arXiv:2106.10207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10207</id>
        <link href="http://arxiv.org/abs/2106.10207"/>
        <updated>2021-06-21T02:07:39.863Z</updated>
        <summary type="html"><![CDATA[Modern deep learning applications require increasingly more compute to train
state-of-the-art models. To address this demand, large corporations and
institutions use dedicated High-Performance Computing clusters, whose
construction and maintenance are both environmentally costly and well beyond
the budget of most organizations. As a result, some research directions become
the exclusive domain of a few large industrial and even fewer academic actors.
To alleviate this disparity, smaller groups may pool their computational
resources and run collaborative experiments that benefit all participants. This
paradigm, known as grid- or volunteer computing, has seen successful
applications in numerous scientific areas. However, using this approach for
machine learning is difficult due to high latency, asymmetric bandwidth, and
several challenges unique to volunteer computing. In this work, we carefully
analyze these constraints and propose a novel algorithmic framework designed
specifically for collaborative training. We demonstrate the effectiveness of
our approach for SwAV and ALBERT pretraining in realistic conditions and
achieve performance comparable to traditional setups at a fraction of the cost.
Finally, we provide a detailed report of successful collaborative language
model pretraining with 40 participants.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diskin_M/0/1/0/all/0/1"&gt;Michael Diskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bukhtiyarov_A/0/1/0/all/0/1"&gt;Alexey Bukhtiyarov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryabinin_M/0/1/0/all/0/1"&gt;Max Ryabinin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saulnier_L/0/1/0/all/0/1"&gt;Lucile Saulnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1"&gt;Quentin Lhoest&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinitsin_A/0/1/0/all/0/1"&gt;Anton Sinitsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popov_D/0/1/0/all/0/1"&gt;Dmitry Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyrkin_D/0/1/0/all/0/1"&gt;Dmitry Pyrkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashirin_M/0/1/0/all/0/1"&gt;Maxim Kashirin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borzunov_A/0/1/0/all/0/1"&gt;Alexander Borzunov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1"&gt;Albert Villanova del Moral&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazur_D/0/1/0/all/0/1"&gt;Denis Mazur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobelev_I/0/1/0/all/0/1"&gt;Ilia Kobelev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1"&gt;Yacine Jernite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1"&gt;Thomas Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pekhimenko_G/0/1/0/all/0/1"&gt;Gennady Pekhimenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees. (arXiv:2002.05551v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05551</id>
        <link href="http://arxiv.org/abs/2002.05551"/>
        <updated>2021-06-21T02:07:39.845Z</updated>
        <summary type="html"><![CDATA[Meta-learning can successfully acquire useful inductive biases from data.
Yet, its generalization properties to unseen learning tasks are poorly
understood. Particularly if the number of meta-training tasks is small, this
raises concerns about overfitting. We provide a theoretical analysis using the
PAC-Bayesian framework and derive novel generalization bounds for
meta-learning. Using these bounds, we develop a class of PAC-optimal
meta-learning algorithms with performance guarantees and a principled
meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our
method results in a standard stochastic optimization problem which can be
solved efficiently and scales well. When instantiating our PAC-optimal
hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as
base learners, the resulting methods yield state-of-the-art performance, both
in terms of predictive accuracy and the quality of uncertainty estimates.
Thanks to their principled treatment of uncertainty, our meta-learners can also
be successfully employed for sequential decision problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1"&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1"&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1"&gt;Martin Josifoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A learned conditional prior for the VAE acoustic space of a TTS system. (arXiv:2106.10229v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10229</id>
        <link href="http://arxiv.org/abs/2106.10229"/>
        <updated>2021-06-21T02:07:39.838Z</updated>
        <summary type="html"><![CDATA[Many factors influence speech yielding different renditions of a given
sentence. Generative models, such as variational autoencoders (VAEs), capture
this variability and allow multiple renditions of the same sentence via
sampling. The degree of prosodic variability depends heavily on the prior that
is used when sampling. In this paper, we propose a novel method to compute an
informative prior for the VAE latent space of a neural text-to-speech (TTS)
system. By doing so, we aim to sample with more prosodic variability, while
gaining controllability over the latent space's structure.

By using as prior the posterior distribution of a secondary VAE, which we
condition on a speaker vector, we can sample from the primary VAE taking
explicitly the conditioning into account and resulting in samples from a
specific region of the latent space for each condition (i.e. speaker). A formal
preference test demonstrates significant preference of the proposed approach
over standard Conditional VAE. We also provide visualisations of the latent
space where well-separated condition-specific clusters appear, as well as
ablation studies to better understand the behaviour of the system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1"&gt;Penny Karanasou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1"&gt;Sri Karlapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1"&gt;Alexis Moinet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1"&gt;Arnaud Joly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1"&gt;Ammar Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Slangen_S/0/1/0/all/0/1"&gt;Simon Slangen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Trueba_J/0/1/0/all/0/1"&gt;Jaime Lorenzo Trueba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1"&gt;Thomas Drugman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Training Helps Transfer Learning via Better Representations. (arXiv:2106.10189v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10189</id>
        <link href="http://arxiv.org/abs/2106.10189"/>
        <updated>2021-06-21T02:07:39.830Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to leverage models pre-trained on source data to
efficiently adapt to target setting, where only limited data are available for
model fine-tuning. Recent works empirically demonstrate that adversarial
training in the source data can improve the ability of models to transfer to
new domains. However, why this happens is not known. In this paper, we provide
a theoretical model to rigorously analyze how adversarial training helps
transfer learning. We show that adversarial training in the source data
generates provably better representations, so fine-tuning on top of this
representation leads to a more accurate predictor of the target data. We
further demonstrate both theoretically and empirically that semi-supervised
learning in the source data can also improve transfer learning by similarly
improving the representation. Moreover, performing adversarial training on top
of semi-supervised learning can further improve transferability, suggesting
that the two approaches have complementary benefits on representations. We
support our theories with experiments on popular data sets and deep learning
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhun Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1"&gt;Kailas Vodrahalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1"&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Black-Box Importance Sampling for VaR and CVaR Estimation. (arXiv:2106.10236v1 [q-fin.RM])]]></title>
        <id>http://arxiv.org/abs/2106.10236</id>
        <link href="http://arxiv.org/abs/2106.10236"/>
        <updated>2021-06-21T02:07:39.822Z</updated>
        <summary type="html"><![CDATA[This paper considers Importance Sampling (IS) for the estimation of tail
risks of a loss defined in terms of a sophisticated object such as a machine
learning feature map or a mixed integer linear optimisation formulation.
Assuming only black-box access to the loss and the distribution of the
underlying random vector, the paper presents an efficient IS algorithm for
estimating the Value at Risk and Conditional Value at Risk. The key challenge
in any IS procedure, namely, identifying an appropriate change-of-measure, is
automated with a self-structuring IS transformation that learns and replicates
the concentration properties of the conditional excess from less rare samples.
The resulting estimators enjoy asymptotically optimal variance reduction when
viewed in the logarithmic scale. Simulation experiments highlight the efficacy
and practicality of the proposed scheme]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Deo_A/0/1/0/all/0/1"&gt;Anand Deo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10199</id>
        <link href="http://arxiv.org/abs/2106.10199"/>
        <updated>2021-06-21T02:07:39.815Z</updated>
        <summary type="html"><![CDATA[We show that with small-to-medium training data, fine-tuning only the bias
terms (or a subset of the bias terms) of pre-trained BERT models is competitive
with (and sometimes better than) fine-tuning the entire model. For larger data,
bias-only fine-tuning is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1"&gt;Elad Ben Zaken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MADE: Exploration via Maximizing Deviation from Explored Regions. (arXiv:2106.10268v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10268</id>
        <link href="http://arxiv.org/abs/2106.10268"/>
        <updated>2021-06-21T02:07:39.809Z</updated>
        <summary type="html"><![CDATA[In online reinforcement learning (RL), efficient exploration remains
particularly challenging in high-dimensional environments with sparse rewards.
In low-dimensional environments, where tabular parameterization is possible,
count-based upper confidence bound (UCB) exploration methods achieve minimax
near-optimal rates. However, it remains unclear how to efficiently implement
UCB in realistic RL tasks that involve non-linear function approximation. To
address this, we propose a new exploration approach via \textit{maximizing} the
deviation of the occupancy of the next policy from the explored regions. We add
this term as an adaptive regularizer to the standard RL objective to balance
exploration vs. exploitation. We pair the new objective with a provably
convergent algorithm, giving rise to a new intrinsic reward that adjusts
existing bonuses. The proposed intrinsic reward is easy to implement and
combine with other existing RL algorithms to conduct exploration. As a proof of
concept, we evaluate the new intrinsic reward on tabular examples across a
variety of model-based and model-free algorithms, showing improvements over
count-only exploration strategies. When tested on navigation and locomotion
tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach
significantly improves sample efficiency over state-of-the-art methods. Our
code is available at https://github.com/tianjunz/MADE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashidinejad_P/0/1/0/all/0/1"&gt;Paria Rashidinejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1"&gt;Stuart Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10169</id>
        <link href="http://arxiv.org/abs/2106.10169"/>
        <updated>2021-06-21T02:07:39.790Z</updated>
        <summary type="html"><![CDATA[By implicitly recognizing a user based on his/her speech input, speaker
identification enables many downstream applications, such as personalized
system behavior and expedited shopping checkouts. Based on whether the speech
content is constrained or not, both text-dependent (TD) and text-independent
(TI) speaker recognition models may be used. We wish to combine the advantages
of both types of models through an ensemble system to make more reliable
predictions. However, any such combined approach has to be robust to incomplete
inputs, i.e., when either TD or TI input is missing. As a solution we propose a
fusion of embeddings network foenet architecture, combining joint learning with
neural attention. We compare foenet with four competitive baseline methods on a
dataset of voice assistant inputs, and show that it achieves higher accuracy
than the baseline and score fusion methods, especially in the presence of
incomplete inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea J.-T. Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Hongda Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Incremental Deep Graph Learning for Ethereum Phishing Scam Detection. (arXiv:2106.10176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10176</id>
        <link href="http://arxiv.org/abs/2106.10176"/>
        <updated>2021-06-21T02:07:39.783Z</updated>
        <summary type="html"><![CDATA[In recent years, phishing scams have become the crime type with the largest
money involved on Ethereum, the second-largest blockchain platform. Meanwhile,
graph neural network (GNN) has shown promising performance in various node
classification tasks. However, for Ethereum transaction data, which could be
naturally abstracted to a real-world complex graph, the scarcity of labels and
the huge volume of transaction data make it difficult to take advantage of GNN
methods. Here in this paper, to address the two challenges, we propose a
Self-supervised Incremental deep Graph learning model (SIEGE), for the phishing
scam detection problem on Ethereum. In our model, two pretext tasks designed
from spatial and temporal perspectives help us effectively learn useful node
embedding from the huge amount of unlabelled transaction data. And the
incremental paradigm allows us to efficiently handle large-scale transaction
data and help the model maintain good performance when the data distribution is
drastically changing. We collect transaction records about half a year from
Ethereum and our extensive experiments show that our model consistently
outperforms strong baselines in both transductive and inductive settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shucheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fengyuan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Runchuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1"&gt;Sheng Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks. (arXiv:2106.10252v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10252</id>
        <link href="http://arxiv.org/abs/2106.10252"/>
        <updated>2021-06-21T02:07:39.776Z</updated>
        <summary type="html"><![CDATA[A common observation regarding adversarial attacks is that they mostly give
rise to false activation at the penultimate layer to fool the classifier.
Assuming that these activation values correspond to certain features of the
input, the objective becomes choosing the features that are most useful for
classification. Hence, we propose a novel approach to identify the important
features by employing counter-adversarial attacks, which highlights the
consistency at the penultimate layer with respect to perturbations on input
samples. First, we empirically show that there exist a subset of features,
classification based in which bridge the gap between the clean and robust
accuracy. Second, we propose a simple yet efficient mechanism to identify those
features by searching the neighborhood of input sample. We then select features
by observing the consistency of the activation values at the penultimate layer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_E/0/1/0/all/0/1"&gt;Emre Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hameed_M/0/1/0/all/0/1"&gt;Muhammad Zaid Hameed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozfatura_K/0/1/0/all/0/1"&gt;Kerem Ozfatura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz Gunduz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09857</id>
        <link href="http://arxiv.org/abs/2106.09857"/>
        <updated>2021-06-21T02:07:39.768Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long training and inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer. The inefficiency of the algorithms reduces the achievable sparsity
level. In addition, many algorithms still require pre-trained dense models and
thus suffer from large memory footprint and long training time. In this paper,
we propose a novel scheduled grow-and-prune (GaP) methodology without
pre-training the dense models. It addresses the shortcomings of the previous
works by repeatedly growing a subset of layers to dense and then pruning back
to sparse after some training. Experiments have shown that such models can
match or beat the quality of highly optimized dense models at 80% sparsity on a
variety of tasks, such as image classification, objective detection, 3D object
part segmentation, and translation. They also outperform other state-of-the-art
(SOTA) pruning methods, including pruning from pre-trained dense models. As an
example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy
on ImageNet, improving the SOTA results by 1.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1"&gt;Minghai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zejiang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Kuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHARP: Shape-Aware Reconstruction of People In Loose Clothing. (arXiv:2106.04778v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04778</id>
        <link href="http://arxiv.org/abs/2106.04778"/>
        <updated>2021-06-21T02:07:39.749Z</updated>
        <summary type="html"><![CDATA[3D human body reconstruction from monocular images is an interesting and
ill-posed problem in computer vision with wider applications in multiple
domains. In this paper, we propose SHARP, a novel end-to-end trainable network
that accurately recovers the detailed geometry and appearance of 3D people in
loose clothing from a monocular image. We propose a sparse and efficient fusion
of a parametric body prior with a non-parametric peeled depth map
representation of clothed models. The parametric body prior constraints our
model in two ways: first, the network retains geometrically consistent body
parts that are not occluded by clothing, and second, it provides a body shape
context that improves prediction of the peeled depth maps. This enables SHARP
to recover fine-grained 3D geometrical details with just L1 losses on the 2D
maps, given an input image. We evaluate SHARP on publicly available Cloth3D and
THuman datasets and report superior performance to state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1"&gt;Sai Sagar Jinka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chacko_R/0/1/0/all/0/1"&gt;Rohan Chacko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1"&gt;Astitva Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Avinash Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;P.J. Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Robustness Propagation: Sharing Adversarial Robustness in Federated Learning. (arXiv:2106.10196v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10196</id>
        <link href="http://arxiv.org/abs/2106.10196"/>
        <updated>2021-06-21T02:07:39.730Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) emerges as a popular distributed learning schema that
learns a model from a set of participating users without requiring raw data to
be shared. One major challenge of FL comes from heterogeneity in users, which
may have distributionally different (or non-iid) data and varying computation
resources. Just like in centralized learning, FL users also desire model
robustness against malicious attackers at test time. Whereas adversarial
training (AT) provides a sound solution for centralized learning, extending its
usage for FL users has imposed significant challenges, as many users may have
very limited training data as well as tight computational budgets, to afford
the data-hungry and costly AT. In this paper, we study a novel learning setting
that propagates adversarial robustness from high-resource users that can afford
AT, to those low-resource users that cannot afford it, during the FL process.
We show that existing FL techniques cannot effectively propagate adversarial
robustness among non-iid users, and propose a simple yet effective propagation
approach that transfers robustness through carefully designed
batch-normalization statistics. We demonstrate the rationality and
effectiveness of our method through extensive experiments. Especially, the
proposed method is shown to grant FL remarkable robustness even when only a
small portion of users afford AT during learning. Codes will be published upon
acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Junyuan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jiayu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Investigation into Mini-Batch Rule Learning. (arXiv:2106.10202v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10202</id>
        <link href="http://arxiv.org/abs/2106.10202"/>
        <updated>2021-06-21T02:07:39.723Z</updated>
        <summary type="html"><![CDATA[We investigate whether it is possible to learn rule sets efficiently in a
network structure with a single hidden layer using iterative refinements over
mini-batches of examples. A first rudimentary version shows an acceptable
performance on all but one dataset, even though it does not yet reach the
performance levels of Ripper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Beck_F/0/1/0/all/0/1"&gt;Florian Beck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furnkranz_J/0/1/0/all/0/1"&gt;Johannes F&amp;#xfc;rnkranz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[pyWATTS: Python Workflow Automation Tool for Time Series. (arXiv:2106.10157v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10157</id>
        <link href="http://arxiv.org/abs/2106.10157"/>
        <updated>2021-06-21T02:07:39.716Z</updated>
        <summary type="html"><![CDATA[Time series data are fundamental for a variety of applications, ranging from
financial markets to energy systems. Due to their importance, the number and
complexity of tools and methods used for time series analysis is constantly
increasing. However, due to unclear APIs and a lack of documentation,
researchers struggle to integrate them into their research projects and
replicate results. Additionally, in time series analysis there exist many
repetitive tasks, which are often re-implemented for each project,
unnecessarily costing time. To solve these problems we present
\texttt{pyWATTS}, an open-source Python-based package that is a non-sequential
workflow automation tool for the analysis of time series data. pyWATTS includes
modules with clearly defined interfaces to enable seamless integration of new
or existing methods, subpipelining to easily reproduce repetitive tasks, load
and save functionality to simply replicate results, and native support for key
Python machine learning libraries such as scikit-learn, PyTorch, and Keras.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidrich_B/0/1/0/all/0/1"&gt;Benedikt Heidrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartschat_A/0/1/0/all/0/1"&gt;Andreas Bartschat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turowski_M/0/1/0/all/0/1"&gt;Marian Turowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_O/0/1/0/all/0/1"&gt;Oliver Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phipps_K/0/1/0/all/0/1"&gt;Kaleb Phipps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meisenbacher_S/0/1/0/all/0/1"&gt;Stefan Meisenbacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmieder_K/0/1/0/all/0/1"&gt;Kai Schmieder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludwig_N/0/1/0/all/0/1"&gt;Nicole Ludwig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1"&gt;Ralf Mikut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenmeyer_V/0/1/0/all/0/1"&gt;Veit Hagenmeyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks. (arXiv:2106.10147v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.10147</id>
        <link href="http://arxiv.org/abs/2106.10147"/>
        <updated>2021-06-21T02:07:39.710Z</updated>
        <summary type="html"><![CDATA[Trigger set-based watermarking schemes have gained emerging attention as they
provide a means to prove ownership for deep neural network model owners. In
this paper, we argue that state-of-the-art trigger set-based watermarking
algorithms do not achieve their designed goal of proving ownership. We posit
that this impaired capability stems from two common experimental flaws that the
existing research practice has committed when evaluating the robustness of
watermarking algorithms: (1) incomplete adversarial evaluation and (2)
overlooked adaptive attacks.

We conduct a comprehensive adversarial evaluation of 10 representative
watermarking schemes against six of the existing attacks and demonstrate that
each of these watermarking schemes lacks robustness against at least two
attacks. We also propose novel adaptive attacks that harness the adversary's
knowledge of the underlying watermarking algorithm of a target model. We
demonstrate that the proposed attacks effectively break all of the 10
watermarking schemes, consequently allowing adversaries to obscure the
ownership of any watermarked model. We encourage follow-up studies to consider
our guidelines when evaluating the robustness of their watermarking schemes via
conducting comprehensive adversarial evaluation that include our adaptive
attacks to demonstrate a meaningful upper bound of watermark robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Suyoung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1"&gt;Wonho Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_S/0/1/0/all/0/1"&gt;Suman Jana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1"&gt;Meeyoung Cha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1"&gt;Sooel Son&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Posterior Distributions under Vessel-Mixing: A Regularization for Cross-Domain Retinal Artery/Vein Classification. (arXiv:2103.09097v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09097</id>
        <link href="http://arxiv.org/abs/2103.09097"/>
        <updated>2021-06-21T02:07:39.678Z</updated>
        <summary type="html"><![CDATA[Retinal artery/vein (A/V) classification is a critical technique for
diagnosing diabetes and cardiovascular diseases. Although deep learning based
methods achieve impressive results in A/V classification, their performances
usually degrade severely when being directly applied to another database, due
to the domain shift, e.g., caused by the variations in imaging protocols. In
this paper, we propose a novel vessel-mixing based consistency regularization
framework, for cross-domain learning in retinal A/V classification. Specially,
to alleviate the severe bias to source domain, based on the label smooth prior,
the model is regularized to give consistent predictions for unlabeled
target-domain inputs that are under perturbation. This consistency
regularization implicitly introduces a mechanism where the model and the
perturbation is opponent to each other, where the model is pushed to be robust
enough to cope with the perturbation. Thus, we investigate a more difficult
opponent to further inspire the robustness of model, in the scenario of retinal
A/V, called vessel-mixing perturbation. Specially, it effectively disturbs the
fundus images especially the vessel structures by mixing two images regionally.
We conduct extensive experiments on cross-domain A/V classification using four
public datasets, which are collected by diverse institutions and imaging
devices. The results demonstrate that our method achieves the state-of-the-art
cross-domain performance, which is also close to the upper bound obtained by
fully supervised learning on target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhehan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wenao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xinghao Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Offline Policy Selection. (arXiv:2106.10251v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10251</id>
        <link href="http://arxiv.org/abs/2106.10251"/>
        <updated>2021-06-21T02:07:39.660Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and healthcare domain among others.
Several off-policy evaluation (OPE) techniques have been proposed to assess the
value of policies using only logged data. However, there is still a big gap
between the evaluation by OPE and the full online evaluation in the real
environment. To reduce this gap, we introduce a novel \emph{active offline
policy selection} problem formulation, which combined logged data and limited
online interactions to identify the best policy. We rely on the advances in OPE
to warm start the evaluation. We build upon Bayesian optimization to
iteratively decide which policies to evaluate in order to utilize the limited
environment interactions wisely. Many candidate policies could be proposed,
thus, we focus on making our approach scalable and introduce a kernel function
to model similarity between policies. We use several benchmark environments to
show that the proposed approach improves upon state-of-the-art OPE estimates
and fully online policy evaluation with limited budget. Additionally, we show
that each component of the proposed method is important, it works well with
various number and quality of OPE estimates and even with a large number of
candidate policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konyushkova_K/0/1/0/all/0/1"&gt;Ksenia Konyushkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yutian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1"&gt;Thomas Paine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1"&gt;Caglar Gulcehre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1"&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1"&gt;Daniel J Mankowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1"&gt;Misha Denil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1"&gt;Nando de Freitas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CompositeTasking: Understanding Images by Spatial Composition of Tasks. (arXiv:2012.09030v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09030</id>
        <link href="http://arxiv.org/abs/2012.09030"/>
        <updated>2021-06-21T02:07:39.653Z</updated>
        <summary type="html"><![CDATA[We define the concept of CompositeTasking as the fusion of multiple,
spatially distributed tasks, for various aspects of image understanding.
Learning to perform spatially distributed tasks is motivated by the frequent
availability of only sparse labels across tasks, and the desire for a compact
multi-tasking network. To facilitate CompositeTasking, we introduce a novel
task conditioning model -- a single encoder-decoder network that performs
multiple, spatially varying tasks at once. The proposed network takes an image
and a set of pixel-wise dense task requests as inputs, and performs the
requested prediction task for each pixel. Moreover, we also learn the
composition of tasks that needs to be performed according to some
CompositeTasking rules, which includes the decision of where to apply which
task. It not only offers us a compact network for multi-tasking, but also
allows for task-editing. Another strength of the proposed method is
demonstrated by only having to supply sparse supervision per task. The obtained
results are on par with our baselines that use dense supervision and a
multi-headed multi-tasking design. The source code will be made publicly
available at www.github.com/nikola3794/composite-tasking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1"&gt;Nikola Popovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1"&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1"&gt;Thomas Probst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09832</id>
        <link href="http://arxiv.org/abs/2106.09832"/>
        <updated>2021-06-21T02:07:39.646Z</updated>
        <summary type="html"><![CDATA[In this work we address the problem of landmark-based segmentation for
anatomical structures. We propose HybridGNet, an encoder-decoder neural
architecture which combines standard convolutions for image feature encoding,
with graph convolutional neural networks to decode plausible representations of
anatomical structures. We benchmark the proposed architecture considering other
standard landmark and pixel-based models for anatomical segmentation in chest
x-ray images, and found that HybridGNet is more robust to image occlusions. We
also show that it can be used to construct landmark-based segmentations from
pixel level annotations. Our experimental results suggest that HybridGNet
produces accurate and anatomically plausible landmark-based segmentations, by
naturally incorporating shape constraints within the decoding process via
spectral convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Gaggion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Being a Bit Frequentist Improves Bayesian Neural Networks. (arXiv:2106.10065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10065</id>
        <link href="http://arxiv.org/abs/2106.10065"/>
        <updated>2021-06-21T02:07:39.628Z</updated>
        <summary type="html"><![CDATA[Despite their compelling theoretical properties, Bayesian neural networks
(BNNs) tend to perform worse than frequentist methods in classification-based
uncertainty quantification (UQ) tasks such as out-of-distribution (OOD)
detection and dataset-shift robustness. In this work, based on empirical
findings in prior works, we hypothesize that this issue is due to the avoidance
of Bayesian methods in the so-called "OOD training" -- a family of techniques
for incorporating OOD data during training process, which has since been an
integral part of state-of-the-art frequentist UQ methods. To validate this, we
treat OOD data as a first-class citizen in BNN training by exploring four
different ways of incorporating OOD data in Bayesian inference. We show in
extensive experiments that OOD-trained BNNs are competitive to, if not better
than recent frequentist baselines. This work thus provides strong baselines for
future work in both Bayesian and frequentist UQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1"&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Problem Dependent View on Structured Thresholding Bandit Problems. (arXiv:2106.10166v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10166</id>
        <link href="http://arxiv.org/abs/2106.10166"/>
        <updated>2021-06-21T02:07:39.620Z</updated>
        <summary type="html"><![CDATA[We investigate the problem dependent regime in the stochastic Thresholding
Bandit problem (TBP) under several shape constraints. In the TBP, the objective
of the learner is to output, at the end of a sequential game, the set of arms
whose means are above a given threshold. The vanilla, unstructured, case is
already well studied in the literature. Taking $K$ as the number of arms, we
consider the case where (i) the sequence of arm's means $(\mu_k)_{k=1}^K$ is
monotonically increasing (MTBP) and (ii) the case where $(\mu_k)_{k=1}^K$ is
concave (CTBP). We consider both cases in the problem dependent regime and
study the probability of error - i.e. the probability to mis-classify at least
one arm. In the fixed budget setting, we provide upper and lower bounds for the
probability of error in both the concave and monotone settings, as well as
associated algorithms. In both settings the bounds match in the problem
dependent regime up to universal constants in the exponential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cheshire_J/0/1/0/all/0/1"&gt;James Cheshire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Menard_P/0/1/0/all/0/1"&gt;Pierre M&amp;#xe9;nard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carpentier_A/0/1/0/all/0/1"&gt;Alexandra Carpentier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rational Shapley Values. (arXiv:2106.10191v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10191</id>
        <link href="http://arxiv.org/abs/2106.10191"/>
        <updated>2021-06-21T02:07:39.612Z</updated>
        <summary type="html"><![CDATA[Explaining the predictions of opaque machine learning algorithms is an
important and challenging task, especially as complex models are increasingly
used to assist in high-stakes decisions such as those arising in healthcare and
finance. Most popular tools for post-hoc explainable artificial intelligence
(XAI) are either insensitive to context (e.g., feature attributions) or
difficult to summarize (e.g., counterfactuals). In this paper, I introduce
\emph{rational Shapley values}, a novel XAI method that synthesizes and extends
these seemingly incompatible approaches in a rigorous, flexible manner. I
leverage tools from decision theory and causal modeling to formalize and
implement a pragmatic approach that resolves a number of known challenges in
XAI. By pairing the distribution of random variables with the appropriate
reference class for a given explanation task, I illustrate through theory and
experiments how user goals and knowledge can inform and constrain the solution
set in an iterative fashion. The method compares favorably to state of the art
XAI tools in a range of quantitative and qualitative comparisons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_D/0/1/0/all/0/1"&gt;David S. Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Principles of Deep Learning Theory. (arXiv:2106.10165v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10165</id>
        <link href="http://arxiv.org/abs/2106.10165"/>
        <updated>2021-06-21T02:07:39.603Z</updated>
        <summary type="html"><![CDATA[This book develops an effective theory approach to understanding deep neural
networks of practical relevance. Beginning from a first-principles
component-level picture of networks, we explain how to determine an accurate
description of the output of trained networks by solving layer-to-layer
iteration equations and nonlinear learning dynamics. A main result is that the
predictions of networks are described by nearly-Gaussian distributions, with
the depth-to-width aspect ratio of the network controlling the deviations from
the infinite-width Gaussian description. We explain how these effectively-deep
networks learn nontrivial representations from training and more broadly
analyze the mechanism of representation learning for nonlinear models. From a
nearly-kernel-methods perspective, we find that the dependence of such models'
predictions on the underlying learning algorithm can be expressed in a simple
and universal way. To obtain these results, we develop the notion of
representation group flow (RG flow) to characterize the propagation of signals
through the network. By tuning networks to criticality, we give a practical
solution to the exploding and vanishing gradient problem. We further explain
how RG flow leads to near-universal behavior and lets us categorize networks
built from different activation functions into universality classes.
Altogether, we show that the depth-to-width ratio governs the effective model
complexity of the ensemble of trained networks. By using information-theoretic
techniques, we estimate the optimal aspect ratio at which we expect the network
to be practically most useful and show how residual connections can be used to
push this scale to arbitrary depths. With these tools, we can learn in detail
about the inductive bias of architectures, hyperparameters, and optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1"&gt;Daniel A. Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaida_S/0/1/0/all/0/1"&gt;Sho Yaida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1"&gt;Boris Hanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data. (arXiv:2106.09973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09973</id>
        <link href="http://arxiv.org/abs/2106.09973"/>
        <updated>2021-06-21T02:07:39.596Z</updated>
        <summary type="html"><![CDATA[We study the fundamental question of the sample complexity of learning a good
policy in finite Markov decision processes (MDPs) when the data available for
learning is obtained by following a logging policy that must be chosen without
knowledge of the underlying MDP. Our main results show that the sample
complexity, the minimum number of transitions necessary and sufficient to
obtain a good policy, is an exponential function of the relevant quantities
when the planning horizon $H$ is finite. In particular, we prove that the
sample complexity of obtaining $\epsilon$-optimal policies is at least
$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H+1)})$ for $\gamma$-discounted
problems, where $\mathrm{S}$ is the number of states, $\mathrm{A}$ is the
number of actions, and $H$ is the effective horizon defined as $H=\lfloor
\tfrac{\ln(1/\epsilon)}{\ln(1/\gamma)} \rfloor$; and it is at least
$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ for finite horizon
problems, where $H$ is the planning horizon of the problem. This lower bound is
essentially matched by an upper bound. For the average-reward setting we show
that there is no algorithm finding $\epsilon$-optimal policies with a finite
amount of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Ilbin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1"&gt;Dale Schuurmans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1"&gt;Csaba Szepesvari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Labelling Drifts in a Fault Detection System for Wind Turbine Maintenance. (arXiv:2106.09951v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09951</id>
        <link href="http://arxiv.org/abs/2106.09951"/>
        <updated>2021-06-21T02:07:39.589Z</updated>
        <summary type="html"><![CDATA[A failure detection system is the first step towards predictive maintenance
strategies. A popular data-driven method to detect incipient failures and
anomalies is the training of normal behaviour models by applying a machine
learning technique like feed-forward neural networks (FFNN) or extreme learning
machines (ELM). However, the performance of any of these modelling techniques
can be deteriorated by the unexpected rise of non-stationarities in the dynamic
environment in which industrial assets operate. This unpredictable statistical
change in the measured variable is known as concept drift. In this article a
wind turbine maintenance case is presented, where non-stationarities of various
kinds can happen unexpectedly. Such concept drift events are desired to be
detected by means of statistical detectors and window-based approaches.
However, in real complex systems, concept drifts are not as clear and evident
as in artificially generated datasets. In order to evaluate the effectiveness
of current drift detectors and also to design an appropriate novel technique
for this specific industrial application, it is essential to dispose beforehand
of a characterization of the existent drifts. Under the lack of information in
this regard, a methodology for labelling concept drift events in the lifetime
of wind turbines is proposed. This methodology will facilitate the creation of
a drift database that will serve both as a training ground for concept drift
detectors and as a valuable information to enhance the knowledge about
maintenance of complex systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_I/0/1/0/all/0/1"&gt;I&amp;#xf1;igo Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viles_E/0/1/0/all/0/1"&gt;Elisabeth Viles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cabrejas_I/0/1/0/all/0/1"&gt;I&amp;#xf1;aki Cabrejas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Semantic Segmentation Augmented with Image-Level Weak Annotations. (arXiv:2007.01496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.01496</id>
        <link href="http://arxiv.org/abs/2007.01496"/>
        <updated>2021-06-21T02:07:39.582Z</updated>
        <summary type="html"><![CDATA[Despite the great progress made by deep neural networks in the semantic
segmentation task, traditional neural-networkbased methods typically suffer
from a shortage of large amounts of pixel-level annotations. Recent progress in
fewshot semantic segmentation tackles the issue by only a few pixel-level
annotated examples. However, these few-shot approaches cannot easily be applied
to multi-way or weak annotation settings. In this paper, we advance the
few-shot segmentation paradigm towards a scenario where image-level annotations
are available to help the training process of a few pixel-level annotations.
Our key idea is to learn a better prototype representation of the class by
fusing the knowledge from the image-level labeled data. Specifically, we
propose a new framework, called PAIA, to learn the class prototype
representation in a metric space by integrating image-level annotations.
Furthermore, by considering the uncertainty of pseudo-masks, a distilled soft
masked average pooling strategy is designed to handle distractions in
image-level annotations. Extensive empirical results on two datasets show
superior performance of PAIA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Shuo Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianfeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Context Encoder: Graph Feature Inpainting for Graph Generation and Self-supervised Pretraining. (arXiv:2106.10124v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10124</id>
        <link href="http://arxiv.org/abs/2106.10124"/>
        <updated>2021-06-21T02:07:39.575Z</updated>
        <summary type="html"><![CDATA[We propose the Graph Context Encoder (GCE), a simple but efficient approach
for graph representation learning based on graph feature masking and
reconstruction.

GCE models are trained to efficiently reconstruct input graphs similarly to a
graph autoencoder where node and edge labels are masked. In particular, our
model is also allowed to change graph structures by masking and reconstructing
graphs augmented by random pseudo-edges.

We show that GCE can be used for novel graph generation, with applications
for molecule generation. Used as a pretraining method, we also show that GCE
improves baseline performances in supervised classification tasks tested on
multiple standard benchmark graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Plan via a Multi-Step Policy Regression Method. (arXiv:2106.10075v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10075</id>
        <link href="http://arxiv.org/abs/2106.10075"/>
        <updated>2021-06-21T02:07:39.547Z</updated>
        <summary type="html"><![CDATA[We propose a new approach to increase inference performance in environments
that require a specific sequence of actions in order to be solved. This is for
example the case for maze environments where ideally an optimal path is
determined. Instead of learning a policy for a single step, we want to learn a
policy that can predict n actions in advance. Our proposed method called policy
horizon regression (PHR) uses knowledge of the environment sampled by A2C to
learn an n dimensional policy vector in a policy distillation setup which
yields n sequential actions per observation. We test our method on the MiniGrid
and Pong environments and show drastic speedup during inference time by
successfully predicting sequences of actions on a single observation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1"&gt;Stefan Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janschek_M/0/1/0/all/0/1"&gt;Michael Janschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1"&gt;Stefan Harmeling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09947</id>
        <link href="http://arxiv.org/abs/2106.09947"/>
        <updated>2021-06-21T02:07:39.539Z</updated>
        <summary type="html"><![CDATA[Evaluating robustness of machine-learning models to adversarial examples is a
challenging problem. Many defenses have been shown to provide a false sense of
security by causing gradient-based attacks to fail, and they have been broken
under more rigorous evaluations. Although guidelines and best practices have
been suggested to improve current adversarial robustness evaluations, the lack
of automatic testing and debugging tools makes it difficult to apply these
recommendations in a systematic manner. In this work, we overcome these
limitations by (i) defining a set of quantitative indicators which unveil
common failures in the optimization of gradient-based attacks, and (ii)
proposing specific mitigation strategies within a systematic evaluation
protocol. Our extensive experimental analysis shows that the proposed
indicators of failure can be used to visualize, debug and improve current
adversarial robustness evaluations, providing a first concrete step towards
automatizing and systematizing current adversarial robustness evaluations. Our
open-source code is available at:
https://github.com/pralab/IndicatorsOfAttackFailure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1"&gt;Luca Demetrio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1"&gt;Angelo Sotgiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1"&gt;Giovanni Manca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Dimpled Manifold Model of Adversarial Examples in Machine Learning. (arXiv:2106.10151v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10151</id>
        <link href="http://arxiv.org/abs/2106.10151"/>
        <updated>2021-06-21T02:07:39.525Z</updated>
        <summary type="html"><![CDATA[The extreme fragility of deep neural networks when presented with tiny
perturbations in their inputs was independently discovered by several research
groups in 2013, but in spite of enormous effort these adversarial examples
remained a baffling phenomenon with no clear explanation. In this paper we
introduce a new conceptual framework (which we call the Dimpled Manifold Model)
which provides a simple explanation for why adversarial examples exist, why
their perturbations have such tiny norms, why these perturbations look like
random noise, and why a network which was adversarially trained with
incorrectly labeled images can still correctly classify test images. In the
last part of the paper we describe the results of numerous experiments which
strongly support this new model, and in particular our assertion that
adversarial perturbations are roughly perpendicular to the low dimensional
manifold which contains all the training examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Adi Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melamed_O/0/1/0/all/0/1"&gt;Odelia Melamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+BenShmuel_O/0/1/0/all/0/1"&gt;Oriel BenShmuel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Vertical Federated Learning Framework for Horizontally Partitioned Labels. (arXiv:2106.10056v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10056</id>
        <link href="http://arxiv.org/abs/2106.10056"/>
        <updated>2021-06-21T02:07:39.510Z</updated>
        <summary type="html"><![CDATA[Vertical federated learning is a collaborative machine learning framework to
train deep leaning models on vertically partitioned data with
privacy-preservation. It attracts much attention both from academia and
industry. Unfortunately, applying most existing vertical federated learning
methods in real-world applications still faces two daunting challenges. First,
most existing vertical federated learning methods have a strong assumption that
at least one party holds the complete set of labels of all data samples, while
this assumption is not satisfied in many practical scenarios, where labels are
horizontally partitioned and the parties only hold partial labels. Existing
vertical federated learning methods can only utilize partial labels, which may
lead to inadequate model update in end-to-end backpropagation. Second,
computational and communication resources vary in parties. Some parties with
limited computational and communication resources will become the stragglers
and slow down the convergence of training. Such straggler problem will be
exaggerated in the scenarios of horizontally partitioned labels in vertical
federated learning. To address these challenges, we propose a novel vertical
federated learning framework named Cascade Vertical Federated Learning (CVFL)
to fully utilize all horizontally partitioned labels to train neural networks
with privacy-preservation. To mitigate the straggler problem, we design a novel
optimization objective which can increase straggler's contribution to the
trained models. We conduct a series of qualitative experiments to rigorously
verify the effectiveness of CVFL. It is demonstrated that CVFL can achieve
comparable performance (e.g., accuracy for classification tasks) with
centralized training. The new optimization objective can further mitigate the
straggler problem comparing with only using the asynchronous aggregation
mechanism during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1"&gt;Wensheng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Ying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaoyong Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Differentially Private Federated Learning: Efficient Algorithms with Tight Risk Bounds. (arXiv:2106.09779v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09779</id>
        <link href="http://arxiv.org/abs/2106.09779"/>
        <updated>2021-06-21T02:07:39.480Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a distributed learning paradigm in which many
clients with heterogeneous, unbalanced, and often sensitive local data,
collaborate to learn a model. Local Differential Privacy (LDP) provides a
strong guarantee that each client's data cannot be leaked during and after
training, without relying on a trusted third party. While LDP is often believed
to be too stringent to allow for satisfactory utility, our paper challenges
this belief. We consider a general setup with unbalanced, heterogeneous data,
disparate privacy needs across clients, and unreliable communication, where a
random number/subset of clients is available each round. We propose three LDP
algorithms for smooth (strongly) convex FL; each are noisy variations of
distributed minibatch SGD. One is accelerated and one involves novel
time-varying noise, which we use to obtain the first non-trivial LDP excess
risk bound for the fully general non-i.i.d. FL problem. Specializing to i.i.d.
clients, our risk bounds interpolate between the best known and/or optimal
bounds in the centralized setting and the cross-device setting, where each
client represents just one person's data. Furthermore, we show that in certain
regimes, our convergence rate (nearly) matches the corresponding non-private
lower bound or outperforms state of the art non-private algorithms (``privacy
for free''). Finally, we validate our theoretical results and illustrate the
practical utility of our algorithm with numerical experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lowy_A/0/1/0/all/0/1"&gt;Andrew Lowy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razaviyayn_M/0/1/0/all/0/1"&gt;Meisam Razaviyayn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10163</id>
        <link href="http://arxiv.org/abs/2106.10163"/>
        <updated>2021-06-21T02:07:39.473Z</updated>
        <summary type="html"><![CDATA[Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1"&gt;Erik Jenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning Models Predict Visual Responses in the Brain: A Preliminary Result. (arXiv:2106.10112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10112</id>
        <link href="http://arxiv.org/abs/2106.10112"/>
        <updated>2021-06-21T02:07:39.466Z</updated>
        <summary type="html"><![CDATA[Supervised deep convolutional neural networks (DCNNs) are currently one of
the best computational models that can explain how the primate ventral visual
stream solves object recognition. However, embodied cognition has not been
considered in the existing visual processing models. From the ecological
standpoint, humans learn to recognize objects by interacting with them,
allowing better classification, specialization, and generalization. Here, we
ask if computational models under the embodied learning framework can explain
mechanisms underlying object recognition in the primate visual system better
than the existing supervised models? To address this question, we use
reinforcement learning to train neural network models to play a 3D computer
game and we find that these reinforcement learning models achieve neural
response prediction accuracy scores in the early visual areas (e.g., V1 and V2)
in the levels that are comparable to those accomplished by the supervised
neural network model. In contrast, the supervised neural network models yield
better neural response predictions in the higher visual areas, compared to the
reinforcement learning models. Our preliminary results suggest the future
direction of visual neuroscience in which deep reinforcement learning should be
included to fill the missing embodiment concept.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piriyajitakonkij_M/0/1/0/all/0/1"&gt;Maytus Piriyajitakonkij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itthipuripat_S/0/1/0/all/0/1"&gt;Sirawaj Itthipuripat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilaiprasitporn_T/0/1/0/all/0/1"&gt;Theerawit Wilaiprasitporn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilokthanakul_N/0/1/0/all/0/1"&gt;Nat Dilokthanakul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09875</id>
        <link href="http://arxiv.org/abs/2106.09875"/>
        <updated>2021-06-21T02:07:39.460Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view subspace clustering has achieved impressive
performance due to the exploitation of complementary imformation across
multiple views. However, multi-view data can be very complicated and are not
easy to cluster in real-world applications. Most existing methods operate on
raw data and may not obtain the optimal solution. In this work, we propose a
novel multi-view clustering method named smoothed multi-view subspace
clustering (SMVSC) by employing a novel technique, i.e., graph filtering, to
obtain a smooth representation for each view, in which similar data points have
similar feature values. Specifically, it retains the graph geometric features
through applying a low-pass filter. Consequently, it produces a
``clustering-friendly" representation and greatly facilitates the downstream
clustering task. Extensive experiments on benchmark datasets validate the
superiority of our approach. Analysis shows that graph filtering increases the
separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models. (arXiv:2106.10121v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10121</id>
        <link href="http://arxiv.org/abs/2106.10121"/>
        <updated>2021-06-21T02:07:39.445Z</updated>
        <summary type="html"><![CDATA[Multivariate time series prediction has attracted a lot of attention because
of its wide applications such as intelligence transportation, AIOps. Generative
models have achieved impressive results in time series modeling because they
can model data distribution and take noise into consideration. However, many
existing works can not be widely used because of the constraints of functional
form of generative models or the sensitivity to hyperparameters. In this paper,
we propose ScoreGrad, a multivariate probabilistic time series forecasting
framework based on continuous energy-based generative models. ScoreGrad is
composed of time series feature extraction module and conditional stochastic
differential equation based score matching module. The prediction can be
achieved by iteratively solving reverse-time SDE. To the best of our knowledge,
ScoreGrad is the first continuous energy based generative model used for time
series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on
six real-world datasets. The impact of hyperparameters and sampler types on the
performance are also explored. Code is available at
https://github.com/yantijin/ScoreGradPred.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1"&gt;Tijin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1"&gt;Yufeng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuanqing Xia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Effects of Compression with Hyperdimensional Computing in Distributed Randomized Neural Networks. (arXiv:2106.09831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09831</id>
        <link href="http://arxiv.org/abs/2106.09831"/>
        <updated>2021-06-21T02:07:39.332Z</updated>
        <summary type="html"><![CDATA[A change of the prevalent supervised learning techniques is foreseeable in
the near future: from the complex, computational expensive algorithms to more
flexible and elementary training ones. The strong revitalization of randomized
algorithms can be framed in this prospect steering. We recently proposed a
model for distributed classification based on randomized neural networks and
hyperdimensional computing, which takes into account cost of information
exchange between agents using compression. The use of compression is important
as it addresses the issues related to the communication bottleneck, however,
the original approach is rigid in the way the compression is used. Therefore,
in this work, we propose a more flexible approach to compression and compare it
to conventional compression algorithms, dimensionality reduction, and
quantization techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1"&gt;Antonello Rosato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1"&gt;Massimo Panella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osipov_E/0/1/0/all/0/1"&gt;Evgeny Osipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented KRnet for density estimation and approximation. (arXiv:2105.12866v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12866</id>
        <link href="http://arxiv.org/abs/2105.12866"/>
        <updated>2021-06-21T02:07:39.325Z</updated>
        <summary type="html"><![CDATA[In this work, we have proposed augmented KRnets including both discrete and
continuous models. One difficulty in flow-based generative modeling is to
maintain the invertibility of the transport map, which is often a trade-off
between effectiveness and robustness. The exact invertibility has been achieved
in the real NVP using a specific pattern to exchange information between two
separated groups of dimensions. KRnet has been developed to enhance the
information exchange among data dimensions by incorporating the
Knothe-Rosenblatt rearrangement into the structure of the transport map. Due to
the maintenance of exact invertibility, a full nonlinear update of all data
dimensions needs three iterations in KRnet. To alleviate this issue, we will
add augmented dimensions that act as a channel for communications among the
data dimensions. In the augmented KRnet, a fully nonlinear update is achieved
in two iterations. We also show that the augmented KRnet can be reformulated as
the discretization of a neural ODE, where the exact invertibility is kept such
that the adjoint method can be formulated with respect to the discretized ODE
to obtain the exact gradient. Numerical experiments have been implemented to
demonstrate the effectiveness of our models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wan_X/0/1/0/all/0/1"&gt;Xiaoliang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_K/0/1/0/all/0/1"&gt;Kejun Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs. (arXiv:2106.06300v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06300</id>
        <link href="http://arxiv.org/abs/2106.06300"/>
        <updated>2021-06-21T02:07:39.319Z</updated>
        <summary type="html"><![CDATA[Performing reliable Bayesian inference on a big data scale is becoming a
keystone in the modern era of machine learning. A workhorse class of methods to
achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their
design to handle distributed datasets has been the subject of many works.
However, existing methods are not completely either reliable or computationally
efficient. In this paper, we propose to fill this gap in the case where the
dataset is partitioned and stored on computing nodes within a cluster under a
master/slaves architecture. We derive a user-friendly centralised distributed
MCMC algorithm with provable scaling in high-dimensional settings. We
illustrate the relevance of the proposed methodology on both synthetic and real
data experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Plassier_V/0/1/0/all/0/1"&gt;Vincent Plassier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vono_M/0/1/0/all/0/1"&gt;Maxime Vono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Connections between Counterfactual Explanations and Adversarial Examples. (arXiv:2106.09992v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09992</id>
        <link href="http://arxiv.org/abs/2106.09992"/>
        <updated>2021-06-21T02:07:39.312Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations and adversarial examples have emerged as critical
research areas for addressing the explainability and robustness goals of
machine learning (ML). While counterfactual explanations were developed with
the goal of providing recourse to individuals adversely impacted by algorithmic
decisions, adversarial examples were designed to expose the vulnerabilities of
ML models. While prior research has hinted at the commonalities between these
frameworks, there has been little to no work on systematically exploring the
connections between the literature on counterfactual explanations and
adversarial examples. In this work, we make one of the first attempts at
formalizing the connections between counterfactual explanations and adversarial
examples. More specifically, we theoretically analyze salient counterfactual
explanation and adversarial example generation methods, and highlight the
conditions under which they behave similarly. Our analysis demonstrates that
several popular counterfactual explanation and adversarial example generation
methods such as the ones proposed by Wachter et. al. and Carlini and Wagner
(with mean squared error loss), and C-CHVAE and natural adversarial examples by
Zhao et. al. are equivalent. We also bound the distance between counterfactual
explanations and adversarial examples generated by Wachter et. al. and DeepFool
methods for linear models. Finally, we empirically validate our theoretical
findings using extensive experimentation with synthetic and real world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1"&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1"&gt;Shalmali Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_C/0/1/0/all/0/1"&gt;Chirag Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1"&gt;Sohini Upadhyay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SoK: Privacy-Preserving Collaborative Tree-based Model Learning. (arXiv:2103.08987v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08987</id>
        <link href="http://arxiv.org/abs/2103.08987"/>
        <updated>2021-06-21T02:07:39.305Z</updated>
        <summary type="html"><![CDATA[Tree-based models are among the most efficient machine learning techniques
for data mining nowadays due to their accuracy, interpretability, and
simplicity. The recent orthogonal needs for more data and privacy protection
call for collaborative privacy-preserving solutions. In this work, we survey
the literature on distributed and privacy-preserving training of tree-based
models and we systematize its knowledge based on four axes: the learning
algorithm, the collaborative model, the protection mechanism, and the threat
model. We use this to identify the strengths and limitations of these works and
provide for the first time a framework analyzing the information leakage
occurring in distributed tree-based model learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatel_S/0/1/0/all/0/1"&gt;Sylvain Chatel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pyrgelis_A/0/1/0/all/0/1"&gt;Apostolos Pyrgelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Troncoso_Pastoriza_J/0/1/0/all/0/1"&gt;Juan Ramon Troncoso-Pastoriza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubaux_J/0/1/0/all/0/1"&gt;Jean-Pierre Hubaux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fitting summary statistics of neural data with a differentiable spiking network simulator. (arXiv:2106.10064v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.10064</id>
        <link href="http://arxiv.org/abs/2106.10064"/>
        <updated>2021-06-21T02:07:39.287Z</updated>
        <summary type="html"><![CDATA[Fitting network models to neural activity is becoming an important tool in
neuroscience. A popular approach is to model a brain area with a probabilistic
recurrent spiking network whose parameters maximize the likelihood of the
recorded activity. Although this is widely used, we show that the resulting
model does not produce realistic neural activity and wrongly estimates the
connectivity matrix when neurons that are not recorded have a substantial
impact on the recorded network. To correct for this, we suggest to augment the
log-likelihood with terms that measure the dissimilarity between simulated and
recorded activity. This dissimilarity is defined via summary statistics
commonly used in neuroscience, and the optimization is efficient because it
relies on back-propagation through the stochastically simulated spike trains.
We analyze this method theoretically and show empirically that it generates
more realistic activity statistics and recovers the connectivity matrix better
than other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bellec_G/0/1/0/all/0/1"&gt;Guillaume Bellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Modirshanechi_A/0/1/0/all/0/1"&gt;Alireza Modirshanechi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Brea_J/0/1/0/all/0/1"&gt;Johanni Brea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gerstner_W/0/1/0/all/0/1"&gt;Wulfram Gerstner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. (arXiv:2106.00198v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00198</id>
        <link href="http://arxiv.org/abs/2106.00198"/>
        <updated>2021-06-21T02:07:39.281Z</updated>
        <summary type="html"><![CDATA[We study the performance of the gradient play algorithm for multi-agent
tabular Markov decision processes (MDPs), which are also known as stochastic
games (SGs), where each agent tries to maximize its own total discounted reward
by making decisions independently based on current state information which is
shared between agents. Policies are directly parameterized by the probability
of choosing a certain action at a given state. We show that Nash equilibria
(NEs) and first order stationary policies are equivalent in this setting, and
give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for
a subclass of multi-agent MDPs called Markov potential games, which includes
the cooperative setting with identical rewards among agents as an important
special case. Our result shows that the number of iterations to reach an
$\epsilon$-NE scales linearly, instead of exponentially, with the number of
agents. Local geometry and local stability are also considered. For Markov
potential games, we prove that strict NEs are local maxima of the total
potential function and fully-mixed NEs are saddle points. We also give a local
convergence rate around strict NEs for more general settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Runyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaolin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information criteria for non-normalized models. (arXiv:1905.05976v4 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.05976</id>
        <link href="http://arxiv.org/abs/1905.05976"/>
        <updated>2021-06-21T02:07:39.272Z</updated>
        <summary type="html"><![CDATA[Many statistical models are given in the form of non-normalized densities
with an intractable normalization constant. Since maximum likelihood estimation
is computationally intensive for these models, several estimation methods have
been developed which do not require explicit computation of the normalization
constant, such as noise contrastive estimation (NCE) and score matching.
However, model selection methods for general non-normalized models have not
been proposed so far. In this study, we develop information criteria for
non-normalized models estimated by NCE or score matching. They are
approximately unbiased estimators of discrepancy measures for non-normalized
models. Simulation results and applications to real data demonstrate that the
proposed criteria enable selection of the appropriate non-normalized model in a
data-driven manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takeru Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Uehara_M/0/1/0/all/0/1"&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hyvarinen_A/0/1/0/all/0/1"&gt;Aapo Hyvarinen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10070</id>
        <link href="http://arxiv.org/abs/2106.10070"/>
        <updated>2021-06-21T02:07:39.266Z</updated>
        <summary type="html"><![CDATA[The breakthrough of contrastive learning (CL) has fueled the recent success
of self-supervised learning (SSL) in high-level vision tasks on RGB images.
However, CL is still ill-defined for low-level vision tasks, such as joint
demosaicking and denoising (JDD), in the RAW domain. To bridge this
methodological gap, we present a novel CL approach on RAW images, residual
contrastive learning (RCL), which aims to learn meaningful representations for
JDD. Our work is built on the assumption that noise contained in each RAW image
is signal-dependent, thus two crops from the same RAW image should have more
similar noise distribution than two crops from different RAW images. We use
residuals as a discriminative feature and the earth mover's distance to measure
the distribution divergence for the contrastive loss. To evaluate the proposed
CL strategy, we simulate a series of unsupervised JDD experiments with
large-scale data corrupted by synthetic signal-dependent noise, where we set a
new benchmark for unsupervised JDD tasks with unknown (random) noise variance.
Our empirical study not only validates that CL can be applied on distributions
(c.f. features), but also exposes the lack of robustness of previous non-ML and
SSL JDD methods when the statistics of the noise are unknown, thus providing
some further insight into signal-dependent noise problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1"&gt;Matteo Maggioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1"&gt;Steven McDonagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Control for Decentralized Deep Learning. (arXiv:2102.04828v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04828</id>
        <link href="http://arxiv.org/abs/2102.04828"/>
        <updated>2021-06-21T02:07:39.259Z</updated>
        <summary type="html"><![CDATA[Decentralized training of deep learning models enables on-device learning
over networks, as well as efficient scaling to large compute clusters.
Experiments in earlier works reveal that, even in a data-center setup,
decentralized training often suffers from the degradation in the quality of the
model: the training and test performance of models trained in a decentralized
fashion is in general worse than that of models trained in a centralized
fashion, and this performance drop is impacted by parameters such as network
size, communication topology and data partitioning. We identify the changing
consensus distance between devices as a key parameter to explain the gap
between centralized and decentralized training.

We show in theory that when the training consensus distance is lower than a
critical quantity, decentralized training converges as fast as the centralized
counterpart. We empirically validate that the relation between generalization
performance and consensus distance is consistent with this theoretical
observation. Our empirical insights allow the principled design of better
decentralized training schemes that mitigate the performance drop. To this end,
we provide practical training guidelines and exemplify its effectiveness on the
data-center setup as the important first step.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingjing Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koloskova_A/0/1/0/all/0/1"&gt;Anastasia Koloskova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1"&gt;Martin Jaggi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1"&gt;Sebastian U. Stich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting the Function Approximation Architecture in Online Reinforcement Learning. (arXiv:2106.09776v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09776</id>
        <link href="http://arxiv.org/abs/2106.09776"/>
        <updated>2021-06-21T02:07:39.240Z</updated>
        <summary type="html"><![CDATA[The performance of a reinforcement learning (RL) system depends on the
computational architecture used to approximate a value function. Deep learning
methods provide both optimization techniques and architectures for
approximating nonlinear functions from noisy, high-dimensional observations.
However, prevailing optimization techniques are not designed for
strictly-incremental online updates. Nor are standard architectures designed
for observations with an a priori unknown structure: for example, light sensors
randomly dispersed in space. This paper proposes an online RL prediction
algorithm with an adaptive architecture that efficiently finds useful nonlinear
features. The algorithm is evaluated in a spatial domain with high-dimensional,
stochastic observations. The algorithm outperforms non-adaptive baseline
architectures and approaches the performance of an architecture given
side-channel information. These results are a step towards scalable RL
algorithms for more general problems, where the observation structure is not
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1"&gt;John D. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Modayil_J/0/1/0/all/0/1"&gt;Joseph Modayil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10351</id>
        <link href="http://arxiv.org/abs/2008.10351"/>
        <updated>2021-06-21T02:07:39.234Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that deep learning models can be used to classify
land-use data from geospatial satellite imagery. We show that when these deep
learning models are trained on data from specific continents/seasons, there is
a high degree of variability in model performance on out-of-sample
continents/seasons. This suggests that just because a model accurately predicts
land-use classes in one continent or season does not mean that the model will
accurately predict land-use classes in a different continent or season. We then
use clustering techniques on satellite imagery from different continents to
visualize the differences in landscapes that make geospatial generalization
particularly difficult, and summarize our takeaways for future satellite
imagery-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lucas Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Approximations for a Class of Sequential Testing Problems. (arXiv:2102.07030v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07030</id>
        <link href="http://arxiv.org/abs/2102.07030"/>
        <updated>2021-06-21T02:07:39.226Z</updated>
        <summary type="html"><![CDATA[We consider a decision maker who must choose an action in order to maximize a
reward function that depends also on an unknown parameter {\Theta}. The
decision maker can delay taking the action in order to experiment and gather
additional information on {\Theta}. We model the decision maker's problem using
a Bayesian sequential experimentation framework and use dynamic programming and
diffusion-asymptotic analysis to solve it. For that, we scale our problem in a
way that both the average number of experiments that is conducted per unit of
time is large and the informativeness of each individual experiment is low.
Under such regime, we derive a diffusion approximation for the sequential
experimentation problem, which provides a number of important insights about
the nature of the problem and its solution. Our solution method also shows that
the complexity of the problem grows only quadratically with the cardinality of
the set of actions from which the decision maker can choose. We illustrate our
methodology and results using a concrete application in the context of
assortment selection and new product introduction. Specifically, we study the
problem of a seller who wants to select an optimal assortment of products to
launch into the marketplace and is uncertain about consumers' preferences.
Motivated by emerging practices in e-commerce, we assume that the seller is
able to use a crowdvoting system to learn these preferences before a final
assortment decision is made. In this context, we undertake an extensive
numerical analysis to assess the value of learning and demonstrate the
effectiveness and robustness of the heuristics derived from the diffusion
approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Araman_V/0/1/0/all/0/1"&gt;Victor F. Araman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Caldentey_R/0/1/0/all/0/1"&gt;Rene Caldentey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01607</id>
        <link href="http://arxiv.org/abs/2005.01607"/>
        <updated>2021-06-21T02:07:39.216Z</updated>
        <summary type="html"><![CDATA[Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'
image from a pathological one. Such images can be helpful in tasks such as
anomaly detection and understanding changes induced by pathology and disease.
In this paper, we present a model that is encouraged to disentangle the
information of pathology from what seems to be healthy. We disentangle what
appears to be healthy and where disease is as a segmentation map, which are
then recombined by a network to reconstruct the input disease image. We train
our models adversarially using either paired or unpaired settings, where we
pair disease images and maps when available. We quantitatively and
subjectively, with a human study, evaluate the quality of pseudo-healthy images
using several criteria. We show in a series of experiments, performed on ISLES,
BraTS and Cam-CAN datasets, that our method is better than several baselines
and methods from the literature. We also show that due to better training
processes we could recover deformations, on surrounding tissue, caused by
disease. Our implementation is publicly available at
https://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been
accepted by Medical Image Analysis:
https://doi.org/10.1016/j.media.2020.101719.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09946</id>
        <link href="http://arxiv.org/abs/2106.09946"/>
        <updated>2021-06-21T02:07:39.208Z</updated>
        <summary type="html"><![CDATA[Limited availability of labeled-data makes any supervised learning problem
challenging. Alternative learning settings like semi-supervised and universum
learning alleviate the dependency on labeled data, but still require a large
amount of unlabeled data, which may be unavailable or expensive to acquire.
GAN-based synthetic data generation methods have recently shown promise by
generating synthetic samples to improve task at hand. However, these samples
cannot be used for other purposes. In this paper, we propose a GAN game which
provides improved discriminator accuracy under limited data settings, while
generating realistic synthetic data. This provides the added advantage that now
the generated data can be used for other similar tasks. We provide the
theoretical guarantees and empirical results in support of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1"&gt;Sauptik Dhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1"&gt;Javad Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Samarth Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1"&gt;Unmesh Kurup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Message Passing in Graph Convolution Networks via Adaptive Filter Banks. (arXiv:2106.09910v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09910</id>
        <link href="http://arxiv.org/abs/2106.09910"/>
        <updated>2021-06-21T02:07:39.189Z</updated>
        <summary type="html"><![CDATA[Graph convolution networks, like message passing graph convolution networks
(MPGCNs), have been a powerful tool in representation learning of networked
data. However, when data is heterogeneous, most architectures are limited as
they employ a single strategy to handle multi-channel graph signals and they
typically focus on low-frequency information. In this paper, we present a novel
graph convolution operator, termed BankGCN, which keeps benefits of message
passing models, but extends their capabilities beyond `low-pass' features. It
decomposes multi-channel signals on graphs into subspaces and handles
particular information in each subspace with an adapted filter. The filters of
all subspaces have different frequency responses and together form a filter
bank. Furthermore, each filter in the spectral domain corresponds to a message
passing scheme, and diverse schemes are implemented via the filter bank.
Importantly, the filter bank and the signal decomposition are jointly learned
to adapt to the spectral characteristics of data and to target applications.
Furthermore, this is implemented almost without extra parameters in comparison
with most existing MPGCNs. Experimental results show that the proposed
convolution operator permits to achieve excellent performance in graph
classification on a collection of benchmark graph datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xing Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Armed Bandits for Minesweeper: Profiting from Exploration-Exploitation Synergy. (arXiv:2007.12824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12824</id>
        <link href="http://arxiv.org/abs/2007.12824"/>
        <updated>2021-06-21T02:07:39.182Z</updated>
        <summary type="html"><![CDATA[A popular computer puzzle, the game of Minesweeper requires its human players
to have a mix of both luck and strategy to succeed. Analyzing these aspects
more formally, in our research we assessed the feasibility of a novel
methodology based on Reinforcement Learning as an adequate approach to tackle
the problem presented by this game. For this purpose we employed Multi-Armed
Bandit algorithms which were carefully adapted in order to enable their use to
define autonomous computational players, targeting to make the best use of some
game peculiarities. After experimental evaluation, results showed that this
approach was indeed successful, especially in smaller game boards, such as the
standard beginner level. Despite this fact the main contribution of this work
is a detailed examination of Minesweeper from a learning perspective, which led
to various original insights which are thoroughly discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lordeiro_I/0/1/0/all/0/1"&gt;Igor Q. Lordeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1"&gt;Diego B. Haddad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cardoso_D/0/1/0/all/0/1"&gt;Douglas O. Cardoso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10258</id>
        <link href="http://arxiv.org/abs/2106.10258"/>
        <updated>2021-06-21T02:07:39.175Z</updated>
        <summary type="html"><![CDATA[When interacting with objects through cameras, or pictures, users often have
a specific intent. For example, they may want to perform a visual search.
However, most object detection models ignore the user intent, relying on image
pixels as their only input. This often leads to incorrect results, such as lack
of a high-confidence detection on the object of interest, or detection with a
wrong class label. In this paper we investigate techniques to modulate standard
object detectors to explicitly account for the user intent, expressed as an
embedding of a simple query. Compared to standard object detectors,
query-modulated detectors show superior performance at detecting objects for a
given label of interest. Thanks to large-scale training data synthesized from
standard object detection annotations, query-modulated detectors can also
outperform specialized referring expression recognition systems. Furthermore,
they can be simultaneously trained to solve for both query-modulated detection
and standard object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1"&gt;Marco Fornoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chaochao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Liangchen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1"&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1"&gt;Alex Stark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1"&gt;Andrew Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Minimax Problems. (arXiv:2106.10022v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10022</id>
        <link href="http://arxiv.org/abs/2106.10022"/>
        <updated>2021-06-21T02:07:39.168Z</updated>
        <summary type="html"><![CDATA[Large scale convex-concave minimax problems arise in numerous applications,
including game theory, robust training, and training of generative adversarial
networks. Despite their wide applicability, solving such problems efficiently
and effectively is challenging in the presence of large amounts of data using
existing stochastic minimax methods. We study a class of stochastic minimax
methods and develop a communication-efficient distributed stochastic
extragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable
for solving convex-concave minimax problem in the Parameter-Server model.
LocalAdaSEG has three main features: (i) periodic communication strategy
reduces the communication cost between workers and the server; (ii) an adaptive
learning rate that is computed locally and allows for tuning-free
implementation; and (iii) theoretically, a nearly linear speed-up with respect
to the dominant variance term, arising from estimation of the stochastic
gradient, is proven in both the smooth and nonsmooth convex-concave settings.
LocalAdaSEG is used to solve a stochastic bilinear game, and train generative
adversarial network. We compare LocalAdaSEG against several existing optimizers
for minimax problems and demonstrate its efficacy through several experiments
in both the homogeneous and heterogeneous settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1"&gt;Luofeng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Li Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jia Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolar_M/0/1/0/all/0/1"&gt;Mladen Kolar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[It's FLAN time! Summing feature-wise latent representations for interpretability. (arXiv:2106.10086v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10086</id>
        <link href="http://arxiv.org/abs/2106.10086"/>
        <updated>2021-06-21T02:07:39.162Z</updated>
        <summary type="html"><![CDATA[Interpretability has become a necessary feature for machine learning models
deployed in critical scenarios, e.g. legal systems, healthcare. In these
situations, algorithmic decisions may have (potentially negative) long-lasting
effects on the end-user affected by the decision. In many cases, the
representational power of deep learning models is not needed, therefore simple
and interpretable models (e.g. linear models) should be preferred. However, in
high-dimensional and/or complex domains (e.g. computer vision), the universal
approximation capabilities of neural networks is required. Inspired by linear
models and the Kolmogorov-Arnol representation theorem, we propose a novel
class of structurally-constrained neural networks, which we call FLANs
(Feature-wise Latent Additive Networks). Crucially, FLANs process each input
feature separately, computing for each of them a representation in a common
latent space. These feature-wise latent representations are then simply summed,
and the aggregated representation is used for prediction. These constraints
(which are at the core of the interpretability of linear models) allow an user
to estimate the effect of each individual feature independently from the
others, enhancing interpretability. In a set of experiments across different
domains, we show how without compromising excessively the test performance, the
structural constraints proposed in FLANs indeed increase the interpretability
of deep learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1"&gt;An-phi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1"&gt;Maria Rodriguez Martinez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-healthy synthesis with pathology disentanglement and adversarial learning. (arXiv:2005.01607v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01607</id>
        <link href="http://arxiv.org/abs/2005.01607"/>
        <updated>2021-06-21T02:07:39.155Z</updated>
        <summary type="html"><![CDATA[Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'
image from a pathological one. Such images can be helpful in tasks such as
anomaly detection and understanding changes induced by pathology and disease.
In this paper, we present a model that is encouraged to disentangle the
information of pathology from what seems to be healthy. We disentangle what
appears to be healthy and where disease is as a segmentation map, which are
then recombined by a network to reconstruct the input disease image. We train
our models adversarially using either paired or unpaired settings, where we
pair disease images and maps when available. We quantitatively and
subjectively, with a human study, evaluate the quality of pseudo-healthy images
using several criteria. We show in a series of experiments, performed on ISLES,
BraTS and Cam-CAN datasets, that our method is better than several baselines
and methods from the literature. We also show that due to better training
processes we could recover deformations, on surrounding tissue, caused by
disease. Our implementation is publicly available at
https://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been
accepted by Medical Image Analysis:
https://doi.org/10.1016/j.media.2020.101719.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10270</id>
        <link href="http://arxiv.org/abs/2106.10270"/>
        <updated>2021-06-21T02:07:39.136Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation (``AugReg'' for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1"&gt;Andreas Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1"&gt;Ross Wightman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10156</id>
        <link href="http://arxiv.org/abs/2106.10156"/>
        <updated>2021-06-21T02:07:39.129Z</updated>
        <summary type="html"><![CDATA[Predicting gender by the name is not a simple task. In many applications,
especially in the natural language processing (NLP) field, this task may be
necessary, mainly when considering foreign names. Some machine learning
algorithms can satisfactorily perform the prediction. In this paper, we
examined and implemented feedforward and recurrent deep neural network models,
such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first
name. A dataset of Brazilian names is used to train and evaluate the models. We
analyzed the accuracy, recall, precision, and confusion matrix to measure the
models' performances. The results indicate that the gender prediction can be
performed from the feature extraction strategy looking at the names as a set of
strings. Some models accurately predict the gender in more than 90% of the
cases. The recurrent models overcome the feedforward models in this binary
classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1"&gt;Rosana C. B. Rego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Ver&amp;#xf4;nica M. L. Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09756</id>
        <link href="http://arxiv.org/abs/2106.09756"/>
        <updated>2021-06-21T02:07:39.121Z</updated>
        <summary type="html"><![CDATA[Machine learning is a general-purpose technology holding promises for many
interdisciplinary research problems. However, significant barriers exist in
crossing disciplinary boundaries when most machine learning tools are developed
in different areas separately. We present Pykale - a Python library for
knowledge-aware machine learning on graphs, images, texts, and videos to enable
and accelerate interdisciplinary research. We formulate new green machine
learning guidelines based on standard software engineering practices and
propose a novel pipeline-based application programming interface (API). PyKale
focuses on leveraging knowledge from multiple sources for accurate and
interpretable prediction, thus supporting multimodal learning and transfer
learning (particularly domain adaptation) with latest deep learning and
dimensionality reduction models. We build PyKale on PyTorch and leverage the
rich PyTorch ecosystem. Our pipeline-based API design enforces standardization
and minimalism, embracing green machine learning concepts via reducing
repetitions and redundancy, reusing existing resources, and recycling learning
models across areas. We demonstrate its interdisciplinary nature via examples
in bioinformatics, knowledge graph, image/video recognition, and medical
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Robert Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peizhen Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo E Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1"&gt;Mustafa Chasmai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1"&gt;Lawrence Schobs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09898</id>
        <link href="http://arxiv.org/abs/2106.09898"/>
        <updated>2021-06-21T02:07:39.113Z</updated>
        <summary type="html"><![CDATA[Several years of research have shown that machine-learning systems are
vulnerable to adversarial examples, both in theory and in practice. Until now,
such attacks have primarily targeted visual models, exploiting the gap between
human and machine perception. Although text-based models have also been
attacked with adversarial examples, such attacks struggled to preserve semantic
meaning and indistinguishability. In this paper, we explore a large class of
adversarial examples that can be used to attack text-based models in a
black-box setting without making any human-perceptible visual modification to
inputs. We use encoding-specific perturbations that are imperceptible to the
human eye to manipulate the outputs of a wide range of Natural Language
Processing (NLP) systems from neural machine-translation pipelines to web
search engines. We find that with a single imperceptible encoding injection --
representing one invisible character, homoglyph, reordering, or deletion -- an
attacker can significantly reduce the performance of vulnerable models, and
with three injections most models can be functionally broken. Our attacks work
against currently-deployed commercial systems, including those produced by
Microsoft and Google, in addition to open source models published by Facebook
and IBM. This novel series of attacks presents a significant threat to many
language processing systems: an attacker can affect systems in a targeted
manner without any assumptions about the underlying model. We conclude that
text-based NLP systems require careful input sanitization, just like
conventional applications, and that given such systems are now being deployed
rapidly at scale, the urgent attention of architects and operators is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1"&gt;Nicholas Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Federated Learning with New Classes for Audio Classification. (arXiv:2106.10019v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10019</id>
        <link href="http://arxiv.org/abs/2106.10019"/>
        <updated>2021-06-21T02:07:39.095Z</updated>
        <summary type="html"><![CDATA[Federated learning is an effective way of extracting insights from different
user devices while preserving the privacy of users. However, new classes with
completely unseen data distributions can stream across any device in a
federated learning setting, whose data cannot be accessed by the global server
or other users. To this end, we propose a unified zero-shot framework to handle
these aforementioned challenges during federated learning. We simulate two
scenarios here -- 1) when the new class labels are not reported by the user,
the traditional FL setting is used; 2) when new class labels are reported by
the user, we synthesize Anonymized Data Impressions by calculating class
similarity matrices corresponding to each device's new classes followed by
unsupervised clustering to distinguish between new classes across different
users. Moreover, our proposed framework can also handle statistical
heterogeneities in both labels and models across the participating users. We
empirically evaluate our framework on-device across different communication
rounds (FL iterations) with new classes in both local and global updates, along
with heterogeneous labels and models, on two widely used audio classification
applications -- keyword spotting and urban sound classification, and observe an
average deterministic accuracy increase of ~4.041% and ~4.258% respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gudur_G/0/1/0/all/0/1"&gt;Gautham Krishna Gudur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perepu_S/0/1/0/all/0/1"&gt;Satheesh K. Perepu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal-Directed Planning by Reinforcement Learning and Active Inference. (arXiv:2106.09938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09938</id>
        <link href="http://arxiv.org/abs/2106.09938"/>
        <updated>2021-06-21T02:07:39.088Z</updated>
        <summary type="html"><![CDATA[What is the difference between goal-directed and habitual behavior? We
propose a novel computational framework of decision making with Bayesian
inference, in which everything is integrated as an entire neural network model.
The model learns to predict environmental state transitions by self-exploration
and generating motor actions by sampling stochastic internal states $z$.
Habitual behavior, which is obtained from the prior distribution of $z$, is
acquired by reinforcement learning. Goal-directed behavior is determined from
the posterior distribution of $z$ by planning, using active inference, to
minimize the free energy for goal observation. We demonstrate the effectiveness
of the proposed framework by experiments in a sensorimotor navigation task with
camera observations and continuous motor actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1"&gt;Dongqi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doya_K/0/1/0/all/0/1"&gt;Kenji Doya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tani_J/0/1/0/all/0/1"&gt;Jun Tani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Bias Quantification for Continuous Treatment. (arXiv:2106.09762v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.09762</id>
        <link href="http://arxiv.org/abs/2106.09762"/>
        <updated>2021-06-21T02:07:39.077Z</updated>
        <summary type="html"><![CDATA[In this work we develop a novel characterization of marginal causal effect
and causal bias in the continuous treatment setting. We show they can be
expressed as an expectation with respect to a conditional probability
distribution, which can be estimated via standard statistical and probabilistic
methods. All terms in the expectations can be computed via automatic
differentiation, also for highly non-linear models. We further develop a new
complete criterion for identifiability of causal effects via covariate
adjustment, showing the bias equals zero if the criterion is met. We study the
effectiveness of our framework in three different scenarios: linear models
under confounding, overcontrol and endogenous selection bias; a non-linear
model where full identifiability cannot be achieved because of missing data; a
simulated medical study of statins and atherosclerotic cardiovascular disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Detommaso_G/0/1/0/all/0/1"&gt;Gianluca Detommaso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bruckner_M/0/1/0/all/0/1"&gt;Michael Br&amp;#xfc;ckner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schulz_P/0/1/0/all/0/1"&gt;Philip Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping strict saddle points of the Moreau envelope in nonsmooth optimization. (arXiv:2106.09815v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.09815</id>
        <link href="http://arxiv.org/abs/2106.09815"/>
        <updated>2021-06-21T02:07:39.068Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that stochastically perturbed gradient methods can
efficiently escape strict saddle points of smooth functions. We extend this
body of work to nonsmooth optimization, by analyzing an inexact analogue of a
stochastically perturbed gradient method applied to the Moreau envelope. The
main conclusion is that a variety of algorithms for nonsmooth optimization can
escape strict saddle points of the Moreau envelope at a controlled rate. The
main technical insight is that typical algorithms applied to the proximal
subproblem yield directions that approximate the gradient of the Moreau
envelope in relative terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Davis_D/0/1/0/all/0/1"&gt;Damek Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diaz_M/0/1/0/all/0/1"&gt;Mateo D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Drusvyatskiy_D/0/1/0/all/0/1"&gt;Dmitriy Drusvyatskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boolean Matrix Factorization with SAT and MaxSAT. (arXiv:2106.10105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10105</id>
        <link href="http://arxiv.org/abs/2106.10105"/>
        <updated>2021-06-21T02:07:39.056Z</updated>
        <summary type="html"><![CDATA[The Boolean matrix factorization problem consists in approximating a matrix
by the Boolean product of two smaller Boolean matrices. To obtain optimal
solutions when the matrices to be factorized are small, we propose SAT and
MaxSAT encoding; however, when the matrices to be factorized are large, we
propose a heuristic based on the search for maximal biclique edge cover. We
experimentally demonstrate that our approaches allow a better factorization
than existing approaches while keeping reasonable computation times. Our
methods also allow the handling of incomplete matrices with missing entries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avellaneda_F/0/1/0/all/0/1"&gt;Florent Avellaneda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villemaire_R/0/1/0/all/0/1"&gt;Roger Villemaire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09993</id>
        <link href="http://arxiv.org/abs/2106.09993"/>
        <updated>2021-06-21T02:07:39.049Z</updated>
        <summary type="html"><![CDATA[Collecting training data from untrusted sources exposes machine learning
services to poisoning adversaries, who maliciously manipulate training data to
degrade the model accuracy. When trained on offline datasets, poisoning
adversaries have to inject the poisoned data in advance before training, and
the order of feeding these poisoned batches into the model is stochastic. In
contrast, practical systems are more usually trained/fine-tuned on sequentially
captured real-time data, in which case poisoning adversaries could dynamically
poison each data batch according to the current model state. In this paper, we
focus on the real-time settings and propose a new attacking strategy, which
affiliates an accumulative phase with poisoning attacks to secretly (i.e.,
without affecting accuracy) magnify the destructive effect of a (poisoned)
trigger batch. By mimicking online learning and federated learning on CIFAR-10,
we show that the model accuracy will significantly drop by a single update step
on the trigger batch after the accumulative phase. Our work validates that a
well-designed but straightforward attacking strategy can dramatically amplify
the poisoning effects, with no need to explore complex techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anomaly Detection in Dynamic Graphs via Transformer. (arXiv:2106.09876v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09876</id>
        <link href="http://arxiv.org/abs/2106.09876"/>
        <updated>2021-06-21T02:07:39.031Z</updated>
        <summary type="html"><![CDATA[Detecting anomalies for dynamic graphs has drawn increasing attention due to
their wide applications in social networks, e-commerce, and cybersecurity. The
recent deep learning-based approaches have shown promising results over shallow
methods. However, they fail to address two core challenges of anomaly detection
in dynamic graphs: the lack of informative encoding for unattributed nodes and
the difficulty of learning discriminate knowledge from coupled spatial-temporal
dynamic graphs. To overcome these challenges, in this paper, we present a novel
Transformer-based Anomaly Detection framework for DYnamic graph (TADDY). Our
framework constructs a comprehensive node encoding strategy to better represent
each node's structural and temporal roles in an evolving graphs stream.
Meanwhile, TADDY captures informative representation from dynamic graphs with
coupled spatial-temporal patterns via a dynamic graph transformer model. The
extensive experimental results demonstrate that our proposed TADDY framework
outperforms the state-of-the-art methods by a large margin on four real-world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Guang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1"&gt;Fei Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1"&gt;Vincent CS Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Being Properly Improper. (arXiv:2106.09920v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09920</id>
        <link href="http://arxiv.org/abs/2106.09920"/>
        <updated>2021-06-21T02:07:39.025Z</updated>
        <summary type="html"><![CDATA[In today's ML, data can be twisted (changed) in various ways, either for bad
or good intent. Such twisted data challenges the founding theory of properness
for supervised losses which form the basis for many popular losses for class
probability estimation. Unfortunately, at its core, properness ensures that the
optimal models also learn the twist. In this paper, we analyse such class
probability-based losses when they are stripped off the mandatory properness;
we define twist-proper losses as losses formally able to retrieve the optimum
(untwisted) estimate off the twists, and show that a natural extension of a
half-century old loss introduced by S. Arimoto is twist proper. We then turn to
a theory that has provided some of the best off-the-shelf algorithms for proper
losses, boosting. Boosting can require access to the derivative of the convex
conjugate of a loss to compute examples weights. Such a function can be hard to
get, for computational or mathematical reasons; this turns out to be the case
for Arimoto's loss. We bypass this difficulty by inverting the problem as
follows: suppose a blueprint boosting algorithm is implemented with a general
weight update function. What are the losses for which boosting-compliant
minimisation happens? Our answer comes as a general boosting algorithm which
meets the optimal boosting dependence on the number of calls to the weak
learner; when applied to Arimoto's loss, it leads to a simple optimisation
algorithm whose performances are showcased on several domains and twists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1"&gt;Richard Nock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sypherd_T/0/1/0/all/0/1"&gt;Tyler Sypherd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sankar_L/0/1/0/all/0/1"&gt;Lalitha Sankar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09794</id>
        <link href="http://arxiv.org/abs/2106.09794"/>
        <updated>2021-06-21T02:07:39.009Z</updated>
        <summary type="html"><![CDATA[To evaluate clustering results is a significant part of cluster analysis.
Since there are no true class labels for clustering in typical unsupervised
learning, many internal cluster validity indices (CVIs), which use predicted
labels and data, have been created. Without true labels, to design an effective
CVI is as difficult as to create a clustering method. And it is crucial to have
more CVIs because there are no universal CVIs that can be used to measure all
datasets and no specific methods of selecting a proper CVI for clusters without
true labels. Therefore, to apply a variety of CVIs to evaluate clustering
results is necessary. In this paper, we propose a novel internal CVI -- the
Distance-based Separability Index (DSI), based on a data separability measure.
We compared the DSI with eight internal CVIs including studies from early Dunn
(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using
clustering results of five clustering algorithms on 12 real and 97 synthetic
datasets. Results show DSI is an effective, unique, and competitive CVI to
other compared CVIs. We also summarized the general process to evaluate CVIs
and created the rank-difference metric for comparison of CVIs' results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Shuyue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09785</id>
        <link href="http://arxiv.org/abs/2106.09785"/>
        <updated>2021-06-21T02:07:39.002Z</updated>
        <summary type="html"><![CDATA[This paper investigates two techniques for developing efficient
self-supervised vision transformers (EsViT) for visual representation learning.
First, we show through a comprehensive empirical study that multi-stage
architectures with sparse self-attentions can significantly reduce modeling
complexity but with a cost of losing the ability to capture fine-grained
correspondences between image regions. Second, we propose a new pre-training
task of region matching which allows the model to capture fine-grained region
dependencies and as a result significantly improves the quality of the learned
vision representations. Our results show that combining the two techniques,
EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,
outperforming prior arts with around an order magnitude of higher throughput.
When transferring to downstream linear classification tasks, EsViT outperforms
its supervised counterpart on 17 out of 18 datasets. The code and models will
be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shuffle Private Stochastic Convex Optimization. (arXiv:2106.09805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09805</id>
        <link href="http://arxiv.org/abs/2106.09805"/>
        <updated>2021-06-21T02:07:38.982Z</updated>
        <summary type="html"><![CDATA[In shuffle privacy, each user sends a collection of randomized messages to a
trusted shuffler, the shuffler randomly permutes these messages, and the
resulting shuffled collection of messages must satisfy differential privacy.
Prior work in this model has largely focused on protocols that use a single
round of communication to compute algorithmic primitives like means,
histograms, and counts. In this work, we present interactive shuffle protocols
for stochastic convex optimization. Our optimization protocols rely on a new
noninteractive protocol for summing vectors of bounded $\ell_2$ norm. By
combining this sum subroutine with techniques including mini-batch stochastic
gradient descent, accelerated gradient descent, and Nesterov's smoothing
method, we obtain loss guarantees for a variety of convex loss functions that
significantly improve on those of the local model and sometimes match those of
the central model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheu_A/0/1/0/all/0/1"&gt;Albert Cheu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joseph_M/0/1/0/all/0/1"&gt;Matthew Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jieming Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Binghui Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09871</id>
        <link href="http://arxiv.org/abs/2106.09871"/>
        <updated>2021-06-21T02:07:38.976Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop active learning
workflows for finding relevant documents in large collections. These workflows
often must meet a target for the proportion of relevant documents found (i.e.
recall) while also holding down costs. A variety of heuristic stopping rules
have been suggested for striking this tradeoff in particular settings, but none
have been tested against a range of recall targets and tasks. We propose two
new heuristic stopping rules, Quant and QuantCI based on model-based estimation
techniques from survey research. We compare them against a range of proposed
heuristics and find they are accurate at hitting a range of recall targets
while substantially reducing review costs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09834</id>
        <link href="http://arxiv.org/abs/2106.09834"/>
        <updated>2021-06-21T02:07:38.970Z</updated>
        <summary type="html"><![CDATA[By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT
reconstruction is a holy grail to minimize cancer risks and genetic damages,
especially for children. With the development of medical CT technologies, the
iterative algorithms are widely used to reconstruct decent CT images from a
low-dose scan. Recently, artificial intelligence (AI) techniques have shown a
great promise in further reducing CT radiation dose to the next level. In this
paper, we demonstrate that AI-powered CT reconstruction offers diagnostic image
quality at an ultra-low-dose level comparable to that of radiography.
Specifically, here we develop a Split Unrolled Grid-like Alternative
Reconstruction (SUGAR) network, in which deep learning, physical modeling and
image prior are integrated. The reconstruction results from clinical datasets
show that excellent images can be reconstructed using SUGAR from 36
projections. This approach has a potential to change future healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weiwen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1"&gt;Shadi Ebrahimian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1"&gt;Mannu Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Invariance Penalties for Risk Minimization. (arXiv:2106.09777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09777</id>
        <link href="http://arxiv.org/abs/2106.09777"/>
        <updated>2021-06-21T02:07:38.963Z</updated>
        <summary type="html"><![CDATA[The Invariant Risk Minimization (IRM) principle was first proposed by
Arjovsky et al. [2019] to address the domain generalization problem by
leveraging data heterogeneity from differing experimental conditions.
Specifically, IRM seeks to find a data representation under which an optimal
classifier remains invariant across all domains. Despite the conceptual appeal
of IRM, the effectiveness of the originally proposed invariance penalty has
recently been brought into question. In particular, there exists
counterexamples for which that invariance penalty can be arbitrarily small for
non-invariant data representations. We propose an alternative invariance
penalty by revisiting the Gramian matrix of the data representation. We discuss
the role of its eigenvalues in the relationship between the risk and the
invariance penalty, and demonstrate that it is ill-conditioned for said
counterexamples. The proposed approach is guaranteed to recover an invariant
representation for linear settings under mild non-degeneracy conditions. Its
effectiveness is substantiated by experiments on DomainBed and
InvarianceUnitTest, two extensive test beds for domain generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1"&gt;Kia Khezeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blaas_A/0/1/0/all/0/1"&gt;Arno Blaas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soboczenski_F/0/1/0/all/0/1"&gt;Frank Soboczenski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chia_N/0/1/0/all/0/1"&gt;Nicholas Chia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalantari_J/0/1/0/all/0/1"&gt;John Kalantari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Indoor Mapping through WiFi Signals. (arXiv:2106.09789v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.09789</id>
        <link href="http://arxiv.org/abs/2106.09789"/>
        <updated>2021-06-21T02:07:38.953Z</updated>
        <summary type="html"><![CDATA[The ubiquitous presence of WiFi access points and mobile devices capable of
measuring WiFi signal strengths allow for real-world applications in indoor
localization and mapping. In particular, no additional infrastructure is
required. Previous approaches in this field were, however, often hindered by
problems such as effortful map-building processes, changing environments and
hardware differences. We tackle these problems focussing on topological maps.
These represent discrete locations, such as rooms, and their relations, e.g.,
distances and transition frequencies. In our unsupervised method, we employ
WiFi signal strength distributions, dimension reduction and clustering. It can
be used in settings where users carry mobile devices and follow their normal
routine. We aim for applications in short-lived indoor events such as
conferences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schaefermeier_B/0/1/0/all/0/1"&gt;Bastian Schaefermeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1"&gt;Gerd Stumme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1"&gt;Tom Hanika&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09994</id>
        <link href="http://arxiv.org/abs/2106.09994"/>
        <updated>2021-06-21T02:07:38.946Z</updated>
        <summary type="html"><![CDATA[Kernel mean embeddings are a popular tool that consists in representing
probability measures by their infinite-dimensional mean embeddings in a
reproducing kernel Hilbert space. When the kernel is characteristic, mean
embeddings can be used to define a distance between probability measures, known
as the maximum mean discrepancy (MMD). A well-known advantage of mean
embeddings and MMD is their low computational cost and low sample complexity.
However, kernel mean embeddings have had limited applications to problems that
consist in optimizing distributions, due to the difficulty of characterizing
which Hilbert space vectors correspond to a probability distribution. In this
note, we propose to leverage the kernel sums-of-squares parameterization of
positive functions of Marteau-Ferey et al. [2020] to fit distributions in the
MMD geometry. First, we show that when the kernel is characteristic,
distributions with a kernel sum-of-squares density are dense. Then, we provide
algorithms to optimize such distributions in the finite-sample setting, which
we illustrate in a density fitting numerical experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1"&gt;Boris Muzellec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09914</id>
        <link href="http://arxiv.org/abs/2106.09914"/>
        <updated>2021-06-21T02:07:38.927Z</updated>
        <summary type="html"><![CDATA[We propose a novel GAN training scheme that can handle any level of labeling
in a unified manner. Our scheme introduces a form of artificial labeling that
can incorporate manually defined labels, when available, and induce an
alignment between them. To define the artificial labels, we exploit the
assumption that neural network generators can be trained more easily to map
nearby latent vectors to data with semantic similarities, than across separate
categories. We use generated data samples and their corresponding artificial
conditioning labels to train a classifier. The classifier is then used to
self-label real data. To boost the accuracy of the self-labeling, we also use
the exponential moving average of the classifier. However, because the
classifier might still make mistakes, especially at the beginning of the
training, we also refine the labels through self-attention, by using the
labeling of real data samples only when the classifier outputs a high
classification probability score. We evaluate our approach on CIFAR-10, STL-10
and SVHN, and show that both self-labeling and self-attention consistently
improve the quality of generated data. More surprisingly, we find that the
proposed scheme can even outperform class-conditional GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Tomoki Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradual Domain Adaptation via Self-Training of Auxiliary Models. (arXiv:2106.09890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09890</id>
        <link href="http://arxiv.org/abs/2106.09890"/>
        <updated>2021-06-21T02:07:38.921Z</updated>
        <summary type="html"><![CDATA[Domain adaptation becomes more challenging with increasing gaps between
source and target domains. Motivated from an empirical analysis on the
reliability of labeled source data for the use of distancing target domains, we
propose self-training of auxiliary models (AuxSelfTrain) that learns models for
intermediate domains and gradually combats the distancing shifts across
domains. We introduce evolving intermediate domains as combinations of
decreasing proportion of source data and increasing proportion of target data,
which are sampled to minimize the domain distance between consecutive domains.
Then the source model could be gradually adapted for the use in the target
domain by self-training of auxiliary models on evolving intermediate domains.
We also introduce an enhanced indicator for sample selection via implicit
ensemble and extend the proposed method to semi-supervised domain adaptation.
Experiments on benchmark datasets of unsupervised and semi-supervised domain
adaptation verify its efficacy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Bin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Many Agent Reinforcement Learning Under Partial Observability. (arXiv:2106.09825v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09825</id>
        <link href="http://arxiv.org/abs/2106.09825"/>
        <updated>2021-06-21T02:07:38.914Z</updated>
        <summary type="html"><![CDATA[Recent renewed interest in multi-agent reinforcement learning (MARL) has
generated an impressive array of techniques that leverage deep reinforcement
learning, primarily actor-critic architectures, and can be applied to a limited
range of settings in terms of observability and communication. However, a
continuing limitation of much of this work is the curse of dimensionality when
it comes to representations based on joint actions, which grow exponentially
with the number of agents. In this paper, we squarely focus on this challenge
of scalability. We apply the key insight of action anonymity, which leads to
permutation invariance of joint actions, to two recently presented deep MARL
algorithms, MADDPG and IA2C, and compare these instantiations to another recent
technique that leverages action anonymity, viz., mean-field MARL. We show that
our instantiations can learn the optimal behavior in a broader class of agent
networks than the mean-field method, using a recently introduced pragmatic
domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1"&gt;Keyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doshi_P/0/1/0/all/0/1"&gt;Prashant Doshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1"&gt;Bikramjit Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers. (arXiv:2106.10153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10153</id>
        <link href="http://arxiv.org/abs/2106.10153"/>
        <updated>2021-06-21T02:07:38.907Z</updated>
        <summary type="html"><![CDATA[Combining Natural Language with Vision represents a unique and interesting
challenge in the domain of Artificial Intelligence. The AI City Challenge Track
5 for Natural Language-Based Vehicle Retrieval focuses on the problem of
combining visual and textual information, applied to a smart-city use case. In
this paper, we present All You Can Embed (AYCE), a modular solution to
correlate single-vehicle tracking sequences with natural language. The main
building blocks of the proposed architecture are (i) BERT to provide an
embedding of the textual descriptions, (ii) a convolutional backbone along with
a Transformer model to embed the visual information. For the training of the
retrieval model, a variation of the Triplet Margin Loss is proposed to learn a
distance measure between the visual and language embeddings. The code is
publicly available at https://github.com/cscribano/AYCE_2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scribano_C/0/1/0/all/0/1"&gt;Carmelo Scribano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sapienza_D/0/1/0/all/0/1"&gt;Davide Sapienza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franchini_G/0/1/0/all/0/1"&gt;Giorgia Franchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verucchi_M/0/1/0/all/0/1"&gt;Micaela Verucchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bertogna_M/0/1/0/all/0/1"&gt;Marko Bertogna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09759</id>
        <link href="http://arxiv.org/abs/2106.09759"/>
        <updated>2021-06-21T02:07:38.900Z</updated>
        <summary type="html"><![CDATA[We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for
training machine learning models. The dataset consists of 21,295 synthetic
COVID-19 chest X-ray images to be used for computer-aided diagnosis. These
images, generated via an unsupervised domain adaptation approach, are of high
quality. We find that the synthetic images not only improve performance of
various deep learning architectures when used as additional training data under
heavy imbalance conditions, but also detect the target class with high
confidence. We also find that comparable performance can also be achieved when
trained only on synthetic images. Further, salient features of the synthetic
COVID-19 images indicate that the distribution is significantly different from
Non-COVID-19 classes, enabling a proper decision boundary. We hope the
availability of such high fidelity chest X-ray images of COVID-19 will
encourage advances in the development of diagnostic and/or management tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers. (arXiv:2106.10270v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10270</id>
        <link href="http://arxiv.org/abs/2106.10270"/>
        <updated>2021-06-21T02:07:38.882Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViT) have been shown to attain highly competitive
performance for a wide range of vision applications, such as image
classification, object detection and semantic image segmentation. In comparison
to convolutional neural networks, the Vision Transformer's weaker inductive
bias is generally found to cause an increased reliance on model regularization
or data augmentation (``AugReg'' for short) when training on smaller training
datasets. We conduct a systematic empirical study in order to better understand
the interplay between the amount of training data, AugReg, model size and
compute budget. As one result of this study we find that the combination of
increased compute and AugReg can yield models with the same performance as
models trained on an order of magnitude more training data: we train ViT models
of various sizes on the public ImageNet-21k dataset which either match or
outperform their counterparts trained on the larger, but not publicly available
JFT-300M dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1"&gt;Andreas Steiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1"&gt;Ross Wightman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Multi-Fidelity Bayesian Optimization with Deep Auto-Regressive Networks. (arXiv:2106.09884v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09884</id>
        <link href="http://arxiv.org/abs/2106.09884"/>
        <updated>2021-06-21T02:07:38.875Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization (BO) is a powerful approach for optimizing black-box,
expensive-to-evaluate functions. To enable a flexible trade-off between the
cost and accuracy, many applications allow the function to be evaluated at
different fidelities. In order to reduce the optimization cost while maximizing
the benefit-cost ratio, in this paper, we propose Batch Multi-fidelity Bayesian
Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of
Bayesian neural networks to construct a fully auto-regressive model, which is
expressive enough to capture strong yet complex relationships across all the
fidelities, so as to improve the surrogate learning and optimization
performance. Furthermore, to enhance the quality and diversity of queries, we
develop a simple yet efficient batch querying method, without any combinatorial
search over the fidelities. We propose a batch acquisition function based on
Max-value Entropy Search (MES) principle, which penalizes highly correlated
queries and encourages diversity. We use posterior samples and moment matching
to fulfill efficient computation of the acquisition function and conduct
alternating optimization over every fidelity-input pair, which guarantees an
improvement at each step. We demonstrate the advantage of our approach on four
real-world hyperparameter optimization applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shibo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirby_R/0/1/0/all/0/1"&gt;Robert M. Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1"&gt;Shandian Zhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSEC: Large-scale spectral ensemble clustering. (arXiv:2106.09852v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09852</id>
        <link href="http://arxiv.org/abs/2106.09852"/>
        <updated>2021-06-21T02:07:38.868Z</updated>
        <summary type="html"><![CDATA[Ensemble clustering is a fundamental problem in the machine learning field,
combining multiple base clusterings into a better clustering result. However,
most of the existing methods are unsuitable for large-scale ensemble clustering
tasks due to the efficiency bottleneck. In this paper, we propose a large-scale
spectral ensemble clustering (LSEC) method to strike a good balance between
efficiency and effectiveness. In LSEC, a large-scale spectral clustering based
efficient ensemble generation framework is designed to generate various base
clusterings within a low computational complexity. Then all based clustering
are combined through a bipartite graph partition based consensus function into
a better consensus clustering result. The LSEC method achieves a lower
computational complexity than most existing ensemble clustering methods.
Experiments conducted on ten large-scale datasets show the efficiency and
effectiveness of the LSEC method. The MATLAB code of the proposed method and
experimental datasets are available at https://github.com/Li-
Hongmin/MyPaperWithCode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongmin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiucai Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Imakura_A/0/1/0/all/0/1"&gt;Akira Imakura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sakurai_T/0/1/0/all/0/1"&gt;Tetsuya Sakurai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machining Cycle Time Prediction: Data-driven Modelling of Machine Tool Feedrate Behavior with Neural Networks. (arXiv:2106.09719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09719</id>
        <link href="http://arxiv.org/abs/2106.09719"/>
        <updated>2021-06-21T02:07:38.862Z</updated>
        <summary type="html"><![CDATA[Accurate prediction of machining cycle times is important in the
manufacturing industry. Usually, Computer Aided Manufacturing (CAM) software
estimates the machining times using the commanded feedrate from the toolpath
file using basic kinematic settings. Typically, the methods do not account for
toolpath geometry or toolpath tolerance and therefore under estimate the
machining cycle times considerably. Removing the need for machine specific
knowledge, this paper presents a data-driven feedrate and machining cycle time
prediction method by building a neural network model for each machine tool
axis. In this study, datasets composed of the commanded feedrate, nominal
acceleration, toolpath geometry and the measured feedrate were used to train a
neural network model. Validation trials using a representative industrial thin
wall structure component on a commercial machining centre showed that this
method estimated the machining time with more than 90% accuracy. This method
showed that neural network models have the capability to learn the behavior of
a complex machine tool system and predict cycle times. Further integration of
the methods will be critical in the implantation of digital twins in Industry
4.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dominguez_Caballero_J/0/1/0/all/0/1"&gt;Javier Dominguez-Caballero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ward_R/0/1/0/all/0/1"&gt;Rob Ward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayvar_Soberanis_S/0/1/0/all/0/1"&gt;Sabino Ayvar-Soberanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Curtis_D/0/1/0/all/0/1"&gt;David Curtis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAC Prediction Sets Under Covariate Shift. (arXiv:2106.09848v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09848</id>
        <link href="http://arxiv.org/abs/2106.09848"/>
        <updated>2021-06-21T02:07:38.855Z</updated>
        <summary type="html"><![CDATA[An important challenge facing modern machine learning is how to rigorously
quantify the uncertainty of model predictions. Conveying uncertainty is
especially important when there are changes to the underlying data distribution
that might invalidate the predictive model. Yet, most existing uncertainty
quantification algorithms break down in the presence of such shifts. We propose
a novel approach that addresses this challenge by constructing \emph{probably
approximately correct (PAC)} prediction sets in the presence of covariate
shift. Our approach focuses on the setting where there is a covariate shift
from the source distribution (where we have labeled training examples) to the
target distribution (for which we want to quantify uncertainty). Our algorithm
assumes given importance weights that encode how the probabilities of the
training examples change under the covariate shift. In practice, importance
weights typically need to be estimated; thus, we extend our algorithm to the
setting where we are given confidence intervals for the importance weights
rather than their true value. We demonstrate the effectiveness of our approach
on various covariate shifts designed based on the DomainNet and ImageNet
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangdon Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1"&gt;Edgar Dobriban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1"&gt;Osbert Bastani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient-free optimization of chaotic acoustics with reservoir computing. (arXiv:2106.09780v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2106.09780</id>
        <link href="http://arxiv.org/abs/2106.09780"/>
        <updated>2021-06-21T02:07:38.849Z</updated>
        <summary type="html"><![CDATA[We develop a versatile optimization method, which finds the design parameters
that minimize time-averaged acoustic cost functionals. The method is
gradient-free, model-informed, and data-driven with reservoir computing based
on echo state networks. First, we analyse the predictive capabilities of echo
state networks both in the short- and long-time prediction of the dynamics. We
find that both fully data-driven and model-informed architectures learn the
chaotic acoustic dynamics, both time-accurately and statistically. Informing
the training with a physical reduced-order model with one acoustic mode
markedly improves the accuracy and robustness of the echo state networks,
whilst keeping the computational cost low. Echo state networks offer accurate
predictions of the long-time dynamics, which would be otherwise expensive by
integrating the governing equations to evaluate the time-averaged quantity to
optimize. Second, we couple echo state networks with a Bayesian technique to
explore the design thermoacoustic parameter space. The computational method is
minimally intrusive. Third, we find the set of flame parameters that minimize
the time-averaged acoustic energy of chaotic oscillations, which are caused by
the positive feedback with a heat source, such as a flame in gas turbines or
rocket motors. These oscillations are known as thermoacoustic oscillations. The
optimal set of flame parameters is found with the same accuracy as brute-force
grid search, but with a convergence rate that is more than one order of
magnitude faster. This work opens up new possibilities for non-intrusive
(``hands-off'') optimization of chaotic systems, in which the cost of
generating data, for example from high-fidelity simulations and experiments, is
high.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Huhn_F/0/1/0/all/0/1"&gt;Francisco Huhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Magri_L/0/1/0/all/0/1"&gt;Luca Magri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Resource Allocation with Graph Neural Networks. (arXiv:2106.09761v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09761</id>
        <link href="http://arxiv.org/abs/2106.09761"/>
        <updated>2021-06-21T02:07:38.831Z</updated>
        <summary type="html"><![CDATA[We present an approach for maximizing a global utility function by learning
how to allocate resources in an unsupervised way. We expect interactions
between allocation targets to be important and therefore propose to learn the
reward structure for near-optimal allocation policies with a GNN. By relaxing
the resource constraint, we can employ gradient-based optimization in contrast
to more standard evolutionary algorithms. Our algorithm is motivated by a
problem in modern astronomy, where one needs to select-based on limited initial
information-among $10^9$ galaxies those whose detailed measurement will lead to
optimal inference of the composition of the universe. Our technique presents a
way of flexibly learning an allocation strategy by only requiring forward
simulators for the physics of interest and the measurement process. We
anticipate that our technique will also find applications in a range of
resource allocation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1"&gt;Miles Cranmer&lt;/a&gt; (Princeton), &lt;a href="http://arxiv.org/find/cs/1/au:+Melchior_P/0/1/0/all/0/1"&gt;Peter Melchior&lt;/a&gt; (Princeton), &lt;a href="http://arxiv.org/find/cs/1/au:+Nord_B/0/1/0/all/0/1"&gt;Brian Nord&lt;/a&gt; (Fermilab)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments. (arXiv:2106.09913v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09913</id>
        <link href="http://arxiv.org/abs/2106.09913"/>
        <updated>2021-06-21T02:07:38.824Z</updated>
        <summary type="html"><![CDATA[Domain generalization aims at performing well on unseen test environments
with data from a limited number of training environments. Despite a
proliferation of proposal algorithms for this task, assessing their
performance, both theoretically and empirically is still very challenging.
Moreover, recent approaches such as Invariant Risk Minimization (IRM) require a
prohibitively large number of training environments - linear in the dimension
of the spurious feature space $d_s$ - even on simple data models like the one
proposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show
that both ERM and IRM cannot generalize with $o(d_s)$ environments. We then
present a new algorithm based on performing iterative feature matching that is
guaranteed with high probability to yield a predictor that generalizes after
seeing only $O(\log{d_s})$ environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1"&gt;Elan Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09788</id>
        <link href="http://arxiv.org/abs/2106.09788"/>
        <updated>2021-06-21T02:07:38.816Z</updated>
        <summary type="html"><![CDATA[Integrated Gradients (IG) is a commonly used feature attribution method for
deep neural networks. While IG has many desirable properties, the method often
produces spurious/noisy pixel attributions in regions that are not related to
the predicted class when applied to visual models. While this has been
previously noted, most existing solutions are aimed at addressing the symptoms
by explicitly reducing the noise in the resulting attributions. In this work,
we show that one of the causes of the problem is the accumulation of noise
along the IG path. To minimize the effect of this source of noise, we propose
adapting the attribution path itself -- conditioning the path not just on the
image but also on the model being explained. We introduce Adaptive Path Methods
(APMs) as a generalization of path methods, and Guided IG as a specific
instance of an APM. Empirically, Guided IG creates saliency maps better aligned
with the model's prediction and the input image that is being explained. We
show through qualitative and quantitative experiments that Guided IG
outperforms other, related methods in nearly every experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1"&gt;Andrei Kapishnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1"&gt;Besim Avci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1"&gt;Ben Wedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1"&gt;Michael Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1"&gt;Tolga Bolukbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07617</id>
        <link href="http://arxiv.org/abs/2106.07617"/>
        <updated>2021-06-21T02:07:38.810Z</updated>
        <summary type="html"><![CDATA[Recently, Vision Transformers (ViTs) have achieved impressive results on
various vision tasks. Yet, their generalization ability under different
distribution shifts is rarely understood. In this work, we provide a
comprehensive study on the out-of-distribution generalization of ViTs. To
support a systematic investigation, we first present a taxonomy of distribution
shifts by categorizing them into five conceptual groups: corruption shift,
background shift, texture shift, destruction shift, and style shift. Then we
perform extensive evaluations of ViT variants under different groups of
distribution shifts and compare their generalization ability with CNNs. Several
important observations are obtained: 1) ViTs generalize better than CNNs under
multiple distribution shifts. With the same or fewer parameters, ViTs are ahead
of corresponding CNNs by more than 5% in top-1 accuracy under most distribution
shifts. 2) Larger ViTs gradually narrow the in-distribution and
out-of-distribution performance gap. To further improve the generalization of
ViTs, we design the Generalization-Enhanced ViTs by integrating adversarial
learning, information theory, and self-supervised learning. By investigating
three types of generalization-enhanced ViTs, we observe their
gradient-sensitivity and design a smoother learning strategy to achieve a
stable training process. With modified training schemes, we achieve
improvements on performance towards out-of-distribution data by 4% from vanilla
ViTs. We comprehensively compare three generalization-enhanced ViTs with their
corresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs
still benefit more for the out-of-distribution generalization. 2)
generalization-enhanced ViTs are more sensitive to the hyper-parameters than
corresponding CNNs. We hope our comprehensive study could shed light on the
design of more generalizable learning architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongzhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shanghang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1"&gt;Daisheng Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhongang Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1"&gt;Shuai Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianglong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Hashing Methods. (arXiv:2003.03369v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03369</id>
        <link href="http://arxiv.org/abs/2003.03369"/>
        <updated>2021-06-21T02:07:38.792Z</updated>
        <summary type="html"><![CDATA[Nearest neighbor search is to find the data points in the database such that
the distances from them to the query are the smallest, which is a fundamental
problem in various domains, such as computer vision, recommendation systems and
machine learning. Hashing is one of the most widely used methods for its
computational and storage efficiency. With the development of deep learning,
deep hashing methods show more advantages than traditional methods. In this
paper, we present a comprehensive survey of the deep hashing algorithms.
Specifically, we categorize deep supervised hashing methods into pairwise
similarity preserving, multiwise similarity preserving, implicit similarity
preserving, classification-oriented preserving as well as quantization
according to the manners of preserving the similarities. In addition, we also
introduce some other topics such as deep unsupervised hashing and multi-modal
deep hashing methods. Meanwhile, we also present some commonly used public
datasets and the scheme to measure the performance of deep hashing algorithms.
Finally, we discussed some potential research directions in conclusion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Daqing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xian-Sheng Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10013</id>
        <link href="http://arxiv.org/abs/2105.10013"/>
        <updated>2021-06-21T02:07:38.785Z</updated>
        <summary type="html"><![CDATA[Image classification methods are usually trained to perform predictions
taking into account a predefined group of known classes. Real-world problems,
however, may not allow for a full knowledge of the input and label spaces,
making failures in recognition a hazard to deep visual learning. Open set
recognition methods are characterized by the ability to correctly identifying
inputs of known and unknown classes. In this context, we propose GeMOS: simple
and plug-and-play open set recognition modules that can be attached to
pretrained Deep Neural Networks for visual recognition. The GeMOS framework
pairs pre-trained Convolutional Neural Networks with generative models for open
set recognition to extract open set scores for each sample, allowing for
failure recognition in object recognition tasks. We conduct a thorough
evaluation of the proposed method in comparison with state-of-the-art open set
algorithms, finding that GeMOS either outperforms or is statistically
indistinguishable from more complex and costly models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1"&gt;Marcos Vendramini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1"&gt;Hugo Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1"&gt;Alexei Machado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1"&gt;Jefersson A. dos Santos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Consensual Collaborative Learning Method for Remote Sensing Image Classification Under Noisy Multi-Labels. (arXiv:2105.05496v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05496</id>
        <link href="http://arxiv.org/abs/2105.05496"/>
        <updated>2021-06-21T02:07:38.778Z</updated>
        <summary type="html"><![CDATA[Collecting a large number of reliable training images annotated by multiple
land-cover class labels in the framework of multi-label classification is
time-consuming and costly in remote sensing (RS). To address this problem,
publicly available thematic products are often used for annotating RS images
with zero-labeling-cost. However, such an approach may result in constructing a
training set with noisy multi-labels, distorting the learning process. To
address this problem, we propose a Consensual Collaborative Multi-Label
Learning (CCML) method. The proposed CCML identifies, ranks and corrects
training images with noisy multi-labels through four main modules: 1)
discrepancy module; 2) group lasso module; 3) flipping module; and 4) swap
module. The discrepancy module ensures that the two networks learn diverse
features, while obtaining the same predictions. The group lasso module detects
the potentially noisy labels by estimating the label uncertainty based on the
aggregation of two collaborative networks. The flipping module corrects the
identified noisy labels, whereas the swap module exchanges the ranking
information between the two networks. The experimental results confirm the
success of the proposed CCML under high (synthetically added) multi-label noise
rates. The code of the proposed method is publicly available at
https://noisy-labels-in-rs.org]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aksoy_A/0/1/0/all/0/1"&gt;Ahmet Kerem Aksoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1"&gt;Mahdyar Ravanbakhsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreuziger_T/0/1/0/all/0/1"&gt;Tristan Kreuziger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1"&gt;Begum Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autoencoder-based cleaning in probabilistic databases. (arXiv:2106.09764v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.09764</id>
        <link href="http://arxiv.org/abs/2106.09764"/>
        <updated>2021-06-21T02:07:38.763Z</updated>
        <summary type="html"><![CDATA[In the field of data integration, data quality problems are often encountered
when extracting, combining, and merging data. The probabilistic data
integration approach represents information about such problems as
uncertainties in a probabilistic database. In this paper, we propose a
data-cleaning autoencoder capable of near-automatic data quality improvement.
It learns the structure and dependencies in the data to identify and correct
doubtful values. A theoretical framework is provided, and experiments show that
it can remove significant amounts of noise from categorical and numeric
probabilistic data. Our method does not require clean data. We do, however,
show that manually cleaning a small fraction of the data significantly improves
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1"&gt;R.R. Mauritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1"&gt;F.P.J. Nijweide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1"&gt;J. Goseling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1"&gt;M. van Keulen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields. (arXiv:2106.05187v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05187</id>
        <link href="http://arxiv.org/abs/2106.05187"/>
        <updated>2021-06-21T02:07:38.756Z</updated>
        <summary type="html"><![CDATA[We present implicit displacement fields, a novel representation for detailed
3D geometry. Inspired by a classic surface deformation technique, displacement
mapping, our method represents a complex surface as a smooth base surface plus
a displacement along the base's normal directions, resulting in a
frequency-based shape decomposition, where the high frequency signal is
constrained geometrically by the low frequency signal. Importantly, this
disentanglement is unsupervised thanks to a tailored architectural design that
has an innate frequency hierarchy by construction. We explore implicit
displacement field surface reconstruction and detail transfer and demonstrate
superior representational power, training stability and generalizability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1"&gt;Wang Yifan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmann_L/0/1/0/all/0/1"&gt;Lukas Rahmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1"&gt;Olga Sorkine-Hornung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10271</id>
        <link href="http://arxiv.org/abs/2106.10271"/>
        <updated>2021-06-21T02:07:38.741Z</updated>
        <summary type="html"><![CDATA[Temporal action detection (TAD) aims to determine the semantic label and the
boundaries of every action instance in an untrimmed video. It is a fundamental
task in video understanding and significant progress has been made in TAD.
Previous methods involve multiple stages or networks and hand-designed rules or
operations, which fall short in efficiency and flexibility. Here, we construct
an end-to-end framework for TAD upon Transformer, termed \textit{TadTR}, which
simultaneously predicts all action instances as a set of labels and temporal
locations in parallel. TadTR is able to adaptively extract temporal context
information needed for making action predictions, by selectively attending to a
number of snippets in a video. It greatly simplifies the pipeline of TAD and
runs much faster than previous detectors. Our method achieves state-of-the-art
performance on HACS Segments and THUMOS14 and competitive performance on
ActivityNet-1.3. Our code will be made available at
\url{https://github.com/xlliu7/TadTR}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaolong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qimeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xu Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Song Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning of Generalized Game Representations. (arXiv:2106.10060v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10060</id>
        <link href="http://arxiv.org/abs/2106.10060"/>
        <updated>2021-06-21T02:07:38.710Z</updated>
        <summary type="html"><![CDATA[Representing games through their pixels offers a promising approach for
building general-purpose and versatile game models. While games are not merely
images, neural network models trained on game pixels often capture differences
of the visual style of the image rather than the content of the game. As a
result, such models cannot generalize well even within similar games of the
same genre. In this paper we build on recent advances in contrastive learning
and showcase its benefits for representation learning in games. Learning to
contrast images of games not only classifies games in a more efficient manner;
it also yields models that separate games in a more meaningful fashion by
ignoring the visual style and focusing, instead, on their content. Our results
in a large dataset of sports video games containing 100k images across 175
games and 10 game genres suggest that contrastive learning is better suited for
learning generalized game representations compared to conventional supervised
learning. The findings of this study bring us closer to universal visual
encoders for games that can be reused across previously unseen games without
requiring retraining or fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1"&gt;Chintan Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1"&gt;Antonios Liapis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1"&gt;Georgios N. Yannakakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Relationships between Object Categories via Universal Canonical Maps. (arXiv:2106.09758v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09758</id>
        <link href="http://arxiv.org/abs/2106.09758"/>
        <updated>2021-06-21T02:07:38.680Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of learning the geometry of multiple categories of
deformable objects jointly. Recent work has shown that it is possible to learn
a unified dense pose predictor for several categories of related objects.
However, training such models requires to initialize inter-category
correspondences by hand. This is suboptimal and the resulting models fail to
maintain correct correspondences as individual categories are learned. In this
paper, we show that improved correspondences can be learned automatically as a
natural byproduct of learning category-specific dense pose predictors. To do
this, we express correspondences between different categories and between
images and categories using a unified embedding. Then, we use the latter to
enforce two constraints: symmetric inter-category cycle consistency and a new
asymmetric image-to-category cycle consistency. Without any manual annotations
for the inter-category correspondences, we obtain state-of-the-art alignment
results, outperforming dedicated methods for matching 3D shapes. Moreover, the
new model is also better at the task of dense pose prediction than prior work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1"&gt;Natalia Neverova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1"&gt;Artsiom Sanakoyeu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1"&gt;Patrick Labatut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1"&gt;David Novotny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Meshing from Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching. (arXiv:2106.10031v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10031</id>
        <link href="http://arxiv.org/abs/2106.10031"/>
        <updated>2021-06-21T02:07:38.672Z</updated>
        <summary type="html"><![CDATA[Reconstruction of object or scene surfaces has tremendous applications in
computer vision, computer graphics, and robotics. In this paper, we study a
fundamental problem in this context about recovering a surface mesh from an
implicit field function whose zero-level set captures the underlying surface.
To achieve the goal, existing methods rely on traditional meshing algorithms;
while promising, they suffer from loss of precision learned in the implicit
surface networks, due to the use of discrete space sampling in marching cubes.
Given that an MLP with activations of Rectified Linear Unit (ReLU) partitions
its input space into a number of linear regions, we are motivated to connect
this local linearity with a same property owned by the desired result of
polygon mesh. More specifically, we identify from the linear regions,
partitioned by an MLP based implicit function, the analytic cells and analytic
faces that are associated with the function's zero-level isosurface. We prove
that under mild conditions, the identified analytic faces are guaranteed to
connect and form a closed, piecewise planar surface. Based on the theorem, we
propose an algorithm of analytic marching, which marches among analytic cells
to exactly recover the mesh captured by an implicit surface network. We also
show that our theory and algorithm are equally applicable to advanced MLPs with
shortcut connections and max pooling. Given the parallel nature of analytic
marching, we contribute AnalyticMesh, a software package that supports
efficient meshing of implicit surface networks via CUDA parallel computing, and
mesh simplification for efficient downstream processing. We apply our method to
different settings of generative shape modeling using implicit surface
networks. Extensive experiments demonstrate our advantages over existing
methods in terms of both meshing accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiabao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10118</id>
        <link href="http://arxiv.org/abs/2106.10118"/>
        <updated>2021-06-21T02:07:38.655Z</updated>
        <summary type="html"><![CDATA[This paper explores the potential for performing temporal semantic
segmentation in the context of agricultural robotics without temporally
labelled data. We achieve this by proposing to generate virtual temporal
samples from labelled still images. This allows us, with no extra annotation
effort, to generate virtually labelled temporal sequences. Normally, to train a
recurrent neural network (RNN), labelled samples from a video (temporal)
sequence are required which is laborious and has stymied work in this
direction. By generating virtual temporal samples, we demonstrate that it is
possible to train a lightweight RNN to perform semantic segmentation on two
challenging agricultural datasets. Our results show that by training a temporal
semantic segmenter using virtual samples we can increase the performance by an
absolute amount of 4.6 and 4.9 on sweet pepper and sugar beet datasets,
respectively. This indicates that our virtual data augmentation technique is
able to accurately classify agricultural images temporally without the use of
complicated synthetic data generation techniques nor with the overhead of
labelling large amounts of temporal sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Alireza Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1"&gt;Michael Halstead&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1"&gt;Chris McCool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Novelty Detection via Contrastive Learning with Negative Data Augmentation. (arXiv:2106.09958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09958</id>
        <link href="http://arxiv.org/abs/2106.09958"/>
        <updated>2021-06-21T02:07:38.648Z</updated>
        <summary type="html"><![CDATA[Novelty detection is the process of determining whether a query example
differs from the learned training distribution. Previous methods attempt to
learn the representation of the normal samples via generative adversarial
networks (GANs). However, they will suffer from instability training, mode
dropping, and low discriminative ability. Recently, various pretext tasks (e.g.
rotation prediction and clustering) have been proposed for self-supervised
learning in novelty detection. However, the learned latent features are still
low discriminative. We overcome such problems by introducing a novel
decoder-encoder framework. Firstly, a generative network (a.k.a. decoder)
learns the representation by mapping the initialized latent vector to an image.
In particular, this vector is initialized by considering the entire
distribution of training data to avoid the problem of mode-dropping. Secondly,
a contrastive network (a.k.a. encoder) aims to ``learn to compare'' through
mutual information estimation, which directly helps the generative network to
obtain a more discriminative representation by using a negative data
augmentation strategy. Extensive experiments show that our model has
significant superiority over cutting-edge novelty detectors and achieves new
state-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and
DCASE. Moreover, our model is more stable for training in a non-adversarial
manner, compared to other adversarial based novelty detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chengwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shaohui Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1"&gt;Ruizhi Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Contrastive Learning for Joint Demosaicking and Denoising. (arXiv:2106.10070v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10070</id>
        <link href="http://arxiv.org/abs/2106.10070"/>
        <updated>2021-06-21T02:07:38.640Z</updated>
        <summary type="html"><![CDATA[The breakthrough of contrastive learning (CL) has fueled the recent success
of self-supervised learning (SSL) in high-level vision tasks on RGB images.
However, CL is still ill-defined for low-level vision tasks, such as joint
demosaicking and denoising (JDD), in the RAW domain. To bridge this
methodological gap, we present a novel CL approach on RAW images, residual
contrastive learning (RCL), which aims to learn meaningful representations for
JDD. Our work is built on the assumption that noise contained in each RAW image
is signal-dependent, thus two crops from the same RAW image should have more
similar noise distribution than two crops from different RAW images. We use
residuals as a discriminative feature and the earth mover's distance to measure
the distribution divergence for the contrastive loss. To evaluate the proposed
CL strategy, we simulate a series of unsupervised JDD experiments with
large-scale data corrupted by synthetic signal-dependent noise, where we set a
new benchmark for unsupervised JDD tasks with unknown (random) noise variance.
Our empirical study not only validates that CL can be applied on distributions
(c.f. features), but also exposes the lack of robustness of previous non-ML and
SSL JDD methods when the statistics of the noise are unknown, thus providing
some further insight into signal-dependent noise problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1"&gt;Nanqing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1"&gt;Matteo Maggioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1"&gt;Steven McDonagh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Longitudinal Neighbourhood Embedding. (arXiv:2103.03840v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03840</id>
        <link href="http://arxiv.org/abs/2103.03840"/>
        <updated>2021-06-21T02:07:38.633Z</updated>
        <summary type="html"><![CDATA[Longitudinal MRIs are often used to capture the gradual deterioration of
brain structure and function caused by aging or neurological diseases.
Analyzing this data via machine learning generally requires a large number of
ground-truth labels, which are often missing or expensive to obtain. Reducing
the need for labels, we propose a self-supervised strategy for representation
learning named Longitudinal Neighborhood Embedding (LNE). Motivated by concepts
in contrastive learning, LNE explicitly models the similarity between
trajectory vectors across different subjects. We do so by building a graph in
each training iteration defining neighborhoods in the latent space so that the
progression direction of a subject follows the direction of its neighbors. This
results in a smooth trajectory field that captures the global morphological
change of the brain while maintaining the local continuity. We apply LNE to
longitudinal T1w MRIs of two neuroimaging studies: a dataset composed of 274
healthy subjects, and Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632). The visualization of the smooth trajectory vector field and superior
performance on downstream tasks demonstrate the strength of the proposed method
over existing self-supervised methods in extracting information associated with
normal aging and in revealing the impact of neurodegenerative disorders. The
code is available at
\url{https://github.com/ouyangjiahong/longitudinal-neighbourhood-embedding.git}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1"&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1"&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1"&gt;Edith V Sullivan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1"&gt;Adolf Pfefferbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharchuk_G/0/1/0/all/0/1"&gt;Greg Zaharchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1"&gt;Kilian M Pohl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09862</id>
        <link href="http://arxiv.org/abs/2106.09862"/>
        <updated>2021-06-21T02:07:38.626Z</updated>
        <summary type="html"><![CDATA[Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly
used to visualize and quantify left atrial (LA) scars. The position and extent
of scars provide important information of the pathophysiology and progression
of atrial fibrillation (AF). Hence, LA scar segmentation and quantification
from LGE MRI can be useful in computer-assisted diagnosis and treatment
stratification of AF patients. Since manual delineation can be time-consuming
and subject to intra- and inter-expert variability, automating this computing
is highly desired, which nevertheless is still challenging and
under-researched.

This paper aims to provide a systematic review on computing methods for LA
cavity, wall, scar and ablation gap segmentation and quantification from LGE
MRI, and the related literature for AF studies. Specifically, we first
summarize AF-related imaging techniques, particularly LGE MRI. Then, we review
the methodologies of the four computing tasks in detail, and summarize the
validation strategies applied in each task. Finally, the possible future
developments are outlined, with a brief survey on the potential clinical
applications of the aforementioned methods. The review shows that the research
into this topic is still in early stages. Although several methods have been
proposed, especially for LA segmentation, there is still large scope for
further algorithmic developments due to performance issues related to the high
variability of enhancement appearance and differences in image acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1"&gt;Veronika A. Zimmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1"&gt;Xiahai Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay. (arXiv:2106.09835v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09835</id>
        <link href="http://arxiv.org/abs/2106.09835"/>
        <updated>2021-06-21T02:07:38.617Z</updated>
        <summary type="html"><![CDATA[This paper proposes two novel knowledge transfer techniques for
class-incremental learning (CIL). First, we propose data-free generative replay
(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples
from a generative model. In the conventional generative replay, the generative
model is pre-trained for old data and shared in extra memory for later
incremental learning. In our proposed DF-GR, we train a generative model from
scratch without using any training data, based on the pre-trained
classification model from the past, so we curtail the cost of sharing
pre-trained generative models. Second, we introduce dual-teacher information
distillation (DT-ID) for knowledge distillation from two teachers to one
student. In CIL, we use DT-ID to learn new classes incrementally based on the
pre-trained model for old classes and another model (pre-)trained on the new
data for new classes. We implemented the proposed schemes on top of one of the
state-of-the-art CIL methods and showed the performance improvement on
CIFAR-100 and ImageNet datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yoojin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Khamy_M/0/1/0/all/0/1"&gt;Mostafa El-Khamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungwon Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting. (arXiv:2106.10137v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10137</id>
        <link href="http://arxiv.org/abs/2106.10137"/>
        <updated>2021-06-21T02:07:38.609Z</updated>
        <summary type="html"><![CDATA[Instance-level contrastive learning techniques, which rely on data
augmentation and a contrastive loss function, have found great success in the
domain of visual representation learning. They are not suitable for exploiting
the rich dynamical structure of video however, as operations are done on many
augmented instances. In this paper we propose "Video Cross-Stream Prototypical
Contrasting", a novel method which predicts consistent prototype assignments
from both RGB and optical flow views, operating on sets of samples.
Specifically, we alternate the optimization process; while optimizing one of
the streams, all views are mapped to one set of stream prototype vectors. Each
of the assignments is predicted with all views except the one matching the
prediction, pushing representations closer to their assigned prototypes. As a
result, more efficient video embeddings with ingrained motion information are
learned, without the explicit need for optical flow computation during
inference. We obtain state-of-the-art results on nearest neighbour video
retrieval and action recognition, outperforming previous best by +3.2% on
UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and
+15.1% on HMDB51 using the R(2+1)D backbone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toering_M/0/1/0/all/0/1"&gt;Martine Toering&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gatopoulos_I/0/1/0/all/0/1"&gt;Ioannis Gatopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stol_M/0/1/0/all/0/1"&gt;Maarten Stol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1"&gt;Vincent Tao Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResDepth: Learned Residual Stereo Reconstruction. (arXiv:2001.08026v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.08026</id>
        <link href="http://arxiv.org/abs/2001.08026"/>
        <updated>2021-06-21T02:07:38.601Z</updated>
        <summary type="html"><![CDATA[We propose an embarrassingly simple but very effective scheme for
high-quality dense stereo reconstruction: (i) generate an approximate
reconstruction with your favourite stereo matcher; (ii) rewarp the input images
with that approximate model; (iii) with the initial reconstruction and the
warped images as input, train a deep network to enhance the reconstruction by
regressing a residual correction; and (iv) if desired, iterate the refinement
with the new, improved reconstruction. The strategy to only learn the residual
greatly simplifies the learning problem. A standard Unet without bells and
whistles is enough to reconstruct even small surface details, like dormers and
roof substructures in satellite images. We also investigate residual
reconstruction with less information and find that even a single image is
enough to greatly improve an approximate reconstruction. Our full model reduces
the mean absolute error of state-of-the-art stereo reconstruction systems by
>50%, both in our target domain of satellite stereo and on stereo pairs from
the ETH3D benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stucker_C/0/1/0/all/0/1"&gt;Corinne Stucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Error: a New Performance Measure for Adversarial Robustness. (arXiv:2106.10212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10212</id>
        <link href="http://arxiv.org/abs/2106.10212"/>
        <updated>2021-06-21T02:07:38.577Z</updated>
        <summary type="html"><![CDATA[Despite the significant advances in deep learning over the past decade, a
major challenge that limits the wide-spread adoption of deep learning has been
their fragility to adversarial attacks. This sensitivity to making erroneous
predictions in the presence of adversarially perturbed data makes deep neural
networks difficult to adopt for certain real-world, mission-critical
applications. While much of the research focus has revolved around adversarial
example creation and adversarial hardening, the area of performance measures
for assessing adversarial robustness is not well explored. Motivated by this,
this study presents the concept of residual error, a new performance measure
for not only assessing the adversarial robustness of a deep neural network at
the individual sample level, but also can be used to differentiate between
adversarial and non-adversarial examples to facilitate for adversarial example
detection. Furthermore, we introduce a hybrid model for approximating the
residual error in a tractable manner. Experimental results using the case of
image classification demonstrates the effectiveness and efficacy of the
proposed residual error metric for assessing several well-known deep neural
network architectures. These results thus illustrate that the proposed measure
could be a useful tool for not only assessing the robustness of deep neural
networks used in mission-critical scenarios, but also in the design of
adversarially robust models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1"&gt;Hossein Aboutalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1"&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karg_M/0/1/0/all/0/1"&gt;Michelle Karg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scharfenberger_C/0/1/0/all/0/1"&gt;Christian Scharfenberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10026</id>
        <link href="http://arxiv.org/abs/2106.10026"/>
        <updated>2021-06-21T02:07:38.549Z</updated>
        <summary type="html"><![CDATA[In this report, we describe the technical details of our submission to the
2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action
Recognition. Leveraging multiple modalities has been proved to benefit the
Unsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal
Mutual Enhancement Module (M3EM), a deep module for jointly considering
information from multiple modalities to find the most transferable
representations across domains. We achieve this by implementing two sub-modules
for enhancing each modality using the context of other modalities. The first
sub-module exchanges information across modalities through the semantic space,
while the second sub-module finds the most transferable spatial region based on
the consensus of all modalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lijin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1"&gt;Yusuke Sugano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1"&gt;Yoichi Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partition-Guided GANs. (arXiv:2104.00816v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00816</id>
        <link href="http://arxiv.org/abs/2104.00816"/>
        <updated>2021-06-21T02:07:38.540Z</updated>
        <summary type="html"><![CDATA[Despite the success of Generative Adversarial Networks (GANs), their training
suffers from several well-known problems, including mode collapse and
difficulties learning a disconnected set of manifolds. In this paper, we break
down the challenging task of learning complex high dimensional distributions,
supporting diverse data samples, to simpler sub-tasks. Our solution relies on
designing a partitioner that breaks the space into smaller regions, each having
a simpler distribution, and training a different generator for each partition.
This is done in an unsupervised manner without requiring any labels.

We formulate two desired criteria for the space partitioner that aid the
training of our mixture of generators: 1) to produce connected partitions and
2) provide a proxy of distance between partitions and data samples, along with
a direction for reducing that distance. These criteria are developed to avoid
producing samples from places with non-existent data density, and also
facilitate training by providing additional direction to the generators. We
develop theoretical constraints for a space partitioner to satisfy the above
criteria. Guided by our theoretical analysis, we design an effective neural
architecture for the space partitioner that empirically assures these
conditions. Experimental results on various standard benchmarks show that the
proposed unsupervised model outperforms several recent methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1"&gt;Mohammadreza Armandpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1"&gt;Ali Sadeghian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Granularity Network with Modal Attention for Dense Affective Understanding. (arXiv:2106.09964v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09964</id>
        <link href="http://arxiv.org/abs/2106.09964"/>
        <updated>2021-06-21T02:07:38.523Z</updated>
        <summary type="html"><![CDATA[Video affective understanding, which aims to predict the evoked expressions
by the video content, is desired for video creation and recommendation. In the
recent EEV challenge, a dense affective understanding task is proposed and
requires frame-level affective prediction. In this paper, we propose a
multi-granularity network with modal attention (MGN-MA), which employs
multi-granularity features for better description of the target frame.
Specifically, the multi-granularity features could be divided into frame-level,
clips-level and video-level features, which corresponds to visual-salient
content, semantic-context and video theme information. Then the modal attention
fusion module is designed to fuse the multi-granularity features and emphasize
more affection-relevant modals. Finally, the fused feature is fed into a
Mixtures Of Experts (MOE) classifier to predict the expressions. Further
employing model-ensemble post-processing, the proposed method achieves the
correlation score of 0.02292 in the EEV challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Baoming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1"&gt;Ke Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ban_C/0/1/0/all/0/1"&gt;Chao Ban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaobo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-06-21T02:07:38.505Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1"&gt;Md Mamunur Rahaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jiquan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yudong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space: a Semantic Perspective. (arXiv:2106.09872v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09872</id>
        <link href="http://arxiv.org/abs/2106.09872"/>
        <updated>2021-06-21T02:07:38.492Z</updated>
        <summary type="html"><![CDATA[The vulnerability of deep neural networks to adversarial examples, which are
crafted maliciously by modifying the inputs with imperceptible perturbations to
misled the network produce incorrect outputs, reveals the lack of robustness
and poses security concerns. Previous works study the adversarial robustness of
image classifiers on image level and use all the pixel information in an image
indiscriminately, lacking of exploration of regions with different semantic
meanings in the pixel space of an image. In this work, we fill this gap and
explore the pixel space of the adversarial image by proposing an algorithm to
looking for possible perturbations pixel by pixel in different regions of the
segmented image. The extensive experimental results on CIFAR-10 and ImageNet
verify that searching for the modified pixel in only some pixels of an image
can successfully launch the one-pixel adversarial attacks without requiring all
the pixels of the entire image, and there exist multiple vulnerable points
scattered in different regions of an image. We also demonstrate that the
adversarial robustness of different regions on the image varies with the amount
of semantic information contained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lina Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yawei Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xuemei Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIR: Self-supervised Image Rectification via Seeing the Same Scene from Multiple Different Lenses. (arXiv:2011.14611v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14611</id>
        <link href="http://arxiv.org/abs/2011.14611"/>
        <updated>2021-06-21T02:07:38.474Z</updated>
        <summary type="html"><![CDATA[Deep learning has demonstrated its power in image rectification by leveraging
the representation capacity of deep neural networks via supervised training
based on a large-scale synthetic dataset. However, the model may overfit the
synthetic images and generalize not well on real-world fisheye images due to
the limited universality of a specific distortion model and the lack of
explicitly modeling the distortion and rectification process. In this paper, we
propose a novel self-supervised image rectification (SIR) method based on an
important insight that the rectified results of distorted images of a same
scene from different lens should be the same. Specifically, we devise a new
network architecture with a shared encoder and several prediction heads, each
of which predicts the distortion parameter of a specific distortion model. We
further leverage a differentiable warping module to generate the rectified
images and re-distorted images from the distortion parameters and exploit the
intra- and inter-model consistency between them during training, thereby
leading to a self-supervised learning scheme without the need for ground-truth
distortion parameters or normal images. Experiments on synthetic dataset and
real-world fisheye images demonstrate that our method achieves comparable or
even better performance than the supervised baseline method and representative
state-of-the-art methods. Self-supervised learning also improves the
universality of distortion models while keeping their self-consistency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jinlong Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:38.455Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Expressions as a Vulnerability in Face Recognition. (arXiv:2011.08809v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08809</id>
        <link href="http://arxiv.org/abs/2011.08809"/>
        <updated>2021-06-21T02:07:38.440Z</updated>
        <summary type="html"><![CDATA[This work explores facial expression bias as a security vulnerability of face
recognition systems. Despite the great performance achieved by state-of-the-art
face recognition systems, the algorithms are still sensitive to a large range
of covariates. We present a comprehensive analysis of how facial expression
bias impacts the performance of face recognition technologies. Our study
analyzes: i) facial expression biases in the most popular face recognition
databases; and ii) the impact of facial expression in face recognition
performances. Our experimental framework includes two face detectors, three
face recognition models, and three different databases. Our results demonstrate
a huge facial expression bias in the most widely used databases, as well as a
related impact of face expression in the performance of state-of-the-art
algorithms. This work opens the door to new research lines focused on
mitigating the observed vulnerability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pena_A/0/1/0/all/0/1"&gt;Alejandro Pe&amp;#xf1;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1"&gt;Ignacio Serna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1"&gt;Agata Lapedriza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00549</id>
        <link href="http://arxiv.org/abs/2008.00549"/>
        <updated>2021-06-21T02:07:38.432Z</updated>
        <summary type="html"><![CDATA[Traffic near-crash events serve as critical data sources for various smart
transportation applications, such as being surrogate safety measures for
traffic safety research and corner case data for automated vehicle testing.
However, there are several key challenges for near-crash detection. First,
extracting near-crashes from original data sources requires significant
computing, communication, and storage resources. Also, existing methods lack
efficiency and transferability, which bottlenecks prospective large-scale
applications. To this end, this paper leverages the power of edge computing to
address these challenges by processing the video streams from existing dashcams
onboard in a real-time manner. We design a multi-thread system architecture
that operates on edge devices and model the bounding boxes generated by object
detection and tracking in linear complexity. The method is insensitive to
camera parameters and backward compatible with different vehicles. The edge
computing system has been evaluated with recorded videos and real-world tests
on two cars and four buses for over ten thousand hours. It filters out
irrelevant videos in real-time thereby saving labor cost, processing time,
network bandwidth, and data storage. It collects not only event videos but also
other valuable data such as road user type, event location, time to collision,
vehicle trajectory, vehicle speed, brake switch, and throttle. The experiments
demonstrate the promising performance of the system regarding efficiency,
accuracy, reliability, and transferability. It is among the first efforts in
applying edge computing for real-time traffic video analytics and is expected
to benefit multiple sub-fields in smart transportation research and
applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1"&gt;Ruimin Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1"&gt;Zhiyong Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Meixin Zhu&lt;/a&gt;, Hao (Frank) &lt;a href="http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1"&gt;Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yinhai Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embodied Language Grounding with 3D Visual Feature Representations. (arXiv:1910.01210v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.01210</id>
        <link href="http://arxiv.org/abs/1910.01210"/>
        <updated>2021-06-21T02:07:38.413Z</updated>
        <summary type="html"><![CDATA[We propose associating language utterances to 3D visual abstractions of the
scene they describe. The 3D visual abstractions are encoded as 3-dimensional
visual feature maps. We infer these 3D visual scene feature maps from RGB
images of the scene via view prediction: when the generated 3D scene feature
map is neurally projected from a camera viewpoint, it should match the
corresponding RGB image. We present generative models that condition on the
dependency tree of an utterance and generate a corresponding visual 3D feature
map as well as reason about its plausibility, and detector models that
condition on both the dependency tree of an utterance and a related image and
localize the object referents in the 3D feature map inferred from the image.
Our model outperforms models of language and vision that associate language
with 2D CNN activations or 2D images by a large margin in a variety of tasks,
such as, classifying plausibility of utterances, detecting referential
expressions, and supplying rewards for trajectory optimization of object
placement policies from language instructions. We perform numerous ablations
and show the improved performance of our detectors is due to its better
generalization across camera viewpoints and lack of object interferences in the
inferred 3D feature space, and the improved performance of our generators is
due to their ability to spatially reason about objects and their configurations
in 3D when mapping from language to scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Prabhudesai_M/0/1/0/all/0/1"&gt;Mihir Prabhudesai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tung_H/0/1/0/all/0/1"&gt;Hsiao-Yu Fish Tung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1"&gt;Syed Ashar Javed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sieb_M/0/1/0/all/0/1"&gt;Maximilian Sieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1"&gt;Adam W. Harley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1"&gt;Katerina Fragkiadaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar Camera Fusion via Representation Learning in Autonomous Driving. (arXiv:2103.07825v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07825</id>
        <link href="http://arxiv.org/abs/2103.07825"/>
        <updated>2021-06-21T02:07:38.394Z</updated>
        <summary type="html"><![CDATA[Radars and cameras are mature, cost-effective, and robust sensors and have
been widely used in the perception stack of mass-produced autonomous driving
systems. Due to their complementary properties, outputs from radar detection
(radar pins) and camera perception (2D bounding boxes) are usually fused to
generate the best perception results. The key to successful radar-camera fusion
is the accurate data association. The challenges in the radar-camera
association can be attributed to the complexity of driving scenes, the noisy
and sparse nature of radar measurements, and the depth ambiguity from 2D
bounding boxes. Traditional rule-based association methods are susceptible to
performance degradation in challenging scenarios and failure in corner cases.
In this study, we propose to address radar-camera association via deep
representation learning, to explore feature-level interaction and global
reasoning. Additionally, we design a loss sampling mechanism and an innovative
ordinal loss to overcome the difficulty of imperfect labeling and to enforce
critical human-like reasoning. Despite being trained with noisy labels
generated by a rule-based algorithm, our proposed method achieves a performance
of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,
this data-driven method also lends itself to continuous improvement via corner
case mining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Binnan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yunxiang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Langechuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences. (arXiv:2011.14579v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14579</id>
        <link href="http://arxiv.org/abs/2011.14579"/>
        <updated>2021-06-21T02:07:38.383Z</updated>
        <summary type="html"><![CDATA[3D Point cloud registration is still a very challenging topic due to the
difficulty in finding the rigid transformation between two point clouds with
partial correspondences, and it's even harder in the absence of any initial
estimation information. In this paper, we present an end-to-end deep-learning
based approach to resolve the point cloud registration problem. Firstly, the
revised LPD-Net is introduced to extract features and aggregate them with the
graph network. Secondly, the self-attention mechanism is utilized to enhance
the structure information in the point cloud and the cross-attention mechanism
is designed to enhance the corresponding information between the two input
point clouds. Based on which, the virtual corresponding points can be generated
by a soft pointer based method, and finally, the point cloud registration
problem can be solved by implementing the SVD method. Comparison results in
ModelNet40 dataset validate that the proposed approach reaches the
state-of-the-art in point cloud registration tasks and experiment resutls in
KITTI dataset validate the effectiveness of the proposed approach in real
applications.Our source code is available at
\url{https://github.com/qiaozhijian/VCR-Net.git}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Huanshu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suo_C/0/1/0/all/0/1"&gt;Chuanzhe Suo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10163</id>
        <link href="http://arxiv.org/abs/2106.10163"/>
        <updated>2021-06-21T02:07:38.376Z</updated>
        <summary type="html"><![CDATA[Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jenner_E/0/1/0/all/0/1"&gt;Erik Jenner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiler_M/0/1/0/all/0/1"&gt;Maurice Weiler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Clustering-friendly Representations: Subspace Clustering via Graph Filtering. (arXiv:2106.09874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09874</id>
        <link href="http://arxiv.org/abs/2106.09874"/>
        <updated>2021-06-21T02:07:38.368Z</updated>
        <summary type="html"><![CDATA[Finding a suitable data representation for a specific task has been shown to
be crucial in many applications. The success of subspace clustering depends on
the assumption that the data can be separated into different subspaces.
However, this simple assumption does not always hold since the raw data might
not be separable into subspaces. To recover the ``clustering-friendly''
representation and facilitate the subsequent clustering, we propose a graph
filtering approach by which a smooth representation is achieved. Specifically,
it injects graph similarity into data features by applying a low-pass filter to
extract useful data representations for clustering. Extensive experiments on
image and document clustering datasets demonstrate that our method improves
upon state-of-the-art subspace clustering techniques. Especially, its
comparable performance with deep learning methods emphasizes the effectiveness
of the simple graph filtering scheme for many real-world applications. An
ablation study shows that graph filtering can remove noise, preserve structure
in the image, and increase the separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1"&gt;Guangchun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1"&gt;Ling Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Graph based Trajectory Predictor with Pseudo Oracle. (arXiv:2002.00391v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00391</id>
        <link href="http://arxiv.org/abs/2002.00391"/>
        <updated>2021-06-21T02:07:38.351Z</updated>
        <summary type="html"><![CDATA[Pedestrian trajectory prediction in dynamic scenes remains a challenging and
critical problem in numerous applications, such as self-driving cars and
socially aware robots. Challenges concentrate on capturing pedestrians' motion
patterns and social interactions, as well as handling the future uncertainties.
Recent studies focus on modeling pedestrians' motion patterns with recurrent
neural networks, capturing social interactions with pooling-based or
graph-based methods, and handling future uncertainties by using random Gaussian
noise as the latent variable. However, they do not integrate specific obstacle
avoidance experience (OAE) that may improve prediction performance. For
example, pedestrians' future trajectories are always influenced by others in
front. Here we propose GTPPO (Graph-based Trajectory Predictor with Pseudo
Oracle), an encoder-decoder-based method conditioned on pedestrians' future
behaviors. Pedestrians' motion patterns are encoded with a long short-term
memory unit, which introduces the temporal attention to highlight specific time
steps. Their interactions are captured by a graph-based attention mechanism,
which draws OAE into the data-driven learning process of graph attention.
Future uncertainties are handled by generating multi-modal outputs with an
informative latent variable. Such a variable is generated by a novel pseudo
oracle predictor, which minimizes the knowledge gap between historical and
ground-truth trajectories. Finally, the GTPPO is evaluated on ETH, UCY and
Stanford Drone datasets, and the results demonstrate state-of-the-art
performance. Besides, the qualitative evaluations show successful cases of
handling sudden motion changes in the future. Such findings indicate that GTPPO
can peek into the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Biao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1"&gt;Guocheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1"&gt;Chingyao Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation. (arXiv:2103.04717v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04717</id>
        <link href="http://arxiv.org/abs/2103.04717"/>
        <updated>2021-06-21T02:07:38.343Z</updated>
        <summary type="html"><![CDATA[Multi-source unsupervised domain adaptation~(MSDA) aims at adapting models
trained on multiple labeled source domains to an unlabeled target domain. In
this paper, we propose a novel multi-source domain adaptation framework based
on collaborative learning for semantic segmentation. Firstly, a simple image
translation method is introduced to align the pixel value distribution to
reduce the gap between source domains and target domain to some extent. Then,
to fully exploit the essential semantic information across source domains, we
propose a collaborative learning method for domain adaptation without seeing
any data from target domain. In addition, similar to the setting of
unsupervised domain adaptation, unlabeled target domain data is leveraged to
further improve the performance of domain adaptation. This is achieved by
additionally constraining the outputs of multiple adaptation models with pseudo
labels online generated by an ensembled model. Extensive experiments and
ablation studies are conducted on the widely-used domain adaptation benchmark
datasets in semantic segmentation. Our proposed method achieves 59.0\% mIoU on
the validation set of Cityscapes by training on the labeled Synscapes and GTA5
datasets and unlabeled training set of Cityscapes. It significantly outperforms
all previous state-of-the-arts single-source and multi-source unsupervised
domain adaptation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianzhong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuaijun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Diverse-Structured Networks for Adversarial Robustness. (arXiv:2102.01886v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01886</id>
        <link href="http://arxiv.org/abs/2102.01886"/>
        <updated>2021-06-21T02:07:38.336Z</updated>
        <summary type="html"><![CDATA[In adversarial training (AT), the main focus has been the objective and
optimizer while the model has been less studied, so that the models being used
are still those classic ones in standard training (ST). Classic network
architectures (NAs) are generally worse than searched NAs in ST, which should
be the same in AT. In this paper, we argue that NA and AT cannot be handled
independently, since given a dataset, the optimal NA in ST would be no longer
optimal in AT. That being said, AT is time-consuming itself; if we directly
search NAs in AT over large search spaces, the computation will be practically
infeasible. Thus, we propose a diverse-structured network (DS-Net), to
significantly reduce the size of the search space: instead of low-level
operations, we only consider predefined atomic blocks, where an atomic block is
a time-tested building block like the residual block. There are only a few
atomic blocks and thus we can weight all atomic blocks rather than find the
best one in a searched block of DS-Net, which is an essential trade-off between
exploring diverse structures and exploiting the best structures. Empirical
results demonstrate the advantages of DS-Net, i.e., weighting the atomic
blocks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xuefeng Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CT Image Synthesis Using Weakly Supervised Segmentation and Geometric Inter-Label Relations For COVID Image Analysis. (arXiv:2106.10230v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10230</id>
        <link href="http://arxiv.org/abs/2106.10230"/>
        <updated>2021-06-21T02:07:38.328Z</updated>
        <summary type="html"><![CDATA[While medical image segmentation is an important task for computer aided
diagnosis, the high expertise requirement for pixelwise manual annotations
makes it a challenging and time consuming task. Since conventional data
augmentations do not fully represent the underlying distribution of the
training set, the trained models have varying performance when tested on images
captured from different sources. Most prior work on image synthesis for data
augmentation ignore the interleaved geometric relationship between different
anatomical labels. We propose improvements over previous GAN-based medical
image synthesis methods by learning the relationship between different
anatomical labels. We use a weakly supervised segmentation method to obtain
pixel level semantic label map of images which is used learn the intrinsic
relationship of geometry and shape across semantic labels. Latent space
variable sampling results in diverse generated images from a base image and
improves robustness. We use the synthetic images from our method to train
networks for segmenting COVID-19 infected areas from lung CT images. The
proposed method outperforms state-of-the-art segmentation methods on a public
dataset. Ablation studies also demonstrate benefits of integrating geometry and
diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1"&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1"&gt;Ankur Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Reuse and Fusion for Real-time Semantic segmentation. (arXiv:2105.12964v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12964</id>
        <link href="http://arxiv.org/abs/2105.12964"/>
        <updated>2021-06-21T02:07:38.308Z</updated>
        <summary type="html"><![CDATA[For real-time semantic segmentation, how to increase the speed while
maintaining high resolution is a problem that has been discussed and solved.
Backbone design and fusion design have always been two essential parts of
real-time semantic segmentation. We hope to design a light-weight network based
on previous design experience and reach the level of state-of-the-art real-time
semantic segmentation without any pre-training. To achieve this goal, a
encoder-decoder architectures are proposed to solve this problem by applying a
decoder network onto a backbone model designed for real-time segmentation tasks
and designed three different ways to fuse semantics and detailed information in
the aggregation phase. We have conducted extensive experiments on two semantic
segmentation benchmarks. Experiments on the Cityscapes and CamVid datasets show
that the proposed FRFNet strikes a balance between speed calculation and
accuracy. It achieves 72% Mean Intersection over Union (mIoU%) on the
Cityscapes test dataset with the speed of 144 on a single RTX 1080Ti card. The
Code is available at https://github.com/favoMJ/FRFNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sixiang_T/0/1/0/all/0/1"&gt;Tan Sixiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.09624</id>
        <link href="http://arxiv.org/abs/2105.09624"/>
        <updated>2021-06-21T02:07:38.300Z</updated>
        <summary type="html"><![CDATA[Photoacoustic imaging has the potential to revolutionise healthcare due to
the valuable information on tissue physiology that is contained in
multispectral photoacoustic measurements. Clinical translation of the
technology requires conversion of the high-dimensional acquired data into
clinically relevant and interpretable information. In this work, we present a
deep learning-based approach to semantic segmentation of multispectral
photoacoustic images to facilitate the interpretability of recorded images.
Manually annotated multispectral photoacoustic imaging data are used as gold
standard reference annotations and enable the training of a deep learning-based
segmentation algorithm in a supervised manner. Based on a validation study with
experimentally acquired data of healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualisations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a processing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Grohl_J/0/1/0/all/0/1"&gt;Janek Gr&amp;#xf6;hl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1"&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dreher_K/0/1/0/all/0/1"&gt;Kris Dreher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Holzwarth_N/0/1/0/all/0/1"&gt;Niklas Holzwarth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10197</id>
        <link href="http://arxiv.org/abs/2106.10197"/>
        <updated>2021-06-21T02:07:38.292Z</updated>
        <summary type="html"><![CDATA[Recently, autonomous vehicles and those equipped with an Advanced Driver
Assistance System (ADAS) are emerging. They share the road with regular ones
operated by human drivers entirely. To ensure guaranteed safety for passengers
and other road users, it becomes essential for autonomous vehicles and ADAS to
anticipate traffic accidents from natural driving scenes. The dynamic
spatial-temporal interaction of the traffic agents is complex, and visual cues
for predicting a future accident are embedded deeply in dashcam video data.
Therefore, early anticipation of traffic accidents remains a challenge. To this
end, the paper presents a dynamic spatial-temporal attention (DSTA) network for
early anticipation of traffic accidents from dashcam videos. The proposed
DSTA-network learns to select discriminative temporal segments of a video
sequence with a module named Dynamic Temporal Attention (DTA). It also learns
to focus on the informative spatial regions of frames with another module named
Dynamic Spatial Attention (DSA). The spatial-temporal relational features of
accidents, along with scene appearance features, are learned jointly with a
Gated Recurrent Unit (GRU) network. The experimental evaluation of the
DSTA-network on two benchmark datasets confirms that it has exceeded the
state-of-the-art performance. A thorough ablation study evaluates the
contributions of individual components of the DSTA-network, revealing how the
network achieves such performance. Furthermore, this paper proposes a new
strategy that fuses the prediction scores from two complementary models and
verifies its effectiveness in further boosting the performance of early
accident anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Muhammad Monjurul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruwen Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zhaozheng Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exponential Moving Average Normalization for Self-supervised and Semi-supervised Learning. (arXiv:2101.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08482</id>
        <link href="http://arxiv.org/abs/2101.08482"/>
        <updated>2021-06-21T02:07:38.285Z</updated>
        <summary type="html"><![CDATA[We present a plug-in replacement for batch normalization (BN) called
exponential moving average normalization (EMAN), which improves the performance
of existing student-teacher based self- and semi-supervised learning
techniques. Unlike the standard BN, where the statistics are computed within
each batch, EMAN, used in the teacher, updates its statistics by exponential
moving average from the BN statistics of the student. This design reduces the
intrinsic cross-sample dependency of BN and enhances the generalization of the
teacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2
points and semi-supervised learning by about 7/2 points, when 1%/10% supervised
labels are available on ImageNet. These improvements are consistent across
methods, network architectures, training duration, and datasets, demonstrating
the general effectiveness of this technique. The code is available at
https://github.com/amazon-research/exponential-moving-average-normalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhaowei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1"&gt;Charless Fowlkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Level Set Stereo for Cooperative Grouping with Occlusion. (arXiv:2006.16094v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16094</id>
        <link href="http://arxiv.org/abs/2006.16094"/>
        <updated>2021-06-21T02:07:38.277Z</updated>
        <summary type="html"><![CDATA[Localizing stereo boundaries is difficult because matching cues are absent in
the occluded regions that are adjacent to them. We introduce an energy and
level-set optimizer that improves boundaries by encoding the essential geometry
of occlusions: The spatial extent of an occlusion must equal the amplitude of
the disparity jump that causes it. In a collection of figure-ground scenes from
Middlebury and Falling Things stereo datasets, the model provides more accurate
boundaries than previous occlusion-handling techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jialiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1"&gt;Todd Zickler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discerning Generic Event Boundaries in Long-Form Wild Videos. (arXiv:2106.10090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10090</id>
        <link href="http://arxiv.org/abs/2106.10090"/>
        <updated>2021-06-21T02:07:38.258Z</updated>
        <summary type="html"><![CDATA[Detecting generic, taxonomy-free event boundaries invideos represents a major
stride forward towards holisticvideo understanding. In this paper we present a
technique forgeneric event boundary detection based on a two stream in-flated
3D convolutions architecture, which can learn spatio-temporal features from
videos. Our work is inspired from theGeneric Event Boundary Detection Challenge
(part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the
paper we provide an in-depth analysis ofthe experiments performed along with an
interpretation ofthe results obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1"&gt;Ayush K Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_T/0/1/0/all/0/1"&gt;Tarun Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dietlmeier_J/0/1/0/all/0/1"&gt;Julia Dietlmeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1"&gt;Alan F Smeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1"&gt;Noel E O&amp;#x27;Connor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Iterative Phase Retrieval With Cascaded Neural Networks. (arXiv:2106.10195v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10195</id>
        <link href="http://arxiv.org/abs/2106.10195"/>
        <updated>2021-06-21T02:07:38.251Z</updated>
        <summary type="html"><![CDATA[Fourier phase retrieval is the problem of reconstructing a signal given only
the magnitude of its Fourier transformation. Optimization-based approaches,
like the well-established Gerchberg-Saxton or the hybrid input output
algorithm, struggle at reconstructing images from magnitudes that are not
oversampled. This motivates the application of learned methods, which allow
reconstruction from non-oversampled magnitude measurements after a learning
phase. In this paper, we want to push the limits of these learned methods by
means of a deep neural network cascade that reconstructs the image successively
on different resolutions from its non-oversampled Fourier magnitude. We
evaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST,
and KMNIST) and demonstrate that it yields improved performance over other
non-iterative methods and optimization-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Uelwer_T/0/1/0/all/0/1"&gt;Tobias Uelwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hoffmann_T/0/1/0/all/0/1"&gt;Tobias Hoffmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harmeling_S/0/1/0/all/0/1"&gt;Stefan Harmeling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLSNet: Skin lesion segmentation using a lightweight generative adversarial network. (arXiv:1907.00856v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.00856</id>
        <link href="http://arxiv.org/abs/1907.00856"/>
        <updated>2021-06-21T02:07:38.240Z</updated>
        <summary type="html"><![CDATA[The determination of precise skin lesion boundaries in dermoscopic images
using automated methods faces many challenges, most importantly, the presence
of hair, inconspicuous lesion edges and low contrast in dermoscopic images, and
variability in the color, texture and shapes of skin lesions. Existing deep
learning-based skin lesion segmentation algorithms are expensive in terms of
computational time and memory. Consequently, running such segmentation
algorithms requires a powerful GPU and high bandwidth memory, which are not
available in dermoscopy devices. Thus, this article aims to achieve precise
skin lesion segmentation with minimum resources: a lightweight, efficient
generative adversarial network (GAN) model called SLSNet, which combines 1-D
kernel factorized networks, position and channel attention, and multiscale
aggregation mechanisms with a GAN model. The 1-D kernel factorized network
reduces the computational cost of 2D filtering. The position and channel
attention modules enhance the discriminative ability between the lesion and
non-lesion feature representations in spatial and channel dimensions,
respectively. A multiscale block is also used to aggregate the coarse-to-fine
features of input skin images and reduce the effect of the artifacts. SLSNet is
evaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018.
Although SLSNet has only 2.35 million parameters, the experimental results
demonstrate that it achieves segmentation results on a par with the
state-of-the-art skin lesion segmentation methods with an accuracy of 97.61%,
and Dice and Jaccard similarity coefficients of 90.63% and 81.98%,
respectively. SLSNet can run at more than 110 frames per second (FPS) in a
single GTX1080Ti GPU, which is faster than well-known deep learning-based image
segmentation models, such as FCN. Therefore, SLSNet can be used for practical
dermoscopic applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sarker_M/0/1/0/all/0/1"&gt;Md. Mostafa Kamal Sarker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rashwan_H/0/1/0/all/0/1"&gt;Hatem A. Rashwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akram_F/0/1/0/all/0/1"&gt;Farhan Akram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Singh_V/0/1/0/all/0/1"&gt;Vivek Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banu_S/0/1/0/all/0/1"&gt;Syeda Furruka Banu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_F/0/1/0/all/0/1"&gt;Forhad U H Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Choudhury_K/0/1/0/all/0/1"&gt;Kabir Ahmed Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chambon_S/0/1/0/all/0/1"&gt;Sylvie Chambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Radeva_P/0/1/0/all/0/1"&gt;Petia Radeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Puig_D/0/1/0/all/0/1"&gt;Domenec Puig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdel_Nasser_M/0/1/0/all/0/1"&gt;Mohamed Abdel-Nasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Generalization in Deep Learning Applications for Land Cover Mapping. (arXiv:2008.10351v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10351</id>
        <link href="http://arxiv.org/abs/2008.10351"/>
        <updated>2021-06-21T02:07:38.212Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that deep learning models can be used to classify
land-use data from geospatial satellite imagery. We show that when these deep
learning models are trained on data from specific continents/seasons, there is
a high degree of variability in model performance on out-of-sample
continents/seasons. This suggests that just because a model accurately predicts
land-use classes in one continent or season does not mean that the model will
accurately predict land-use classes in a different continent or season. We then
use clustering techniques on satellite imagery from different continents to
visualize the differences in landscapes that make geospatial generalization
particularly difficult, and summarize our takeaways for future satellite
imagery-related applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1"&gt;Lucas Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1"&gt;Caleb Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Pollution Reduction in Nighttime Photography. (arXiv:2106.10046v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10046</id>
        <link href="http://arxiv.org/abs/2106.10046"/>
        <updated>2021-06-21T02:07:38.197Z</updated>
        <summary type="html"><![CDATA[Nighttime photographers are often troubled by light pollution of unwanted
artificial lights. Artificial lights, after scattered by aerosols in the
atmosphere, can inundate the starlight and degrade the quality of nighttime
images, by reducing contrast and dynamic range and causing hazes. In this paper
we develop a physically-based light pollution reduction (LPR) algorithm that
can substantially alleviate the aforementioned degradations of perceptual
quality and restore the pristine state of night sky. The key to the success of
the proposed LPR algorithm is an inverse method to estimate the spatial
radiance distribution and spectral signature of ground artificial lights.
Extensive experiments are carried out to evaluate the efficacy and limitations
of the LPR algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaolin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning. (arXiv:2102.04960v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04960</id>
        <link href="http://arxiv.org/abs/2102.04960"/>
        <updated>2021-06-21T02:07:38.175Z</updated>
        <summary type="html"><![CDATA[Place recognition is critical for both offline mapping and online
localization. However, current single-sensor based place recognition still
remains challenging in adverse conditions. In this paper, a heterogeneous
measurements based framework is proposed for long-term place recognition, which
retrieves the query radar scans from the existing lidar maps. To achieve this,
a deep neural network is built with joint training in the learning stage, and
then in the testing stage, shared embeddings of radar and lidar are extracted
for heterogeneous place recognition. To validate the effectiveness of the
proposed method, we conduct tests and generalization experiments on the
multi-session public datasets compared to other competitive methods. The
experimental results indicate that our model is able to perform multiple place
recognitions: lidar-to-lidar, radar-to-radar and radar-to-lidar, while the
learned model is trained only once. We also release the source code publicly:
https://github.com/ZJUYH/radar-to-lidar-place-recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xuecheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Rong Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise2Sim -- Similarity-based Self-Learning for Image Denoising. (arXiv:2011.03384v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.03384</id>
        <link href="http://arxiv.org/abs/2011.03384"/>
        <updated>2021-06-21T02:07:38.167Z</updated>
        <summary type="html"><![CDATA[Despite its best performance in image denoising, the supervised deep
denoising methods require paired noise-clean data, which are often unavailable.
To address this challenge, Noise2Noise was designed based on the fact that
paired noise-clean images can be replaced by paired noise-noise images that are
easier to collect. However, in many scenarios the collection of paired
noise-noise images is still impractical. To bypass labeled images, Noise2Void
methods predict masked pixels from their surroundings with single noisy images
only and give improved denoising results that still need improvements. An
observation on classic denoising methods is that non-local mean (NLM) outcomes
are typically superior to locally denoised results. In contrast, Noise2Void and
its variants do not utilize self-similarities in an image as the NLM-based
methods do. Here we propose Noise2Sim, an NLM-inspired self-learning method for
image denoising. Specifically, Noise2Sim leverages the self-similarity of image
pixels to train the denoising network, requiring single noisy images only. Our
theoretical analysis shows that Noise2Sim tends to be equivalent to Noise2Noise
under mild conditions. To efficiently manage the computational burden for
globally searching similar pixels, we design a two-step procedure to provide
data for Noise2Sim training. Extensive experiments demonstrate the superiority
of Noise2Sim on common benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1"&gt;Fenglei Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1"&gt;Qing Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Routing Needed Between Capsules. (arXiv:2001.09136v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.09136</id>
        <link href="http://arxiv.org/abs/2001.09136"/>
        <updated>2021-06-21T02:07:38.160Z</updated>
        <summary type="html"><![CDATA[Most capsule network designs rely on traditional matrix multiplication
between capsule layers and computationally expensive routing mechanisms to deal
with the capsule dimensional entanglement that the matrix multiplication
introduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise
multiplication rather than matrix multiplication, the dimensions of the
capsules remain unentangled. In this work, we study HVCs as applied to the
highly structured MNIST dataset in order to produce a direct comparison to the
capsule research direction of Geoffrey Hinton, et al. In our study, we show
that a simple convolutional neural network using HVCs performs as well as the
prior best performing capsule network on MNIST using 5.5x fewer parameters, 4x
fewer training epochs, no reconstruction sub-network, and requiring no routing
mechanism. The addition of multiple classification branches to the network
establishes a new state of the art for the MNIST dataset with an accuracy of
99.87% for an ensemble of these models, as well as establishing a new state of
the art for a single model (99.83% accurate).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Byerly_A/0/1/0/all/0/1"&gt;Adam Byerly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalganova_T/0/1/0/all/0/1"&gt;Tatiana Kalganova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dear_I/0/1/0/all/0/1"&gt;Ian Dear&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training or Architecture? How to Incorporate Invariance in Neural Networks. (arXiv:2106.10044v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10044</id>
        <link href="http://arxiv.org/abs/2106.10044"/>
        <updated>2021-06-21T02:07:38.153Z</updated>
        <summary type="html"><![CDATA[Many applications require the robustness, or ideally the invariance, of a
neural network to certain transformations of input data. Most commonly, this
requirement is addressed by either augmenting the training data, using
adversarial training, or defining network architectures that include the
desired invariance automatically. Unfortunately, the latter often relies on the
ability to enlist all possible transformations, which make such approaches
largely infeasible for infinite sets of transformations, such as arbitrary
rotations or scaling. In this work, we propose a method for provably invariant
network architectures with respect to group actions by choosing one element
from a (possibly continuous) orbit based on a fixed criterion. In a nutshell,
we intend to 'undo' any possible transformation before feeding the data into
the actual network. We analyze properties of such approaches, extend them to
equivariant networks, and demonstrate their advantages in terms of robustness
as well as computational efficiency in several numerical examples. In
particular, we investigate the robustness with respect to rotations of images
(which can possibly hold up to discretization artifacts only) as well as the
provable rotational and scaling invariance of 3D point cloud classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1"&gt;Kanchana Vaishnavi Gandikota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1"&gt;Jonas Geiping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lahner_Z/0/1/0/all/0/1"&gt;Zorah L&amp;#xe4;hner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czaplinski_A/0/1/0/all/0/1"&gt;Adam Czapli&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Object Detection and User Intent via Query-Modulation. (arXiv:2106.10258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10258</id>
        <link href="http://arxiv.org/abs/2106.10258"/>
        <updated>2021-06-21T02:07:38.146Z</updated>
        <summary type="html"><![CDATA[When interacting with objects through cameras, or pictures, users often have
a specific intent. For example, they may want to perform a visual search.
However, most object detection models ignore the user intent, relying on image
pixels as their only input. This often leads to incorrect results, such as lack
of a high-confidence detection on the object of interest, or detection with a
wrong class label. In this paper we investigate techniques to modulate standard
object detectors to explicitly account for the user intent, expressed as an
embedding of a simple query. Compared to standard object detectors,
query-modulated detectors show superior performance at detecting objects for a
given label of interest. Thanks to large-scale training data synthesized from
standard object detection annotations, query-modulated detectors can also
outperform specialized referring expression recognition systems. Furthermore,
they can be simultaneously trained to solve for both query-modulated detection
and standard object detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fornoni_M/0/1/0/all/0/1"&gt;Marco Fornoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chaochao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1"&gt;Liangchen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_K/0/1/0/all/0/1"&gt;Kimberly Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stark_A/0/1/0/all/0/1"&gt;Alex Stark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yin Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1"&gt;Andrew Howard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10240</id>
        <link href="http://arxiv.org/abs/2106.10240"/>
        <updated>2021-06-21T02:07:38.126Z</updated>
        <summary type="html"><![CDATA[We present VSAC, a RANSAC-type robust estimator with a number of novelties.
It benefits from the introduction of the concept of independent inliers that
improves significantly the efficacy of the dominant plane handling and, also,
allows near error-free rejection of incorrect models, without false positives.
The local optimization process and its application is improved so that it is
run on average only once. Further technical improvements include adaptive
sequential hypothesis verification and efficient model estimation via Gaussian
elimination. Experiments on four standard datasets show that VSAC is
significantly faster than all its predecessors and runs on average in 1-2 ms,
on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,
the currently most accurate estimator of two-view geometry. In the repeated
runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1"&gt;Maksym Ivashechkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1"&gt;Daniel Barath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1"&gt;Jiri Matas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern Recognition. (arXiv:2106.10102v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10102</id>
        <link href="http://arxiv.org/abs/2106.10102"/>
        <updated>2021-06-21T02:07:38.111Z</updated>
        <summary type="html"><![CDATA[In this paper we present our preliminary work on model-based behavioral
analysis of horse motion. Our approach is based on the SMAL model, a 3D
articulated statistical model of animal shape. We define a novel SMAL model for
horses based on a new template, skeleton and shape space learned from $37$
horse toys. We test the accuracy of our hSMAL model in reconstructing a horse
from 3D mocap data and images. We apply the hSMAL model to the problem of
lameness detection from video, where we fit the model to images to recover 3D
pose and train an ST-GCN network on pose data. A comparison with the same
network trained on mocap points illustrates the benefit of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Ci Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1"&gt;Nima Ghorbani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1"&gt;Sofia Broom&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1"&gt;Maheen Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hernlund_E/0/1/0/all/0/1"&gt;Elin Hernlund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1"&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1"&gt;Silvia Zuffi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Coarse-to-Fine Instance Segmentation Network with Learning Boundary Representation. (arXiv:2106.10213v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10213</id>
        <link href="http://arxiv.org/abs/2106.10213"/>
        <updated>2021-06-21T02:07:38.104Z</updated>
        <summary type="html"><![CDATA[Boundary-based instance segmentation has drawn much attention since of its
attractive efficiency. However, existing methods suffer from the difficulty in
long-distance regression. In this paper, we propose a coarse-to-fine module to
address the problem. Approximate boundary points are generated at the coarse
stage and then features of these points are sampled and fed to a refined
regressor for fine prediction. It is end-to-end trainable since differential
sampling operation is well supported in the module. Furthermore, we design a
holistic boundary-aware branch and introduce instance-agnostic supervision to
assist regression. Equipped with ResNet-101, our approach achieves 31.7\% mask
AP on COCO dataset with single-scale training and testing, outperforming the
baseline 1.3\% mask AP with less than 1\% additional parameters and GFLOPs.
Experiments also show that our proposed method achieves competitive performance
compared to existing boundary-based methods with a lightweight design and a
simple pipeline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Bin-Bin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jiangpeng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiased Subjective Assessment of Real-World Image Enhancement. (arXiv:2106.10080v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.10080</id>
        <link href="http://arxiv.org/abs/2106.10080"/>
        <updated>2021-06-21T02:07:38.096Z</updated>
        <summary type="html"><![CDATA[In real-world image enhancement, it is often challenging (if not impossible)
to acquire ground-truth data, preventing the adoption of distance metrics for
objective quality assessment. As a result, one often resorts to subjective
quality assessment, the most straightforward and reliable means of evaluating
image enhancement. Conventional subjective testing requires manually
pre-selecting a small set of visual examples, which may suffer from three
sources of biases: 1) sampling bias due to the extremely sparse distribution of
the selected samples in the image space; 2) algorithmic bias due to potential
overfitting the selected samples; 3) subjective bias due to further potential
cherry-picking test results. This eventually makes the field of real-world
image enhancement more of an art than a science. Here we take steps towards
debiasing conventional subjective assessment by automatically sampling a set of
adaptive and diverse images for subsequent testing. This is achieved by casting
sample selection into a joint maximization of the discrepancy between the
enhancers and the diversity among the selected input images. Careful visual
inspection on the resulting enhanced images provides a debiased ranking of the
enhancement algorithms. We demonstrate our subjective assessment method using
three popular and practically demanding image enhancement tasks: dehazing,
super-resolution, and low-light enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhangyang_C/0/1/0/all/0/1"&gt;Cao Peibei. Wang Zhangyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kede_M/0/1/0/all/0/1"&gt;Ma Kede&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping. (arXiv:2106.09965v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09965</id>
        <link href="http://arxiv.org/abs/2106.09965"/>
        <updated>2021-06-21T02:07:38.088Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a high fidelity face swapping method, called
HifiFace, which can well preserve the face shape of the source face and
generate photo-realistic results. Unlike other existing face swapping works
that only use face recognition model to keep the identity similarity, we
propose 3D shape-aware identity to control the face shape with the geometric
supervision from 3DMM and 3D face reconstruction method. Meanwhile, we
introduce the Semantic Facial Fusion module to optimize the combination of
encoder and decoder features and make adaptive blending, which makes the
results more photo-realistic. Extensive experiments on faces in the wild
demonstrate that our method can preserve better identity, especially on the
face shape, and can generate more photo-realistic results than previous
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Junwei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1"&gt;Wenqing Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation, and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic Lanes. (arXiv:2106.09932v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09932</id>
        <link href="http://arxiv.org/abs/2106.09932"/>
        <updated>2021-06-21T02:07:38.068Z</updated>
        <summary type="html"><![CDATA[In a previous study, we presented VT-Lane, a three-step framework for
real-time vehicle detection, tracking, and turn movement classification at
urban intersections. In this study, we present a case study incorporating the
highly accurate trajectories and movement classification obtained via VT-Lane
for the purpose of speed estimation and driver behavior calibration for traffic
at urban intersections. First, we use a highly instrumented vehicle to verify
the estimated speeds obtained from video inference. The results of the speed
validation show that our method can estimate the average travel speed of
detected vehicles in real-time with an error of 0.19 m/sec, which is equivalent
to 2% of the average observed travel speeds in the intersection of the study.
Instantaneous speeds (at the resolution of 30 Hz) were found to be estimated
with an average error of 0.21 m/sec and 0.86 m/sec respectively for
free-flowing and congested traffic conditions. We then use the estimated speeds
to calibrate the parameters of a driver behavior model for the vehicles in the
area of study. The results show that the calibrated model replicates the
driving behavior with an average error of 0.45 m/sec, indicating the high
potential for using this framework for automated, large-scale calibration of
car-following models from roadside traffic video data, which can lead to
substantial improvements in traffic modeling via microscopic simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdelhalim_A/0/1/0/all/0/1"&gt;Awad Abdelhalim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1"&gt;Montasir Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotha_B/0/1/0/all/0/1"&gt;Bhavi Bharat Kotha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wicks_A/0/1/0/all/0/1"&gt;Alfred Wicks&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Fault Detection in Industrial Welding Processes with Deep Learning and Data Augmentation. (arXiv:2106.10160v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10160</id>
        <link href="http://arxiv.org/abs/2106.10160"/>
        <updated>2021-06-21T02:07:38.061Z</updated>
        <summary type="html"><![CDATA[With the rise of deep learning models in the field of computer vision, new
possibilities for their application in industrial processes proves to return
great benefits. Nevertheless, the actual fit of machine learning for highly
standardised industrial processes is still under debate. This paper addresses
the challenges on the industrial realization of the AI tools, considering the
use case of Laser Beam Welding quality control as an example. We use object
detection algorithms from the TensorFlow object detection API and adapt them to
our use case using transfer learning. The baseline models we develop are used
as benchmarks and evaluated and compared to models that undergo dataset scaling
and hyperparameter tuning. We find that moderate scaling of the dataset via
image augmentation leads to improvements in intersection over union (IoU) and
recall, whereas high levels of augmentation and scaling may lead to
deterioration of results. Finally, we put our results into perspective of the
underlying use case and evaluate their fit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1"&gt;Jibinraj Antony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlather_D/0/1/0/all/0/1"&gt;Dr. Florian Schlather&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Safronov_G/0/1/0/all/0/1"&gt;Georgij Safronov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitz_M/0/1/0/all/0/1"&gt;Markus Schmitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laerhoven_P/0/1/0/all/0/1"&gt;Prof. Dr. Kristof Van Laerhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolving GANs: When Contradictions Turn into Compliance. (arXiv:2106.09946v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09946</id>
        <link href="http://arxiv.org/abs/2106.09946"/>
        <updated>2021-06-21T02:07:38.054Z</updated>
        <summary type="html"><![CDATA[Limited availability of labeled-data makes any supervised learning problem
challenging. Alternative learning settings like semi-supervised and universum
learning alleviate the dependency on labeled data, but still require a large
amount of unlabeled data, which may be unavailable or expensive to acquire.
GAN-based synthetic data generation methods have recently shown promise by
generating synthetic samples to improve task at hand. However, these samples
cannot be used for other purposes. In this paper, we propose a GAN game which
provides improved discriminator accuracy under limited data settings, while
generating realistic synthetic data. This provides the added advantage that now
the generated data can be used for other similar tasks. We provide the
theoretical guarantees and empirical results in support of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhar_S/0/1/0/all/0/1"&gt;Sauptik Dhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heydari_J/0/1/0/all/0/1"&gt;Javad Heydari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Samarth Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurup_U/0/1/0/all/0/1"&gt;Unmesh Kurup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mohak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards interpreting computer vision based on transformation invariant optimization. (arXiv:2106.09982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09982</id>
        <link href="http://arxiv.org/abs/2106.09982"/>
        <updated>2021-06-21T02:07:38.047Z</updated>
        <summary type="html"><![CDATA[Interpreting how does deep neural networks (DNNs) make predictions is a vital
field in artificial intelligence, which hinders wide applications of DNNs.
Visualization of learned representations helps we humans understand the vision
of DNNs. In this work, visualized images that can activate the neural network
to the target classes are generated by back-propagation method. Here, rotation
and scaling operations are applied to introduce the transformation invariance
in the image generating process, which we find a significant improvement on
visualization effect. Finally, we show some cases that such method can help us
to gain insight into neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jinzhe Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tonghuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yaqian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Dongdong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;RenGang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[World-GAN: a Generative Model for Minecraft Worlds. (arXiv:2106.10155v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10155</id>
        <link href="http://arxiv.org/abs/2106.10155"/>
        <updated>2021-06-21T02:07:38.024Z</updated>
        <summary type="html"><![CDATA[This work introduces World-GAN, the first method to perform data-driven
Procedural Content Generation via Machine Learning in Minecraft from a single
example. Based on a 3D Generative Adversarial Network (GAN) architecture, we
are able to create arbitrarily sized world snippets from a given sample. We
evaluate our approach on creations from the community as well as structures
generated with the Minecraft World Generator. Our method is motivated by the
dense representations used in Natural Language Processing (NLP) introduced with
word2vec [1]. The proposed block2vec representations make World-GAN independent
from the number of different blocks, which can vary a lot in Minecraft, and
enable the generation of larger levels. Finally, we demonstrate that changing
this new representation space allows us to change the generated style of an
already trained generator. World-GAN enables its users to generate Minecraft
worlds based on parts of their creations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awiszus_M/0/1/0/all/0/1"&gt;Maren Awiszus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1"&gt;Frederik Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Distraction-Robust Active Visual Tracking. (arXiv:2106.10110v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10110</id>
        <link href="http://arxiv.org/abs/2106.10110"/>
        <updated>2021-06-21T02:07:38.000Z</updated>
        <summary type="html"><![CDATA[In active visual tracking, it is notoriously difficult when distracting
objects appear, as distractors often mislead the tracker by occluding the
target or bringing a confusing appearance. To address this issue, we propose a
mixed cooperative-competitive multi-agent game, where a target and multiple
distractors form a collaborative team to play against a tracker and make it
fail to follow. Through learning in our game, diverse distracting behaviors of
the distractors naturally emerge, thereby exposing the tracker's weakness,
which helps enhance the distraction-robustness of the tracker. For effective
learning, we then present a bunch of practical methods, including a reward
function for distractors, a cross-modal teacher-student learning strategy, and
a recurrent attention mechanism for the tracker. The experimental results show
that our tracker performs desired distraction-robust active visual tracking and
can be well generalized to unseen environments. We also show that the
multi-agent game can be used to adversarially test the robustness of trackers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1"&gt;Fangwei Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1"&gt;Peng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wenhan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1"&gt;Tingyun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object Detection. (arXiv:2106.10013v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10013</id>
        <link href="http://arxiv.org/abs/2106.10013"/>
        <updated>2021-06-21T02:07:37.993Z</updated>
        <summary type="html"><![CDATA[Pseudo-LiDAR based 3D object detectors have gained popularity due to their
high accuracy. However, these methods need dense depth supervision and suffer
from inferior speed. To solve these two issues, a recently introduced RTS3D
builds an efficient 4D Feature-Consistency Embedding (FCE) space for the
intermediate representation of object without depth supervision. FCE space
splits the entire object region into 3D uniform grid latent space for feature
sampling point generation, which ignores the importance of different object
regions. However, we argue that, compared with the inner region, the outer
region plays a more important role for accurate 3D detection. To encode more
information from the outer region, we propose a shape prior non-uniform
sampling strategy that performs dense sampling in outer region and sparse
sampling in inner region. As a result, more points are sampled from the outer
region and more useful features are extracted for 3D detection. Further, to
enhance the feature discrimination of each sampling point, we propose a
high-level semantic enhanced FCE module to exploit more contextual information
and suppress noise better. Experiments on the KITTI dataset are performed to
show the effectiveness of the proposed method. Compared with the baseline
RTS3D, our proposed method has 2.57% improvement on AP3d almost without extra
network parameters. Moreover, our proposed method outperforms the
state-of-the-art methods without extra supervision at a real-time speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1"&gt;A. Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;J. Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1"&gt;Y. Pang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advanced Hough-based method for on-device document localization. (arXiv:2106.09987v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09987</id>
        <link href="http://arxiv.org/abs/2106.09987"/>
        <updated>2021-06-21T02:07:37.973Z</updated>
        <summary type="html"><![CDATA[The demand for on-device document recognition systems increases in
conjunction with the emergence of more strict privacy and security
requirements. In such systems, there is no data transfer from the end device to
a third-party information processing servers. The response time is vital to the
user experience of on-device document recognition. Combined with the
unavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on
consumer-grade end devices such as smartphones, the time limitations put
significant constraints on the computational complexity of the applied
algorithms for on-device execution.

In this work, we consider document location in an image without prior
knowledge of the document content or its internal structure. In accordance with
the published works, at least 5 systems offer solutions for on-device document
location. All these systems use a location method which can be considered
Hough-based. The precision of such systems seems to be lower than that of the
state-of-the-art solutions which were not designed to account for the limited
computational resources.

We propose an advanced Hough-based method. In contrast with other approaches,
it accounts for the geometric invariants of the central projection model and
combines both edge and color features for document boundary detection. The
proposed method allowed for the second best result for SmartDoc dataset in
terms of precision, surpassed by U-net like neural network. When evaluated on a
more challenging MIDV-500 dataset, the proposed algorithm guaranteed the best
precision compared to published methods. Our method retained the applicability
to on-device computations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1"&gt;D.V. Tropin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ershov_A/0/1/0/all/0/1"&gt;A.M. Ershov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1"&gt;D.P. Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1"&gt;V.V. Arlazarov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariance-bridged SO(2)-Invariant Representation Learning using Graph Convolutional Network. (arXiv:2106.09996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09996</id>
        <link href="http://arxiv.org/abs/2106.09996"/>
        <updated>2021-06-21T02:07:37.966Z</updated>
        <summary type="html"><![CDATA[Training a Convolutional Neural Network (CNN) to be robust against rotation
has mostly been done with data augmentation. In this paper, another progressive
vision of research direction is highlighted to encourage less dependence on
data augmentation by achieving structural rotational invariance of a network.
The deep equivariance-bridged SO(2) invariant network is proposed to echo such
vision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network
(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the
graph representation of an image to acquire rotationally equivariant
representation, as GCN is more suitable for constructing deeper network than
spectral graph convolution-based approaches. Then, invariant representation is
eventually obtained with Global Average Pooling (GAP), a permutation-invariant
operation suitable for aggregating high-dimensional representations, over the
equivariant set of vertices retrieved from SWN-GCN. Our method achieves the
state-of-the-art image classification performance on rotated MNIST and CIFAR-10
images, where the models are trained with a non-augmented dataset only.
Quantitative validations over invariance of the representations also
demonstrate strong invariance of deep representations of SWN-GCN over
rotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sungwon Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1"&gt;Hyungtae Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1"&gt;Hyun Myung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention. (arXiv:2106.09914v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09914</id>
        <link href="http://arxiv.org/abs/2106.09914"/>
        <updated>2021-06-21T02:07:37.958Z</updated>
        <summary type="html"><![CDATA[We propose a novel GAN training scheme that can handle any level of labeling
in a unified manner. Our scheme introduces a form of artificial labeling that
can incorporate manually defined labels, when available, and induce an
alignment between them. To define the artificial labels, we exploit the
assumption that neural network generators can be trained more easily to map
nearby latent vectors to data with semantic similarities, than across separate
categories. We use generated data samples and their corresponding artificial
conditioning labels to train a classifier. The classifier is then used to
self-label real data. To boost the accuracy of the self-labeling, we also use
the exponential moving average of the classifier. However, because the
classifier might still make mistakes, especially at the beginning of the
training, we also refine the labels through self-attention, by using the
labeling of real data samples only when the classifier outputs a high
classification probability score. We evaluate our approach on CIFAR-10, STL-10
and SVHN, and show that both self-labeling and self-attention consistently
improve the quality of generated data. More surprisingly, we find that the
proposed scheme can even outperform class-conditional GANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Tomoki Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1"&gt;Paolo Favaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Radar Localization on Lidar Maps Using Shared Embedding. (arXiv:2106.10000v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.10000</id>
        <link href="http://arxiv.org/abs/2106.10000"/>
        <updated>2021-06-21T02:07:37.939Z</updated>
        <summary type="html"><![CDATA[We present a heterogeneous localization framework for solving radar global
localization and pose tracking on pre-built lidar maps. To bridge the gap of
sensing modalities, deep neural networks are constructed to create shared
embedding space for radar scans and lidar maps. Herein learned feature
embeddings are supportive for similarity measurement, thus improving map
retrieval and data matching respectively. In RobotCar and MulRan datasets, we
demonstrate the effectiveness of the proposed framework with the comparison to
Scan Context and RaLL. In addition, the proposed pose tracking pipeline is with
less neural networks compared to the original RaLL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Huan Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1"&gt;Rong Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combined Person Classification with Airborne Optical Sectioning. (arXiv:2106.10077v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.10077</id>
        <link href="http://arxiv.org/abs/2106.10077"/>
        <updated>2021-06-21T02:07:37.932Z</updated>
        <summary type="html"><![CDATA[Fully autonomous drones have been demonstrated to find lost or injured
persons under strongly occluding forest canopy. Airborne Optical Sectioning
(AOS), a novel synthetic aperture imaging technique, together with
deep-learning-based classification enables high detection rates under realistic
search-and-rescue conditions. We demonstrate that false detections can be
significantly suppressed and true detections boosted by combining
classifications from multiple AOS rather than single integral images. This
improves classification rates especially in the presence of occlusion. To make
this possible, we modified the AOS imaging process to support large overlaps
between subsequent integrals, enabling real-time and on-board scanning and
processing of groundspeeds up to 10 m/s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1"&gt;Indrajit Kurmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_D/0/1/0/all/0/1"&gt;David C. Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1"&gt;Oliver Bimber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accumulative Poisoning Attacks on Real-time Data. (arXiv:2106.09993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09993</id>
        <link href="http://arxiv.org/abs/2106.09993"/>
        <updated>2021-06-21T02:07:37.919Z</updated>
        <summary type="html"><![CDATA[Collecting training data from untrusted sources exposes machine learning
services to poisoning adversaries, who maliciously manipulate training data to
degrade the model accuracy. When trained on offline datasets, poisoning
adversaries have to inject the poisoned data in advance before training, and
the order of feeding these poisoned batches into the model is stochastic. In
contrast, practical systems are more usually trained/fine-tuned on sequentially
captured real-time data, in which case poisoning adversaries could dynamically
poison each data batch according to the current model state. In this paper, we
focus on the real-time settings and propose a new attacking strategy, which
affiliates an accumulative phase with poisoning attacks to secretly (i.e.,
without affecting accuracy) magnify the destructive effect of a (poisoned)
trigger batch. By mimicking online learning and federated learning on CIFAR-10,
we show that the model accuracy will significantly drop by a single update step
on the trigger batch after the accumulative phase. Our work validates that a
well-designed but straightforward attacking strategy can dramatically amplify
the poisoning effects, with no need to explore complex techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples. (arXiv:2106.09947v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09947</id>
        <link href="http://arxiv.org/abs/2106.09947"/>
        <updated>2021-06-21T02:07:37.888Z</updated>
        <summary type="html"><![CDATA[Evaluating robustness of machine-learning models to adversarial examples is a
challenging problem. Many defenses have been shown to provide a false sense of
security by causing gradient-based attacks to fail, and they have been broken
under more rigorous evaluations. Although guidelines and best practices have
been suggested to improve current adversarial robustness evaluations, the lack
of automatic testing and debugging tools makes it difficult to apply these
recommendations in a systematic manner. In this work, we overcome these
limitations by (i) defining a set of quantitative indicators which unveil
common failures in the optimization of gradient-based attacks, and (ii)
proposing specific mitigation strategies within a systematic evaluation
protocol. Our extensive experimental analysis shows that the proposed
indicators of failure can be used to visualize, debug and improve current
adversarial robustness evaluations, providing a first concrete step towards
automatizing and systematizing current adversarial robustness evaluations. Our
open-source code is available at:
https://github.com/pralab/IndicatorsOfAttackFailure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1"&gt;Luca Demetrio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotgiu_A/0/1/0/all/0/1"&gt;Angelo Sotgiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manca_G/0/1/0/all/0/1"&gt;Giovanni Manca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1"&gt;Ambra Demontis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1"&gt;Nicholas Carlini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Lies: Optical Adversarial Attack. (arXiv:2106.09908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09908</id>
        <link href="http://arxiv.org/abs/2106.09908"/>
        <updated>2021-06-21T02:07:37.880Z</updated>
        <summary type="html"><![CDATA[A significant amount of work has been done on adversarial attacks that inject
imperceptible noise to images to deteriorate the image classification
performance of deep models. However, most of the existing studies consider
attacks in the digital (pixel) domain where an image acquired by an image
sensor with sampling and quantization has been recorded. This paper, for the
first time, introduces an optical adversarial attack, which physically alters
the light field information arriving at the image sensor so that the
classification model yields misclassification. More specifically, we modulate
the phase of the light in the Fourier domain using a spatial light modulator
placed in the photographic system. The operative parameters of the modulator
are obtained by gradient-based optimization to maximize cross-entropy and
minimize distortions. We present experiments based on both simulation and a
real hardware optical system, from which the feasibility of the proposed
optical attack is demonstrated. It is also verified that the proposed attack is
completely different from common optical-domain distortions such as spherical
aberration, defocus, and astigmatism in terms of both perturbation patterns and
classification results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyu-Lim Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jeong-Soo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Seung-Ri Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jun-Ho Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_C/0/1/0/all/0/1"&gt;Chul-Min Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jong-Seok Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Matting: A New Perspective on Medical Segmentation with Uncertainty. (arXiv:2106.09887v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09887</id>
        <link href="http://arxiv.org/abs/2106.09887"/>
        <updated>2021-06-21T02:07:37.814Z</updated>
        <summary type="html"><![CDATA[In medical image segmentation, it is difficult to mark ambiguous areas
accurately with binary masks, especially when dealing with small lesions.
Therefore, it is a challenge for radiologists to reach a consensus by using
binary masks under the condition of multiple annotations. However, these areas
may contain anatomical structures that are conducive to diagnosis. Uncertainty
is introduced to study these situations. Nevertheless, the uncertainty is
usually measured by the variances between predictions in a multiple trial way.
It is not intuitive, and there is no exact correspondence in the image.
Inspired by image matting, we introduce matting as a soft segmentation method
and a new perspective to deal with and represent uncertain regions into medical
scenes, namely medical matting. More specifically, because there is no
available medical matting dataset, we first labeled two medical datasets with
alpha matte. Secondly, the matting method applied to the natural image is not
suitable for the medical scene, so we propose a new architecture to generate
binary masks and alpha matte in a row. Thirdly, the uncertainty map is
introduced to highlight the ambiguous regions from the binary results and
improve the matting performance. Evaluated on these datasets, the proposed
model outperformed state-of-the-art matting algorithms by a large margin, and
alpha matte is proved to be a more efficient labeling form than a binary mask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lie Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1"&gt;Wanji He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yelin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhiwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1"&gt;Xuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiufen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python. (arXiv:2106.09756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09756</id>
        <link href="http://arxiv.org/abs/2106.09756"/>
        <updated>2021-06-21T02:07:37.799Z</updated>
        <summary type="html"><![CDATA[Machine learning is a general-purpose technology holding promises for many
interdisciplinary research problems. However, significant barriers exist in
crossing disciplinary boundaries when most machine learning tools are developed
in different areas separately. We present Pykale - a Python library for
knowledge-aware machine learning on graphs, images, texts, and videos to enable
and accelerate interdisciplinary research. We formulate new green machine
learning guidelines based on standard software engineering practices and
propose a novel pipeline-based application programming interface (API). PyKale
focuses on leveraging knowledge from multiple sources for accurate and
interpretable prediction, thus supporting multimodal learning and transfer
learning (particularly domain adaptation) with latest deep learning and
dimensionality reduction models. We build PyKale on PyTorch and leverage the
rich PyTorch ecosystem. Our pipeline-based API design enforces standardization
and minimalism, embracing green machine learning concepts via reducing
repetitions and redundancy, reusing existing resources, and recycling learning
models across areas. We demonstrate its interdisciplinary nature via examples
in bioinformatics, knowledge graph, image/video recognition, and medical
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Haiping Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1"&gt;Robert Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_P/0/1/0/all/0/1"&gt;Peizhen Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1"&gt;Raivo E Koot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuo Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chasmai_M/0/1/0/all/0/1"&gt;Mustafa Chasmai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schobs_L/0/1/0/all/0/1"&gt;Lawrence Schobs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep reinforcement learning with automated label extraction from clinical reports accurately classifies 3D MRI brain volumes. (arXiv:2106.09812v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09812</id>
        <link href="http://arxiv.org/abs/2106.09812"/>
        <updated>2021-06-21T02:07:37.792Z</updated>
        <summary type="html"><![CDATA[Purpose: Image classification is perhaps the most fundamental task in imaging
AI. However, labeling images is time-consuming and tedious. We have recently
demonstrated that reinforcement learning (RL) can classify 2D slices of MRI
brain images with high accuracy. Here we make two important steps toward
speeding image classification: Firstly, we automatically extract class labels
from the clinical reports. Secondly, we extend our prior 2D classification work
to fully 3D image volumes from our institution. Hence, we proceed as follows:
in Part 1, we extract labels from reports automatically using the SBERT natural
language processing approach. Then, in Part 2, we use these labels with RL to
train a classification Deep-Q Network (DQN) for 3D image volumes.

Methods: For Part 1, we trained SBERT with 90 radiology report impressions.
We then used the trained SBERT to predict class labels for use in Part 2. In
Part 2, we applied multi-step image classification to allow for combined Deep-Q
learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90
images. We tested on a separate set of 61 images, again using the classes
predicted from patient reports by the trained SBERT in Part 1. For comparison,
we also trained and tested a supervised deep learning classification network on
the same set of training and testing images using the same labels.

Results: Part 1: Upon training with the corpus of radiology reports, the
SBERT model had 100% accuracy for both normal and metastasis-containing scans.
Part 2: Then, using these labels, whereas the supervised approach quickly
overfit the training data and as expected performed poorly on the testing set
(66% accuracy, just over random guessing), the reinforcement learning approach
achieved an accuracy of 92%. The results were found to be statistically
significant, with a p-value of 3.1 x 10^-5.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1"&gt;Joseph Stember&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantized Neural Networks via {-1, +1} Encoding Decomposition and Acceleration. (arXiv:2106.09886v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09886</id>
        <link href="http://arxiv.org/abs/2106.09886"/>
        <updated>2021-06-21T02:07:37.784Z</updated>
        <summary type="html"><![CDATA[The training of deep neural networks (DNNs) always requires intensive
resources for both computation and data storage. Thus, DNNs cannot be
efficiently applied to mobile phones and embedded devices, which severely
limits their applicability in industrial applications. To address this issue,
we propose a novel encoding scheme using {-1, +1} to decompose quantized neural
networks (QNNs) into multi-branch binary networks, which can be efficiently
implemented by bitwise operations (i.e., xnor and bitcount) to achieve model
compression, computational acceleration, and resource saving. By using our
method, users can achieve different encoding precisions arbitrarily according
to their requirements and hardware resources. The proposed mechanism is highly
suitable for the use of FPGA and ASIC in terms of data storage and computation,
which provides a feasible idea for smart chips. We validate the effectiveness
of our method on large-scale image classification (e.g., ImageNet), object
detection, and semantic segmentation tasks. In particular, our method with
low-bit encoding can still achieve almost the same performance as its high-bit
counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1"&gt;Qigong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiufang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1"&gt;Fanhua Shang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongying Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Model Sparsification by Scheduled Grow-and-Prune Methods. (arXiv:2106.09857v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09857</id>
        <link href="http://arxiv.org/abs/2106.09857"/>
        <updated>2021-06-21T02:07:37.777Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) are effective in solving many real-world
problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but
their excessive computation results in long training and inference time. Model
sparsification can reduce the computation and memory cost while maintaining
model quality. Most existing sparsification algorithms unidirectionally remove
weights, while others randomly or greedily explore a small subset of weights in
each layer. The inefficiency of the algorithms reduces the achievable sparsity
level. In addition, many algorithms still require pre-trained dense models and
thus suffer from large memory footprint and long training time. In this paper,
we propose a novel scheduled grow-and-prune (GaP) methodology without
pre-training the dense models. It addresses the shortcomings of the previous
works by repeatedly growing a subset of layers to dense and then pruning back
to sparse after some training. Experiments have shown that such models can
match or beat the quality of highly optimized dense models at 80% sparsity on a
variety of tasks, such as image classification, objective detection, 3D object
part segmentation, and translation. They also outperform other state-of-the-art
(SOTA) pruning methods, including pruning from pre-trained dense models. As an
example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy
on ImageNet, improving the SOTA results by 1.5%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaolong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1"&gt;Minghai Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1"&gt;Fei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1"&gt;Zejiang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Kuang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuan Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance-based Separability Measure for Internal Cluster Validation. (arXiv:2106.09794v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09794</id>
        <link href="http://arxiv.org/abs/2106.09794"/>
        <updated>2021-06-21T02:07:37.759Z</updated>
        <summary type="html"><![CDATA[To evaluate clustering results is a significant part of cluster analysis.
Since there are no true class labels for clustering in typical unsupervised
learning, many internal cluster validity indices (CVIs), which use predicted
labels and data, have been created. Without true labels, to design an effective
CVI is as difficult as to create a clustering method. And it is crucial to have
more CVIs because there are no universal CVIs that can be used to measure all
datasets and no specific methods of selecting a proper CVI for clusters without
true labels. Therefore, to apply a variety of CVIs to evaluate clustering
results is necessary. In this paper, we propose a novel internal CVI -- the
Distance-based Separability Index (DSI), based on a data separability measure.
We compared the DSI with eight internal CVIs including studies from early Dunn
(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using
clustering results of five clustering algorithms on 12 real and 97 synthetic
datasets. Results show DSI is an effective, unique, and competitive CVI to
other compared CVIs. We also summarized the general process to evaluate CVIs
and created the rank-difference metric for comparison of CVIs' results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1"&gt;Shuyue Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepLab2: A TensorFlow Library for Deep Labeling. (arXiv:2106.09748v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09748</id>
        <link href="http://arxiv.org/abs/2106.09748"/>
        <updated>2021-06-21T02:07:37.711Z</updated>
        <summary type="html"><![CDATA[DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a
state-of-the-art and easy-to-use TensorFlow codebase for general dense pixel
prediction problems in computer vision. DeepLab2 includes all our recently
developed DeepLab model variants with pretrained checkpoints as well as model
training and evaluation code, allowing the community to reproduce and further
improve upon the state-of-art systems. To showcase the effectiveness of
DeepLab2, our Panoptic-DeepLab employing Axial-SWideRNet as network backbone
achieves 68.0% PQ or 83.5% mIoU on Cityscaspes validation set, with only
single-scale inference and ImageNet-1K pretrained checkpoints. We hope that
publicly sharing our library could facilitate future research on dense pixel
labeling tasks and envision new applications of this technology. Code is made
publicly available at \url{https://github.com/google-research/deeplab2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1"&gt;Mark Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huiyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1"&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1"&gt;Maxwell D. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yukun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Liangzhe Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dahun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1"&gt;Daniel Cremers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1"&gt;Laura Leal-Taixe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan L. Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1"&gt;Florian Schroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1"&gt;Hartwig Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang-Chieh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RSG: A Simple but Effective Module for Learning Imbalanced Datasets. (arXiv:2106.09859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09859</id>
        <link href="http://arxiv.org/abs/2106.09859"/>
        <updated>2021-06-21T02:07:37.694Z</updated>
        <summary type="html"><![CDATA[Imbalanced datasets widely exist in practice and area great challenge for
training deep neural models with agood generalization on infrequent classes. In
this work, wepropose a new rare-class sample generator (RSG) to solvethis
problem. RSG aims to generate some new samplesfor rare classes during training,
and it has in particularthe following advantages: (1) it is convenient to use
andhighly versatile, because it can be easily integrated intoany kind of
convolutional neural network, and it works wellwhen combined with different
loss functions, and (2) it isonly used during the training phase, and
therefore, no ad-ditional burden is imposed on deep neural networks duringthe
testing phase. In extensive experimental evaluations, weverify the
effectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive
results on Imbalanced CIFARand new state-of-the-art results on Places-LT,
ImageNet-LT, and iNaturalist 2018. The source code is available at
https://github.com/Jianf-Wang/RSG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhenghua Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid graph convolutional neural networks for landmark-based anatomical segmentation. (arXiv:2106.09832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09832</id>
        <link href="http://arxiv.org/abs/2106.09832"/>
        <updated>2021-06-21T02:07:37.671Z</updated>
        <summary type="html"><![CDATA[In this work we address the problem of landmark-based segmentation for
anatomical structures. We propose HybridGNet, an encoder-decoder neural
architecture which combines standard convolutions for image feature encoding,
with graph convolutional neural networks to decode plausible representations of
anatomical structures. We benchmark the proposed architecture considering other
standard landmark and pixel-based models for anatomical segmentation in chest
x-ray images, and found that HybridGNet is more robust to image occlusions. We
also show that it can be used to construct landmark-based segmentations from
pixel level annotations. Our experimental results suggest that HybridGNet
produces accurate and anatomically plausible landmark-based segmentations, by
naturally incorporating shape constraints within the decoding process via
spectral convolutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1"&gt;Nicol&amp;#xe1;s Gaggion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-Enabled Ultra-Low-Dose CT Reconstruction. (arXiv:2106.09834v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09834</id>
        <link href="http://arxiv.org/abs/2106.09834"/>
        <updated>2021-06-21T02:07:37.644Z</updated>
        <summary type="html"><![CDATA[By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT
reconstruction is a holy grail to minimize cancer risks and genetic damages,
especially for children. With the development of medical CT technologies, the
iterative algorithms are widely used to reconstruct decent CT images from a
low-dose scan. Recently, artificial intelligence (AI) techniques have shown a
great promise in further reducing CT radiation dose to the next level. In this
paper, we demonstrate that AI-powered CT reconstruction offers diagnostic image
quality at an ultra-low-dose level comparable to that of radiography.
Specifically, here we develop a Split Unrolled Grid-like Alternative
Reconstruction (SUGAR) network, in which deep learning, physical modeling and
image prior are integrated. The reconstruction results from clinical datasets
show that excellent images can be reconstructed using SUGAR from 36
projections. This approach has a potential to change future healthcare.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1"&gt;Weiwen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1"&gt;Chuang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebrahimian_S/0/1/0/all/0/1"&gt;Shadi Ebrahimian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hengyong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1"&gt;Mannu Kalra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1"&gt;Ge Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Self-supervised Vision Transformers for Representation Learning. (arXiv:2106.09785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09785</id>
        <link href="http://arxiv.org/abs/2106.09785"/>
        <updated>2021-06-21T02:07:37.637Z</updated>
        <summary type="html"><![CDATA[This paper investigates two techniques for developing efficient
self-supervised vision transformers (EsViT) for visual representation learning.
First, we show through a comprehensive empirical study that multi-stage
architectures with sparse self-attentions can significantly reduce modeling
complexity but with a cost of losing the ability to capture fine-grained
correspondences between image regions. Second, we propose a new pre-training
task of region matching which allows the model to capture fine-grained region
dependencies and as a result significantly improves the quality of the learned
vision representations. Our results show that combining the two techniques,
EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,
outperforming prior arts with around an order magnitude of higher throughput.
When transferring to downstream linear classification tasks, EsViT outperforms
its supervised counterpart on 17 out of 18 datasets. The code and models will
be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pengchuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1"&gt;Bin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1"&gt;Xiyang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Lu Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Integrated Gradients: An Adaptive Path Method for Removing Noise. (arXiv:2106.09788v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09788</id>
        <link href="http://arxiv.org/abs/2106.09788"/>
        <updated>2021-06-21T02:07:37.630Z</updated>
        <summary type="html"><![CDATA[Integrated Gradients (IG) is a commonly used feature attribution method for
deep neural networks. While IG has many desirable properties, the method often
produces spurious/noisy pixel attributions in regions that are not related to
the predicted class when applied to visual models. While this has been
previously noted, most existing solutions are aimed at addressing the symptoms
by explicitly reducing the noise in the resulting attributions. In this work,
we show that one of the causes of the problem is the accumulation of noise
along the IG path. To minimize the effect of this source of noise, we propose
adapting the attribution path itself -- conditioning the path not just on the
image but also on the model being explained. We introduce Adaptive Path Methods
(APMs) as a generalization of path methods, and Guided IG as a specific
instance of an APM. Empirically, Guided IG creates saliency maps better aligned
with the model's prediction and the input image that is being explained. We
show through qualitative and quantitative experiments that Guided IG
outperforms other, related methods in nearly every experiment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapishnikov_A/0/1/0/all/0/1"&gt;Andrei Kapishnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1"&gt;Subhashini Venugopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1"&gt;Besim Avci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedin_B/0/1/0/all/0/1"&gt;Ben Wedin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1"&gt;Michael Terry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1"&gt;Tolga Bolukbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smoothed Multi-View Subspace Clustering. (arXiv:2106.09875v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.09875</id>
        <link href="http://arxiv.org/abs/2106.09875"/>
        <updated>2021-06-21T02:07:37.609Z</updated>
        <summary type="html"><![CDATA[In recent years, multi-view subspace clustering has achieved impressive
performance due to the exploitation of complementary imformation across
multiple views. However, multi-view data can be very complicated and are not
easy to cluster in real-world applications. Most existing methods operate on
raw data and may not obtain the optimal solution. In this work, we propose a
novel multi-view clustering method named smoothed multi-view subspace
clustering (SMVSC) by employing a novel technique, i.e., graph filtering, to
obtain a smooth representation for each view, in which similar data points have
similar feature values. Specifically, it retains the graph geometric features
through applying a low-pass filter. Consequently, it produces a
``clustering-friendly" representation and greatly facilitates the downstream
clustering task. Extensive experiments on benchmark datasets validate the
superiority of our approach. Analysis shows that graph filtering increases the
separability of classes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1"&gt;Zhao Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis. (arXiv:2106.09759v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.09759</id>
        <link href="http://arxiv.org/abs/2106.09759"/>
        <updated>2021-06-21T02:07:37.584Z</updated>
        <summary type="html"><![CDATA[We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for
training machine learning models. The dataset consists of 21,295 synthetic
COVID-19 chest X-ray images to be used for computer-aided diagnosis. These
images, generated via an unsupervised domain adaptation approach, are of high
quality. We find that the synthetic images not only improve performance of
various deep learning architectures when used as additional training data under
heavy imbalance conditions, but also detect the target class with high
confidence. We also find that comparable performance can also be achieved when
trained only on synthetic images. Further, salient features of the synthetic
COVID-19 images indicate that the distribution is significantly different from
Non-COVID-19 classes, enabling a proper decision boundary. We hope the
availability of such high fidelity chest X-ray images of COVID-19 will
encourage advances in the development of diagnostic and/or management tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zunair_H/0/1/0/all/0/1"&gt;Hasib Zunair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hamza_A/0/1/0/all/0/1"&gt;A. Ben Hamza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Development of a conversing and body temperature scanning autonomously navigating robot to help screen for COVID-19. (arXiv:2106.09894v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.09894</id>
        <link href="http://arxiv.org/abs/2106.09894"/>
        <updated>2021-06-21T02:07:37.576Z</updated>
        <summary type="html"><![CDATA[Throughout the COVID-19 pandemic, the most common symptom displayed by
patients has been a fever, leading to the use of temperature scanning as a
preemptive measure to detect potential carriers of the virus. Human employees
with handheld thermometers have been used to fulfill this task, however this
puts them at risk as they cannot be physically distanced and the sequential
nature of this method leads to great inconveniences and inefficiency. The
proposed solution is an autonomously navigating robot capable of conversing and
scanning people's temperature to detect fevers and help screen for COVID-19. To
satisfy this objective, the robot must be able to (1) navigate autonomously,
(2) detect and track people, and (3) get individuals' temperature reading and
converse with them if it exceeds 38{\deg}C. An autonomously navigating mobile
robot is used with a manipulator controlled using a face tracking algorithm,
and an end effector consisting of a thermal camera, smartphone, and chatbot.
The goal is to develop a functioning solution that performs the above tasks. In
addition, technical challenges encountered and their engineering solutions will
be presented, and recommendations will be made for enhancements that could be
incorporated when approaching commercialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1"&gt;Ryan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02242</id>
        <link href="http://arxiv.org/abs/2106.02242"/>
        <updated>2021-06-21T02:07:37.440Z</updated>
        <summary type="html"><![CDATA[Transformer has been widely adopted in Neural Machine Translation (NMT)
because of its large capacity and parallel training of sequence generation.
However, the deployment of Transformer is challenging because different
scenarios require models of different complexities and scales. Naively training
multiple Transformers is redundant in terms of both computation and memory. In
this paper, we propose a novel Scalable Transformers, which naturally contains
sub-Transformers of different scales and have shared parameters. Each
sub-Transformer can be easily obtained by cropping the parameters of the
largest Transformer. A three-stage training scheme is proposed to tackle the
difficulty of training the Scalable Transformers, which introduces additional
supervisions from word-level and sequence-level self-distillation. Extensive
experiments were conducted on WMT EN-De and En-Fr to validate our proposed
Scalable Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces. (arXiv:2106.07505v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07505</id>
        <link href="http://arxiv.org/abs/2106.07505"/>
        <updated>2021-06-21T02:07:37.433Z</updated>
        <summary type="html"><![CDATA[Hate speech and profanity detection suffer from data sparsity, especially for
languages other than English, due to the subjective nature of the tasks and the
resulting annotation incompatibility of existing corpora. In this study, we
identify profane subspaces in word and sentence representations and explore
their generalization capability on a variety of similar and distant target
tasks in a zero-shot setting. This is done monolingually (German) and
cross-lingually to closely-related (English), distantly-related (French) and
non-related (Arabic) tasks. We observe that, on both similar and distant target
tasks and across all languages, the subspace-based representations transfer
more effectively than standard BERT representations in the zero-shot setting,
with improvements between F1 +10.9 and F1 +42.9 over the baselines across all
tested monolingual and cross-lingual scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1"&gt;Vanessa Hahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1"&gt;Dana Ruiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1"&gt;Thomas Kleinbauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1"&gt;Dietrich Klakow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Speech Translation via Cross-modal Progressive Training. (arXiv:2104.10380v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10380</id>
        <link href="http://arxiv.org/abs/2104.10380"/>
        <updated>2021-06-21T02:07:37.426Z</updated>
        <summary type="html"><![CDATA[End-to-end speech translation models have become a new trend in research due
to their potential of reducing error propagation. However, these models still
suffer from the challenge of data scarcity. How to effectively use unlabeled or
other parallel corpora from machine translation is promising but still an open
problem. In this paper, we propose Cross Speech-Text Network (XSTNet), an
end-to-end model for speech-to-text translation. XSTNet takes both speech and
text as input and outputs both transcription and translation text. The model
benefits from its three key design aspects: a self-supervised pre-trained
sub-network as the audio encoder, a multi-task training objective to exploit
additional parallel bilingual text, and a progressive training procedure. We
evaluate the performance of XSTNet and baselines on the MuST-C En-X and
LibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art
results on all language directions with an average BLEU of 28.8, outperforming
the previous best method by 3.2 BLEU. Code, models, cases, and more detailed
analysis are available at https://github.com/ReneeYe/XSTNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1"&gt;Rong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01547</id>
        <link href="http://arxiv.org/abs/2102.01547"/>
        <updated>2021-06-21T02:07:37.376Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhuoyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhendong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xin Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12906</id>
        <link href="http://arxiv.org/abs/2103.12906"/>
        <updated>2021-06-21T02:07:37.356Z</updated>
        <summary type="html"><![CDATA[Query by Example is a well-known information retrieval task in which a
document is chosen by the user as the search query and the goal is to retrieve
relevant documents from a large collection. However, a document often covers
multiple aspects of a topic. To address this scenario we introduce the task of
faceted Query by Example in which users can also specify a finer grained aspect
in addition to the input query document. We focus on the application of this
task in scientific literature search. We envision models which are able to
retrieve scientific papers analogous to a query scientific paper along
specifically chosen rhetorical structure elements as one solution to this
problem. In this work, the rhetorical structure elements, which we refer to as
facets, indicate backgrounds, methods, or results of a scientific paper. We
introduce and describe an expert annotated test collection to evaluate models
trained to perform this task. Our test collection consists of a diverse set of
50 query documents, drawn from computational linguistics and machine learning
venues. We carefully followed the annotation guideline used by TREC for depth-k
pooling (k = 100 or 250) and the resulting data collection consists of graded
relevance scores with high annotation agreement. The data is freely available
for research purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1"&gt;Sheshera Mysore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1"&gt;Tim O&amp;#x27;Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On-Device Personalization of Automatic Speech Recognition Models for Disordered Speech. (arXiv:2106.10259v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.10259</id>
        <link href="http://arxiv.org/abs/2106.10259"/>
        <updated>2021-06-21T02:07:37.348Z</updated>
        <summary type="html"><![CDATA[While current state-of-the-art Automatic Speech Recognition (ASR) systems
achieve high accuracy on typical speech, they suffer from significant
performance degradation on disordered speech and other atypical speech
patterns. Personalization of ASR models, a commonly applied solution to this
problem, is usually performed in a server-based training environment posing
problems around data privacy, delayed model-update times, and communication
cost for copying data and models between mobile device and server
infrastructure. In this paper, we present an approach to on-device based ASR
personalization with very small amounts of speaker-specific data. We test our
approach on a diverse set of 100 speakers with disordered speech and find
median relative word error rate improvement of 71% with only 50 short
utterances required per speaker. When tested on a voice-controlled home
automation platform, on-device personalized models show a median task success
rate of 81%, compared to only 40% of the unadapted models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1"&gt;Katrin Tomanek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;oise Beaufays&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cattiau_J/0/1/0/all/0/1"&gt;Julie Cattiau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1"&gt;Angad Chandorkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1"&gt;Khe Chai Sim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Self-training for Punctuation Prediction. (arXiv:2104.10339v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10339</id>
        <link href="http://arxiv.org/abs/2104.10339"/>
        <updated>2021-06-21T02:07:37.341Z</updated>
        <summary type="html"><![CDATA[Punctuation prediction for automatic speech recognition (ASR) output
transcripts plays a crucial role for improving the readability of the ASR
transcripts and for improving the performance of downstream natural language
processing applications. However, achieving good performance on punctuation
prediction often requires large amounts of labeled speech transcripts, which is
expensive and laborious. In this paper, we propose a Discriminative
Self-Training approach with weighted loss and discriminative label smoothing to
exploit unlabeled speech transcripts. Experimental results on the English
IWSLT2011 benchmark test set and an internal Chinese spoken language dataset
demonstrate that the proposed approach achieves significant improvement on
punctuation prediction accuracy over strong baselines including BERT, RoBERTa,
and ELECTRA models. The proposed Discriminative Self-Training approach
outperforms the vanilla self-training approach. We establish a new
state-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current
SOTA model by 1.3% absolute gain on F$_1$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mengzhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10357</id>
        <link href="http://arxiv.org/abs/2104.10357"/>
        <updated>2021-06-21T02:07:37.334Z</updated>
        <summary type="html"><![CDATA[In the traditional cascading architecture for spoken language understanding
(SLU), it has been observed that automatic speech recognition errors could be
detrimental to the performance of natural language understanding. End-to-end
(E2E) SLU models have been proposed to directly map speech input to desired
semantic frame with a single model, hence mitigating ASR error propagation.
Recently, pre-training technologies have been explored for these E2E models. In
this paper, we propose a novel joint textual-phonetic pre-training approach for
learning spoken language representations, aiming at exploring the full
potentials of phonetic information to improve SLU robustness to ASR errors. We
explore phoneme labels as high-level speech features, and design and compare
pre-training tasks based on conditional masked language model objectives and
inter-sentence relation objectives. We also investigate the efficacy of
combining textual and phonetic information during fine-tuning. Experimental
results on spoken language understanding benchmarks, Fluent Speech Commands and
SNIPS, show that the proposed approach significantly outperforms strong
baseline models and improves robustness of spoken language understanding to ASR
errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglin Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back Attention Knowledge Transfer for Low-Resource Named Entity Recognition. (arXiv:1906.01183v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01183</id>
        <link href="http://arxiv.org/abs/1906.01183"/>
        <updated>2021-06-21T02:07:37.325Z</updated>
        <summary type="html"><![CDATA[In recent years, great success has been achieved in the field of natural
language processing (NLP), thanks in part to the considerable amount of
annotated resources. For named entity recognition (NER), most languages do not
have such an abundance of labeled data as English, so the performances of those
languages are relatively lower. To improve the performance, we propose a
general approach called Back Attention Network (BAN). BAN uses a translation
system to translate other language sentences into English and then applies a
new mechanism named back attention knowledge transfer to obtain task-specific
information from pre-trained high-resource languages NER model. This strategy
can transfer high-layer features of well-trained model and enrich the semantic
representations of the original language. Experiments on three different
language datasets indicate that the proposed approach outperforms other
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Shengfei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Linghao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1"&gt;Huixiong Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huanhuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEUS: A Data-driven Approach to Estimate User Satisfaction in Multi-turn Dialogues. (arXiv:2103.01287v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01287</id>
        <link href="http://arxiv.org/abs/2103.01287"/>
        <updated>2021-06-21T02:07:37.307Z</updated>
        <summary type="html"><![CDATA[Digital assistants are experiencing rapid growth due to their ability to
assist users with day-to-day tasks where most dialogues are happening
multi-turn. However, evaluating multi-turn dialogues remains challenging,
especially at scale. We suggest a context-sensitive method to estimate the
turn-level satisfaction for dialogue considering various types of user
preferences. The costs of interactions between users and dialogue systems are
formulated using a budget consumption concept. We assume users have an initial
interaction budget for a dialogue formed based on the task complexity and that
each turn has a cost. When the task is completed, or the budget has been
exhausted, users quit the dialogue. We demonstrate our method's effectiveness
by extensive experimentation with a simulated dialogue platform and real
multi-turn dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dookun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1"&gt;Julia Kiseleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition. (arXiv:2106.10169v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10169</id>
        <link href="http://arxiv.org/abs/2106.10169"/>
        <updated>2021-06-21T02:07:37.247Z</updated>
        <summary type="html"><![CDATA[By implicitly recognizing a user based on his/her speech input, speaker
identification enables many downstream applications, such as personalized
system behavior and expedited shopping checkouts. Based on whether the speech
content is constrained or not, both text-dependent (TD) and text-independent
(TI) speaker recognition models may be used. We wish to combine the advantages
of both types of models through an ensemble system to make more reliable
predictions. However, any such combined approach has to be robust to incomplete
inputs, i.e., when either TD or TI input is missing. As a solution we propose a
fusion of embeddings network foenet architecture, combining joint learning with
neural attention. We compare foenet with four competitive baseline methods on a
dataset of voice assistant inputs, and show that it achieves higher accuracy
than the baseline and score fusion methods, especially in the presence of
incomplete inputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruirui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1"&gt;Chelsea J.-T. Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1"&gt;Hongda Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elibol_O/0/1/0/all/0/1"&gt;Oguz Elibol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10199</id>
        <link href="http://arxiv.org/abs/2106.10199"/>
        <updated>2021-06-21T02:07:37.239Z</updated>
        <summary type="html"><![CDATA[We show that with small-to-medium training data, fine-tuning only the bias
terms (or a subset of the bias terms) of pre-trained BERT models is competitive
with (and sometimes better than) fine-tuning the entire model. For larger data,
bias-only fine-tuning is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaken_E/0/1/0/all/0/1"&gt;Elad Ben Zaken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1"&gt;Shauli Ravfogel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1"&gt;Yoav Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Stacking of Layers in Neural Networks: An Application to Neural Machine Translation. (arXiv:2106.10002v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10002</id>
        <link href="http://arxiv.org/abs/2106.10002"/>
        <updated>2021-06-21T02:07:37.141Z</updated>
        <summary type="html"><![CDATA[In deep neural network modeling, the most common practice is to stack a
number of recurrent, convolutional, or feed-forward layers in order to obtain
high-quality continuous space representations which in turn improves the
quality of the network's prediction. Conventionally, each layer in the stack
has its own parameters which leads to a significant increase in the number of
model parameters. In this paper, we propose to share parameters across all
layers thereby leading to a recurrently stacked neural network model. We report
on an extensive case study on neural machine translation (NMT), where we apply
our proposed method to an encoder-decoder based neural network model, i.e., the
Transformer model, and experiment with three Japanese--English translation
datasets. We empirically demonstrate that the translation quality of a model
that recurrently stacks a single layer 6 times, despite having significantly
fewer parameters, approaches that of a model that stacks 6 layers where each
layer has different parameters. We also explore the limits of recurrent
stacking where we train extremely deep NMT models. This paper also examines the
utility of our recurrently stacked model as a student model through transfer
learning via leveraging pre-trained parameters and knowledge distillation, and
shows that it compensates for the performance drops in translation quality that
the direct training of recurrently stacked model brings. We also show how
transfer learning helps in faster decoding on top of the already reduced number
of parameters due to recurrent stacking. Finally, we analyze the effects of
recurrently stacked layers by visualizing the attentions of models that use
recurrently stacked layers and models that do not.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1"&gt;Raj Dabre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1"&gt;Atsushi Fujita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subjective Bias in Abstractive Summarization. (arXiv:2106.10084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10084</id>
        <link href="http://arxiv.org/abs/2106.10084"/>
        <updated>2021-06-21T02:07:37.133Z</updated>
        <summary type="html"><![CDATA[Due to the subjectivity of the summarization, it is a good practice to have
more than one gold summary for each training document. However, many modern
large-scale abstractive summarization datasets have only one-to-one samples
written by different human with different styles. The impact of this phenomenon
is understudied. We formulate the differences among possible multiple
expressions summarizing the same content as subjective bias and examine the
role of this bias in the context of abstractive summarization. In this paper a
lightweight and effective method to extract the feature embeddings of
subjective styles is proposed. Results of summarization models trained on
style-clustered datasets show that there are certain types of styles that lead
to better convergence, abstraction and generalization. The reproducible code
and generated summaries are available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Litvak_M/0/1/0/all/0/1"&gt;Marina Litvak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanetik_N/0/1/0/all/0/1"&gt;Natalia Vanetik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jiacheng Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yinan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1"&gt;Siya Qi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting gender of Brazilian names using deep learning. (arXiv:2106.10156v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10156</id>
        <link href="http://arxiv.org/abs/2106.10156"/>
        <updated>2021-06-21T02:07:37.091Z</updated>
        <summary type="html"><![CDATA[Predicting gender by the name is not a simple task. In many applications,
especially in the natural language processing (NLP) field, this task may be
necessary, mainly when considering foreign names. Some machine learning
algorithms can satisfactorily perform the prediction. In this paper, we
examined and implemented feedforward and recurrent deep neural network models,
such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first
name. A dataset of Brazilian names is used to train and evaluate the models. We
analyzed the accuracy, recall, precision, and confusion matrix to measure the
models' performances. The results indicate that the gender prediction can be
performed from the feature extraction strategy looking at the names as a set of
strings. Some models accurately predict the gender in more than 90% of the
cases. The recurrent models overcome the feedforward models in this binary
classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1"&gt;Rosana C. B. Rego&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1"&gt;Ver&amp;#xf4;nica M. L. Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text. (arXiv:2106.10123v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10123</id>
        <link href="http://arxiv.org/abs/2106.10123"/>
        <updated>2021-06-21T02:07:37.059Z</updated>
        <summary type="html"><![CDATA[Code-mixing is a frequent communication style among multilingual speakers
where they mix words and phrases from two different languages in the same
utterance of text or speech. Identifying and filtering code-mixed text is a
challenging task due to its co-existence with monolingual and noisy text. Over
the years, several code-mixing metrics have been extensively used to identify
and validate code-mixed text quality. This paper demonstrates several inherent
limitations of code-mixing metrics with examples from the already existing
datasets that are popularly used across various experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synchronising speech segments with musical beats in Mandarin and English singing. (arXiv:2106.10045v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.10045</id>
        <link href="http://arxiv.org/abs/2106.10045"/>
        <updated>2021-06-21T02:07:37.051Z</updated>
        <summary type="html"><![CDATA[Generating synthesised singing voice with models trained on speech data has
many advantages due to the models' flexibility and controllability. However,
since the information about the temporal relationship between segments and
beats are lacking in speech training data, the synthesised singing may sound
off-beat at times. Therefore, the availability of the information on the
temporal relationship between speech segments and music beats is crucial. The
current study investigated the segment-beat synchronisation in singing data,
with hypotheses formed based on the linguistics theories of P-centre and
sonority hierarchy. A Mandarin corpus and an English corpus of professional
singing data were manually annotated and analysed. The results showed that the
presence of musical beats was more dependent on segment duration than sonority.
However, the sonority hierarchy and the P-centre theory were highly related to
the location of beats. Mandarin and English demonstrated cross-linguistic
variations despite exhibiting common patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jian Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPBERT: Pre-training BERT on SPARQL Queries for End-to-end Question Answering over Knowledge Graphs. (arXiv:2106.09997v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09997</id>
        <link href="http://arxiv.org/abs/2106.09997"/>
        <updated>2021-06-21T02:07:37.037Z</updated>
        <summary type="html"><![CDATA[We aim to create an unprecedented attempt to build an end-to-end Question
Answering (QA) over Knowledge Graphs (KGs), which can construct SPARQL queries
from natural language questions and generate a verbalized answer to its
queries. Hence, we introduce SPBERT, a Transformer-based language model
pre-trained on massive SPARQL query logs. By incorporating masked language
modelling objective and word structural objective, SPBERT can learn
general-purpose representations in both natural language and SPARQL query
language and make the most of the sequential order of words that are crucial
for structured language like SPARQL. In this paper, we investigate how SPBERT
and encoder-decoder architecture can be adapted for Knowledge-based QA corpora.
We conduct exhaustive experiments on two auxiliary tasks, including SPARQL
Query Construction and Answer Verbalization Generation. Results show that
SPBERT obtains promising performance and achieves state-of-the-art results on
several of these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1"&gt;Long Phan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Truong-Son Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Financial Sentiment Analysis in a South African Landscape. (arXiv:2106.10004v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10004</id>
        <link href="http://arxiv.org/abs/2106.10004"/>
        <updated>2021-06-21T02:07:37.028Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis as a sub-field of natural language processing has received
increased attention in the past decade enabling organisations to more
effectively manage their reputation through online media monitoring. Many
drivers impact reputation, however, this thesis focuses only the aspect of
financial performance and explores the gap with regards to financial sentiment
analysis in a South African context. Results showed that pre-trained sentiment
analysers are least effective for this task and that traditional lexicon-based
and machine learning approaches are best suited to predict financial sentiment
of news articles. The evaluated methods produced accuracies of 84\%-94\%. The
predicted sentiments correlated quite well with share price and highlighted the
potential use of sentiment as an indicator of financial performance. A main
contribution of the study was updating an existing sentiment dictionary for
financial sentiment analysis. Model generalisation was less acceptable due to
the limited amount of training data used. Future work includes expanding the
data set to improve general usability and contribute to an open-source
financial sentiment analyser for South African data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terblanche_M/0/1/0/all/0/1"&gt;Michelle Terblanche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing user creativity: Semantic measures for idea generation. (arXiv:2106.10131v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10131</id>
        <link href="http://arxiv.org/abs/2106.10131"/>
        <updated>2021-06-21T02:07:37.005Z</updated>
        <summary type="html"><![CDATA[Human creativity generates novel ideas to solve real-world problems. This
thereby grants us the power to transform the surrounding world and extend our
human attributes beyond what is currently possible. Creative ideas are not just
new and unexpected, but are also successful in providing solutions that are
useful, efficient and valuable. Thus, creativity optimizes the use of available
resources and increases wealth. The origin of human creativity, however, is
poorly understood, and semantic measures that could predict the success of
generated ideas are currently unknown. Here, we analyze a dataset of design
problem-solving conversations in real-world settings by using 49 semantic
measures based on WordNet 3.1 and demonstrate that a divergence of semantic
similarity, an increased information content, and a decreased polysemy predict
the success of generated ideas. The first feedback from clients also enhances
information content and leads to a divergence of successful ideas in creative
problem solving. These results advance cognitive science by identifying
real-world processes in human problem solving that are relevant to the success
of produced solutions and provide tools for real-time monitoring of problem
solving, student training and skill acquisition. A selected subset of
information content (IC S\'anchez-Batet) and semantic similarity
(Lin/S\'anchez-Batet) measures, which are both statistically powerful and
computationally fast, could support the development of technologies for
computer-assisted enhancements of human creativity or for the implementation of
creativity in machines endowed with general artificial intelligence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1"&gt;Georgi V. Georgiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgiev_D/0/1/0/all/0/1"&gt;Danko D. Georgiev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Label Mask for Multi-Label Text Classification. (arXiv:2106.10076v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.10076</id>
        <link href="http://arxiv.org/abs/2106.10076"/>
        <updated>2021-06-21T02:07:36.996Z</updated>
        <summary type="html"><![CDATA[One of the key problems in multi-label text classification is how to take
advantage of the correlation among labels. However, it is very challenging to
directly model the correlations among labels in a complex and unknown label
space. In this paper, we propose a Label Mask multi-label text classification
model (LM-MTC), which is inspired by the idea of cloze questions of language
model. LM-MTC is able to capture implicit relationships among labels through
the powerful ability of pre-train language models. On the basis, we assign a
different token to each potential label, and randomly mask the token with a
certain probability to build a label based Masked Language Model (MLM). We
train the MTC and MLM together, further improving the generalization ability of
the model. A large number of experiments on multiple datasets demonstrate the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xingbing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zelong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1"&gt;Haining An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiqi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Pre-Training for Multi-Hop Retriever. (arXiv:2106.09983v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09983</id>
        <link href="http://arxiv.org/abs/2106.09983"/>
        <updated>2021-06-21T02:07:36.944Z</updated>
        <summary type="html"><![CDATA[In multi-hop QA, answering complex questions entails iterative document
retrieval for finding the missing entity of the question. The main steps of
this process are sub-question detection, document retrieval for the
sub-question, and generation of a new query for the final document retrieval.
However, building a dataset that contains complex questions with sub-questions
and their corresponding documents requires costly human annotation. To address
the issue, we propose a new method for weakly supervised multi-hop retriever
pre-training without human efforts. Our method includes 1) a pre-training task
for generating vector representations of complex questions, 2) a scalable data
generation method that produces the nested structure of question and
sub-question as weak supervision for pre-training, and 3) a pre-training model
structure based on dense encoders. We conduct experiments to compare the
performance of our pre-trained retriever with several state-of-the-art models
on end-to-end multi-hop QA as well as document retrieval. The experimental
results show that our pre-trained retriever is effective and also robust on
limited data and computational resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seonwoo_Y/0/1/0/all/0/1"&gt;Yeon Seonwoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Woo Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1"&gt;Jung-Woo Ha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1"&gt;Alice Oh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Neural Edge-Editing Approach for Document-Level Relation Graph Extraction. (arXiv:2106.09900v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09900</id>
        <link href="http://arxiv.org/abs/2106.09900"/>
        <updated>2021-06-21T02:07:36.926Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel edge-editing approach to extract relation
information from a document. We treat the relations in a document as a relation
graph among entities in this approach. The relation graph is iteratively
constructed by editing edges of an initial graph, which might be a graph
extracted by another system or an empty graph. The way to edit edges is to
classify them in a close-first manner using the document and
temporally-constructed graph information; each edge is represented with a
document context information by a pretrained transformer model and a graph
context information by a graph convolutional neural network model. We evaluate
our approach on the task to extract material synthesis procedures from
materials science texts. The experimental results show the effectiveness of our
approach in editing the graphs initialized by our in-house rule-based system
and empty graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makino_K/0/1/0/all/0/1"&gt;Kohei Makino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1"&gt;Makoto Miwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasaki_Y/0/1/0/all/0/1"&gt;Yutaka Sasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating the Role of Negatives in Contrastive Representation Learning. (arXiv:2106.09943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.09943</id>
        <link href="http://arxiv.org/abs/2106.09943"/>
        <updated>2021-06-21T02:07:36.915Z</updated>
        <summary type="html"><![CDATA[Noise contrastive learning is a popular technique for unsupervised
representation learning. In this approach, a representation is obtained via
reduction to supervised learning, where given a notion of semantic similarity,
the learner tries to distinguish a similar (positive) example from a collection
of random (negative) examples. The success of modern contrastive learning
pipelines relies on many parameters such as the choice of data augmentation,
the number of negative examples, and the batch size; however, there is limited
understanding as to how these parameters interact and affect downstream
performance. We focus on disambiguating the role of one of these parameters:
the number of negative examples. Theoretically, we show the existence of a
collision-coverage trade-off suggesting that the optimal number of negative
examples should scale with the number of underlying concepts in the data.
Empirically, we scrutinize the role of the number of negatives in both NLP and
vision tasks. In the NLP task, we find that the results broadly agree with our
theory, while our vision experiments are murkier with performance sometimes
even being insensitive to the number of negatives. We discuss plausible
explanations for this behavior and suggest future directions to better align
theory and practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1"&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1"&gt;Surbhi Goel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1"&gt;Akshay Krishnamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1"&gt;Dipendra Misra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking. (arXiv:2106.09795v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09795</id>
        <link href="http://arxiv.org/abs/2106.09795"/>
        <updated>2021-06-21T02:07:36.895Z</updated>
        <summary type="html"><![CDATA[Entity linking (EL), the task of disambiguating mentions in text by linking
them to entities in a knowledge graph, is crucial for text understanding,
question answering or conversational systems. Entity linking on short text
(e.g., single sentence or question) poses particular challenges due to limited
context. While prior approaches use either heuristics or black-box neural
methods, here we propose LNN-EL, a neuro-symbolic approach that combines the
advantages of using interpretable rules based on first-order logic with the
performance of neural learning. Even though constrained to using rules, LNN-EL
performs competitively against SotA black-box neural approaches, with the added
benefits of extensibility and transferability. In particular, we show that we
can easily blend existing rule templates given by a human expert, with multiple
types of features (priors, BERT encodings, box embeddings, etc), and even
scores resulting from previous EL methods, thus improving on such methods. For
instance, on the LC-QuAD-1.0 dataset, we show more than $4$\% increase in F1
score over previous SotA. Finally, we show that the inductive bias offered by
using logic results in learned rules that transfer well across datasets, even
without fine tuning, while maintaining high accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1"&gt;Sairam Gurajada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qiuhao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1"&gt;Sumit Neelam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1"&gt;Lucian Popa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1"&gt;Prithviraj Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1"&gt;Alexander Gray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction. (arXiv:2106.09895v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09895</id>
        <link href="http://arxiv.org/abs/2106.09895"/>
        <updated>2021-06-21T02:07:36.885Z</updated>
        <summary type="html"><![CDATA[Joint extraction of entities and relations from unstructured texts is a
crucial task in information extraction. Recent methods achieve considerable
performance but still suffer from some inherent limitations, such as redundancy
of relation prediction, poor generalization of span-based extraction and
inefficiency. In this paper, we decompose this task into three subtasks,
Relation Judgement, Entity Extraction and Subject-object Alignment from a novel
perspective and then propose a joint relational triple extraction framework
based on Potential Relation and Global Correspondence (PRGC). Specifically, we
design a component to predict potential relations, which constrains the
following entity extraction to the predicted relation subset rather than all
relations; then a relation-specific sequence tagging component is applied to
handle the overlapping problem between subjects and objects; finally, a global
correspondence component is designed to align the subject and object into a
triple with low-complexity. Extensive experiments show that PRGC achieves
state-of-the-art performance on public benchmarks with higher efficiency and
delivers consistent performance gain on complex scenarios of overlapping
triples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hengyi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1"&gt;Rui Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yifan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yunyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1"&gt;Bin Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Ming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations. (arXiv:2106.09896v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09896</id>
        <link href="http://arxiv.org/abs/2106.09896"/>
        <updated>2021-06-21T02:07:36.876Z</updated>
        <summary type="html"><![CDATA[Quotations are crucial for successful explanations and persuasions in
interpersonal communications. However, finding what to quote in a conversation
is challenging for both humans and machines. This work studies automatic
quotation generation in an online conversation and explores how language
consistency affects whether a quotation fits the given context. Here, we
capture the contextual consistency of a quotation in terms of latent topics,
interactions with the dialogue history, and coherence to the query turn's
existing content. Further, an encoder-decoder neural framework is employed to
continue the context with a quotation via language generation. Experiment
results on two large-scale datasets in English and Chinese demonstrate that our
quotation generation model outperforms the state-of-the-art models. Further
analysis shows that topic, interaction, and query consistency are all helpful
to learn how to quote in online conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lingzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haisong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Kam-Fai Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Joint Pandemic Concern and Relation Extraction on Twitter. (arXiv:2106.09929v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09929</id>
        <link href="http://arxiv.org/abs/2106.09929"/>
        <updated>2021-06-21T02:07:36.859Z</updated>
        <summary type="html"><![CDATA[Public concern detection provides potential guidance to the authorities for
crisis management before or during a pandemic outbreak. Detecting people's
concerns and attention from online social media platforms has been widely
acknowledged as an effective approach to relieve public panic and prevent a
social crisis. However, detecting concerns in time from massive information in
social media turns out to be a big challenge, especially when sufficient
manually labeled data is in the absence of public health emergencies, e.g.,
COVID-19. In this paper, we propose a novel end-to-end deep learning model to
identify people's concerns and the corresponding relations based on Graph
Convolutional Network and Bi-directional Long Short Term Memory integrated with
Concern Graph. Except for the sequential features from BERT embeddings, the
regional features of tweets can be extracted by the Concern Graph module, which
not only benefits the concern detection but also enables our model to be high
noise-tolerant. Thus, our model can address the issue of insufficient manually
labeled data. We conduct extensive experiments to evaluate the proposed model
by using both manually labeled tweets and automatically labeled tweets. The
experimental results show that our model can outperform the state-of-art models
on real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jingli Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weihua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yongchareon_S/0/1/0/all/0/1"&gt;Sira Yongchareon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1"&gt;Quan Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bad Characters: Imperceptible NLP Attacks. (arXiv:2106.09898v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09898</id>
        <link href="http://arxiv.org/abs/2106.09898"/>
        <updated>2021-06-21T02:07:36.850Z</updated>
        <summary type="html"><![CDATA[Several years of research have shown that machine-learning systems are
vulnerable to adversarial examples, both in theory and in practice. Until now,
such attacks have primarily targeted visual models, exploiting the gap between
human and machine perception. Although text-based models have also been
attacked with adversarial examples, such attacks struggled to preserve semantic
meaning and indistinguishability. In this paper, we explore a large class of
adversarial examples that can be used to attack text-based models in a
black-box setting without making any human-perceptible visual modification to
inputs. We use encoding-specific perturbations that are imperceptible to the
human eye to manipulate the outputs of a wide range of Natural Language
Processing (NLP) systems from neural machine-translation pipelines to web
search engines. We find that with a single imperceptible encoding injection --
representing one invisible character, homoglyph, reordering, or deletion -- an
attacker can significantly reduce the performance of vulnerable models, and
with three injections most models can be functionally broken. Our attacks work
against currently-deployed commercial systems, including those produced by
Microsoft and Google, in addition to open source models published by Facebook
and IBM. This novel series of attacks presents a significant threat to many
language processing systems: an attacker can affect systems in a targeted
manner without any assumptions about the underlying model. We conclude that
text-based NLP systems require careful input sanitization, just like
conventional applications, and that given such systems are now being deployed
rapidly at scale, the urgent attention of architects and operators is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boucher_N/0/1/0/all/0/1"&gt;Nicholas Boucher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:36.841Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Task Learning and Adapted Knowledge Models for Emotion-Cause Extraction. (arXiv:2106.09790v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09790</id>
        <link href="http://arxiv.org/abs/2106.09790"/>
        <updated>2021-06-21T02:07:36.699Z</updated>
        <summary type="html"><![CDATA[Detecting what emotions are expressed in text is a well-studied problem in
natural language processing. However, research on finer grained emotion
analysis such as what causes an emotion is still in its infancy. We present
solutions that tackle both emotion recognition and emotion cause detection in a
joint fashion. Considering that common-sense knowledge plays an important role
in understanding implicitly expressed emotions and the reasons for those
emotions, we propose novel methods that combine common-sense knowledge via
adapted knowledge models with multi-task learning to perform joint emotion
classification and emotion cause tagging. We show performance improvement on
both tasks when including common-sense reasoning and a multitask framework. We
provide a thorough analysis to gain insights into model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Turcan_E/0/1/0/all/0/1"&gt;Elsbeth Turcan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anubhai_R/0/1/0/all/0/1"&gt;Rishita Anubhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_K/0/1/0/all/0/1"&gt;Kasturi Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Onaizan_Y/0/1/0/all/0/1"&gt;Yaser Al-Onaizan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSFCube -- A Test Collection of Computer Science Research Articles for Faceted Query by Example. (arXiv:2103.12906v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12906</id>
        <link href="http://arxiv.org/abs/2103.12906"/>
        <updated>2021-06-21T02:07:36.688Z</updated>
        <summary type="html"><![CDATA[Query by Example is a well-known information retrieval task in which a
document is chosen by the user as the search query and the goal is to retrieve
relevant documents from a large collection. However, a document often covers
multiple aspects of a topic. To address this scenario we introduce the task of
faceted Query by Example in which users can also specify a finer grained aspect
in addition to the input query document. We focus on the application of this
task in scientific literature search. We envision models which are able to
retrieve scientific papers analogous to a query scientific paper along
specifically chosen rhetorical structure elements as one solution to this
problem. In this work, the rhetorical structure elements, which we refer to as
facets, indicate backgrounds, methods, or results of a scientific paper. We
introduce and describe an expert annotated test collection to evaluate models
trained to perform this task. Our test collection consists of a diverse set of
50 query documents, drawn from computational linguistics and machine learning
venues. We carefully followed the annotation guideline used by TREC for depth-k
pooling (k = 100 or 250) and the resulting data collection consists of graded
relevance scores with high annotation agreement. The data is freely available
for research purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mysore_S/0/1/0/all/0/1"&gt;Sheshera Mysore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1"&gt;Tim O&amp;#x27;Gorman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_H/0/1/0/all/0/1"&gt;Hamed Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Graph Learning for Recommendation. (arXiv:2010.10783v4 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10783</id>
        <link href="http://arxiv.org/abs/2010.10783"/>
        <updated>2021-06-21T02:07:36.677Z</updated>
        <summary type="html"><![CDATA[Representation learning on user-item graph for recommendation has evolved
from using single ID or interaction history to exploiting higher-order
neighbors. This leads to the success of graph convolution networks (GCNs) for
recommendation such as PinSage and LightGCN. Despite effectiveness, we argue
that they suffer from two limitations: (1) high-degree nodes exert larger
impact on the representation learning, deteriorating the recommendations of
low-degree (long-tail) items; and (2) representations are vulnerable to noisy
interactions, as the neighborhood aggregation scheme further enlarges the
impact of observed edges.

In this work, we explore self-supervised learning on user-item graph, so as
to improve the accuracy and robustness of GCNs for recommendation. The idea is
to supplement the classical supervised task of recommendation with an auxiliary
self-supervised task, which reinforces node representation learning via
self-discrimination. Specifically, we generate multiple views of a node,
maximizing the agreement between different views of the same node compared to
that of other nodes. We devise three operators to generate the views -- node
dropout, edge dropout, and random walk -- that change the graph structure in
different manners. We term this new learning paradigm as
\textit{Self-supervised Graph Learning} (SGL), implementing it on the
state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL
has the ability of automatically mining hard negatives. Empirical studies on
three benchmark datasets demonstrate the effectiveness of SGL, which improves
the recommendation accuracy, especially on long-tail items, and the robustness
against interaction noises. Our implementations are available at
\url{https://github.com/wujcan/SGL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiancan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1"&gt;Jianxun Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xing Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval. (arXiv:2104.09791v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09791</id>
        <link href="http://arxiv.org/abs/2104.09791"/>
        <updated>2021-06-21T02:07:36.665Z</updated>
        <summary type="html"><![CDATA[Pre-training and fine-tuning have achieved remarkable success in many
downstream natural language processing (NLP) tasks. Recently, pre-training
methods tailored for information retrieval (IR) have also been explored, and
the latest success is the PROP method which has reached new SOTA on a variety
of ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the
\textit{representative words prediction} (ROP) task for pre-training inspired
by the query likelihood model. Despite its exciting performance, the
effectiveness of PROP might be bounded by the classical unigram language model
adopted in the ROP task construction process. To tackle this problem, we
propose a bootstrapped pre-training method (namely B-PROP) based on BERT for
ad-hoc retrieval. The key idea is to use the powerful contextual language model
BERT to replace the classical unigram language model for the ROP task
construction, and re-train BERT itself towards the tailored objective for IR.
Specifically, we introduce a novel contrastive method, inspired by the
divergence-from-randomness idea, to leverage BERT's self-attention mechanism to
sample representative words from the document. By further fine-tuning on
downstream ad-hoc retrieval tasks, our method achieves significant improvements
over baselines without pre-training or with other pre-training methods, and
further pushes forward the SOTA on a variety of ad-hoc retrieval tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xinyu Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1"&gt;Yixing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xueqi Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-mode Transformer Transducer with Stochastic Future Context. (arXiv:2106.09760v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.09760</id>
        <link href="http://arxiv.org/abs/2106.09760"/>
        <updated>2021-06-21T02:07:36.655Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition (ASR) models make fewer errors when more
surrounding speech information is presented as context. Unfortunately,
acquiring a larger future context leads to higher latency. There exists an
inevitable trade-off between speed and accuracy. Naively, to fit different
latency requirements, people have to store multiple models and pick the best
one under the constraints. Instead, a more desirable approach is to have a
single model that can dynamically adjust its latency based on different
constraints, which we refer to as Multi-mode ASR. A Multi-mode ASR model can
fulfill various latency requirements during inference -- when a larger latency
becomes acceptable, the model can process longer future context to achieve
higher accuracy and when a latency budget is not flexible, the model can be
less dependent on future context but still achieve reliable accuracy. In
pursuit of Multi-mode ASR, we propose Stochastic Future Context, a simple
training procedure that samples one streaming configuration in each iteration.
Through extensive experiments on AISHELL-1 and LibriSpeech datasets, we show
that a Multi-mode ASR model rivals, if not surpasses, a set of competitive
streaming baselines trained with different latency budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwangyoun Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1"&gt;Felix Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sridhar_P/0/1/0/all/0/1"&gt;Prashant Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1"&gt;Kyu J. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1"&gt;Shinji Watanabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09775</id>
        <link href="http://arxiv.org/abs/2106.09775"/>
        <updated>2021-06-21T02:07:36.643Z</updated>
        <summary type="html"><![CDATA[Building a benchmark dataset for hate speech detection presents several
challenges. Firstly, because hate speech is relatively rare -- e.g., less than
3\% of Twitter posts are hateful \citep{founta2018large} -- random sampling of
tweets to annotate is inefficient in capturing hate speech. A common practice
is to only annotate tweets containing known ``hate words'', but this risks
yielding a biased benchmark that only partially captures the real-world
phenomenon of interest. A second challenge is that definitions of hate speech
tend to be highly variable and subjective. Annotators having diverse prior
notions of hate speech may not only disagree with one another but also struggle
to conform to specified labeling guidelines. Our key insight is that the rarity
and subjectivity of hate speech are akin to that of relevance in information
retrieval (IR). This connection suggests that well-established methodologies
for creating IR test collections might also be usefully applied to create
better benchmark datasets for hate speech detection. Firstly, to intelligently
and efficiently select which tweets to annotate, we apply established IR
techniques of {\em pooling} and {\em active learning}. Secondly, to improve
both consistency and value of annotations, we apply {\em task decomposition}
\cite{Zhang-sigir14} and {\em annotator rationale} \cite{mcdonnell16-hcomp}
techniques. Using the above techniques, we create and share a new benchmark
dataset\footnote{We will release the dataset upon publication.} for hate speech
detection with broader coverage than prior datasets. We also show a dramatic
drop in accuracy of existing detection models when tested on these broader
forms of hate. Collected annotator rationales not only provide documented
support for labeling decisions but also create exciting future work
opportunities for dual-supervision and/or explanation generation in modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mustafizur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1"&gt;Dinesh Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1"&gt;Dhiraj Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1"&gt;Mucahid Kutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1"&gt;Matthew Lease&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks. (arXiv:2106.10159v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.10159</id>
        <link href="http://arxiv.org/abs/2106.10159"/>
        <updated>2021-06-21T02:07:36.627Z</updated>
        <summary type="html"><![CDATA[Financial technology (FinTech) has drawn much attention among investors and
companies. While conventional stock analysis in FinTech targets at predicting
stock prices, less effort is made for profitable stock recommendation. Besides,
in existing approaches on modeling time series of stock prices, the
relationships among stocks and sectors (i.e., categories of stocks) are either
neglected or pre-defined. Ignoring stock relationships will miss the
information shared between stocks while using pre-defined relationships cannot
depict the latent interactions or influence of stock prices between stocks. In
this work, we aim at recommending the top-K profitable stocks in terms of
return ratio using time series of stock prices and sector information. We
propose a novel deep learning-based model, Financial Graph Attention Networks
(FinGAT), to tackle the task under the setting that no pre-defined
relationships between stocks are given. The idea of FinGAT is three-fold.
First, we devise a hierarchical learning component to learn short-term and
long-term sequential patterns from stock time series. Second, a fully-connected
graph between stocks and a fully-connected graph between sectors are
constructed, along with graph attention networks, to learn the latent
interactions among stocks and sectors. Third, a multi-task objective is devised
to jointly recommend the profitable stocks and predict the stock movement.
Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit
remarkable recommendation performance of our FinGAT, comparing to
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1"&gt;Yi-Ling Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yu-Che Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng-Te Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic Stopping Rules For Technology-Assisted Review. (arXiv:2106.09871v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09871</id>
        <link href="http://arxiv.org/abs/2106.09871"/>
        <updated>2021-06-21T02:07:36.436Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop active learning
workflows for finding relevant documents in large collections. These workflows
often must meet a target for the proportion of relevant documents found (i.e.
recall) while also holding down costs. A variety of heuristic stopping rules
have been suggested for striking this tradeoff in particular settings, but none
have been tested against a range of recall targets and tasks. We propose two
new heuristic stopping rules, Quant and QuantCI based on model-based estimation
techniques from survey research. We compare them against a range of proposed
heuristics and find they are accurate at hitting a range of recall targets
while substantially reducing review costs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GEM: A General Evaluation Benchmark for Multimodal Tasks. (arXiv:2106.09889v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09889</id>
        <link href="http://arxiv.org/abs/2106.09889"/>
        <updated>2021-06-21T02:07:36.399Z</updated>
        <summary type="html"><![CDATA[In this paper, we present GEM as a General Evaluation benchmark for
Multimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,
XGLUE and XTREME that mainly focus on natural language tasks, GEM is a
large-scale vision-language benchmark, which consists of GEM-I for
image-language tasks and GEM-V for video-language tasks. Comparing with
existing multimodal datasets such as MSCOCO and Flicker30K for image-language
tasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the
largest vision-language dataset covering image-language tasks and
video-language tasks at the same time, but also labeled in multiple languages.
We also provide two baseline models for this benchmark. We will release the
dataset, code and baseline models, aiming to advance the development of
multilingual multimodal research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1"&gt;Lin Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1"&gt;Edward Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chenfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Huaishao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongfei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1"&gt;Ming Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharti_T/0/1/0/all/0/1"&gt;Taroon Bharti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sacheti_A/0/1/0/all/0/1"&gt;Arun Sacheti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Minimizing Cost in Legal Document Review Workflows. (arXiv:2106.09866v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.09866</id>
        <link href="http://arxiv.org/abs/2106.09866"/>
        <updated>2021-06-21T02:07:36.386Z</updated>
        <summary type="html"><![CDATA[Technology-assisted review (TAR) refers to human-in-the-loop machine learning
workflows for document review in legal discovery and other high recall review
tasks. Attorneys and legal technologists have debated whether review should be
a single iterative process (one-phase TAR workflows) or whether model training
and review should be separate (two-phase TAR workflows), with implications for
the choice of active learning algorithm. The relative cost of manual labeling
for different purposes (training vs. review) and of different documents
(positive vs. negative examples) is a key and neglected factor in this debate.
Using a novel cost dynamics analysis, we show analytically and empirically that
these relative costs strongly impact whether a one-phase or two-phase workflow
minimizes cost. We also show how category prevalence, classification task
difficulty, and collection size impact the optimal choice not only of workflow
type, but of active learning method and stopping point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eugene Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_D/0/1/0/all/0/1"&gt;David D. Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frieder_O/0/1/0/all/0/1"&gt;Ophir Frieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Audio-Driven System For Real-Time Music Visualisation. (arXiv:2106.10134v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.10134</id>
        <link href="http://arxiv.org/abs/2106.10134"/>
        <updated>2021-06-21T02:07:36.368Z</updated>
        <summary type="html"><![CDATA[Computer-generated visualisations can accompany recorded or live music to
create novel audiovisual experiences for audiences. We present a system to
streamline the creation of audio-driven visualisations based on audio feature
extraction and mapping interfaces. Its architecture is based on three modular
software components: backend (audio plugin), frontend (3D game-like
environment), and middleware (visual mapping interface). We conducted a user
evaluation comprising two stages. Results from the first stage (34
participants) indicate that music visualisations generated with the system were
significantly better at complementing the music than a baseline visualisation.
Nine participants took part in the second stage involving interactive tasks.
Overall, the system yielded a Creativity Support Index above average (68.1) and
a System Usability Scale index (58.6) suggesting that ease of use can be
improved. Thematic analysis revealed that participants enjoyed the system's
synchronicity and expressive capabilities, but found technical problems and
difficulties understanding the audio feature terminology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Graf_M/0/1/0/all/0/1"&gt;Max Graf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Opara_H/0/1/0/all/0/1"&gt;Harold Chijioke Opara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1"&gt;Mathieu Barthet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point-of-Interest Recommender Systems: A Survey from an Experimental Perspective. (arXiv:2106.10069v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.10069</id>
        <link href="http://arxiv.org/abs/2106.10069"/>
        <updated>2021-06-21T02:07:36.339Z</updated>
        <summary type="html"><![CDATA[Point-of-Interest recommendation is an increasing research and developing
area within the widely adopted technologies known as Recommender Systems. Among
them, those that exploit information coming from Location-Based Social Networks
(LBSNs) are very popular nowadays and could work with different information
sources, which pose several challenges and research questions to the community
as a whole. We present a systematic review focused on the research done in the
last 10 years about this topic. We discuss and categorize the algorithms and
evaluation methodologies used in these works and point out the opportunities
and challenges that remain open in the field. More specifically, we report the
leading recommendation techniques and information sources that have been
exploited more often (such as the geographical signal and deep learning
approaches) while we also alert about the lack of reproducibility in the field
that may hinder real performance improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1"&gt;Pablo S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1"&gt;Alejandro Bellog&amp;#xed;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Information Retrieval Approach to Building Datasets for Hate Speech Detection. (arXiv:2106.09775v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.09775</id>
        <link href="http://arxiv.org/abs/2106.09775"/>
        <updated>2021-06-21T02:07:36.316Z</updated>
        <summary type="html"><![CDATA[Building a benchmark dataset for hate speech detection presents several
challenges. Firstly, because hate speech is relatively rare -- e.g., less than
3\% of Twitter posts are hateful \citep{founta2018large} -- random sampling of
tweets to annotate is inefficient in capturing hate speech. A common practice
is to only annotate tweets containing known ``hate words'', but this risks
yielding a biased benchmark that only partially captures the real-world
phenomenon of interest. A second challenge is that definitions of hate speech
tend to be highly variable and subjective. Annotators having diverse prior
notions of hate speech may not only disagree with one another but also struggle
to conform to specified labeling guidelines. Our key insight is that the rarity
and subjectivity of hate speech are akin to that of relevance in information
retrieval (IR). This connection suggests that well-established methodologies
for creating IR test collections might also be usefully applied to create
better benchmark datasets for hate speech detection. Firstly, to intelligently
and efficiently select which tweets to annotate, we apply established IR
techniques of {\em pooling} and {\em active learning}. Secondly, to improve
both consistency and value of annotations, we apply {\em task decomposition}
\cite{Zhang-sigir14} and {\em annotator rationale} \cite{mcdonnell16-hcomp}
techniques. Using the above techniques, we create and share a new benchmark
dataset\footnote{We will release the dataset upon publication.} for hate speech
detection with broader coverage than prior datasets. We also show a dramatic
drop in accuracy of existing detection models when tested on these broader
forms of hate. Collected annotator rationales not only provide documented
support for labeling decisions but also create exciting future work
opportunities for dual-supervision and/or explanation generation in modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Md Mustafizur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishnan_D/0/1/0/all/0/1"&gt;Dinesh Balakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murthy_D/0/1/0/all/0/1"&gt;Dhiraj Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1"&gt;Mucahid Kutlu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1"&gt;Matthew Lease&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PixInWav: Residual Steganography for Hiding Pixels in Audio. (arXiv:2106.09814v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2106.09814</id>
        <link href="http://arxiv.org/abs/2106.09814"/>
        <updated>2021-06-21T02:07:36.277Z</updated>
        <summary type="html"><![CDATA[Steganography comprises the mechanics of hiding data in a host media that may
be publicly available. While previous works focused on unimodal setups (e.g.,
hiding images in images, or hiding audio in audio), PixInWav targets the
multimodal case of hiding images in audio. To this end, we propose a novel
residual architecture operating on top of short-time discrete cosine transform
(STDCT) audio spectrograms. Among our results, we find that the residual audio
steganography setup we propose allows independent encoding of the hidden image
from the host audio without compromising quality. Accordingly, while previous
works require both host and hidden signals to hide a signal, PixInWav can
encode images offline -- which can be later hidden, in a residual fashion, into
any audio signal. Finally, we test our scheme in a lab setting to transmit
images over airwaves from a loudspeaker to a microphone verifying our
theoretical insights and obtaining promising results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geleta_M/0/1/0/all/0/1"&gt;Margarita Geleta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punti_C/0/1/0/all/0/1"&gt;Cristina Punti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1"&gt;Kevin McGuinness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pons_J/0/1/0/all/0/1"&gt;Jordi Pons&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canton_C/0/1/0/all/0/1"&gt;Cristian Canton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giro_i_Nieto_X/0/1/0/all/0/1"&gt;Xavier Giro-i-Nieto&lt;/a&gt;</name>
        </author>
    </entry>
</feed>